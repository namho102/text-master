
It took a terrorist attack for Google to enter the news business.
On September 11, 2001, after hijackers crashed two commercial jets into the World Trade Center as well as a third plane into the Pentagon and another into a field near Shanksville, Pennsylvania, internet users turned to the search engine for information. Again and again, they typed in terms like “New York Twin Towers,” but found nothing about what had happened that morning. Google’s web crawlers hadn’t indexed “Twin Towers” since the month before, which meant every result that Google returned was, given the context, totally and painfully irrelevant.
Google quickly set up a special page for “News and information about attacks in U.S.” with links to the websites of about four dozen newspapers and news networks, along with links to relief funds, resources, and phone numbers for airlines and hospitals. A link to this makeshift news page stayed there for weeks, just below the search bar on Google’s minimalist homepage. Within a year, Google had incorporated a news filter into its search algorithm so that timely headlines appeared atop a list of search results for relevant keywords.
A new era of personalized news products began, in earnest, as a reaction to horrific global news.
Today, a Google search for news runs through the same algorithmic filtration system as any other Google search: A person’s individual search history, geographic location, and other demographic information affects what Google shows you. Exactly how your search results differ from any other person’s is a mystery, however. Not even the computer scientists who developed the algorithm could precisely reverse engineer it, given the fact that the same result can be achieved through numerous paths, and that ranking factors—deciding which results show up first—are constantly changing, as are the algorithms themselves.
We now get our news in real time, on demand, tailored to our interests, across multiple platforms, without knowing just how much is actually personalized. It was technology companies like Google and Facebook, not traditional newsrooms, that made it so. But news organizations are increasingly betting that offering personalized content can help them draw audiences to their sites—and keep them coming back.
Personalization extends beyond how and where news organizations meet their readers. Already, smartphone users can subscribe to push notifications for the specific coverage areas that interest them. On Facebook, users can decide—to some extent—which organizations’ stories they would like to appear in their news feeds. At the same time, devices and platforms that use machine learning to get to know their users will increasingly play a role in shaping ultra-personalized news products. Meanwhile, voice-activated artificially intelligent devices, such as Google Home and Amazon Echo, are poised to redefine the relationship between news consumers and the news.
While news personalization can help people manage information overload by making individuals’ news diets unique, it also threatens to incite filter bubbles and, in turn, bias. This “creates a bit of an echo chamber,” says Judith Donath, author of The Social Machine: Designs for Living Online and a researcher affiliated with Harvard University ’s Berkman Klein Center for Internet and Society. “You get news that is designed to be palatable to you. It feeds into people’s appetite of expecting the news to be entertaining ... [and] the desire to have news that’s reinforcing your beliefs, as opposed to teaching you about what’s happening in the world and helping you predict the future better.”
As data tracking becomes more sophisticated, voice-recognition software advances, and tech companies leverage personalization for profit, personalization will only become more acute. This is potentially alarming given the growth of websites—news-oriented and otherwise—inhabiting the political extremes, which on Facebook are easy to mistake for valid sources of news. When users can customize their news, and customize to these political and social extremes, civic discourse can suffer. “What’s important is how people use the news to have a discussion,” says Donath. “You may have friends or colleagues, and you read the same things in common. You may decide different things about it. Then you debate with those people. If you’re not even seeing the same news story, it leaves you with a much narrower set of people with whom you share that common ground. You’re losing the common ground of news.”
Information-filtering algorithms, whether those of tech giants or news organizations, are the foundation of personalization efforts. But journalists and technologists approach this info-filtering environment in fundamentally different ways. News organizations share information that is true and hopefully engaging. Technology companies like Google and Facebook enable the sharing of information that is engaging and hopefully true. Emerging technologies will only exacerbate the existing problems with algorithmically promoted junk information.
Still, algorithms have a place in responsible journalism. “An algorithm actually is the modern editorial tool,” says Tamar Charney, the managing editor of NPR One, the organization’s customizable mobile-listening app. A handcrafted hub for audio content from both local and national programs as well as podcasts from sources other than NPR, NPR One employs an algorithm to help populate users’ streams with content that is likely to interest them. But Charney assures there’s still a human hand involved: “The whole editorial vision of NPR One was to take the best of what humans do and take the best of what algorithms do and marry them together.”
In an Inside NPR blog post about the editorial ethics driving NPR One’s personalization (cowritten by Charney, chief digital officer Thomas Hjelm, and senior VP of news and editorial director Michael Oreskes), the so-called secret sauce behind the app is “an editorially responsible algorithm.” Metrics track listener behavior so that, over time, the app can offer content catered to individual preferences. Charney declined to describe exactly what data the NPR One app collects—“We’re a little proprietary,” she says—but she gave some examples of how the algorithm personalizes NPR content.
For instance, NPR One knows when you stop listening, which in the future can help producers decide how to keep listeners interested. It can also tell which listeners heard a story that later had a correction appended to it, and deliver that correction to the top of those listeners’ queues. In at least one case, when a correction was significant, NPR One’s algorithm determined who had heard the original segment. NPR then emailed the correction to that list of users.
NPR One can apply that same principle to multipart stories. If a listener misses the first or second part of a story, the app will be sure to offer the missing part to that listener, something those who listen to NPR on the radio often might miss. “Nobody thinks that’s what personalization algorithms are for,” Charney says. “But we can counter both the filter bubble and we can counter false narratives this way.”
Important news stories—both local and national—are presented to all users, with no options for personalization; the app will always provide the lead story of the day and other important stories selected by editors. So while NPR One enables listeners to choose the “nonessential” stories that are more particular to one’s interests—music reviews, for example, or stories about sports or interviews with artists—and decide on the level of depth they hear on certain topics, dialing up or down the frequency of updates, human editors still ultimately decide what you need to hear.
“You may not be interested in Syria. We’ll tell you if this big thing happened and you need to know about it, but we’ll spare you from the incremental news,” Charney says. “The ability to skim across some stories and to dive into other stories, that may be the power of personalization.”
The skimming and diving Charney describes sounds almost exactly like how Apple and Google approach their distributed-content platforms. With Apple News, users can decide which outlets and topics they are most interested in seeing, with Siri offering suggestions as the algorithm gets better at understanding your preferences. Siri now has help from Safari. The personal assistant can now detect browser history and suggest news items based on what someone’s been looking at—for example, if someone is searching Safari for Reykjavík-related travel information, they will then see Iceland-related news on Apple News. But the For You view of Apple News isn’t 100 percent customizable, as it still spotlights top stories of the day, and trending stories that are popular with other users, alongside those curated just for you.
Similarly, with Google’s latest update to Google News, readers can scan fixed headlines, customize sidebars on the page to their core interests and location—and, of course, search. The latest redesign of Google News makes it look newsier than ever, and adds to many of the personalization features Google first introduced in 2010. There’s also a place where you can preprogram your own interests into the algorithm.
Google says this isn’t an attempt to supplant news organizations, nor is it inspired by them. The design is rather an embodiment of Google’s original ethos, the product manager for Google News Anand Paka says: “Just due to the deluge of information, users do want ways to control information overload. In other words, why should I read the news that I don’t care about?”
That is a question news organizations continue to grapple with. If reactions to The New York Times’ efforts to tailor news consumption to individual subscribers are any indication, some people do want all the news that’s fit to print—and aren’t sold on the idea of news personalization.
The Times has recently introduced, or plans to do so later this year, a number of customization features on its homepage involving the placement of various newsletters and editorial features—like California Today, the Morning Briefing, and The Daily podcast—that depend on whether a person has signed up for those services as well as readers being able to choose prioritized placement of preferred topics or writers. Soon, the biggest news headlines may still dominate the top of the homepage, but much of the surrounding content will be customized to cater to individuals’ interests and habits.
The Times’ algorithm, drawing from data like geolocation, will make many of these choices for people. A person reading the news from, say, India might see news relevant to the Indian subcontinent in a more prominent place online than a person reading from New York City. The site already features a “Recommended for You” box, listing articles that you haven’t yet read, also including those suggestions in emails to some subscribers.
The then-public editor Liz Spayd discussed the changes in a March column, noting that she’d heard from several readers unhappy with the newspaper’s efforts to offer a more unique reader experience, and to document and share subscribers’ activity with them. “I pay for a subscription for a reason: the judgment and experience of the editors and writers that make this paper great. Don’t try to be Facebook ... Be The New York Times and do it right,” commented one reader.
“Don’t try to be Facebook” was a common refrain among the commenters. The social network has had its fair share of issues with news curation in its attempts to become “the best personalized newspaper in the world,” as CEO Mark Zuckerberg put it back in 2013. To say nothing of the fake news that proliferates on users’ news feeds, the “trending topics” section had a very rough few months in 2016. First, news “curators” were accused of bias for burying conservative news stories; then, Facebook laid off the entire editorial staff responsible for writing descriptions of items appearing in the section, with some disastrous results, such as when a made-up story—claiming Megyn Kelly was fired from Fox News for being a supporter of Hillary Clinton—showed up at the top of the “trending” list. The story appeared on the blog USPostman.com, a website registered to an address in Macedonia, known for its robust network of information scammers, PolitiFact reported at the time. In January of this year, Facebook gave up on personalized trending topics altogether, filtering topics by users’ geographic regions rather than interests.
Even more troubling than Facebook’s trending-topics woes was the revelation in September that the social network had sold upwards of 3,000 ads—totaling at least $100,000—to a Russian firm connected to the spread of pro-Kremlin propaganda and fake news. The firm, posing as Americans in a myriad of groups and pages, sought to target U.S. voters during the presidential campaign and, while most of the ads didn’t specifically reference the election or any candidates, they “appeared to focus on amplifying divisive social and political messages across the ideological spectrum,” wrote Alex Stamos, Facebook’s chief security officer, in a blog post. The fact that the topics of the ads were so wide-ranging—varying from immigration and gun rights to the LGBT community and Black Lives Matter—is suggestive of how damaging personalization can be and how it isn’t confined to any particular party line. Soon after, Twitter announced it had found and suspended about 200 accounts linked to Russian operatives, many of whom were identified as the same ad buyers active on Facebook.
Meanwhile, in May, Google briefly tested a personalized search filter that would dip into its trove of data about users with personal Google and Gmail accounts and include results exclusively from their emails, photos, calendar items, and other personal data related to their query. The “personal” tab was supposedly “just an experiment,” a Google spokesperson said, and the option was temporarily removed, but seems to have rolled back out for many users as of August.
Now, Google, in seeking to settle a class-action lawsuit alleging that scanning emails to offer targeted ads amounts to illegal wiretapping, is promising that for the next three years it won’t use the content of its users’ emails to serve up targeted ads in Gmail. The move, which will go into effect at an unspecified date, doesn’t mean users won’t see ads, however. Google will continue to collect data from users’ search histories, YouTube, and Chrome browsing habits, and other activity.
The fear that personalization will encourage filter bubbles by narrowing the selection of stories is a valid one, especially considering that the average internet user or news consumer might not even be aware of such efforts. Elia Powers, an assistant professor of journalism and news media at Towson University in Maryland, studied the awareness of news personalization among students after he noticed those in his own classes didn’t seem to realize the extent to which Facebook and Google customized users’ results. “My sense is that they didn’t really understand ... the role that people that were curating the algorithms [had], how influential that was. And they also didn’t understand that they could play a pretty active role on Facebook in telling Facebook what kinds of news they want them to show and how to prioritize [content] on Google,” he says.
The results of Powers’s study, which was published in Digital Journalism in February, showed that the majority of students had no idea that algorithms were filtering the news content they saw on Facebook and Google. When asked if Facebook shows every news item, posted by organizations or people, in a users’ newsfeed, only 24 percent of those surveyed were aware that Facebook prioritizes certain posts and hides others. Similarly, only a quarter of respondents said Google search results would be different for two different people entering the same search terms at the same time.
This, of course, has implications beyond the classroom, says Powers: “People as news consumers need to be aware of what decisions are being made [for them], before they even open their news sites, by algorithms and the people behind them, and also be able to understand how they can counter the effects or maybe even turn off personalization or make tweaks to their feeds or their news sites so they take a more active role in actually seeing what they want to see in their feeds.”
On Google and Facebook, the algorithm that determines what you see is invisible. With voice-activated assistants, the algorithm suddenly has a persona. “We are being trained to have a relationship with the AI,” says Amy Webb, founder of the Future Today Institute and an adjunct professor at New York University Stern School of Business. “This is so much more catastrophically horrible for news organizations than the internet. At least with the internet, I have options. The voice ecosystem is not built that way. It’s being built so I just get the information I need in a pleasing way.”
Webb argues that voice is the next big threat for journalism, but one that presents news organizations with the opportunity to play an even greater role in people’s everyday lives. Soon, we likely will be able to engage with voice-activated assistants such as Siri and Alexa beyond just asking for the day’s news. We’ll be able to interrupt and ask questions—not just in order to put things in context and deepen our understanding of current events, but to personalize them. To ask, “Why should this matter to me?” or even, “What’s the most important news story of today—for me?”
Today, you can ask the Amazon Echo to read you the news—a bit like the way radio broadcasters simply read straight from the newspaper when radio was in its infancy. But technologists, journalists, and scholars believe that in the near future, artificially intelligent voice-activated devices will offer a genuinely interactive and personalized news experience. “Maybe I want to have a conversation with The Atlantic and not USA Today, so I’m willing to pay for that,” Webb says. “This has to do with technology but also organizational management because suddenly there are like 20 different job titles that need to exist that don’t.”
The Echo’s Flash Briefing comes with preloaded default channels—such as NPR, BBC, and the Associated Press—already enabled, but it’s “very much on the consumer to decide” what they want to hear from the Echo, says Amazon spokeswoman Rachel Hass. Any web developer can include a site in the Flash Briefing category the Echo dips into for the news, but being selected as a default outlet by Amazon gives news organizations a huge competitive advantage. Research shows that most people don’t change default settings on their phones, computers, and software—either because they don’t want to, or more likely, they don’t know how to.
Much like a search engine, Amazon isn’t focused on differentiating material from various sources or fact-checking the information the Echo provides. The Echo does, however, read a quick line of attribution during news briefings. “As Alexa reads out your Flash Briefing, she attributes each headline or news piece by saying ‘from NPR’ or ‘from The Daily Show,’” Hass explains. There’s also tremendous incentive for news organizations to play nice with Amazon as a way to get cemented into the device’s default news settings—a relationship that evokes the damaging dependency newsrooms have on Facebook for traffic.
Because Flash Briefings aren’t limited to traditional news outlets, you could conceivably find briefings available from all kinds of sources—including full-fledged newsrooms and individuals. Even former Vice President Joe Biden now delivers daily news briefings, introducing various news articles of his choosing, which are available on Google Home as well as the Echo.
“There are already more than 3,500 Alexa Flash Briefing” skills, the term Amazon uses for the app-like command-driven programs created by developers to use on the Echo. For example, there’s the skill Trump Golf, which offers updates on President Trump’s golf outings whenever prompted by the command, “Alexa, ask Trump Golf for an update.”
“I suspect these devices are the most important thing to emerge since the advent of the iPhone in 2007,” says Kinsey Wilson, former editor for innovation and strategy at The New York Times, “because they open up spaces—principally in the home and in the car—where it allows for a higher, more informal degree of interaction.”
In some ways, voice seems like a natural extension of search. Devices like the Amazon Echo and Google Home will enable people to dip into search engines without having to type. More than that, though, these new devices are meant to be conversational. “It’s not so much asking them a bunch of questions but having a collaborative exploration of some topic,” says Alex Rudnicky, a computer-science professor at Carnegie Mellon University. “This idea of, ‘Wouldn’t it be really nice if you could call up a friend of yours who is very knowledgeable and just have a conversation with them?’”
The personalization element isn’t just the heightened sense of camaraderie one might feel with a conversational robot versus a stack of broadsheets or a talking head on cable television. Personalization is rooted in the fact that devices like the Echo actively learn about the human user with every new interaction and adjust their behavior accordingly. This is the same personalization technique used by Google and Facebook—slurp up myriad individual data, then tailor services to suit—but it uses devices that are always listening, and therefore always learning.
Media organizations that want to create conversational news products for voice-activated devices will have to figure out how to produce and package entirely new kinds of stories, perhaps including advanced tagging systems for snippets of those stories, and be sure their methods integrate with the operating systems these devices use. “The existing SEO methods that we have might need to be rethought completely from scratch,” says Trushar Barot, a member of the digital-development team at BBC World Service. “There may be new methods that emerge that are native to voice recognition.”
Personalized voice assistants face potential obstacles. Sounding too much like a machine is one problem; sounding too much like a human is another. “It’s very easy for people, psychologically, to start anthropomorphizing the device into a real entity and developing genuine human feelings about it,” says Barot. “Plus, the fact that it’s a device that’s in their home and it’s learning more and more about their lives and potentially becoming much more intelligent about proactively offering you suggestions or ideas. That brings up challenging ethical issues.”
News organizations’ use of voice interfaces raise a host of ethical concerns related to data collection, privacy, and security. We don’t know precisely what data these devices collect about individuals (few people read company privacy policies) but, if smartphones have taught us anything, the rough answer is: everything they possibly can. And there’s not an easy answer to who, exactly, owns this data, but one thing’s for sure—it’s not (just) you. This data has immense value, not just to those generating, capturing, and analyzing it, but to a wide range of companies, tech giants and otherwise.
So what do newsrooms do with audience data? “There are potentially ways for newsrooms to use that personalization [data] in a useful way,” says the Berkman Klein Center’s Donath. It largely depends “on what you think the mission of the newsroom is. Is it to inform people as well as to possibly have its own model of what’s important information that people should be aware of? Or is it much more of an entertainment model?” If the latter, that audience data is incredibly valuable for organizations to make sure they’re creating and distributing the type of content people want each day.
Amazon is considering offering developers raw transcripts of what people say to the Echo, according to a July report in The Information. Newsrooms will have to grapple with whether it’s ethical to use data from those transcripts as a way to make money, a move that would certainly enrage some privacy-minded consumers. For publishers, that could be an important revenue stream, but it could also creep audiences out and lessen trust, not enhance it.
What happens to a person’s perception of information, for instance, if the same voice some day is reading headlines from both Breitbart and The Washington Post? “What does that do to your level of trust in that content?” Barot asks. Plus, “there is a lot of evidence that people inherently trust or believe content or news or information shared by their friends. So if this is a similar type of dynamic that’s developing, what does that do for newsrooms?”
Loss of a sense of sources is a big issue, according to Donath: “What’s useful is knowing where something comes from. Depending on what your perspective is, it can cause you to believe it more or believe it less. When you see everything in this generic feed, you have no idea if it’s being reported by something right-leaning or left-leaning. In a lot of ways, the entire significance of what you’re reading is missing.”
These concerns certainly aren’t unique to voice technology. There’s reason to worry that personalization will only exacerbate existing trust issues around news organizations given the gaping partisan disparity found in a September Gallup survey on Americans’ trust in mass media. Though Democrats’ trust and confidence in the media has actually jumped to the highest level it’s been in the past two decades, from 51 percent in 2016 to 72 percent this year, the opposite can be said for Republicans: Only 14 percent of Republicans have a great or fair deal of trust in the mass media, which ties with 2016 as a record low in Gallup’s polling history.
Although some newspaper readers might like being greeted by name each time a major news organization sends a daily roundup of stories, news organizations run the risk of sounding inauthentic, the way campaign emails from politicians seem impersonal despite their attempts to the contrary.
According to Powers, news organizations should share with audiences that the content they’re seeing may not be the same as what their neighbor is seeing—and how they can opt out of personalization. “There needs to be more transparency about what data they’re actually collecting, and how people can manually turn [personalization efforts] on or off or affect what they see,” says Powers.
Perhaps most importantly, it’s essential for news organizations to remember that they can’t leave personalization up to algorithms alone; doing so will likely only narrow people’s news consumption rather than expand it, and could lead to the spread of misinformation. “You still need to have an actual human editor looking to make sure that what’s popular isn’t bogus or hurtful,” says Powers.
Personalization should be a way to enhance news decisions made by human editors, professionals committed to quality journalism as a crucial component of an open society. The news-filtering algorithms made by companies that refuse to admit they are even in the media business—let alone in a position to do great harm—aren’t bound to even the most basic journalistic standards. And yet they are the dominant forces filtering the world around us in real time.
This post appears courtesy of Nieman Reports.
Eryn Carlson contributed reporting.


Emma Perrier spent the summer of 2015 mending a broken heart, after a recent breakup. By September, the restaurant manager had grown tired of watching The Notebook alone in her apartment in Twickenham, a leafy suburb southwest of London, and decided it was time to get back out there. Despite the horror stories she’d heard about online dating, Emma, 33, downloaded a matchmaking app called Zoosk. The second “o” in the Zoosk logo looks like a diamond engagement ring, which suggested that its 38 million members were seeking more than the one-night stands offered by apps like Tinder.
To hear more feature stories, see our full list or get the Audm iPhone app.
She snapped the three selfies the app required to “verify her identity.” Emma, who is from a volcanic city near the French Alps, not far from the source of Perrier mineral water, is petite, and brunette. She found it difficult to meet men, especially as she avoided pubs and nightclubs, and worked such long hours at a coffee shop in the city’s financial district that she met only stockbrokers, who were mostly looking for cappuccinos, not love.
It was a customer who had caused Emma’s heartache, two months earlier. Connor was one of London’s dashing “city boys,” and 11 years her junior. He had telephoned her at work to ask her on a date, which turned into an eight-month romance. They went night-fishing for carp near his parents’ home in Kent, where they sat holding hands in the darkness, their lines dangling in the water. One day at the train station, Connor told her it wasn’t working; he liked nightclubs more than he liked being in a relationship. When she protested, Connor said that he’d never loved her.
To raise her spirits, Emma huffed and puffed her way through a high-energy barbell class called Bodypump, four times a week. Though she now felt prepared to join the 91 million people worldwide who use dating apps, deep down she did not believe that computers were an instrument of fate. “I’m a romantic,” Emma told me, two years after the internet turned her life upside down. “I love to love,” she said, in a thick French accent. “And I want to be loved too.”
As soon as her dating profile went live, Emma’s phone started to bleep and whistle with interest from strangers. The app allowed her to gaze at a vast assortment of suitors like cakes in a coffee-shop window, but not interact with them until she subscribed. That evening, a private message arrived in her inbox. It was from a dark-haired Italian named Ronaldo “Ronnie” Scicluna, who looked to Emma like a high-school crush. But the text was “floue,” Emma told me, not knowing the English word for “blurred.” The app was holding Ronnie’s message ransom.
That night, Emma FaceTimed her sister and showed her Ronnie’s photos: “Oh my God, look at the guy!” she giggled, as they swiped through his profile pictures. He was boyish yet mysterious, like the kind of dangersome male model who steers sailboats through cologne commercials. But according to his profile, Ronnie was a 34-year-old electrician in England’s West Midlands, just 100 miles away.
Gaëlle, Emma’s twin, lived in France and was married with an 11-year-old daughter. The sisters had gossiped on daily video calls since Emma emigrated to the United Kingdom five years earlier. Emma had to learn English “chop-chop”—as Londoners say—and now she too was ready to meet someone special. Ronnie seemed exciting, so she paid the £25 ($34) subscription to Zoosk.
Ronnie’s message materialized. It said: “You look beautiful.”
A rally followed. Emma discovered that she and Ronnie were two lonely Europeans working blue-collar jobs in England. Charming Ronnie attempted a little French, but when Emma wrote to him in Italian, she was surprised that he didn’t speak it. His mother was English, Ronnie explained, his Italian father spoke English too, “except when he swears.”
Their conversation moved from Zoosk onto WhatsApp, a free messaging app. Each morning on the train to work, Emma sat glued to her iPhone. She wondered how a guy like him was interested in her. “I’m very natural,” Emma said. “I mean, I’m nothing. I’m very simple you know ... so I was flattered.” In her favorite photograph, Ronnie wore a leather jacket that made him look like a pop star. As a teenager, Emma had obsessed over the British boy band Take That. But Ronnie was the opposite of a celebrity; he was down-to-earth.
“You could easily have picked someone else,” Ronnie told her one day.
“No. You’re the only one I wanted to talk to ... I paid because of you,” she replied.
“As soon as I saw your picture I wanted you,” he wrote.
“Makes me happy to know that,” Emma replied.
When four red heart emojis appeared on her screen, Emma was thrilled. Unlike her ex-boyfriend, Ronnie seemed mature and attentive. Ronnie was easy on the eyes, funny, and caring, but there was one problem: He did not exist.
* * *
Ronaldo Scicluna was a fictional character created by Alan Stanley, a short, balding, 53-year-old shop fitter—a decorator of retail stores. Alan lived alone in Stratford-upon-Avon, the birthplace of William Shakespeare. Like one of the Bard’s shape-shifting characters, Alan used a disguise to fool women into romance, and to prevent himself from getting hurt. His alter ego “Ronnie” was a ladies’ man, charming, and attractive—everything Alan was not. “I was in a pretty lonely place,” he told me during an emotional interview. “I wasn’t feeling the most attractive of people, I might say. You know, I always struggled with self-confidence and ... I was going through a messy separation and I was just feeling like I needed somebody to talk to.”
When his marriage of 22 years failed, Alan, who has an adult daughter, was devastated and found himself uninterested in the opposite sex. “I’d just had enough,” he explained. For almost a year, he allowed his decorating work to consume him, but boredom set in. Alan wanted to “mix” with new people, he said, but feared public rejection in his close-knit town. Then one day he noticed the online-dating service Zoosk.
Alan elected to bypass the company’s selfie-based verification system, a spokesperson for Zoosk told me, following an internal investigation. He admitted using photographs of a random male model from Google that he had stolen. “I’m always nervous about posting personal images of myself,” he explained. “I just don’t like pictures of me. It goes back a long way, to be honest.” Emma’s profile was the first he saw. He was captivated.
Alan had done it before, at least five times, he admits. He’d become online pen pals with single women from all over the world, but avoided video calls and meetings. He found the thrill of the chase electrifying, with none of the awkward stuff like first dates. Emma was just another mark, and their flirty exchanges were innocent fun, he said. “Catfishing is prevalent across the internet,” he told me, “Everybody does catfishing.”
Catfishing was added to the Merriam-Webster dictionary in 2014. It refers to a person who creates a fake social-media profile, usually with the goal of making a romantic connection. The term was coined during a 2010 documentary, Catfish, when a subject told a story about the journey of live cod from the United States to China. Apparently, to prevent the cod from becoming lazy and their flesh turning to mush, seafood suppliers add to the tanks their natural enemy, the catfish. A predator creates excitement.
Alan was right. Online, catfishing was growing in popularity. “Now you don’t need the imagination of a Tolstoy or Dickens to create a totally believable but fictional identity,” said the cyber-psychologist Mary Aiken, author of The Cyber Effect, “It’s a matter of cut and paste.” The results can be devastating. In 2006, a 13-year-old girl in Missouri was duped into an online relationship with a fake teenage boy created by neighbors. After their online romance soured, Megan Meier committed suicide. By June of this year, catfishing was so prevalent that Facebook announced it is piloting new tools to prevent people from stealing others’ profile pictures, like Alan did.
His flirting with Emma soon progressed from small talk to in-jokes, pet names, and late-night telephone calls. To Emma, his lilting West Midlands accent somehow fit perfectly with the images of the model. In October of 2015, she wrote how happy she had become since “meeting” him.
“Are you not usually happy, stinky?” he asked.
“I am,” she said, “but you changed something.”
They both agreed to delete the dating app. Emma constantly asked for a physical date, but was crestfallen when Ronnie made excuses. This had happened before. Alan knew how to prolong the relationship with a combination of evasion and false promises. He told Emma that decorating new shops took him all over Europe. Any free time was spent drinking whiskey with his father, or on vacation at his parents’ villa in Spain, he said. Maybe one day she could stay in “bedroom three.” Emma just wanted a local dinner—they lived only 100 miles apart.
“It’s hard to keep everyone happy,” Ronnie complained. “Dad loves me working and wants me to keep doing better. Mum wants me to quit. She worries about me. My health. Stress. Dad thinks I handle it well.”
“I think what you need is a [girlfriend] to look after you,” said Emma, before he changed the subject.
“Do you want to know why I started online dating?” she asked him one night. “Because I wanted to ... meet that someone and to start something with that someone ... not to have a broken heart ... which is even more painful when you have never met someone.”
“Me too,” said Ronnie. “We both want the same thing.”
“Give me a date then,” Emma wrote. “I will suit your availability.”
She waited for his reply.
“I don’t think you realize how difficult it is for me to get time off,” he wrote.
“Just a dinner to start with,” Emma begged. “I can do the travel ... then if the connection is really there we will find a way.”
“Do you think it will be there?” he asked.
“I have never been so sure.”
“Do you have faith in us?” he asked.
“It could work perfectly well,” Emma wrote.
“And I love you,” he wrote.
“And I love you too,” she replied.
* * *
Little scientific research exists about catfishing, but experts say that victims tend to be lonely, vulnerable, or missing something in their lives. John Suler, a clinical psychologist and author of Psychology of the Digital Age, said that victims without a real-world social network can overlook what is too good to be true: “It always helps to have friends and family reality-check relationships online,” he said. But Emma had few close friends or family in London. And Emma was looking for love.
Emma met her first boyfriend at age 15. When their high-school romance ended a decade later, she ran away, high into the French Alps, to find seasonal work. She did not find love there, and decided to keep running, this time to England, where she had dreamed of living since visiting as a child. When she arrived, aged 28, there were 127,601 French-born residents in London, and by 2015 that number had doubled, making it the sixth-biggest French city, according to London’s mayor. But the language barrier nearly made Emma quit after two months. “It’s not like the same as you listening to that song in your bedroom when you’re 16,” she said.
She loved talking to Ronnie, whose conversations were full of construction-site bonhomie, British slang, and flirtation. One day, she received a black-and-white modeling photograph of him wearing a tiny pair of Speedos. Emma fired back emojis with laughing faces stained with tears of joy.
“I love that picture thank you,” she replied, “I saved it.”
Alan, who is a fitness fanatic, was now spending his mornings on long-distance runs. Decades of manual labor had kept him fit, but he was resentful about losing his hair at a young age. “In my 30s it started falling out,” he said. “I was exactly like my dad.”
To him, Emma had become not just a friendly voice on the phone, but a project. When he discovered that Emma spent three hours a day commuting to work, Alan encouraged her to find a local job. “I was on her journey in life, trying to guide her,” Alan said.
By January of 2016, Emma was thrilled to receive a job offer three miles from her home at an Italian chain restaurant. As the new assistant manager of Zizzi in Richmond, she managed a team of Poles, Spaniards, and Greeks (there are no real Italians in this story). When Emma boasted about her “long-distance” love, the busboys asked why they’d never met. Emma told them he was “extremely busy.”
Alan was running out of excuses. “It was eating at me because I knew the longer it went on, the more problematic it would become in the long term,” he said. Like Malvolio in Twelfth Night, Alan had donned a ludicrous disguise to win the affections of his Olivia. And in a world where Alan felt ugly and invisible to the opposite sex, Emma showered him in “adoration.” In his mind, Alan minimized his lie: “Everything I told her about me, apart from who I was, and the age, was true.”
One night, after the last customers left Zizzi, Emma closed the restaurant with a popular, baby-faced Spanish waiter named Abraham. As they shut down the huge pizza oven, and packed away the cutlery, Emma revealed how she longed to meet her mysterious boyfriend. Abraham listened for a while, then turned to his manager and said: “But Emma, the guy doesn’t want to meet you ... maybe it’s not even him.”
Emma insisted that they’d talked on the phone.
Abraham said her boyfriend was “probably an old man.”
Then he said he’d heard about an app that could help.
“He could be a psycho,” he added.
Emma was hurt and confused. After Abraham left, she found herself alone in the restaurant. Looking through the window she watched the happy couples walking along the black cobbles of King Street. She longed for the day when Ronnie would appear at Zizzi, sweep her off her feet, and prove them all wrong.
By the spring of 2016, Emma’s family recommended that she cut off all communications with Ronnie. He had refused to meet her after six months, they said. “I didn’t want to listen to them,” Emma said. But one evening after work, she laid on her bed and downloaded to her iPad an app called Reverse Image Search. It is one of many apps that crawls the internet to find the original source of a profile picture.
“Believe me I was scared to use it for the first time,” Emma said. She uploaded the photograph of Ronnie wearing his leather jacket. The results arrived in seconds: The man in the photographs was a model and actor from Turkey, called Adem Guzel. Emma was confused. She found his model-management website, an official Twitter account, and his Facebook. Adem’s closest connection to the United Kingdom was that he had studied at the Gaiety School of Acting in the nearby Republic of Ireland.
“Do you have anything to tell me about Adem Guzel?” she wrote in a text message.
“It is me,” Alan replied, thinking fast. Those were his modeling pictures, he said. He’d once used another name.
“It was a long time ago,” he promised.
Given the opportunity, Alan couldn’t tell the truth. “I would have lost someone that I really treasured,” he told me. But Emma demanded that he reveal himself. FaceTime was “for teenagers,” he said. When she insisted, he yelled: “Stop! Don’t ask me anymore!”
But Emma still wanted to believe in the fantasy, not the truth.
“I couldn’t believe it because, you know ... when you talk to someone every day, and you share your life ... he was my confidente.”
And why would somebody claim to be someone else online?
Julie Albright, a digital sociologist at the University of Southern California, says catfishing can be addictive: “Suddenly finding success with romantic partners online is exciting, and in fact intoxicating for certain people,” she said, adding that catfish often target more than one victim: “Putting several hooks in the water and getting several relationships going is the way to hedge your bets.”
In August of 2016, nearly a year after his and Emma’s relationship began, Alan had computer troubles. He bought a new one, but set it up using his personal email address. When he sent Emma a message, it sounded like Ronnie, but the email address said “Alan Stanley.”
It was his first mistake.
“I lied,” Alan told me. “I said, no, I bought this computer from somebody else and they haven’t changed it yet.”
Emma was now overwhelmed with doubts.
During that summer of 2016, Emma allowed her long-distance relationship to continue as she started what she proudly calls “my investigation.” One day Ronnie sent her a photograph from an aquarium, the fish from Finding Nemo. It was either a False Percula clownfish or a True Percula clownfish—only a saltwater aquarist could tell the difference—but Emma was more interested in uploading it to her app. “This Nemo sent me to TripAdvisor,” she said. It illustrated a review written by “Alan S.”
“I knew,” Emma told me. She typed Alan’s email address into Google.
I asked what she found.
“Everything, everything,” she sighed. “His Twitter accounts. Where I’ve seen his face.”
“It was devastating and I felt sick,” she said. “You have no idea how much I’ve been hurt inside.”
Alan was in early-morning traffic when his cellphone rang.
“Is your real name Alan?” Emma asked.
“No.” he replied.
“But it is, it is, it is!” Emma said, sobbing. Alan accused her of having trust issues.
“Don’t talk to me about trust, Alan Stanley!” Emma yelled. The call, and Alan’s masquerade, was over.
From a quiet corner of a half-decorated shop, Alan called Emma back. “I could not be any more apologetic,” he told me. “I told her everything.” Emma told him she felt like a fool. They both cried. It was, Alan said, a “big error of judgment, the worst and biggest mistake of my life.” But even in his telling of “the truth,” Alan told Emma he was 50, shaving off a few years.
Emma had questions. Was he a pervert? Alan sent her a real photograph of himself, wrinkles and all. “It might sound cruel what I’m going to say,” Emma told me, “but I carried on talking with him, after I knew who he was, only because I wanted to know why he did that to me,” Emma said. “I’m 34 at the time, but maybe another girl, when she finds out, she could maybe go too far, maybe kill herself.” After the big reveal, Emma asked Alan if he wanted to meet her. “I really wanted to go, to end the story,” she said. But was Alan dangerous?
Emma decided that she needed to protect others from his scam. On September 16, 2016, she wrote a Facebook message to the Turkish model:
“Hello Adem, we don’t know each other but a year ago I met a guy online and that man is using your picture and pretends he is you under another name. I wasn’t sure if getting in touch with you was a good idea but I needed you to know, kind regards, Emma.”
* * *
Adem Guzel nearly ignored the message. The shy, 35-year-old model woke up in the Bohemian district of Cihangir, near Istanbul’s famous Taksim Square, suffering from a cold. This was not the first message he had received of this nature. Adem poured a cup of tea in the kitchen of his aparthotel, a type of bed-and-breakfast that had once been popular with travelers, before political instability and terrorist attacks killed off Turkish tourism. He drew a hot bath, undressed, and sank into the water. Maybe it was the head cold, Adem thought, but it was like an invisible person was yelling in his ear: “Pick up the phone!”
Adem toweled off and found his iPhone. Something about the sincerity of Emma’s message stuck in his mind. He wrote back in broken English. “And the conversation just started,” Adem told me, in a gruff, Turkish voice. When he heard how Alan had tricked Emma, Adem was furious. Emma asked him if he wanted to video call.
Emma was on a bus in Richmond when she read the message. She dashed home and showered, with a strange flutter in her stomach. When Adem’s face appeared on her iPhone, Emma was hysterical. “It was crazy,” she said. “I wasn’t sure it was him, I was always in doubt.” But there he was, talking, smiling, nervously running his fingers through his hair. “I never do FaceTiming,” Adem said. “But somehow I wanted to do it with her.”
“You are so real,” Emma said, crying. “You really exist!”
Emma had questions. In English, their shared second language, Adem explained that he had grown up in a coastal Turkish village, then moved to Istanbul and enjoyed a prosperous modeling career. But his plans to become a television actor had stalled when he refused to enter a Turkish reality show, which he said operated on a “casting-couch” basis. Instead, Adem moved into a friend’s deserted aparthotel as a temporary manager.
As they talked, Emma summoned her sister on FaceTime, and showed the iPad to her iPhone. Gaëlle and the Turkish model waved at each other from opposite sides of Europe. After the call, Adem and Emma exchanged text messages, but Adem soon packed his bags and returned to the village whence he came. Şarköy, pop. 17,000, had the cellphone signal of a small Turkish village, and their conversation fizzled out.
* * *
On Friday, November 11, 2016, Alan Stanley stepped off a train at London’s Paddington Station. He strolled to a nearby row of white-pillared Georgian townhouses and checked into the Arbor, a swanky, boutique hotel with views of Hyde Park. That evening, Alan walked out of his hotel, and into the nearby London Hilton, where Emma was nervously waiting in the lounge. She said she needed closure, and to see the truth with her own eyes. Alan “needed to apologize to her face-to-face,” he said.
His face was red with shame. “The hug went on for about a minute,” he told me, “I was just, like, quite tearful.” Emma pulled up an armchair and they sat uneasily side-by-side, making small talk. Then, Alan said he was sorry.
He said he did it to escape the agony of loneliness. When Emma studied him, she saw a man just two years younger than her own father.
Emma and Alan left the Hilton for some fresh air, and strolled along a tree-lined pathway known as Lover’s Walk. In Alan’s telling, they passed Hyde Park’s “Winter Wonderland” where couples were riding a Ferris wheel or whizzing around an ice-skating rink. The walk—20,000 steps, according to his iPhone’s health app—was one of the longest and best of his life.
“We talk, talk, talk,” Emma said. She asked him about drinking whiskey with his father. Was even that true? “He said his dad passed away a few years ago.”
While Alan considered the evening a date, Emma’s memory of the walk was quite the opposite of romance. The park was “empty” she said. Her only memory was pausing at a memorial to the 52 victims of London’s July 2005 bombings.
“It was a perfect night,” Alan said. “She paid for dinner that evening. Italian restaurant in Paddington.”
Alan even insinuated that Emma had stayed the night at his hotel. “As a gentleman I’m very reluctant to talk about this side of it,” he said. Emma flatly denied it.
“I was pleased I met him obviously,” Emma said curtly, “And that was it.”
But that wasn’t it. Emma could not erase Alan from her life. After their meeting in London, they met several times. Just before Christmas of 2016, Alan presented her with a Swarovski bracelet. “She bought me Hugo Boss socks,” Alan told me, “They’re not cheap.”
“It was a relationship that we built ... You develop a friendship, you talk ...” she explained, her voice breaking as she described their toxic relationship. She was helplessly bonded to Alan and he was obsessed with her, high on virtual validation: “She made me feel like I was a teenager again,” he told me.
I wondered if Alan arrived in London hoping that Emma would overlook the difference between him and the model. Maybe his email slipup was just part of a “bait and switch.”
But Emma could tell the difference. “Things started to get a little bit sour between us,” Alan said. “There was a kind of breakdown after Christmas ... her attention suddenly turned more focused toward finding him.” Alan sensed he was competing with the Turkish model for Emma’s affections. He had deleted his fake accounts, and focused his attention on her. Now, he dreaded he would lose her to the man he had unwittingly thrown in her path—an ironic demise worthy of Shakespeare. “I just put two and two together,” Alan said. “I reckoned that they are talking behind the scenes.”
* * *
By January of 2017, the conversation between Emma and Adem had reignited. “I’m not a religious guy,” Adem said, but it felt like fate had pulled them together. They stopped talking about Alan’s scam, and very slowly the conversation between the shy model and Emma, who had so recently been burned, became emotionally charged. But Emma told her sister, Gaëlle, that she felt like she was just starting another long-distance affair. This time, she wouldn’t be played for a fool, and she wouldn’t waste a moment. She invited Adem to London. “It wasn’t to flirt, believe me,” Emma insisted. Adem said yes immediately. He was curious to meet this beautiful French girl, and sure, in London!
On March 31, 2017, Emma sent her catfish a goodbye text message:
“Alan I wanted to tell you that tomorrow I’m going to pick up Adem at the airport. And I still don’t know if it’s good or bad but I’m going to meet ‘my Ronnie.’ You built up all this shit, I’m not sure if I should thank you or detest you for that. But this is happening.”
It was April Fool’s Day, 2017, when Emma stood beneath the giant arrivals board at London’s Heathrow Airport, searching for Adem’s flight. When a lady beside her noticed her shaking hands, Emma explained that she was waiting for a man from the internet, whom she had never met. The woman froze. “You have to be very careful!” She warned, on the internet not everyone is who they say they are.
“Well actually, I know ...” Emma began, but the Turkish passengers were already flooding into the arrivals hall.
“Oh my God, it’s happening,” she thought.
When the crowd parted, she saw him walking toward her in a white T-shirt and a blue cardigan, the man in her photographs, come to life. Adem was taller than she expected, and when he recognized her, she felt breathless. As they hugged in the middle of the airport, Emma thought that he smelled “fantastique.”
In a quiet corner, Emma produced an egg-and-mayonnaise sandwich, which she had bought in case Adem was hungry. When he lifted it to his mouth, she noticed his hands were shaking too. “I was really nervous,” Adem said. They walked into the bitter cold air, and Emma summoned an Uber. It seemed to take forever. Adem was very quiet and there was a nervous energy between them. When he stepped off the curb to look for their car, Adem turned around and found Emma at eye level.
Inexplicably, she kissed him.
“Three minutes later I felt like I know her a long time,” Adem said. The spark was undeniable. She gave him a key to her apartment, and together they discovered the city like tourists, goofing around with a selfie stick. Later, when Adem opened his suitcase, Emma spotted the leather jacket from her favorite photograph, and felt starstruck. And Adem couldn’t believe his luck—his soul mate had appeared in his inbox as if by magic.
On April 23, 2017, their story became a tabloid sensation in England. “My catfish became cupid,” Emma told the Daily Mirror, “And now we’re living happily ever after.” Soon, other victims of Alan Stanley reached out to Emma. One woman from New York said she had been in a relationship with Ronnie for “years.” When the newspapers described Alan as a “love rat,” he endured summits about his behavior with his colleagues and employer, and an “awful” conversation with his daughter.
“These last few months have been beyond stressful,” he told me. “I don’t think I’ve slept properly for three or four months now.” Overwhelmed by shame, he moved to a faraway town. But even Alan felt relieved that the story ended in comedy, not a tragedy.
“I think it’s brilliant Emma and Adem have met,” he said. “It’s almost like fate.” Alan added that he no longer uses fake identities, and has since met someone special, he said, on Twitter: “A European lady, younger than me, younger than Emma.” There is someone out for there for everyone, he added. “I don’t consider myself to be particularly good-looking ... I’m not a David Beckham, or a Tom Cruise, or an Adem Guzel.”
When I spoke to the couple in September of this year, they had been living together in London for six months. “He’s lovely,” Emma said, “He’s a lovely man.” Currently, Adem is chasing his acting dreams in London, and says he recently auditioned for Aladdin, the original, Arabian catfishing story. He read for the lead, a street urchin who uses a genie’s magic to pass himself off as a prince to win over a princess—before realizing that he must be himself.
At home there has been confusion. Emma was making a coffee one day when she looked over and realized: God, this is Adem, not Ronnie. She says Adem is quite different from the gregarious character invented by Alan—he is quiet and sensitive. There are other challenges: Turkey is not yet in the European Union, so Adem can only stay in London for six months at a time, and cannot work. But Emma now admits that the internet is an instrument of fate.
One evening, not long ago, Emma was closing down Zizzi after a busy shift. Night shifts were once her loneliest times, when she would long for “Ronnie” to materialize from the internet and sweep her off her feet. But that night, she noticed Abraham, the disbelieving Spanish waiter, and the rest of the crew, staring at the handsome gentleman waiting in the doorway, ready to take her home.


The latest experiment in a universal basic income will be coming to Stockton, California, in the next year.
With $1 million in funding from the tech industry–affiliated Economic-Security Project, the Stockton Economic-Empowerment Demonstration (SEED) will be the country’s first municipal pilot program. As currently envisioned, some number of people in Stockton will receive $500 per month. That’s not enough to cover all their expenses, but it could help people with rising housing costs, paying student loans, or simply saving for life’s inevitable problems.
Last year, Stockton rents rose more than 10 percent, putting the city’s rental price growth among the top 10 in the nation. This is quite a surprise in what Time called “America’s most miserable city” just three years ago. The average rent remains a modest-by-Bay-standards $1,051, but Stockton has a per-capita income of just $23,046, more than $6,000 less than the U.S. median and a full $8,500 less than the California median. If you made the per-capita income of the city, average rent alone would eat 55 percent of your income.
As the tech boom that began in the mid-00s continues, its financial blast radius keeps expanding. Tech workers have been streaming into the Bay, yet few homes have been built in the Bay Area’s cities. Home prices and rents have exploded. Longtime residents and newcomers alike have been getting pushed ever further out. And in recent years, Stockton—once one of the cheapest cities to live in California—has become the eastmost outpost of the insane Bay housing market.
“There’s not a shortage of housing. There’s a shortage of money to buy housing,” said Fred Sheil, a member of STAND Affordable Housing in Stockton. “Unless you’ve got Bay Area income, they aren’t interesting in talking to you.”
That’s garnered the attention of city leaders, especially Mayor Michael Tubbs, who became the youngest-ever mayor of a medium-sized city when he won a landslide election in 2016. Tall, gregarious, often besuited with a trim beard, Tubbs could become the new face of universal basic income, or as people abbreviate it, UBI.
Stockton won’t be the first UBI project in the Bay (pilots are already in the field in West Oakland and San Francisco), but it would be the first public attempt to show what a basic income can do for people. Unlike the secretive other projects, both the local government and the participants will be reporting what the cash does for them. And the project will be occurring within the context of a regular city government, with all the community engagement that entails.
“The [UBI] conversation is not being had with the people who are going to be impacted,” Tubbs said. “Mark Zuckerberg don’t need $500 a month.”
So, in Stockton, they are planning a six- to nine-month design process to incorporate the city’s residents into the program design, including precisely how the cash stipends will be awarded.
“My bias is that it should go to people who need it the most, but that’s not truly universal. That’s targeted,” he said. “The way our country is now, for something like this to work, everybody has to feel like they are a part of it.”
One idea they’re kicking around is that a specific number of slots would be reserved for what they call their “promise zone” in south Stockton, where they’ve done a lot of existing economic research and development work.
Tubbs, too, approaches the idea of a minimum income from an entirely different place than Silicon Valley’s scions. Most of the tech proponents of UBI have approached the topic through the lens of automation and the massive devaluation of human labor that they think could result from further developments in artificial intelligence. While giving cash to everyone has an egalitarian ring, when the message is delivered by the ultra-wealthy of Menlo Park and San Francisco, it can feel as if UBI is the crumbs being swept off the real-money table to buy off the masses.
But Tubbs referenced a strain of African American thought expressed by no less a leader than Martin Luther King Jr. “The solution to poverty is to abolish it directly by a now widely discussed measure: the guaranteed income,” King argued in 1967. Though Tubbs didn’t mention them, the previous year, the Black Panthers came out with their famous 10-Point Program. And there it is in point number two: “We believe that the federal government is responsible and obligated to give every man employment or a guaranteed income.”
Perhaps it’s not surprising that different black thinkers in the 1960s came to the conclusion that a guaranteed income would be an effective way to fight the poverty that resulted from structural racism. They’d just seen a generation of federal programs make white Americans much, much wealthier, while also seeing how those same policies discriminated against them. The big programs that were created during the New Deal were boxed in by what historian Ira Katznelson calls “the Southern cage.” In exchange for creating socialistic Federal programs, the then-Democrats of the south required policies that would reinforce the racial hierarchy of the country. Black people’s freedom and economic prospects were the bargaining chip that Franklin Delano Roosevelt and the Congresses he worked with slid over to former slave states in exchange for their support of sweeping legislation.
For example, FDR would create the Federal Housing Authority, but segregation and redlining would combine to create disinvestment in increasingly segregated black neighborhoods across the nation. FDR would get Social Security, but many job categories in which black people predominated would be exempted from inclusion. The GI Bill might have helped black people get an education, but they could not take equal advantage of the Veterans Administration housing benefits because of racist real-estate practices. Job and social programs might seem nice, but the experience of what could happen to nice ideas within American bureaucracy might have made simple cash payments seem more racism-proof than the alternatives.
But Tubbs is not a theoretician or activist. He is the mayor of a poor city, and he knows that people in Stockton need money not just to survive, but to try to lever themselves out of the lower-income brackets through education or entrepreneurship.
In preparation for the UBI project, Tubbs had a convening in his old city-council district (where he grew up) in south Stockton with upper-income, middle-income, and poor people.
“We said, ‘What would you do with an extra $500 a month?’” Tubbs said. “One woman said, ‘It’s summer, so that’d be great because my kids are coming back from college and my bills go up. One person said, ‘I’d probably save that up to start a business. One person said, ‘I’d go back to school.’ It wasn’t: ‘I’m gonna buy a TV or a car.’”
For the poorest people in Stockton, it could help them transition from being on the streets into some kind of housing, or from temporary housing into something more permanent. Extra cash could help people stay in their homes, rather than getting evicted. “Don’t get me started talking about Evicted,” he told me, referencing the surprise hit book by Harvard sociologist Matthew Desmond about the lives of poor people in Milwaukee.
“There was one line where he said, ‘Poor black men were locked up. Poor black women were locked out,’” Tubbs said. Locked into prison, locked out of homes from which they’d been evicted.
The lessons of the book hit close to home. He grew up in south Stockton, spending his elementary-school days in Louis Park Estates, a few blocks of nearly identical two-story condos just across the water from Rough and Ready Island. (Yes, that is its real name.)
“I’m not sure why they call it ‘estates.’ It’s a bunch of condominiums with stray cats walking around,” Tubbs jokes. “Growing up, when I'd throw out the trash, I’d toss it and dart because all the cats would come running. That’s why I still don’t like cats.”
On a recent afternoon, there were kids playing in the small and connected front yards, a few older folks perched on plastic chairs. An ancient gentleman in a brown zoot suit that might have been purchased in that cut’s heyday stepped creakily out of a Cadillac. It was closer to idyllic than dystopian, but every window had a set of heavy bars, even the second-story ones. And on one lawn, a family’s possessions were scattered everywhere, around a U-Haul that had been driven up onto the grass. If it was not an eviction, the scene spoke of some kind of hasty retreat.
* * *
Soon, there will be 1,000 more jobs in South Stockton. Amazon recently committed to building a 600,000-square-foot facility in the area.
That’s on top of a million-square-foot facility in a huge and developing logistics hub in Tracy. That’s about 20 minutes down I-205, right at the base of the Altamont Pass, which separates the Central Valley from the East Bay.
Once a sleepy agricultural area, it finds itself a logistics hub for dozens of companies. The wealthy Bay Area is nearby. There is great highway access. The Port of Oakland is through the pass. The land is cheap. And most importantly, the companies want to access “a laborshed” that extends outside the Bay.
A single developer, the logistics-focused real-estate investment trust Prologis, is developing 1,800 acres next to existing facilities for Costco and Safeway. Their first big lease went to Amazon, which snapped up a million-square-foot building that was the first warehouse to be built in the Central Valley after the Great Recession. Now thousands of people work in the warehouse alongside a fleet of robots.
“When I got in the business 10 years ago, people cared about how many truck stalls do you have, how many doors do you have, what’s your clear height,” Ryan George, the Prologis investment officer working on the Tracy project, told me. “That’s all still important, but what drives the discussion now is where is my labor? How do I compete to attract and retain labor?”
Several logistics-industry publications back up George’s assertion. There is a widely acknowledged labor “shortage” in logistics, which has been exacerbated by Amazon’s growth. That’s driven up wages beyond traditional brick-and-mortar retail jobs, but not high enough to retain employees in high-cost regions.
And that’s why Stockton and the surrounding small towns are so attractive. “Some companies are trading transportation advantages for locations that have a desirable labor pool,” wrote Logistics Management in August of this year.
At the same time, a report from the Material Handling Institute and Deloitte Consulting found that many companies expected a major increase in adoption of automation and robotics over the coming years in part because of how hard it is to find the cheap workers that make e-commerce go.
“The fact is that there are 600,000 [warehouse] jobs that are going unfilled in the United States and that gap is getting bigger and bigger,” Fetch Robotics CEO Melonee Wise told me late last year. “The turnover rate for any manufacturing or warehouse job is about 25 percent. And so, there is a need for automation because people aren’t showing up to do the work.”
And ever more e-commerce, which requires a ton of shipping, has added a new wrinkle to the structural problems: It’s highly seasonal. That’s where places like Tracy come into the equation. It’s close enough to serve the Bay Area’s wealthy, but can tap the labor pool not just in Stockton and Sacramento, but all the way out to the migrant workers of the Central Valley.
“The Central Valley in general has a big advantage. To put it in the simplest terms, there are folks out there picking tomatoes in the summertime,” George told me. “They don’t have anything to do in November, December, January. So that’s when they are helping when Amazon triples their employees. And it’s not unique to Amazon.”
Faced with these labor-market conditions, companies have a few options. They can pay out more in wages and offer more perks. They can add tech, in the form of robotics, trying to drive down the amount of labor they need. They can reduce the amount of training and responsibility the average worker needs, so all the people who churn through are roughly interchangeable.
The problem is that that latter two decisions usually make the jobs even worse, exacerbating the wage problem.
George takes me on a driving tour of the vast development. Out here in the back end of e-commerce, drought-tolerant plants line the boulevards, fed only by recycled water. There are bike paths and glassy office parks and little hints of the area’s previous life: an irrigation canal, a railroad crossing.
George stops so that we can watch the construction of a perfectly flat plane onto which concrete will be poured to create the foundation of another enormous building. We talk about how the town of Tracy has received the new development. Though the city has been supportive, some residents don’t want the new development.
“People don’t realize this is where the future is. No one’s going to shopping malls. Shopping malls are going into here, right?” he says, pointing at the soon-to-be building.
Looking around, this does seem like the perfect place for a warehouse. We’re surrounded by highways, a wind farm, huge transmission lines, aqueducts. This is the shadow infrastructure of the Bay Area, the place where the physical systems that underly even the most phone-dependent life take shape. There are jobs in making those systems work, but they may not be ones that people want to do.
It’s a fascinating paradox. While Mayor Tubbs worries about how to structure UBI and get decent jobs into his city, the logistics people are fretting about not having enough workers to fill the slots and how to purchase more robots to reduce the need for human labor.
Even out here, two hours from Silicon Valley on a good day, the tech industry is shaking up civic and economic life. Would a truly universal UBI make hiring even more difficult, thereby driving even more automation? Given that not enough people seem to want warehouse jobs, is that necessarily a bad thing?
In San Francisco, the idea of a universal basic income can drive derisive snorts as a payoff from the tech overlords, but in Stockton, they’ll take all the help they can get.


A skeleton is a human being in its most naked form. A life stripped down to its essence. As the foundation of our bodies—indeed, of our very being—skeletons provoke equal measures of fascination and terror.
As an archaeologist excavating burials, I’ve felt connected to another person—separated by centuries of time—by touching their remains. I’ve observed how exhibits of Egyptian mummies and plastinated bodies inspire wonder for others. But as a museum curator, I’ve also learned that for many cultures, human remains are not organic material to be exploited for science, but rather the sacred remnants of ancestors to be revered.
Our physical bodies will exist as motionless bones far longer than as animate flesh. And human skeletons evoke powerful reactions, from reverence to fear, when they’re encountered. Those features imbue skeletons with a surprising power. Through them, people can live on through their earthly remains.
* * *
Bones are an ancient obsession. Archaeologists recently revealed an 11,000-year-old “skull cult” in Turkey. Humanity’s first farmers also ritually de-fleshed, carved, and displayed human crania. For a thousand years, Japanese folklore has warned of the gashadokuro, a colossal starving skeleton who feasts on the living in the dark of night. The Chimbu tribe of Papua New Guinea intimidates enemies by painting their entire bodies in frightening versions of skeletons, becoming an army of the dead. In medieval Europe, the skeleton was commonly portrayed as a memento mori—a reminder of the inevitability of death.
From Hamlet’s gaze into the eye sockets of the departed court jester, to Paris’s underground catacombs (where there are 6 million skeletons for the public to view), to the laughing skulls carved on pumpkins for Halloween, human bones continue to haunt the collective imagination.
My own imagination was stirred when I excavated my first grave along Highway 188 in central Arizona more than 20 years ago. The road needed to be realigned, but more than 300 burials stood in its way, left some 750 years ago by a Native American group scholars call the Salado. As cars whizzed by, I dug into the soft dirt to reveal pearl-white bones. The archaeological work was slow and painstaking—not only because of the sheer number of burials, but also because of their dazzling contents. In the Southwest, ancient graves typically consist of the bones of the dead along with a few nonperishable artifacts, such as pottery or stone. Here, the graves were loaded with shell and turquoise jewelry, stone animal carvings, bone hairpins, and whole jars and stone points.
I had already uncovered two bodies in the shallow pit, and now a third skull appeared. When I finished exposing the left hand of this third individual, I gasped. Her hand was situated just below the right hand of the second. I realized that the pair likely died at the same time and were placed in the grave side-by-side, holding hands.
It was a moment that would shape my view of what human remains mean. Seeing those two ancients tenderly touching each other in death, I had an immediate link to their history, previously lost to the past. But I also felt their humanity surround me in the present.
* * *
Science tends to take a cold view of the dead. Bones, which The Anatomy and Biology of the Human Skeleton describes as the “remnants of mineralized connective tissue,” are made up of cells arranged in a matrix like a spiderweb. When living, they are a bank of salts, calcium, and red blood cells. Adult humans have 206 bones, which shelter vital organs while also working in concert with muscles to give humans their characteristic fluid rigidity. Though made from soft tissue, bones are tremendously strong. They can heal themselves. The human skeleton is a brilliant feat of evolution.
It took centuries for humans to understand it. More than 2,000 years ago, in what is now Turkey, the physician and philosopher Galen undertook one of the first systematic studies of human anatomy. He got it mostly right, but also seeded myths—such as the theory that bones consist of the same matter as semen because they share a similar color.
Later, the Persians took great interest in anatomy, advancing its knowledge along with scholars in China, Japan, and India. But it wasn’t until the cusp of the European Renaissance that a renewed interest in human dissection led to detailed studies of the body’s architecture. The greatest researcher of this period was Leonardo da Vinci, whose detailed illustrations accurately revealed the body’s inner workings. By the 16th century, articulated human skeletons hung in anatomy theaters across Europe.
During the centuries that followed, the science of the human skeleton took a darker turn. Between 1839 and 1849, Samuel G. Morton published his three-volume Crania Americana, which purported to prove the superiority or inferiority of races based on measurements of their skulls. Based on these racist ideas, museums collected thousands of skeletons—mostly of Native Americans, since their graveyards were easiest to pillage.
Today’s researchers reject such views, of course. Biological inheritance is intertwined with behavior, environment, and culture. People are born with bones, but those bones respond to the world that contains them and bodies that live atop their scaffolding. This is why the skeleton continues to be so valuable to archaeologists. Excavated remains tell the stories of the dead—a person’s sex and age at death, along with their disorders and diseases, traumas and infections, clues to their diet, what hand they used most, how hard they worked. Bones are also a vessel for DNA, which allows scientists to trace the migrations of ancient humans and even discover who they had sex with.
* * *
Some cultures intentionally display their dead. The Torajans on the Indonesian island of Sulawesi, for example, mummify deceased relatives and keep them in their homes, talking to them and feeding them. Yet many people around the world are distraught that their ancestors lie as specimens on museum shelves.
Some years ago, a group of Native Americans came to visit their ancestors’ remains in the storage area of the museum where I work. They asked me to turn off the lights. We were engulfed in darkness when an elder struck a match and lit a bundle of sage, the sweet smoke filling the air. He then sang a song so loudly that the metal of the cabinets reverberated like an accompanying drumbeat. He said he wanted to be sure that his ancestors’ spirits knew he was there—that he remembered them and cared for them.
Bones are not the same as shards of pottery or beaded moccasins. In 1990, after years of protest, Native Americans secured a federal law that established a process for the return of human remains, funerary offerings, and other cultural items from museums. In the years since, more than 57,000 Native American skeletons and 1.7 million burial goods have been repatriated (although more than 100,000 skeletons and millions more artifacts are still in U.S. museums). This movement has become global, as indigenous peoples in New Zealand, Australia, Canada, and parts of Africa have demanded the return of their dead.
While some scientists and museums have pushed back against such claims, the native peoples and the scientists agree more than they might realize. Most Native Americans and indigenous peoples do not oppose science; they object to the form of science that robs bodies of their humanity, especially without consent. Likewise, Westerners also respect skeletons when given the opportunity. In 2012, workers discovered a shallow, unmarked grave under a parking lot in Leicester, England. The skeleton it held, scholars soon confirmed, belonged to King Richard III. For more than 500 years, no one had known the exact fate of the English monarch, long portrayed as a tyrant and murderer. The discovery was a revelation. In his bones lay vital clues about the monarch’s life and last days.
Unlike Native Americans, however, the king didn’t go into a museum. His remains sparked an outpouring of grief and love. Locals raised more than $250,000 for a funeral. The body was laid in an oak coffin in Leicester’s Anglican cathedral. Thousands came to view him. After three days, in an intricate ceremony, 10 British Army soldiers carried Richard III to a marble tomb.
In this moment, Richard III was made a king once again, given a fleeting but vitalized second life. His skeleton provoked new ideas about his biography and England’s history. The mere presence of the bones got the living to fund and attend a burial with the pomp and circumstance befitting royalty.
* * *
A fork does not eat. A painting does not gaze. A book cannot think. But objects do induce humans to act and feel. A fork affords nourishment; a painting creates the experience of beauty; a book stimulates learning. Through their form, cultural function, historical role, or inherent qualities, objects exert their influence and power.
Perhaps nothing does this more profoundly than human bones. They are the medium through which people live on after death. The sight of skeletons can draw or repel. When used for historical purposes, they provide answers about life. When used spiritually, they provoke questions about what lies after death. Perhaps this is why people feel the power of skeletons so viscerally. They seem alive and dead all at once. That’s why they live on so vibrantly, and why people can’t help to react to them with both awe and fear. You and I and everyone else will surely die, but our bones will live on without us.
This article appears courtesy of Object Lessons.


On Monday, the reporter Taylor Lorenz noticed that Google Maps had a new feature: Walking distances were delivered in terms of calories.
Instead of simply telling her that a walk would take 13 minutes, the app also converted that to an amount of energy, 59 calories. Then a click on that calorie count gave a further conversion, from calories to food.
Specifically, mini cupcakes with pink frosting.
This was not well received.
Responses varied narrowly. An ostensible measure to promote health was interpreted as a tech corporation policing women’s bodies.
The writer Rachel Joy Larris noted: “‘Cupcake?’ Let’s talk about all the signifiers that contains about assumptions of gender, culture, and food.”
The writer Dana Cass said, referring to the Harvey Weinstein-induced Me Too movement: “Lol every woman I know has been sexually assaulted and Google Maps is telling me how many calories I’ll burn on my walk to work.”
The app offered no option to convert calorie counts into Budweiser or raw venison.
Within hours, BuzzFeed News reported that Google was simply testing the change, and that it “is removing this feature due to strong user feedback.”
Despite a boom in fitness apps and $1,200 watches that track physical activity, many people do not want to be reminded of calories unless requested. While this sort of nudge may benefit some people, among others the concern is that overwhelming focus on intake and output can drive bulimia or anorexia. In either case, unsolicited calorie counts and cupcake equivalents have an air of body policing and guilt inducement that do not pair well with a culture that assiduously regulates women’s appearances. As writer Casey Johnston offered, “Any woman could have told you this is a supremely bad thing a) to do b) to not be able to turn off.”
In the spirit of no-one-size-fits-all solutions in health, there is more logic in Google considering this as an opt-in feature rather than a default. Tailoring the experience to users in ways safe and driven by evidence would mean more thought than simply forcing pink-cupcake counts on unsuspecting people.
For instance, Google estimated, “The average person burns 90 calories by walking one mile.” Calorie counts vary widely from person to person—walking a mile is a much less energy-intensive endeavor for a professional endurance athlete than a veteran of World War II. Google presumably has the personal data on most of us to make a much more precise calculation—and to suggest more specific incentives than cupcakes or burning calories.
I’ve argued many times that calorie bartering is not usually an effective approach to weight loss or health. Calories offer no insight into the nutritional value of a food, and they are often used by sellers of junk to convince people that they can eat junk if they simply exercise the calories away. But the metabolic effects of 100 calories of Coke on future hunger and energy storage are not the same as a 100 calorie salad, any more than introducing any two 100-pound people would have the same effect on a dinner party.
All of this is part of the consistent theme that obesity prevention is much less straightforward than other public-health challenges. Metabolic syndrome is unique among deadly preventable conditions—it is not the equivalent to if Google Maps were able to track swarms of Zika-infected mosquitoes and suggest alternate routes.
As our behavior is shaped more and more by interactions with phones, our health is shaped by the world that comes to us through apps. The effects can be beneficial or otherwise, but they will not be neutral. This means a serious burden/opportunity on designers to advocate responsibly and strategically for health. That means reckoning with the individual and societal stigma of states of health that affect our outward appearances, and those which are tied to ideas of guilt and moral judgment, and finding ways to make health easy without compromising any individual’s sense of agency in deciding what degree of health they choose to pursue.


The first recorded example in Western literature of men telling women to shut up and stay in the house, writes classicist Mary Beard in her 2014 essay, “The Public Voice of Women,” is in the Odyssey. Not-yet-grown Telemachus tells his mother, Penelope, to “go back up into your quarters, and take up your own work, the loom and the distaff ... speech will be the business of men, all men, and of me most of all.”
As Beard noted in her essay, centuries on, the voices of women are still considered illegitimate in the public sphere, including the new spaces of social media. That manifests as verbal harassment, death threats, and doxing online; as complaints about the sound of women’s literal voices on the radio, giving talks, or in podcasts; as sexual harassment in the workplace; as catcalls on the street. All of these can be seen as ways to drive women out of the public sphere, and back to their proper domain of Kinder, Küche, Kirche (children, kitchen, church). On Friday, many Twitter users boycotted the platform, in response to the suspension of the actress Rose McGowan’s account for speaking out about sexual harassment by the film executive Harvey Weinstein. The driving force for the boycott was women outraged that hate speech, including misogynist and racial harassment and threats, routinely go unchecked, and yet McGowan’s account was suspended.
These women did indeed remove themselves from a public sphere. Twitter, with its more than 300 million active monthly users, is a communal space in a new and extraordinary way that’s driven by the specific technological decisions of the site, which carry with them specific affordances. “Affordances,” a term popularized in the world of design and user interaction by Donald Norman, is a way of describing the perceived possibilities of how the user can interact with the product. These affordances shape how users behave.
Much of the power of Twitter comes from retweets, which can carry the words of a user to an audience far beyond their own followers (for comparison, see Instagram, where no such function exists—it makes it much more difficult for a specific image to “go viral” on the site). But retweeting also allows for what social-media researchers such as danah boyd and Alice Marwick refer to as “context collapse”: removing tweets from not only their temporal and geographic context, but also their original social and cultural milieu, which is very different from most public spaces. I described it to a friend once on a New York City subway—“we’re talking in public, in that everyone near us in this subway car can hear what we’re saying, but that’s a very different ‘public’ than hearing ourselves on NPR tomorrow.” While readers may literally know nothing about the poster or the context except for what is said in that one tweet, they can still just hit “reply” and their response will likely be seen by the poster.
While nothing is stopping people from finding out more information before responding, the clearest affordance Twitter has is for these “drive-by” responses (I’ve been mansplained to by many people who I presume haven’t even looked at my bio to see the “engineering professor” there before trying to school me on my research field—per Telemachus, “of me most of all”). This amplification and context collapse, coupled with the ease of replying and of creating bots, makes targeted harassment trivially easy, particularly in an environment where users can both mostly live in their own ideological bubble by following people who share their views, however abhorrent, and who can easily forget that there is a real person behind the 140 characters of text.
So while Twitter may consider itself to be merely reflecting the discourse, these technological affordances ease the way for certain types of hostile behavior. If you think of the experience of the generalized, systemic misogyny and racism of our culture as being bathed in sunlight on a scorching hot day, Twitter might say it’s just a mirror. But it’s actually handing out magnifying glasses that can focus the already painful ambient sunlight into a killing ray. The targets of this ire, in our society and on Twitter, are disproportionately not just women but people of color. (Imagine how Telemachus would have responded if, rather than his mother, one of the non-Greek household slaves chose to speak up in visiting company.)
One of the most profound social changes of the last few decades is opening up public discourse to a broader range of speakers than ever before, and social media has been a large part of that. The specific affordances of Twitter make it powerful—it can amplify marginalized voices but it can also amplify harassment. Friday’s boycott was intended to be a unified stand against that.
But the point of harassment is to shut women up, either by self-censorship through fear or by driving them away from Twitter, making it simply the newest wrinkle in that long history of exclusion from public spaces and conversations. Many women, especially women of color, therefore found a protest that mandated their silence to be ironic, if not outright misguided: It takes a certain amount of social power to genuinely believe that your absence would be remarked upon and lamented. After 3,000 years of denying the public sphere to all but a small set of voices, some of the new voices are rightly considering their presence to be a sit-in, an occupation, and they are rightly refusing to be driven away. Ultimately, if Twitter wants to be the public sphere, it needs to act like it, by working to create an environment where all voices can be safely heard. Twitter’s social problems are exacerbated by the affordances of technology; they’ll need to bring both ongoing human effort and better design decisions to improve the experiences of marginalized people, and therefore everyone, in their public sphere.


Last week, Sotheby’s auctioned off 140 little black dresses. The event, “Les Petites Robes Noires, 1921–2010,” featured vintage dresses collected by the fashion antiquarian Didier Ludot. A dazzling mix of silk faille, velvet, jersey, and tulle—all in black—cut simple silhouettes. The collection included iconic pieces from Chanel, Givenchy, and Hermès. The more expensive lots fetched over 20,000 euros.
To introduce the collection, Ludot wrote, “Today I pay tribute to the astonishing story of the little black dress and to the designers who wrote its story, a dizzying tale ... from the Roaring Twenties to the new millennium.” But the most astonishing part of the little black dress’s story might be its prologue, the backstory left out of the auction catalogue, the glossy coffee-table books, and the fashion magazines. The most important acolytes of the little black dress were not designers nor aristocrats, but masses of working-class women.
* * *
In October 1926, Vogue featured a sketch of a long-sleeved, calf-length, black sheath dress by a plucky young designer named Coco Chanel. Dubbed “Chanel’s Ford,” the dress was promoted as equivalent in egalitarianism to the Model T.
At the time, Vogue’s editors wrote that Chanel’s little black dress would “become sort of a uniform for all women of taste.” That seems like an astute prediction, in hindsight. But in 1926, the proclamation was tone-deaf at best, as the little black dress was already the actual uniform of many working-class women. The little black dress (or LBD, as it is commonly abbreviated) was a uniform designed to keep certain women in their place. Only later was it co-opted as haute couture for women of taste.
When the lower classes adopt the fashions of the elite, the elites often respond by changing course abruptly—a neckline or a hemline rises or falls dramatically, perhaps, or a voluminous silhouette narrows. But sometimes, rather than quickly changing styles, the upper classes simply wear the clothes the poor have discarded.
For example, as towns populated in the 14th century, a merchant class arose within them. This middle class had some discretionary income, and they spent it on the most conspicuous consumer good: clothing. Finally, they could afford jewel-studded velvets, gold and silver trimmings, brightly colored coats, and sumptuous furs. As the fashion historian Anne Hollander has explained, when the aristocracy couldn’t outlaw or outspend these medieval nouveau riche, they started wearing baggy and threadbare clothing. This new fashion—looking like one had thrown on any old thing—served as a not-so-subtle reminder to the upstarts that, while money could buy clothes, it couldn’t buy class.
Blue jeans offer a more recent example. Jeans began as cheap and durable work pants for miners and farmers. They were the de facto uniform of the rural working class. But once working-class men had access to ready-to-wear trousers, their jeans started showing up on postwar suburban youths, and then in trendy boutiques. Recently, Nordstrom even sold a $425 pair of jeans with fake mud stains—the ultimate blue-collar costume. Once more, the wealthy turn the tables by appropriating the clothing of the poor.
The LBD also finds its origins among the poor. Before the 19th century, domestic servants wore whatever they could—homemade dresses, often, but also their employers’ hand-me-downs. But in the 1860s, the British upper classes required their maids to wear a common uniform: a white mobcap, an apron, and a simple black dress. Soon after, wealthy American and French families followed suit.
Relationships between upper-class women and their servants had changed, becoming “less intimate and more authoritarian,” as the sociologist Diana Crane puts it. At this time, servants ceased to be “the help,” a somewhat collegial characterization, and became known as “domestics.” And domestics wearing upper-class castoffs, especially young and pretty ones, led to embarrassing mix-ups. A caller mistaking the maid for the mistress of the house raised uncomfortable questions about recently erected class barriers.
Cassell’s Household Guide, which billed itself as an encyclopedia of domestic and social economy, summed up the problem like this, circa 1880: “As a general rule, ladies do not like to see their maids dressed in the clothes they themselves have worn—the difference in the social scale of mistress and maid renders this unpleasing.”
But Cassell’s made one exception: “a black or a dark-colored silk.” Previously, a simple black dress meant a wealthy woman was “dressing down.” But by the 19th century, the black dress had become a staple of the lower and middle classes. It was the perfect hand-me-down for the help.
* * *
There was a time when black signified wealth. It was favored by 15th-century Spanish aristocrats and wealthy Dutch merchants. Later, Baldassare Castiglione’s 1528 The Book of the Courtier advised others to follow their lead, to appear above the petty fads of commoners. Black clothing conveyed plainness and piety, for one thing. But it was also incredibly expensive to produce, requiring vast quantities of imported “oak apples”—a bulbous growth left behind on oak leaves from insect egg sacs. By the early 19th century, a newer dye made from logwood and ferrous sulfate made the color cheap to produce. In 1863, an even cheaper synthetic aniline black dye was developed.
By the 1880s, most awkward maid-or-mistress mix-ups had been eliminated thanks to the trusty black dress. But another sort of working-class woman now had the opportunity to dress above her station. Rapid industrialization gave consumers more disposable income, and they wanted places to spend it. More shops opened in urban centers, and cheap labor was needed to staff them. Unmarried young women began pouring into the cities to work as “shopgirls” in dry-good establishments, dress stores, hat and glove shops, and department stores.
The shopgirl enjoyed more freedom and less supervision than domestic servants did. Often, for the first time in her life, she also enjoyed some disposable income of her own. The sewing machine, invented in 1846 and mass-produced in the 1870s, made it easier than ever to imitate these fashions. Mated to the precut paper pattern, devised by the upscale American designer Ellen Curtis Demorest, women could duplicate the latest fashions from Paris with relative ease. And advances in efficiency at textile factories made a wider variety of fabrics and trims available with which to do so.
The new cheap aniline dyes that made the domestic’s black uniform possible also made brightly colored dresses—the vivid scarlets, blues, and greens that were once only for the upper classes—affordable, too. With a few dollars and a few nights’ work, an enterprising shopgirl could create a passable imitation of a dress from the society pages. Or instead, she could shop the sale rack at her place of employment—one of the large, new department stores—and purchase a ready-to-wear dress. She could then alter and trim the dress with lace, sequins, or buttons to make it appear custom-made.
So attired, she might successfully blend in with a store’s clientele—or even outshine them. This wasn’t a desirable state of affairs. Writing in the June 4, 1910, edition of the International Gazette, a Methodist minister urged that “the craze of the shopgirl ... as fashionably attired as the rich woman she waits on had become a menace.” Even earlier, in response to customer complaints, employers had brainstormed ways to neutralize the threat. In 1890, The Sun declared there was a “revolution in dress” underway, “not by the fashionable folk, but by New York’s army of shopgirls.”
In response, many employers began requiring their female employees to dress like domestic servants, in simple black dresses. An 1892 San Francisco Call headline summarized the reaction among the labor pool: “The Shopgirls Hate It.” Sometimes they even went on strike in response. But threatened with termination, most shopgirls buckled, and by the 1890s the little black dress was the required uniform in New York, London, and Paris.
In the summer of 1894, wearing a black dress became a condition of employment for Jersey City telephone operators, too. The “‘hello’ girls,” as they were called, also protested. Newspapers presented their case sympathetically; in 1892, for example, the Reading Times pointed out that the women were opposed not to the dress itself, but “to the idea of showing by their dress that they are working girls.”
For these reasons, the little black dress became a marker of class. When young working-class women complained that being forced into uniform was “inconsistent with our ideals of freedom and independence,” as the The San Francisco Call reported in 1892, they weren’t just complaining about self-expression. Embedded in their ideals was the promise of social mobility.
These women were the fin de siècle equivalent of medieval merchants. They mixed with the upper classes, whether in drawing rooms or on retail shop floors, and they saw what the wealthy wore up close. Thanks to the sewing machine, the paper pattern, and affordable fabrics, the working classes could finally, feasibly, dress like high society—even if they were now only permitted to do so after work hours.
* * *
Society matrons exacted their revenge by dressing like shopgirls and maids, reappropriating their little black dresses for the upper crust.
Lillie Langtry, a famous British beauty who would go on to become a successful actress, conquered London society in 1886 “dressed in a simple little black frock,” as the Emporia Daily News described it. By the early 1900s, socialites who wanted to appear especially youthful and edgy donned little black dresses. The LBD appeared in fashion magazines and society pages decades before Chanel’s dress appeared in Vogue. It was such an established trend by 1915 that even the wife of the U.S. Secretary of the Treasury appeared in public looking “like a college girl, in her short little black dress.”
While Coco Chanel didn’t invent the little black dress, she was astute enough to pick up on the underlying trend that made it popular—la pauvreté de luxe, she called it, or “luxurious poverty.” It was a look reserved exclusively for those who could “afford” to look poor by pretending that they simply couldn’t be bothered with fashion. But while a rich woman might now better blend into the crowd, on closer inspection, there would be some small detail in her seemingly anonymous garment—a certain cut or fabric or label—that acted as a secret handshake for those in the know.
Today, the fashion industry sometimes celebrates the little black dress as an equal-opportunity fashion—versatile, classic, and chic. But this neutral garment was never ideologically neutral—nor was it the democratic creation of a visionary designer. The little black dress marked and mediated social boundaries, a collaboration between cutting-edge technology and age-old class politics.
Today, in addition to little black-dress auctions, there are LBD-themed dinner parties and wine tastings, galas and charity balls. A little black dress has become a shorthand for instant glamour, promising to disguise both figure flaws and mundane lives. This blue-collar costume has successfully crossed over. Women wear little black dresses to feel more like Audrey Hepburn or Princess Diana or even a model in a Robert Palmer music video. But when they do, those women also conjure other predecessors: the women who wore them while they balanced trays, stocked shelves, folded shirts, worked the switchboards, and wrung out the laundry.
This article appears courtesy of Object Lessons.




Few journalists have gotten a peek inside X, the secretive lab run by Google's parent company Alphabet. Its scientists are researching cold fusion, hover boards, and stratosphere-surfing balloons. Derek Thompson, staff writer at The Atlantic, spent several days with the staff of X. In this episode, he tells Matt and Alex all about what he found, and what it suggests about the future of technological invention.
Links:


There’s a famous viral video in which a diver slowly swims up to a clump of rock and seaweed, only for part of that clump to turn white, open its eye, and jet away, squirting ink behind it. Few videos so dramatically illustrate an octopus’s mastery of camouflage. But ignore, if you can, the creature’s color, and focus on its texture. As its skin shifts from mottled brown to spectral white, it also goes from lumpy to smooth. In literally the blink of an eye, all those little bumps, spikes, and protuberances disappear.
The project was entirely funded by the U.S. Army Research Office—and it’s not hard to imagine why. There are obvious benefits to having materials that can adaptively hide the outlines of vehicles and robots by breaking up their outlines. But there are other applications beyond military ones, Shepherd says. It might cut down on shipping costs if you could deliver materials as flat sheets, and then readily transform them into three-dimensional shapes—like flat-pack furniture, but without the frustrating assembly. Or, as the roboticist Cecilia Laschi notes in a related commentary, biologists could use camouflaged robots to better spy on animals in their natural habitats.
“I don’t see this being implemented in any real application for quite some time,” says Shepherd. Instead, he mainly wants to learn more about how octopuses themselves work, by attempting to duplicate their biological feats with synthetic materials. “I’m just a big nerd who likes biology,” he says.
Octopuses change their texture using small regions in their skin known as papillae. In these structures, muscle fibers run in a spiderweb pattern, with both radial spokes and concentric circles. When these fibers contract, they draw the soft tissue in the papillae towards the center. And since that tissue doesn’t compress very well, the only direction it can go is up. By arranging the muscle fibers in different patterns, the octopus can turn flat, two-dimensional skin into all manner of three-dimensional shapes, including round bumps, sharp spikes, and even branching structures.
Shepherd’s team—which includes the postdoc James Pikul and the octopus expert Roger Hanlon, who took the famous video at the start of this piece—designed their material to work in a similar way. In place of the octopus’s soft flesh, they used a stretchy silicone sheet. And in place of the muscles, they used a mesh of synthetic fibers that were laid down in concentric rings. Normally, the silicone membrane would balloon outward into a sphere when inflated. But the rings of fibers constrain it, limiting its ability to expand and forcing it to shoot upward instead.
By changing the layout of the fibers, the team could create structures that would inflate into various shapes, like round bumps and pointy cones. Pikul grabbed a stone from a local riverbed and programmed the material to mimic its contours. He set the material to create hierarchical shapes—lumps on lumps. He even programmed it to duplicate the more complicated contours of a field of stones, and a plant with spiraling leaves.
For the moment, the material can only be programmed to mimic one predetermined shape at a time. Still, “the results are impressive,” writes Laschi, and “represent a first step toward more general camouflage abilities.” Indeed, Shepherd is now adapting the material so it can transform more flexibly—just like a real octopus. For example, the team could replace the fixed mesh of fibers with rubber tubes, parts of which could be inflated or deflated at whim. That way, they could change which bits of the surface are flexible, to determine how it will eventually inflate.
Shepherd’s team is just one of many groups who are attempting to build soft robots, which eschew the traditional hard surfaces of most machines in favor of materials that are soft, bouncy, and floppy. Such bots would theoretically be better at navigating tough terrain, resisting shocks and injuries, and even caring for people. Often, these researchers use the octopus as an inspiration. Last year, Harvard researchers 3-D printed a soft, autonomous “octobot” that moved by burning small amounts of onboard fuel, and channeling the resulting gas into its arms. Laschi, meanwhile, has built a robot with soft floppy arms that can wiggle through the water.
The robots are certainly cool, but they’re nowhere near as versatile as the real deal. Shepherd’s material, for example, can change texture about as fast as an actual octopus, but it can only make one rough shape at a time. The animal, meanwhile, can produce far finer undulations in its skin, which are tuned to whatever it sees in its environment. For now, nothing we produce comes anywhere close.


In the media world, as in so many other realms, there is a sharp discontinuity in the timeline: before the 2016 election, and after.
Things we thought we understood—narratives, data, software, news events—have had to be reinterpreted in light of Donald Trump’s surprising win as well as the continuing questions about the role that misinformation and disinformation played in his election.
To hear more feature stories, see our full list or get the Audm iPhone app.
Tech journalists covering Facebook had a duty to cover what was happening before, during, and after the election. Reporters tried to see past their often liberal political orientations and the unprecedented actions of Donald Trump to see how 2016 was playing out on the internet. Every component of the chaotic digital campaign has been reported on, here at The Atlantic, and elsewhere: Facebook’s enormous distribution power for political information, rapacious partisanship reinforced by distinct media information spheres, the increasing scourge of “viral” hoaxes and other kinds of misinformation that could propagate through those networks, and the Russian information ops agency.
But no one delivered the synthesis that could have tied together all these disparate threads. It’s not that this hypothetical perfect story would have changed the outcome of the election. The real problem—for all political stripes—is understanding the set of conditions that led to Trump’s victory. The informational underpinnings of democracy have eroded, and no one has explained precisely how.
* * *
We’ve known since at least 2012 that Facebook was a powerful, non-neutral force in electoral politics. In that year, a combined University of California, San Diego and Facebook research team led by James Fowler published a study in Nature, which argued that Facebook’s “I Voted” button had driven a small but measurable increase in turnout, primarily among young people.
Rebecca Rosen’s 2012 story, “Did Facebook Give Democrats the Upper Hand?” relied on new research from Fowler, et al., about the presidential election that year. Again, the conclusion of their work was that Facebook’s get-out-the-vote message could have driven a substantial chunk of the increase in youth voter participation in the 2012 general election. Fowler told Rosen that it was “even possible that Facebook is completely responsible” for the youth voter increase. And because a higher proportion of young people vote Democratic than the general population, the net effect of Facebook’s GOTV effort would have been to help the Dems.
The research showed that a small design change by Facebook could have electoral repercussions, especially with America’s electoral-college format in which a few hotly contested states have a disproportionate impact on the national outcome. And the pro-liberal effect it implied became enshrined as an axiom of how campaign staffers, reporters, and academics viewed social media.
In June 2014, Harvard Law scholar Jonathan Zittrain wrote an essay in New Republic called, “Facebook Could Decide an Election Without Anyone Ever Finding Out,” in which he called attention to the possibility of Facebook selectively depressing voter turnout. (He also suggested that Facebook be seen as an “information fiduciary,” charged with certain special roles and responsibilities because it controls so much personal data.)
In late 2014, The Daily Dot called attention to an obscure Facebook-produced case study on how strategists defeated a statewide measure in Florida by relentlessly focusing Facebook ads on Broward and Dade counties, Democratic strongholds. Working with a tiny budget that would have allowed them to send a single mailer to just 150,000 households, the digital-advertising firm Chong and Koster was able to obtain remarkable results. “Where the Facebook ads appeared, we did almost 20 percentage points better than where they didn’t,” testified a leader of the firm. “Within that area, the people who saw the ads were 17 percent more likely to vote our way than the people who didn’t. Within that group, the people who voted the way we wanted them to, when asked why, often cited the messages they learned from the Facebook ads.”
In April 2016, Rob Meyer published “How Facebook Could Tilt the 2016 Election” after a company meeting in which some employees apparently put the stopping-Trump question to Mark Zuckerberg. Based on Fowler’s research, Meyer reimagined Zittrain’s hypothetical as a direct Facebook intervention to depress turnout among non-college graduates, who leaned Trump as a whole.
Facebook, of course, said it would never do such a thing. “Voting is a core value of democracy and we believe that supporting civic participation is an important contribution we can make to the community,” a spokesperson said. “We as a company are neutral—we have not and will not use our products in a way that attempts to influence how people vote.”
They wouldn’t do it intentionally, at least.
As all these examples show, though, the potential for Facebook to have an impact on an election was clear for at least half a decade before Donald Trump was elected. But rather than focusing specifically on the integrity of elections, most writers—myself included, some observers like Sasha Issenberg, Zeynep Tufekci, and Daniel Kreiss excepted—bundled electoral problems inside other, broader concerns like privacy, surveillance, tech ideology, media-industry competition, or the psychological effects of social media.
The same was true even of people inside Facebook. “If you’d come to me in 2012, when the last presidential election was raging and we were cooking up ever more complicated ways to monetize Facebook data, and told me that Russian agents in the Kremlin’s employ would be buying Facebook ads to subvert American democracy, I’d have asked where your tin-foil hat was,” wrote Antonio García Martínez, who managed ad targeting for Facebook back then. “And yet, now we live in that otherworldly political reality.”
Not to excuse us, but this was back on the Old Earth, too, when electoral politics was not the thing that every single person talked about all the time. There were other important dynamics to Facebook’s growing power that needed to be covered.
* * *
Facebook’s draw is its ability to give you what you want. Like a page, get more of that page’s posts; like a story, get more stories like that; interact with a person, get more of their updates. The way Facebook determines the ranking of the News Feed is the probability that you’ll like, comment on, or share a story. Shares are worth more than comments, which are both worth more than likes, but in all cases, the more likely you are to interact with a post, the higher up it will show in your News Feed. Two thousand kinds of data (or “features” in the industry parlance) get smelted in Facebook’s machine-learning system to make those predictions.
What’s crucial to understand is that, from the system’s perspective, success is correctly predicting what you’ll like, comment on, or share. That’s what matters. People call this “engagement.” There are other factors, as Slate’s Will Oremus noted in this rare story about the News Feed ranking team. But who knows how much weight they actually receive and for how long as the system evolves. For example, one change that Facebook highlighted to Oremus in early 2016—taking into account how long people look at a story, even if they don’t click it—was subsequently dismissed by Lars Backstrom, the VP of engineering in charge of News Feed ranking, as a “noisy” signal that’s also “biased in a few ways” making it “hard to use” in a May 2017 technical talk.
Facebook’s engineers do not want to introduce noise into the system. Because the News Feed, this machine for generating engagement, is Facebook’s most important technical system. Their success predicting what you’ll like is why users spend an average of more than 50 minutes a day on the site, and why even the former creator of the “like” button worries about how well the site captures attention. News Feed works really well.
But as far as “personalized newspapers” go, this one’s editorial sensibilities are limited. Most people are far less likely to engage with viewpoints that they find confusing, annoying, incorrect, or abhorrent. And this is true not just in politics, but the broader culture.
That this could be a problem was apparent to many. Eli Pariser’s The Filter Bubble, which came out in the summer of 2011, became the most widely cited distillation of the effects Facebook and other internet platforms could have on public discourse.
Pariser began the book research when he noticed conservative people, whom he’d befriended on the platform despite his left-leaning politics, had disappeared from his News Feed. “I was still clicking my progressive friends’ links more than my conservative friends’— and links to the latest Lady Gaga videos more than either,” he wrote. “So no conservative links for me.”
Through the book, he traces the many potential problems that the “personalization” of media might bring. Most germane to this discussion, he raised the point that if every one of the billion News Feeds is different, how can anyone understand what other people are seeing and responding to?
“The most serious political problem posed by filter bubbles is that they make it increasingly difficult to have a public argument. As the number of different segments and messages increases, it becomes harder and harder for the campaigns to track who’s saying what to whom,” Pariser wrote. “How does a [political] campaign know what its opponent is saying if ads are only targeted to white Jewish men between 28 and 34 who have expressed a fondness for U2 on Facebook and who donated to Barack Obama’s campaign?”
This did, indeed, become an enormous problem. When I was editor in chief of Fusion, we set about trying to track the “digital campaign” with several dedicated people. What we quickly realized was that there was both too much data—the noisiness of all the different posts by the various candidates and their associates—as well as too little. Targeting made tracking the actual messaging that the campaigns were paying for impossible to track. On Facebook, the campaigns could show ads only to the people they targeted. We couldn’t actually see the messages that were actually reaching people in battleground areas. From the outside, it was a technical impossibility to know what ads were running on Facebook, one that the company had fought to keep intact.
Pariser suggests in his book, “one simple solution to this problem would simply be to require campaigns to immediately disclose all of their online advertising materials and to whom each ad is targeted.” Which could happen in future campaigns.
Imagine if this had happened in 2016. If there were data sets of all the ads that the campaigns and others had run, we’d know a lot more about what actually happened last year. The Filter Bubble is obviously prescient work, but there was one thing that Pariser and most other people did not foresee. And that’s that Facebook became completely dominant as a media distributor.
* * *
About two years after Pariser published his book, Facebook took over the news-media ecosystem. They’ve never publicly admitted it, but in late 2013, they began to serve ads inviting users to “like” media pages. This caused a massive increase in the amount of traffic that Facebook sent to media companies. At The Atlantic and other publishers across the media landscape, it was like a tide was carrying us to new traffic records. Without hiring anyone else, without changing strategy or tactics, without publishing more, suddenly everything was easier.
While traffic to The Atlantic from Facebook.com increased, at the time, most of the new traffic did not look like it was coming from Facebook within The Atlantic’s analytics. It showed up as “direct/bookmarked” or some variation, depending on the software. It looked like what I called “dark social” back in 2012. But as BuzzFeed’s Charlie Warzel pointed out at the time, and as I came to believe, it was primarily Facebook traffic in disguise. Between August and October of 2013, BuzzFeed’s “partner network” of hundreds of websites saw a jump in traffic from Facebook of 69 percent.
At The Atlantic, we ran a series of experiments that showed, pretty definitively from our perspective, that most of the stuff that looked like “dark social” was, in fact, traffic coming from within Facebook’s mobile app. Across the landscape, it began to dawn on people who thought about these kinds of things: Damn, Facebook owns us. They had taken over media distribution.
Why? This is a best guess, proffered by Robinson Meyer as it was happening: Facebook wanted to crush Twitter, which had drawn a disproportionate share of media and media-figure attention. Just as Instagram borrowed Snapchat’s “Stories” to help crush the site’s growth, Facebook decided it needed to own “news” to take the wind out of the newly IPO’d Twitter.
The first sign that this new system had some kinks came with “Upworthy-style” headlines. (And you’ll never guess what happened next!) Things didn’t just go kind of viral, they went ViralNova, a site which, like Upworthy itself, Facebook eventually smacked down. Many of the new sites had, like Upworthy, which was cofounded by Pariser, a progressive bent.
Less noticed was that a right-wing media was developing in opposition to and alongside these left-leaning sites. “By 2014, the outlines of the Facebook-native hard-right voice and grievance spectrum were there,” The New York Times’ media and tech writer John Herrman told me, “and I tricked myself into thinking they were a reaction/counterpart to the wave of soft progressive/inspirational content that had just crested. It ended up a Reaction in a much bigger and destabilizing sense.”
The other sign of algorithmic trouble was the wild swings that Facebook Video underwent. In the early days, just about any old video was likely to generate many, many, many views. The numbers were insane in the early days. Just as an example, a Fortune article noted that BuzzFeed’s video views “grew 80-fold in a year, reaching more than 500 million in April.” Suddenly, all kinds of video—good, bad, and ugly—were doing 1-2-3 million views.
As with news, Facebook’s video push was a direct assault on a competitor, YouTube. Videos changed the dynamics of the News Feed for individuals, for media companies, and for anyone trying to understand what the hell was going on.
Individuals were suddenly inundated with video. Media companies, despite no business model, were forced to crank out video somehow or risk their pages/brands losing relevance as video posts crowded others out.
And on top of all that, scholars and industry observers were used to looking at what was happening in articles to understand how information was flowing. Now, by far the most viewed media objects on Facebook, and therefore on the internet, were videos without transcripts or centralized repositories. In the early days, many successful videos were just “freebooted” (i.e., stolen) videos from other places or reposts. All of which served to confuse and obfuscate the transport mechanisms for information and ideas on Facebook.
Through this messy, chaotic, dynamic situation, a new media rose up through the Facebook burst to occupy the big filter bubbles. On the right, Breitbart is the center of a new conservative network. A study of 1.25 million election news articles found “a right-wing media network anchored around Breitbart developed as a distinct and insulated media system, using social media as a backbone to transmit a hyper-partisan perspective to the world.”
Breitbart, of course, also lent Steve Bannon, its chief, to the Trump campaign, creating another feedback loop between the candidate and a rabid partisan press. Through 2015, Breitbart went from a medium-sized site with a small Facebook page of 100,000 likes into a powerful force shaping the election with almost 1.5 million likes. In the key metric for Facebook’s News Feed, its posts got 886,000 interactions from Facebook users in January. By July, Breitbart had surpassed The New York Times’ main account in interactions. By December, it was doing 10 million interactions per month, about 50 percent of Fox News, which had 11.5 million likes on its main page. Breitbart’s audience was hyper-engaged.
There is no precise equivalent to the Breitbart phenomenon on the left. Rather the big news organizations are classified as center-left, basically, with fringier left-wing sites showing far smaller followings than Breitbart on the right.
And this new, hyperpartisan media created the perfect conditions for another dynamic that influenced the 2016 election, the rise of fake news.
* * *
In a December 2015 article for BuzzFeed, Joseph Bernstein argued that “the dark forces of the internet became a counterculture.” He called it “Chanterculture” after the trolls who gathered at the meme-creating, often-racist 4chan message board. Others ended up calling it the “alt-right.” This culture combined a bunch of people who loved to perpetuate hoaxes with angry Gamergaters with “free-speech” advocates like Milo Yiannopoulos with honest-to-God neo-Nazis and white supremacists. And these people loved Donald Trump.
“This year Chanterculture found its true hero, who makes it plain that what we’re seeing is a genuine movement: the current master of American resentment, Donald Trump,” Bernstein wrote. “Everywhere you look on ‘politically incorrect’ subforums and random chans, he looms.”
When you combine hyper-partisan media with a group of people who love to clown “normies,” you end up with things like Pizzagate, a patently ridiculous and widely debunked conspiracy theory that held there was a child-pedophilia ring linked to Hillary Clinton somehow. It was just the most bizarre thing in the entire world. And many of the figures in Bernstein’s story were all over it, including several who the current president has consorted with on social media.
But Pizzagate was but the most Pynchonian of all the crazy misinformation and hoaxes that spread in the run-up to the election.
BuzzFeed, deeply attuned to the flows of the social web, was all over the story through reporter Craig Silverman. His best-known analysis happened after the election, when he showed that “in the final three months of the U.S. presidential campaign, the top-performing fake election-news stories on Facebook generated more engagement than the top stories from major news outlets such as The New York Times, The Washington Post, The Huffington Post, NBC News, and others.”
But he also tracked fake news before the election, as did other outlets such as The Washington Post, including showing that Facebook’s “Trending” algorithm regularly promoted fake news. By September of 2016, even the Pope himself was talking about fake news, by which we mean actual hoaxes or lies perpetuated by a variety of actors.
The longevity of Snopes shows that hoaxes are nothing new to the internet. Already in January 2015, Robinson Meyer reported about how Facebook was “cracking down on the fake news stories that plague News Feeds everywhere.”
What made the election cycle different was that all of these changes to the information ecosystem had made it possible to develop weird businesses around fake news. Some random website posting aggregated news about the election could not drive a lot of traffic. But some random website announcing that the Pope had endorsed Donald Trump definitely could. The fake news generated a ton of engagement, which meant that it spread far and wide.
A few days before the election Silverman and fellow BuzzFeed contributor Lawrence Alexander traced 100 pro–Donald Trump sites to a town of 45,000 in Macedonia. Some teens there realized they could make money off the election, and just like that, became a node in the information network that helped Trump beat Clinton.
Whatever weird thing you imagine might happen, something weirder probably did happen. Reporters tried to keep up, but it was too strange. As Max Read put it in New York Magazine, Facebook is “like a four-dimensional object, we catch slices of it when it passes through the three-dimensional world we recognize.” No one can quite wrap their heads around what this thing has become, or all the things this thing has become.
“Not even President-Pope-Viceroy Zuckerberg himself seemed prepared for the role Facebook has played in global politics this past year,” Read wrote.
And we haven’t even gotten to the Russians.
* * *
Russia’s disinformation campaigns are well known. During his reporting for a story in The New York Times Magazine, Adrian Chen sat across the street from the headquarters of the Internet Research Agency, watching workaday Russian agents/internet trolls head inside. He heard how the place had “industrialized the art of trolling” from a former employee. “Management was obsessed with statistics—page views, number of posts, a blog’s place on LiveJournal’s traffic charts—and team leaders compelled hard work through a system of bonuses and fines,” he wrote. Of course they wanted to maximize engagement, too!
There were reports that Russian trolls were commenting on American news sites. There were many, many reports of Russia’s propaganda offensive in Ukraine. Ukrainian journalists run a website dedicated to cataloging these disinformation attempts called StopFake. It has hundreds of posts reaching back into 2014.
A Guardian reporter who looked into Russian military doctrine around information war found a handbook that described how it might work. “The deployment of information weapons, [the book] suggests, ‘acts like an invisible radiation’ upon its targets: ‘The population doesn’t even feel it is being acted upon. So the state doesn’t switch on its self-defense mechanisms,’” wrote Peter Pomerantsev.
As more details about the Russian disinformation campaign come to the surface through Facebook’s continued digging, it’s fair to say that it’s not just the state that did not switch on its self-defense mechanisms. The influence campaign just happened on Facebook without anyone noticing.
As many people have noted, the 3,000 ads that have been linked to Russia are a drop in the bucket, even if they did reach millions of people. The real game is simply that Russian operatives created pages that reached people “organically,” as the saying goes. Jonathan Albright, research director of the Tow Center for Digital Journalism at Columbia University, pulled data on the six publicly known Russia-linked Facebook pages. He found that their posts had been shared 340 million times. And those were six of 470 pages that Facebook has linked to Russian operatives. You’re probably talking billions of shares, with who knows how many views, and with what kind of specific targeting.
The Russians are good at engagement! Yet, before the U.S. election, even after Hillary Clinton and intelligence agencies fingered Russian intelligence meddling in the election, even after news reports suggested that a disinformation campaign was afoot, nothing about the actual operations on Facebook came out.
In the aftermath of these discoveries, three Facebook security researchers, Jen Weedon, William Nuland, and Alex Stamos, released a white paper called Information Operations and Facebook. “We have had to expand our security focus from traditional abusive behavior, such as account hacking, malware, spam, and financial scams, to include more subtle and insidious forms of misuse, including attempts to manipulate civic discourse and deceive people,” they wrote.
One key theme of the paper is that they were used to dealing with economic actors, who responded to costs and incentives. When it comes to Russian operatives paid to Facebook, those constraints no longer hold. “The area of information operations does provide a unique challenge,” they wrote, “in that those sponsoring such operations are often not constrained by per-unit economic realities in the same way as spammers and click fraudsters, which increases the complexity of deterrence.” They were not expecting that.
Add everything up. The chaos of a billion-person platform that competitively dominated media distribution. The known electoral efficacy of Facebook. The wild fake news and misinformation rampaging across the internet generally and Facebook specifically. The Russian info operations. All of these things were known.
And yet no one could quite put it all together: The dominant social network had altered the information and persuasion environment of the election beyond recognition while taking a very big chunk of the estimated $1.4 billion worth of digital advertising purchased during the election. There were hundreds of millions of dollars of dark ads doing their work. Fake news all over the place. Macedonian teens campaigning for Trump. Ragingly partisan media infospheres serving up only the news you wanted to hear. Who could believe anything? What room was there for policy positions when all this stuff was eating up News Feed space? Who the hell knew what was going on?
As late as August 20, 2016, the The Washington Post could say this of the campaigns:
Hillary Clinton is running arguably the most digital presidential campaign in U.S. history. Donald Trump is running one of the most analog campaigns in recent memory. The Clinton team is bent on finding more effective ways to identify supporters and ensure they cast ballots; Trump is, famously and unapologetically, sticking to a 1980s-era focus on courting attention and voters via television.
Just a week earlier, Trump’s campaign had hired Cambridge Analytica. Soon, they’d ramped up to $70 million a month in Facebook advertising spending. And the next thing you knew, Brad Parscale, Trump’s digital director, is doing the postmortem rounds talking up his win.
“These social platforms are all invented by very liberal people on the west and east coasts,” Parscale said. “And we figure out how to use it to push conservative values. I don’t think they thought that would ever happen.”
And that was part of the media’s problem, too.
* * *
Before Trump’s election, the impact of internet technology generally and Facebook specifically was seen as favoring Democrats. Even a TechCrunch critique of Rosen’s 2012 article about Facebook’s electoral power argued, “the internet inherently advantages liberals because, on average, their greater psychological embrace of disruption leads to more innovation (after all, nearly every major digital breakthrough, from online fundraising to the use of big data, was pioneered by Democrats).”
Certainly, the Obama tech team that I profiled in 2012 thought this was the case. Of course, social media would benefit the (youthful, diverse, internet-savvy) left. And the political bent of just about all Silicon Valley companies runs Democratic. For all the talk about Facebook employees embedding with the Trump campaign, the former CEO of Google, Eric Schmidt, sat with the Obama tech team on Election Day 2012.
In June 2015, The New York Times ran an article about Republicans trying to ramp up their digital campaigns that began like this: “The criticism after the 2012 presidential election was swift and harsh: Democrats were light-years ahead of Republicans when it came to digital strategy and tactics, and Republicans had serious work to do on the technology front if they ever hoped to win back the White House.”
It cited Sasha Issenberg, the most astute reporter on political technology. “The Republicans have a particular challenge,” Issenberg said, “which is, in these areas they don’t have many people with either the hard skills or the experience to go out and take on this type of work.”
University of North Carolina journalism professor Daniel Kreiss wrote a whole (good) book, Prototype Politics, showing that Democrats had an incredible personnel advantage.  “Drawing on an innovative data set of the professional careers of 629 staffers working in technology on presidential campaigns from 2004 to 2012 and data from interviews with more than 60 party and campaign staffers,” Kriess wrote, “the book details how and explains why the Democrats have invested more in technology, attracted staffers with specialized expertise to work in electoral politics, and founded an array of firms and organizations to diffuse technological innovations down ballot and across election cycles.”
Which is to say: It’s not that no journalists, internet-focused lawyers, or technologists saw Facebook’s looming electoral presence—it was undeniable—but all the evidence pointed to the structural change benefitting Democrats. And let’s just state the obvious: Most reporters and professors are probably about as liberal as your standard Silicon Valley technologist, so this conclusion fit into the comfort zone of those in the field.
By late October, the role that Facebook might be playing in the Trump campaign—and more broadly—was emerging. Joshua Green and Issenberg reported a long feature on the data operation then in motion. The Trump campaign was working to suppress “idealistic white liberals, young women, and African Americans,” and they’d be doing it with targeted, “dark” Facebook ads. These ads are only visible to the buyer, the ad recipients, and Facebook. No one who hasn’t been targeted by then can see them. How was anyone supposed to know what was going on, when the key campaign terrain was literally invisible to outside observers?
Steve Bannon was confident in the operation. “I wouldn’t have come aboard, even for Trump, if I hadn’t known they were building this massive Facebook and data engine,” Bannon told them. “Facebook is what propelled Breitbart to a massive audience. We know its power.”
Issenberg and Green called it “an odd gambit” which had “no scientific basis.” Then again, Trump’s whole campaign had seemed like an odd gambit with no scientific basis. The conventional wisdom was that Trump was going to lose and lose badly. In the days before the election, The Huffington Post’s data team had Clinton’s election probability at 98.3 percent. A member of the team, Ryan Grim, went after Nate Silver for his more conservative probability of 64.7 percent, accusing him of skewing his data for “punditry” reasons. Grim ended his post on the topic, “If you want to put your faith in the numbers, you can relax. She’s got this.”
Narrator: She did not have this.
But the point isn’t that a Republican beat a Democrat. The point is that the very roots of the electoral system—the news people see, the events they think happened, the information they digest—had been destabilized.
In the middle of the summer of the election, the former Facebook ad-targeting product manager, Antonio García Martínez, released an autobiography called Chaos Monkeys. He called his colleagues “chaos monkeys,” messing with industry after industry in their company-creating fervor. “The question for society,” he wrote, “is whether it can survive these entrepreneurial chaos monkeys intact, and at what human cost.” This is the real epitaph of the election.
The information systems that people use to process news have been rerouted through Facebook, and in the process, mostly broken and hidden from view. It wasn’t just liberal bias that kept the media from putting everything together. Much of the hundreds of millions of dollars that was spent during the election cycle came in the form of “dark ads.”
The truth is that while many reporters knew some things that were going on on Facebook, no one knew everything that was going on on Facebook, not even Facebook. And so, during the most significant shift in the technology of politics since the television, the first draft of history is filled with undecipherable whorls and empty pages. Meanwhile, the 2018 midterms loom.
Update: After publication, Adam Mosseri, head of News Feed, sent an email
describing some of the work that Facebook is doing in response to the
problems during the election. They include new software and processes
"to stop the spread of misinformation, click-bait and otherproblematic content on Facebook."
"The truth is we’ve learned things since the election, and we take our
responsibility to protect the community of people who use Facebook
seriously. As a result, we’ve launched a company-wide effort to
improve the integrity of information on our service," he wrote. "It’s
already translated into new products, new protections, and the
commitment of thousands of new people to enforce our policies and
standards... We know there is a lot more work to do, but I’ve never
seen this company more engaged on a single challenge since I joined
almost 10 years ago."


Is there a pillow as useless as the U-shaped travel neck pillow? There is not. This half-ovate, toilet-seat cover-esque object reigns as King of Travel Accessories, while failing miserably at its intended sole use. It is a scourge for reasons that I will outline in this essay and of which, by the end, I will convince you without question.
This past summer, I had occasion to travel by plane with such a pillow—memory foam in a pleasant maroon—and did so thoughtlessly, stuffing it into my carry-on as if it were my passport, or a book to ignore while watching, God willing, episodes of Sex and the City on the tiny television. When it came time to attempt sleep I, like many of my fellow passengers, dutifully placed the U-shaped pillow on my shoulders. As my neck protruded an uncomfortable distance from the seat back, I let my head fall to my left. No good. I let my head fall to my right. No good. I scrunched the pillow up, so it was more like a tiny, oddly-shaped normal pillow, but the damn thing kept bouncing back to U-shape, which, by design, has a hole in it, so that was definitely no good.
This damn pillow was no good.
It might come as a shock to you to hear someone speak the truth about U-shaped neck pillows so plainly, as this sort of pillow has been allowed to exist unchecked since it was patented in 1929. I understand and will allow you a moment to compose yourself. Have you taken it? Okay. The U-shaped neck pillow is an unsupportive abomination; a pernicious, deceitful, recklessly ubiquitous travel trinket lulling the masses not to sleep but to a zombielike restlessness for which they have been trained to blame themselves, i.e., “I can’t sleep on airplanes.” The U-shaped travel neck pillow is a useless trash pillow for nobody.
But not everyone agrees. “I bought this pillow for the long-weekend holiday trip. The memory foam is the perfect firmness, and it is so soft and comfortable,” says someone named Ivan in an Amazon review of a neck pillow similar to that which failed me on my recent flight. Okay, Ivan. Someone named Allen says, “I use this in the car. I fall asleep very easy. This keeps my neck comfortable and I don't wake up with neck pain.” Okay, Allen. Someone named Cass says, “I returned it as it had a horrible chemical smell, plus whatever was inside was a solid piece. I wanted something that had little pellets.” Well. This one seems like more of a “Cass” issue, actually.
Brad John, the cofounder of Flight 001, a popular chain of travel stores about which Martha Stewart has allegedly commented, “I love this store, it looks like an airplane,” told me the U-shaped travel pillow sells very well, even though there hasn’t been much innovation in the market. “They’re basically the same as they’ve always been. We sell the heated ones, the inflatable ones, the foam ones.” The main advancement, he said, and the top seller at the moment, is a convertible travel pillow “which you can either make into a regular pillow or a U-neck.” Very interesting that the top-selling U-shaped neck pillow is one that has the ability to function as a normal, non-U-shaped neck pillow.
Brad John himself uses a normal pillow on flights. “I just don’t find the neck pillow comfortable,” he said, “but that’s just personal preference.”
Everyone I spoke with agreed that the U-shaped neck pillow stinks, notably my friend Megan Reynolds who said, “We have one in the house but the boy cat uses it for sex.” My friend Lindsay Robertson, to whom I was referred explicitly because she regularly uses a U-shaped neck pillow on flights, proved to secretly be a member of the U-shaped-neck-pillow resistance: “I never actually use it as a neck pillow, because I can't sleep that way—I'm not sure anyone can,” she told me. Instead, she puts her neck pillow on the tray table in front of her, takes off her glasses, puts her hands in her lap, and “[lets her] face fall completely forward into the pillow, as if [she has] expired.”
What accounts for why some derive comfort from the U-shaped neck pillow—(liars)—and some do not? I asked Mary O’Connor, who is a professor of orthopedics and rehabilitation and the director of the Center for Musculoskeletal Care at Yale. “I’m unaware that there is any clinical data that shows they’re effective in reducing neck strain or neck discomfort,” she said, “However, many of us who travel have experienced falling asleep with our neck in a weird position and it bothering us thereafter. So, I think they can be helpful, but that depends on how they’re used and whether they support the neck.”
The ideal pillow, she said, would keep your head and neck in neutral alignment with your spine, so you’re not too far forward, or backward, or too far to one side or the other. “But how do you know, when you’re in the airport, that the pillow you’re going to purchase is going to give you the right support?” O’Connor asks. “The pillows are all the same. Some people have short necks, some people have long necks, and there’s no ability to look and say, ‘I need this design or this size pillow for my neck, to really work well for me.’ And that’s part of the challenge. Could one of those pillows help someone? Yes, they could. Will they help everyone? Probably not.”
I attempted to find research pointing to the uselessness or usefulness of the dreaded U-shaped neck pillow, and came up empty-handed. However I did find a study titled “The Use of Neck-Support Pillows and Postural Exercises in the Management of Chronic Neck Pain,” which was published in The Journal of Rheumatology in 2016 and dealt with the positive effects of bed-specific neck-support pillows for people with chronic neck pain. I spoke to the study’s coauthor Brian Feldman, a senior scientist and head of the Division of Rheumatology at Toronto’s Hospital for Sick Children, who made sure I understood that his study was not, actually, about the U-shaped travel pillows people use on planes. I understand. I thought he might be able to offer some insight, anyway.
In, he stressed, his own opinion of U-shaped travel pillows, he said, “I can’t stand them. I never use them. They’re not built strongly enough or firm enough. There are all kinds of new gizmos that people have been developing for pillows for sleep in transportation, and they tend to be more like straps that hold your head in place, or boxlike structures that you can sit forward and place your head in, or neck collars, which give you much more support around your neck. Those kinds of things are probably all much better than the typical U-shaped pillow.”
Keeping your neck in a nice physiological position while sleeping is a wonderful thing to do, he said, but the issue with U-shaped pillows is that they aren’t built to be firm enough or high enough to help most people, plus they don’t circle around the neck properly. “They just don’t do the job they’re supposed to do,” Feldman says. In order to work, he thinks they’d have to look more like the kind of rigid neck collar you see on someone who has recently injured their neck, one “that presses up into the head and keeps the chin up and supported so the head doesn’t flop over in any way once you’ve fallen asleep” while sitting up.
Also, don’t they look like the the first-ever stone pillow used by Mesopotamians in 7,000 BC? Seems like we should not still be using a pillow that looks like the first-ever stone pillow used by Mesopotamians in 7,000 BC, but that’s just my opinion.
If I could leave you with one piece of advice, it would be: Take a hard look at whether or not your U-shaped travel pillow is worth toting on your next flight. Are you stuffing it into your carry-on out of usefulness, or out of habit? Is it taking up precious storage space because it will help you sleep, or because you thought you should buy it even though there you’ve encountered no evidence, either personal or scientific, to suggest that this thought is correct? Are you wrong, or do you agree with me? Ask yourself these questions, and then leave the U-shaped pillow behind.
(Unless you’re a boy cat and you’d like to use it for sex.)


ADELPHI, Maryland—In a quiet voice and in her native Spanish, the woman explained to Dr. Shantanu Nundy that she had been feeling dizzy whenever she stood up.
She cleaned houses and worked in a store. There was a lot going on at home—and now this. She choked up describing it all.
Nundy’s clinic, called Mary’s Center, is a primary-care practice, and hers was a classic primary-care problem: common, yet strange; vague, yet worrisome—troubling enough to send the woman to the emergency room the day before, sticking her with a $200 bill. Still, the dizzy spells were not definitive enough for the ER to do anything about.
Nundy suspected she had something deep inside her ear that was throwing off her balance. To make sure, he had her perform something called the Dix-Hallpike test: From a sitting position, he asked her to fall back onto the exam table, then toss her head to one side. That would help determine whether the source of the dizziness was a problem in the inner ear.
It didn’t really work. When she sat back up, she felt fine.
Nundy stepped into the hallway and wrote up her case in the clinic’s electronic medical record. But he still wanted to be sure the cause of the dizziness wasn’t a small stroke or something more serious.
He opened a new tab on his computer and went to a new website that he helps design and run: the Human Diagnosis Project, or Human Dx. The project allows primary-care doctors to ask for assistance on difficult cases from an online network of physicians all over the world.
He clicked “get help on a case” and, on a checklist-style page, input that she was “43f”—a 43-year-old female—with episodic dizziness for the past two months. He then submitted the case to a doctor at another Mary’s Center clinic, as well as to Human Dx’s entire database of nearly 7,000 doctors.
Trained in internal medicine, Nundy now leads the nonprofit arm of Human Dx, but he spends Fridays at the clinic as its only provider for adults. (Other doctors and nurses see children there the other days of the week.)
Mary’s Center is a safety-net clinic, so its patients pay according to their income. At just after 8:30, the waiting room was bustling. The staff issued each patient a number and called them back in English and Spanish—“Twenty-six ... veintiseis!”
Nundy says about 80 percent of his patients are uninsured, in some cases because of their immigration status. Even for those with insurance, a specialist might be out of reach because of high deductibles and co-pays or long wait times.
“For you and me, someone who has insurance, the standard of care is that you see an expert who lives and breathes ... your diagnosis,” Nundy says. But for the 28 million uninsured Americans, seeing, say, a dermatologist or a neurologist usually means getting on long waiting lists for a doctor who is willing to volunteer his or her time.
Human Dx might help doctors confirm their suspected diagnoses or think of things to rule out. At Mary’s Center, one man came in complaining of headaches and nausea, and the Human Dx physicians suggested a blood test called an ESR. Another time, Nundy used it to confirm a suspected case of rheumatoid arthritis before putting a low-income patient on a heavy-duty course of medications.
Experienced doctors use Human Dx for their most difficult cases, and newer providers use it to hone their skills. Johns Hopkins Hospital and other teaching hospitals are now using it to train medical residents. Georgia Lewis, a nurse practitioner who works with Nundy, used Human Dx when, two months into her stint at Mary’s Center, all the other providers went on vacation. Rashes can be confounding, so she’ll upload them to Human Dx along with a photo.
The contributors to the project are vetted based on how accurately they’ve solved past cases. Human Dx uses machine learning, which means that eventually the algorithms powering the diagnosis suggestions will become “smarter” based on the input of the doctors using it. The hope is that, over time, Human Dx can help reduce misdiagnoses, which according to studies happen up to 20 percent of the time.
Human Dx hopes to soon roll out to all 1,300 safety-net clinics in the United States. Ron Yee, the chief medical officer of the National Association of Community Health Centers, is helping clinics like Mary’s Center start using the platform. “We thought we can really help our communities because we have challenges getting specialty care,” he said.
Yee and his colleagues are still figuring out how to fit Human Dx into so many primary-care doctors’ workflows. They’re also puzzling through that eternal health-care question: how to get paid for it. “Does insurance accept this?” Yee said. “I don’t know what it looks like.”
Nundy acknowledges that Human Dx adds time to a doctor’s day. But he says researching difficult cases already adds time, as does reading reference materials or calling his med-school friends for their advice. He hopes that as the project progresses, it could count toward doctors’ continuing medical education, licensing requirements, or student loans. Eventually, he hopes to get all the area’s specialists who treat the uninsured on Human Dx, so they can offer their counsel digitally and save their charity care for those who really need to be seen in person.
It usually takes about six hours to get a response through Human Dx, but a little over an hour after Nundy had seen the woman with the dizzy spells, a few responses had already trickled in. The relative likelihood of the doctors’ guesses were represented by little green bars, like a Wi-Fi signal. The most common suggested diagnosis was dehydration, followed by stress, a ministroke, or Ménière’s disease, a disorder of the inner ear. “Now when I see a person with dizziness,” Nundy said, “I’ll think about Ménière’s disease.”
Most likely, the woman was just stressed and tired. But for Nundy and other primary-care doctors using Human Dx, it’s worth carefully considering “the consequences of being wrong. If this was my mom or my sister ... that’s what we would want,” he said. “That’s what patients deserve.”


The morning of January 27, 1967, Gus Grissom and his Apollo 1 crew put on their flight suits. Foil-clad, with breathing masks, they looked like a mid-century vision of the future brought to you by Reynolds Wrap. The crew of three were to perform a launch test that day, to see what it would be like when they rode a metal cone to space.*
Grissom had been to space before during the Gemini program. That day’s practice wasn’t going great, not like one would hope an actual launch would go. First, the astronauts smelled something rotten in their oxygen. That delayed them by more than an hour. Then, their communications system began to fritz out. Of this, Grissom famously groused, “How are we going to get to the moon if we can't talk between two or three buildings?”
Later, though—into that same microphone and over those same lines—came a single word: “fire.”
It was true: Damaged wires had likely ignited a spark, which fed on the all-oxygen air, growing with its consumption of space-age material—nylon, foam.
The crew tried to escape the capsule. But the hatch wouldn’t open. All three astronauts suffocated inside the vessel that was supposed to carry them—and with them, us—into the future.**
The agency’s two other fatal accidents occurred during the same January week as Apollo’s: Challenger 19 years later, Columbia 17 years after that.*** And just three years ago, the private-spaceflight industry endured its first human loss, when Virgin Galactic’s SpaceShipTwo lost its copilot.****
After each fatal incident, the nation has responded with shock and grief. These explorers—our explorers, Earth’s explorers—paid for that exploration with their lives. Questions arose. Some—How did this happen?—are left to inspectors and investigators. But others—How big a cost are humans willing to bear to leave the planet?—lie in the public domain. The answers seem to have changed throughout the decades, as space travel seemed to evolve from something novel to something routine.
Today, industry and government are both upshifting gears, back into novelty, which means the public’s attitudes toward space travel and its inevitable accidents may return to what they were in NASA’s early, more adventurous days. After decades in a stable and predictable orbit, American spaceflight will return to new vehicles and, maybe, new destinations. The country is deciding which far-off world to point ships toward next, with the moon and Mars the most likely candidates. Private companies are doing the same, and preparing to take high rollers on suborbital romps. And with that leap into the unknown, Americans may become more tolerant of the loss of astronaut life. If they don’t, the government and private industry might not be able to make the leap at all.
We all know people probably will die on these new missions, especially if they become commonplace, as many hope. What no one knows is how we will all respond to those losses.
* * *
When Grissom and his compatriots signed on to the astronaut corps, times were different. They were different: cowboy test pilots—military men, mostly, with that rightest of stuff. Space, and the flight paths to and through it, was basically uncharted. Rockets blew up—a lot—listing sideways, spinning tail over teakettle, exploding heads in the ground like ostriches.
And the astronauts themselves were, for the most part, inured to their mortality. In The Right Stuff, Tom Wolfe repeatedly references the euphemism the early astronauts used to describe fatal crashes: The fliers had “screwed the pooch.” It’s gallows humor: The pilots and astronauts couldn’t completely control their survival—but they could at once face death, distance themselves from it, and use tone to strip it of power.
The public perceived these guys (and they were all guys) as all-American swaggerers, laying their lives on the line for the primacy of the country.
“It was a battle in the Cold War,” says Rand Simberg, author of Safe Is Not An Option: Overcoming the Futile Obsession With Getting Everyone Back Alive That Is Killing Our Expansion Into Space.
The nation, of course, mourned the Apollo 1 crew’s loss—especially given its gruesome nature. But the public and the government were perhaps not surprised, or philosophically disturbed, that people had to die if Americans were to get to the moon in a decade. In an article called “Space Travel: Risk, Ethics, and Governance in Commercial Human Spaceflight,” space strategist Sara Langston looks to other fields to understand attitudes and regulations about space exploration. “In the Encyclopedia of Public Health, [Daniel] Krewski defines acceptable risk as the likelihood of an event whose probability of occurrence is small, whose consequences are so slight, or whose benefits (perceived or real) are so great that individuals or groups in society are willing to take or be subjected to the risk that the event might occur,” she writes. The risk of space accidents, by inference, are subject to the same kind of cost-benefit analysis.
After Apollo, though, came the staid shuttle program. And with it, the tenor of spaceflight changed. The Cold War ended in the ’90s. The spacecraft was called a shuttle. You know, like the bus that takes you to the airport. The Americans had already conquered spaceflight—we got to the moon, which was very hard and very far away and involved orbiting other bodies and sometimes landing. Spinning ellipses around our own planet in a sturdy vehicle? Easy.
The shuttle program left Americans—and perhaps the world—with the false sense that the space-pioneer days were over.
* * *
In technical terms, as the shuttle program developed, people began to think of its flights as operational rather than experimental. In experimental mode, engineers are still figuring the details out, fingering the edges of a craft’s envelope and seeing how hard and fast they can press before they get a cut. In operational mode, though, engineers are supposed to know most everything—the ups, downs, ins, and outs of performance given sundry contingencies.
While the shuttle mostly functioned well, that performance was never actually a given. The vehicle remained, to its last days, experimental, a status reflected in its success/failure rate. “I think people that know our industry kind of understand the edge we're on here, because these systems are tremendously complex,” says David Bearden, general manager of the NASA and civil-space division at the Aerospace Corporation. “If you look back, empirically or historically, at any launch system, about the best you're ever going to get is 1 in 200. On an airline it is a one-in-a-million chance. People who know the industry and know the way those systems operate understand that, I think.”
* * *
I was only six months old when space-shuttle mission STS-51-L sat on the launchpad on January 28, 1986. Aboard were six astronauts and Christa McAuliffe, a teacher from Concord, New Hampshire. The shuttle lifted off on the cold Florida morning. But then, nine miles above Earth’s surface, that seemingly reliable spacecraft broke apart, undone by the uncharacteristic chill at Cape Canaveral that day.
As a Miami Herald Tropic investigation later detailed, the astronauts didn’t die right away: The crew vehicle stayed intact, and continued to go up, before tipping back toward Earth, traveling 12 miles of sky before crashing into the cold ocean water—hard as the cement on the launchpad. The astronauts, the article said, were very likely alive until the very end and might have even been conscious.
Coverage from the days after the tragedy expresses, of course, sadness. “The Shuttle Explosion; Suddenly, Flash of Fear Dashes Watchers’ Hopes,” read a New York Times headline.
“What Went Wrong? Shuttle Loss in Ball of Fire Stuns Nation,” went one from the local Orlando Sentinel.
Both papers, though, declared that the show must go on: “Reflecting on Loss: Welling of Tears, a Desire to Press On,” said the Times.
“Three shuttle veterans lose peers but not faith in program,” said the Sentinel.
The losses, while tragic and (as the Rogers Commission Report would later reveal) avoidable, shouldn’t squash the program. Sacrifices, after all, must be made, for a new program whose utility the nation was still proving.
* * *
I was 17 when NASA lost the space shuttle Columbia in 2003. I’d grown up in Central Florida, not far from Kennedy Space Center. I’d seen almost every shuttle launch in person—with my classmates outside on the sidewalks of my schools, with my sisters in the backyard, and very occasionally from the far side of Cape Canaveral, with my parents. The sonic booms from landings sometimes set off the burglar alarms that hung from our door handles.
But in 2003, I had things to do that didn’t include watching out for spacecraft. I was on my way to a band (marching band, not the cool kind of band) rehearsal session when I heard about Columbia. News of the accident came slow and halting over whatever alt-rock station I was blasting from my Grand Am.
Later, investigators would reveal that a piece of foam had fallen from the shuttle’s wing during launch, leaving a hole that let gas come through when the shuttle re-entered the atmosphere. The shuttle was going Mach 18, 37 miles above the ground, when it broke apart, shaking debris across thousands of square miles.
I remember sitting in my car in a church parking lot, thinking how it couldn’t be real. I remember thinking the radio host didn’t sound like he thought it was real. We’d probably both watched shuttles launch and land safely for much of our lives. To us, the whole program seemed routine—operational. It had moved into that realm of seeming safety, and risks seemed not just less likely but also less justified. And while we always knew this could happen, we never thought it would.
The Columbia disaster represented, unlike the Challenger explosion before it, the start of the finish for the shuttle program. NASA announced its end the very next year. Two strikes, shuttle’s out.
* * *
Sometimes, you hear the phrase “Failure is not an option” associated with NASA. But it was never a slogan at the agency; no one in mission control, that we know of, ever said it, and no manager passed it down. It was just a line in the movie Apollo 13. Failure is always an option: It has to be.
Of course, no one wants a rocket to blow up or a crew capsule to fall to Earth. But to undertake space travel, the undertakers have to acknowledge those possibilities and mitigate the risks. As NASA administrator William Gerstenmaier said in his paper “Staying Hungry: the Interminable Management of Risk in Human Spaceflight,” “We never simply accept it, but NASA, our stakeholders, and the public must acknowledge the risk as we move forward.”
The public, to some extent, also knows that’s the equation. But a 1/200 mission-failure rate means that one doesn’t happen very often, which means that every one comes as a shock.
Still, astronauts’ deaths don’t always cause communal moral outrage. “A particularly risky venture can become socially acceptable in correlation with the value placed on it,” Langston wrote in her risk paper. If people value a space-exploration program, in other words, they’re okay with others risking their lives to push it forward.
Simberg contends that wasn’t fully true with the shuttle, as compared to Apollo—an inspiring and aspiring mission with political importance. “The reason we were so upset about losing these seven astronauts was that what they were doing was kind of trivial,” he says of Columbia.
We don’t always demand, though, that people be doing something Valuable that Benefits Humanity to let them risk their lives (and there were lots of ways the shuttle and, in particular, its trips to the always-peaceful International Space Station did benefit humanity). About 1 percent of people who try to climb Mount Everest historically die in the attempt, for example. And this despite the fact that topping Everest is not exactly exploration, with its $40,000 price tag and Sherpa guides and commercial expeditions. And it’s been done before.
Shuttle astronauts, meanwhile, have a 1.5 percent chance of dying on a given trip to space. And trying to keep them at least as safe as that—or safer—means the agency can’t go as boldly as private industry can.
* * *
The major players in the crewed-commercial space are SpaceX, which wants to eventually build a martian colony; Blue Origin, whose Jeff Bezos envisions an outer space full of industry and long-term habitation; and Virgin Galactic, which wants to democratize access to space closer to home, with a carrier plane that rides up to 50,000 feet, then travels up on its own and glides back down at the behest and guidance of its pilots.
On October 31, 2014, Virgin Galactic paid a human price for that system. During that October test flight, copilot Michael Alsbury unlocked SpaceShip’s feathering system, which changes the shape of the plane to aid reentry, early. The wind then pushed the system open, and the vehicle destabilized. While pilot Peter Siebold parachuted to safety, Alsbury remained with the ship, and died on impact.
After the accident, Virgin allowed its already-booked customers to back out, but just 3 percent did.
SpaceX, meanwhile, has had its own explosive setbacks, and yet the company and leader Elon Musk still remain the industry’s darlings. SpaceX blew up an uncrewed Falcon 9 rocket in September 2016. In June of the year before, the company lost another Falcon that was supposed to resupply the Space Station. In test launches and landings of its reusable rockets, SpaceX has also had a vessel tip over into the ocean and explode (January 2016); crash into a ship (January 2015); and land “too hard for survival” (April 2015).
Based on this, the NewSpace industry seems to exist firmly in the experimental phase. But, more than that, the public seems to know—and accept—that status. “You understand that you're in a test-pilot phase,” says Bearden. “The public can process that and say, ‘That's not me. By the time I fly, they're going to have worked it out.’”
The public permits mistakes for the private space companies—because they produce rockets and results on non-geologic timescales, and lay out visions like “you can go to space” and “you can have a house on Mars.”
The FAA, which regulates commercial space activity via the Human Spaceflight Requirements for Crew and Spaceflight Participants, is also relatively forgiving. “Congress mandated these regulations in the Commercial Space-Launch Amendments Act of 2004,” says the FAA’s description of this law. “Recognizing that this is a fledgling industry, the law required a phased approach in regulating commercial human spaceflight, with regulatory standards evolving as the industry matures,” attempting not to crush innovation with regulation. Flight providers do, though, have to get extremely informed consent from would-be astronauts.
NASA recognizes the value in this model, and in its different posture toward risk. The agency has teamed up with such space companies—letting them, among other things, shuttle cargo and crew to low Earth orbit. NASA no longer has to be all things to all people and missions, and can let those experimental upstarts do a little legwork.
The agency may also see, though, that the public perceives New Space cadets as pioneers—a lens through which they don’t see NASA like they used to—and so forgive their mistakes, tallying them as the cost of innovation, rather than a cost not worth bearing. And perhaps the agency hopes the same thing for itself, as it turned those duties over to private companies so that it can focus on its own bold goals, its own new risky, experimental phase of operations with both the costs and the benefits that come with that.
* This article originally stated that there were four crew members aboard Apollo 1.
** This article originally misstated the cause of death for the Apollo 1 crew.
*** This article originally implied that the Columbia disaster occurred 36 years after the Challenger explosion.
**** This article originally stated that the Virgin Galactic crash resulted in the death of the craft’s pilot. We regret the errors.


You kids don’t understand. You could never understand.
You walk around in habitats of text, pop-up cathedrals of social language whose cornerstone is the rectangle in your pocket. The words and the alert sounds swirl around you and you know how to read them and hear them because our culture—that we made—taught you how. We were the first generation to spend two hours typing at our closest friends instead of finishing our homework, parsing and analyzing and worrying over “u were so funny in class today” or “nah lol youre pretty cool.”
That thing you know how to do, that cerebellum-wracking attentiveness to every character of the text message and what it might mean—we invented that. But when we invented it, we didn’t have text messages, we didn’t have Snapchat, we didn’t have group chats or Instagram DMs or school-provided Gmail accounts. We had AIM. We had AOL Instant Messenger.
“How did AIM work?” you ask. It was like Gchat or iMessage, but you could only do it from a desktop computer. (Since we didn’t have smartphones back then, its desktop-delimited-ness was self-explanatory.) You could set lengthy status messages with animated icons in them. And iconic alert noises played at certain actions: the door-opening squeak when someone logged on, the door-closing click when they logged off, the boodleoop for every new message.
“Those status messages,” you say. “What were they like?” As thunderous piano-accompanied art songs were to the sad young men of Romantic Germany, so were status messages to us. They might have a succinct description of our emotional state. Often they consisted of the quotation of vitally important song lyrics: from The Postal Service, from Dashboard Confessional, from blink-182, from Green Day, from The Beatles (only after Across the Universe came out), from RENT and Spring Awakening and The Last Five Years. (We didn’t have Hamilton back then—I shudder to imagine what 2008 would’ve been like if we had.) From Brand New or Taking Back Sunday if you were pissed at your crush.
And then there were, sometimes concurrently with the song lyrics, the pained, cryptic, and egocentric recountings of the emotional trials of the day. Our parents wronged us. Our best friend wronged us. Our chemistry teacher wronged us. But we never actually said that outright; instead, we hinted at their sins and petty slights through suggestion and understatement. That’s right: AIM was so fertile and life-giving that we invented subtweeting to use it. (Gen X-ers: Don’t @ me about how you all proto-subtweeted on CompuServe or Usenet or ENIAC or whatever.)
But status messages were just the golden filigree of the gorgeous AIM tapestry. AIM was everything to us. I really mean that: As 9/11-jittered American parents were restricting access to the places where we could meet in public—the sociologist danah boyd writes about this in her book, It’s Complicated—we had to turn to AIM. So AIM became the original public-private space. AIM was the mall. AIM was the study carrel. AIM was our best friend’s finished basement. AIM was the side of the library where everyone smoked. AIM was the club (see, Hobbes, Calvin and) and da club (see Cent, Fifty). AIM was the original dark social.
We didn’t ask for someone’s number, at least not then—an errant month of texting in 2005 could still cost $45, an exorbitant figure to the teenage mind—so we asked for their AIM. Or we got their AIM from someone else. (We usually had to tread carefully around the ask.) And over a couple months, we assembled buddy lists of our friends and teammates and crushes and classmates. Their away lights twinkled in a constellation of teenage social possibility.
“What did you even talk about?” All the same stuff you text about now. We asked if they had copied down the math problem sets. We asked how far you were supposed to read tonight in Gatsby. (Then we didn’t do the reading.) We complained about how Mr. O’Brien was mean to freshmen. We talked about the high-school musical, about the ending of Donnie Darko, about God and religion. We used lol to stand in not only for laughter or humor, but for any inarticulable mass of any emotion at all. We talked about who had sex with who. We talked a lot about love. We felt the world shiver and transform when our crush logged on and—boodleoop—started messaging us.
We made our first attempts, on AIM, of transfiguring our mysterious and unpredictable thoughts into lively and personable textual performances. We were witty and dramatic. We invented our online selves—we invented ourselves.
We got bored. Myspace and Xanga helped us set up temporary and ramshackle museums of our tastes. Then Facebook came along, with all the frisson of “only college students use it,” and we drifted there. Its pseudo-maturity and time-delayed interactions allured us. Our AIM status messages went to Facebook instead: It was where we mourned the end of the field-hockey season or the final showing of the winter musical. We posted photos of each other on Facebook and liked them and commented on them—but sometimes still chatted about them on AIM. We asked homework questions via each other’s walls. We wrote subtweety openings as our Facebook status, hoping our crush would comment there instead. Eventually Facebook had its own chat product too, and it made more sense to use that, or Gchat, or to just text.
And then we graduated from high school, and some of us moved far away, and as mobile semi-adults spread across campus, AIM didn’t make logistical sense anymore. Our usernames, laden with Harry Potter and Hot Topic references, were kind of embarrassing anyway. We got bored with the sweet and secret internet of our youth, and we began the hard adult work of building our personal brands, watching prestige television, and purchasing different forms of financial insurance (renter’s, medical, dental, life).
But for years AIM was still there—simply, silently, warmly beckoning for anyone to return. You didn’t hear it. You texted instead, or made Instagram stories. We texted instead, too. It’s how we navigate our lives now.
So now, on December 15, AIM will leave us forever. “AIM is signing off for the last time,” said the product team in a tweet on Friday. “Thanks to our buddies for making chat history with us!”
AIM showed us how to live online, for good and for ill. We all live our whole lives in text chains and group threads now. We plan every hangout, we send every news article, we proclaim every relationship in the river of text it taught us to sail. Honestly, that river has been a little scary lately. Instant messaging, once a special thrill, now sets the texture of our common life. But AIM taught us how to live online first. So AIM, my old buddy, don’t feel bad if you see us shedding a tear. We know what you have to do. For we’ll see you waving from such great heights—
“Come down now,” we’ll say.
But everything looks perfect from far away.
“Come down now,” but you’ll stay.


“Death spiral!” President Trump tweeted in May, about the Affordable Care Act. It had been a common accusation of Republicans even earlier. Media, pundits, and think tanks all weighed in on whether or not the label applies to Obamacare and its health-care exchanges.
Today, death spiral means “a marketplace spinning out of control,” as FiveThirtyEight’s Anna Maria Barry-Jester puts it. It’s an accusation that demands an urgent response. In a death spiral, destruction is so near and so inevitable that any attempt to avoid it becomes valid. By evoking the dwindling seconds before a plane crash, every other option looks better by comparison.
Yet death spirals have another story to tell. Before the death spiral was a figure of speech, it was a physical problem aviators needed to solve: how to keep from crashing when they flew through clouds or fog. How they solved real death spirals in the air might help explain how to resist the narrowed choices metaphorical death spirals impose.
* * *
In the early decades of flight, aviators were bedeviled by bad weather. Those who encountered poor visibility mid-flight told harrowing tales of disorientation and confusion. Surrounded on all sides by milk-white fog or hazy darkness, pilots entered a world where nothing behaved as it should. When they observed the plane slipping into a gentle descent, they corrected to gain altitude, only to find the plane diving downward faster. Or, when they were certain the plane was flying level, the turn indicator would register a turn to the right. What the instrument registered as level, meanwhile, felt like a turn to the left.
Under these conditions, bailing out often became the best option. Those who didn’t often joined their plane as it crashed into the ground.
Lost in the clouds, these pilots had fallen prey to a form of sensory disorientation known as a death spiral, or, more commonly, a graveyard spiral. The term describes an almost instinctive set of maneuvers pilots undertake when they lose sight of the horizon. The graveyard spiral begins when a plane flying in these conditions enters a gentle turn. As it turns, the plane will begin to descend, picking up speed.
Death spirals occur because the pilot feels the descent but not the turn. That has to do with the way the human body relies both on the visual and vestibular systems to perceive its orientation in space. As fluid moves through the small canals in the inner ear, the brain registers the body’s shifts in position. The fluid moves when the head turns, creating the sensation that the vessel under control is doing the turning. In mid-flight, though, the fluid can settle in place. If this happens, a turn can feel like level flight. In this situation, a pilot who follows the instruments and levels the plane’s wings feels, with absolute certainty, that the craft is turning in the opposite direction.
A pilot who recorrects to what feels level in his or her body simply reinitiates the spiral dive. Likewise, pulling back on the yoke to gain altitude without leveling the wings only tightens the plane’s downward spiral. Without a clear view of the horizon to correct against, the pilot can become so disoriented that a total loss of control results, ending in a crash. An Air Safety Institute scare-tactic training video, “178 Seconds to Live,” follows a pilot through the disorientation of a classic graveyard spiral.
Once it became clear that aviators were becoming disoriented in the clouds, they set themselves to the task of figuring out how to avoid it. This was the birth of what is known as “instrument flight.” Planes already carried basic instruments, such as turn and bank indicators, but these were primarily seen as navigational devices—implements that helped pilots reach a destination rather than keep the plane in the air. To tame the death spiral, these devices had to become part of how aviators kept control of the plane.
In 1932, William Ocker and Carl Crane published Blind Flight in Theory and Practice, a detailed guide to flying by instruments through darkness and fog. Ocker and Crane’s method relied on giving the pilot a visual reference against which to double-check the body’s fallible sensations. A turn and bank indicator shows the wings’ departure from level flight, and an artificial horizon visually represents the plane’s relation to the ground.
But designing and implementing instruments was the easy part. It was harder to teach pilots to believe what their instruments reported over (and against) the persuasive sensations they felt in their bodies. Here Ocker and Crane ran up against aviators’ long-standing belief that they controlled the plane, at least in part, through their superior “air sense”—their body’s special ability to maintain its equilibrium in flight. The idea that skillful flight depended on the body’s perception of its own weight and relative position, sometimes called “deep sensibility” or kinesthesia, was a truism among pilots. (Aviators referred to this skill as their ability to “fly by the seat of the pants,” a phrase that connoted, perhaps falsely, skill more than luck.)
Ocker and Crane started demonstrating the limits of the pilot’s body, spinning skeptical pilots in chairs until they were dizzy, or showing them the curves their bodies traced when they tried to walk a straight line without the aid of vision. They even blindfolded homing pigeons and threw them out of a plane to demonstrate that even nature’s best fliers would lose all sense of orientation without sight. (The pigeons spiraled helplessly, Ocker and Crane reported, until they finally spread their wings parachute-style and floated, unharmed, to the ground.) This “inherent spiral tendency” lived in everyone, Ocker and Crane argued, and it would show itself if not restrained by a competing vision of the horizon. Hence the aviator’s need for instruments: They gave back the horizon clouds and fog had obscured.
A wary stance toward bodily perceptions would become a guiding principle for instrument flight. Early U.S. military training documents instructed pilots that their inner ears provided information that was “not at all reliable,” for instance. Ocker and Crane gave pilots a set of practical lessons in how to reference them to keep control of the plane. As pilots learned to trust their instruments, flight through clouds and fog became commonplace, safe, and mundane. The death spiral, meanwhile, was replaced by a simpler imperative: Check your instruments, and believe them.
* * *
Pilots still talk about death spirals, especially to warn amateurs of the dangers of flying into fog and haze. More commonly, though, the term claims that a social organization is on the brink of collapse: small towns, department stores, utility markets, liberal-arts colleges, Apple before Steve Jobs’s return as CEO, the island of Puerto Rico (pre- and post-Hurricane Maria), even the State Department under Rex Tillerson.
The use that is most resonant today—the death spiral as what ails insurance markets—traces back to a 1998 article by two economists describing an “adverse-selection death spiral,” in which insurance plans become financially unsustainable when there are too few healthy, low-cost subscribers enrolled. Economists and businesspeople have played a leading role in the death spiral’s transition to metaphor, converting the individual danger pilots faced into a shorthand for market forces endowed with the inevitability of natural law. They draw on the death spiral’s sense of urgency, meanwhile, to heighten the stakes of corporate failures. The term demands drastic action while rationalizing choices that might follow that imperative.
But its metaphorical life abandons the work that made death spirals in aviation avoidable—the steady, mundane habit of cross-referencing one’s fallible perceptions to the reality of the horizon. As a metaphor, the death spiral is all problem and no solution; it preserves the original’s diagnosis but abandons its cure.
This absence seems particularly lamentable in current discussions of the ACA, given how intensely felt most people’s policy positions seem to be. The death spiral works as a metaphor in this case because it fits neatly into a larger narrative of scarcity. That young healthy people are not buying health insurance on the exchanges seems a rational choice, given their precarious financial state. When the majority of Americans worry they will be unable to maintain their standard of living, the idea that benefits like Obamacare are about to collapse under their own weight makes intuitive sense similarly to how aviators’ bodies rationalize their false perceptions in the air.
The death spiral’s lesson is that logic that seems intuitive needs to be calibrated against measured reality. The perception that the ACA is in a death spiral, for example, requires calibration against the realities of spending decisions and wealth distribution. America pays more than any other industrialized country for its health care, which nevertheless does less to extend its citizens’ lives. About half of the nation’s discretionary spending goes to the military. Great wealth is concentrating in the hands of a diminishing few.
Against this horizon, the urgency and narrowness implied by the ACA’s supposed death spiral looks less insistent. If the aviators win more options when checking their bodily impulses against the horizon, so too the citizenry might find more room to maneuver by expanding its view of possible maneuvers.
I don’t mean to make this process sound easy; it’s not. There’s a reason that the aviator and writer Wolfgang Langewiesche, writing in Harper’s in 1941, described instrument flight as “the castigation of the flesh translated into aeronautical terms.” Orienting their bodies to a horizon that was obscured required pilots to resist the sensations that keep humans upright at every moment. Likewise, resisting the death spiral as metaphor requires pushing back against the normal and the everyday.
Metaphorical death spirals lure people toward forced (and false) choices—choices that endorse actions in concord with fear. It’s not that it feels good to believe disaster is imminent; it’s that it feels real—the perceptions bodies and minds feel intuitively ground people’s thoughts and actions. Perhaps this is why the death spiral is such a powerful metaphor today, when catastrophe feels like the background to everyday life. But there’s also hope in the death spiral: Crashes aren’t inevitable—so long as there’s instrumentation to help find a horizon.
This article appears courtesy of Object Lessons.


The surveillance state has a Blogspot.
At least that’s what it looks like in the opening credits of Gossip Girl, when the titular website flashes on the screen, and Kristen Bell, the narrating omniscient voice of Gossip Girl herself, intones: “Gossip Girl here: your one and only source into the scandalous lives of Manhattan’s elite.”
The site that obsessively monitors, and regularly ruins, the characters’ lives looks like it was made on the classic Blogger platform: There’s a header, a series of bordered posts that run straight down the middle, and a left rail full of links. It’s the most iconic of the many ways that the show, which turns 10 years old this year, is a perfect time capsule of the technology of its time. And while it now feels dated in some respects, it was remarkably prescient about the compulsive relationship people would end up having with their devices.
Gossip Girl was a show about ultra-privileged teens and their infinitely morphing romantic entanglements and high-society social battles. But it was also a show about lives lived in the spotlight of the internet, in the liminal era just before most of America dove headfirst into palm-sized screens.
Technology was integral to Gossip Girl’s premise and plots. Without cameraphone-wielding looky-loos invading the privacy of Serena, Blair, Dan, Chuck, and Nate, there would be no show. So many plotlines hinged on secrets, but it usually only took a couple episodes before Gossip Girl ensured those secrets were revealed, and the writers had to find something new for the characters to hide.
The show’s creators treated technology with the detailed attention befitting its central role, to the point that “we would have companies like Verizon come in and show us prototypes of new models coming up in the future,” Joshua Safran, the show’s executive producer, told Vulture. “We would come up with plotlines based on what we knew would be tech coming out in the future.” Nothing but the newest and shiniest for Manhattan’s elite.
One interesting thing about the Gossip Girl era was the sheer variety of phones available. Before we all coalesced around a touch-screen rectangle as the best possible mobile-phone design, there were BlackBerries with their full keyboards; the Motorola Razr, a super-skinny flip phone; the LG Chocolate, which came in fun colors and slid open to reveal its number pad. All of these appear in the show’s first season, a reflection of the technological diversity of the time.
If the show were filmed today, all the Constance Billard/St. Jude’s students would have iPhones. (Serena’s would be gold and Blair’s would be rose gold. I’m certain of this.)
This was a show in which text messages were often major plot points, but this was before anyone had thought to depict texts as free-floating typography in a shot (an idea widely credited to Sherlock), which meant there were a lot of close-ups of cellphone screens.
The attention the show paid to technology was both incredible production design and a great opportunity for product placement. Watching it today, it feels extremely evocative of 2007, in a good way, but also sometimes in a hilariously dated way.
There is a plot point mid-first season that revolves around a videotape. A literal tape, from a camcorder:
Blair studies for the SATs with this handheld Princeton Review device:
And at one point Serena bonds with her boyfriend Dan’s best friend, Vanessa, over a round of Guitar Hero. Is there anything more 2007 than Guitar Hero?
Sure, one can certainly get one’s jollies by watching Blake Lively pretend to be totally crushing it playing “Free Bird” on that plastic guitar. (Her fingers barely move! As someone who devoted way too much time to getting good at Guitar Hero, I’m offended by this shallow performance.) But where these Upper East Siders were ahead of the curve was in the tightness of the grip technology had on their lives.
The teens of Gossip Girl had codependent, toxic relationships with their phones in a way that would be intimately familiar to many people now, even those who aren’t constantly living in fear of their personal lives being blogged about. Though it was possible in the late ’00s to subscribe to text-message updates from RSS feeds, or SMS alerts from news organizations, for the most part cellphones were still thought to be just for calling and texting people you knew. But Gossip Girl’s characters were using their phones to monitor the news. (By “news” I mean rumors about their very small social circle, but still.) It was unclear whether they’d signed up to get notifications from the Gossip Girl blog, or whether the anonymous blogger just had everyone’s numbers to send “e-blasts” to. These e-blasts were also inconsistent in form—sometimes they appeared as emails:
And sometimes as texts:
It was not uncommon for all the characters to be in a room together, probably at a lavish penthouse party, and for all their phones to go off simultaneously. Then they’d all check them at once, creating a tableau that was strikingly similar to a modern group of people reacting to a breaking-news notification:
If I encountered this in real life today I’d be more likely to expect that North Korea had launched a missile than that my friend’s ex had been spotted with another woman.
Several of the characters—well, let’s be real, mostly Blair Waldorf—exhibited a double standard around privacy. Blair fiercely protected her own secrets, and was devastated when Gossip Girl revealed embarrassing facts about her private life. But she also frequently sent tips in to the blog about others, for her own ends. And all the characters, however they may have hated the blog, still read it regularly. This is a more extreme version of how anybody today might engage in Facebook-stalking, or other digital dirt-gathering, on people in their lives, even as they might worry about what’s discoverable about themselves online.
People have only entrusted more of their personal information to the internet—especially to their smartphones—over time. “It was once said that a person’s eyes were a window to their soul,” Blair says at one point in season one, as she’s forwarding messages from a stolen phone to herself. “That was before people had cellphones.” That certainly hasn’t become less true since then.
The role of the actual Gossip Girl blog diminished as the seasons went on, and the show’s quality declined as well. At the end, the nonsensical reveal of which main character was behind the blog entirely missed the point. That wasn’t a mystery that needed to be solved. The point of Gossip Girl wasn’t who she was; it was that she was watching.
The show was about scandal, and privilege, and the greatest love affair in 21st-century television history (Blair + Chuck 4eva), but it was also about the ways a person’s public and private life can blur in the internet age, with or without their consent. And that’s a theme that feels more relevant than ever. XOXO.


At just about the halfway point of Lynn Novick and Ken Burns’s monumental documentary on the Vietnam War, an army advisor tells an anecdote that seems to sum up the relationship between the military and computers during the mid-1960s.
“There’s the old apocryphal story that in 1967, they went to the basement of the Pentagon, when the mainframe computers took up the whole basement, and they put on the old punch cards everything you could quantify. Numbers of ships, numbers of tanks, numbers of helicopters, artillery, machine gun, ammo—everything you could quantify,” says James Willbanks, the chair of military history at U.S. Army Command and General Staff College. “They put it in the hopper and said, ‘When will we win in Vietnam?’ They went away on Friday and the thing ground away all weekend. [They] came back on Monday and there was one card in the output tray. And it said, 'You won in 1965.’”
This is, first and foremost, a joke. But given the emphasis that Secretary of Defense Robert McNamara placed on data and running the number—I began to wonder if there was actually some software that tried to calculate precisely when the United States would win the war. And if it was possible that it once gave such an answer.
The most prominent citation for the apocryphal story comes in Harry G. Summers’ study of the war, American Strategy in Vietnam: A Critical Analysis. In this telling, however, it is not the Johnson administration doing the calculation but the incoming Nixon officials:
When the Nixon Administration took over in 1969 all the data on North Vietnam and on the United States was fed into a Pentagon computer—population, gross national product, manufacturing capability, number of tanks, ships, and aircraft, size of the armed forces, and the like. The computer was then asked, “When will we win?” It took only a moment to give the answer: “You won in 1964!”
He said “the bitter little story” circulated “during the closing days of the Vietnam War.” It made the point that there “was more to war, even limited war, than those things that could be measured, quantified, and computerized.”
There’s no doubt that Vietnam was quantified in new ways. McNamara had brought what a historian called “computer-based quantitative business-analysis techniques” that “offered new and ingenious procedures for the collection, manipulation, and analysis of military data.”
In practice, this meant creating vast amounts of data, which had to be sent to computing centers and entered on punch cards. One massive program was the Hamlet Evaluation System, which sought to quantify how the American program of “pacification” was proceeding by surveying 12,000 villages in the Vietnamese countryside. “Every month, the HES produced approximately 90,000 pages of data and reports,” a RAND report found. “This means that over the course of just four of the years in which the system was fully functional, it produced more than 4.3 million pages of information.”
Once a baseline was established, decision makers could see progress. And they wanted to see progress, which created pressure on data gatherers to paint a rosy picture of the portrait on the ground. The slippage between reality and the model of reality based on data became one of the key themes of the war.
“The crucial factors were always the intentions of Hanoi, the will of the Viet Cong, the state of South Vietnamese politics, and the loyalties of the peasants. Not only were we deeply ignorant of these factors, but because they could never be reduced to charts and calculations, no serious effort was made to explore them,” wrote Richard N. Goodwin, a speechwriter for Presidents Kennedy and Johnson. “No expert on Vietnamese culture sat at the conference table. Intoxicated by charts and computers, the Pentagon and war games, we have been publicly relying on and calculating the incalculable.”
All of which the “apocryphal story” condenses into a biting joke.
But was there actually a computer somewhere in the Pentagon that was cranking out “When will we win the war?” calculations?
On October 27, 1967, The Wall Street Journal ran an un-bylined blurb from its Washington, D.C., bureau on the front page talking about a “victory index.”
U.S. strategists seek a “victory index” to measure progress in the Vietnam War. They want a single statistic reflecting enemy infiltration, casualties, recruiting, hamlets pacified, roads cleared. Top U.S. intelligence officials, in a recent secret huddle, couldn’t work out an index; they get orders to keep trying.
Now, a victory index is not quite a computer program you can ask “When will we win the war?” But it’s pretty close! A chart could be plotted. Projections could be made from current progress to future ultimate success. At the very least, we can say that officials tried to build a system that could be the kernel of truth at the center of a certainly embellished story.
And it doesn’t seem out of the question that the specific error—showing the United States had already won—could have actually occurred. As the intelligence officials tried different models to make sense of all their numbers, it certainly seems possible that some statistical runs would, in fact, return the result that the peak of the victory index had already occurred. That the war had been won.
In a world besotted by data, the apocryphal story about the Pentagon computers reminds us that the model is not the world, and that ignoring that reality can have terrible consequences.


On October 4, 1957, a beach ball-shaped satellite launched into space from the Kazakh desert. The satellite joined Earth’s journey around the sun, which is why its creators named it Sputnik, Russian for “traveling companion.” Sputnik circled the planet about every hour-and-a-half, traveling at 18,000 miles per hour as it emitted a steady beep, beep, beep. On the ground, people watched Sputnik through binoculars or listened to its pings on ham radios. By January of the following year, Earth’s traveling companion fell out of its orbit and burned up in the planet’s atmosphere.  
Sputnik’s spectators could not have anticipated that this event—the launch of the first human-made satellite into space—would ignite a race to the stars between the United States and the Soviet Union. Nor could they have known that they were, all of them, standing at the precipice of a new era in human history of near-complete reliance on satellite technology. For them, Sputnik was a sudden flash of innovation, something at which to marvel briefly. For their children and grandchildren and generations after, satellites would become the quiet infrastructure that powered the technology that runs their world.
“Many people grasp that satellites are important in our lives, but they may not see exactly in what ways,” said Martin Collins, a curator at the space-history department of the Smithsonian National Air and Space Museum.
So what would happen if all the satellites orbiting Earth suddenly, all at once, stopped working?
The effects would be felt unevenly around the world, Collins said. In communities that don’t rely on satellite technology, particularly in the developing world, potential disruptions to daily life likely would be less severe. In other places, like in the United States, the results would be severe at best. If the blackout persisted long enough, they’d be catastrophic.
If the satellites shut down, “tentacles of disruption,” as Collins put it, would begin to unfurl.
Without operational communications satellites, most television would disappear. People in one country would be cut off from the news reports in another. The satellite phones used by people in remote areas, like at a research station in Antarctica or on a cargo ship in the Atlantic, would be useless. Space agencies would be unable to talk to the International Space Station, leaving six people effectively stranded in space. Militaries around the world would lose contact with troops in conflict zones. Air-traffic controllers couldn’t talk to pilots flying aircraft over oceans.
Richard Hollingham described how this loss of would feel in a Wellesian story in the BBC in 2013: “The rapid-communications systems that tied the world together were unraveling. Rather than shrinking, it seemed as if the Earth was getting larger.”
Without global navigation satellites, the Global Positioning System (GPS)—the network of satellites and ground stations that tell us exactly where we are—would crumble. Some of the immediate effects would be frustrating, but not debilitating, like not being able to use a smartphone to find your way around a new city or track a run in a fitness app. Other effects would have far-reaching consequences. Millions of truckers and other workers in the delivery industry rely on GPS to crisscross the country each day, delivering food, medicine, and important goods.
The loss of GPS also would have disastrous results for our sense of time. GPS satellites are equipped with atomic clocks, which provide the very precise time that satellites need to calculate distance on Earth and tell GPS-enabled devices about their location. Satellites transmit this time to receivers on the ground, where power companies, banks, computer networks, and other institutions synchronize their operations to it. Without these clocks, the electrical grid, financial transactions, and, yes, the internet would start to fall apart. So too would the internet of things, the vast web of devices that talk to each other on our behalf.
“GPS is staggeringly integrated into our lives,” Collins said.
The shutdown of weather and remote-sensing satellites would gravely hamper our ability to predict weather events, like the major hurricanes that have swept across the Caribbean and southeastern United States this year. Farmers couldn’t get information that informs their crop and water management, and scientists wouldn’t have data for their studies of Earth’s features or climate change.
The World Needs a Terrestrial Sputnik Moment
The disruption of every one of the hundreds of operational satellites orbiting Earth is unlikely, but even the loss of one or a few satellites could have powerful effects. When one communications satellite fell out of its orbit in 1998, 80 percent of pager users in the United States——about 45 million people—lost service. An article in The Los Angeles Times a couple of days later sought to emphasize the fragility of the nation’s behind-the-scenes satellite infrastructure. “Paging is hardly the only consumer convenience delivered via satellite technology,” it warned.
Satellite operations could get knocked out by natural phenomena, like powerful solar storms, or human activity, like one nation’s intentional destruction of another’s  fleet of satellites, or an all-out global war. Space junk could also set off a series of collisions that damage any satellites in their path. Collins said that the cause of a complete blackout of satellites would likely determine how people respond to it. Chaos, for a time, is likely inevitable, and there are plenty of suggestions for this doomsday scenario in apocalyptic science-fiction writing.
“Would it severely disrupt the way we live right now? Yes,” Collins said. “Would people be starving in the streets or would there be civil disobedience? That’s hard to say. Potentially.”
Would anything good come out of it? Perhaps, Collins said, when the power grid fails and people are left in the darkness, they could see, many of them for the first time, the unobstructed night sky, with the stars of the Milky Way stretching out before them. They could look up and gaze at the place where their traveling companions, now silent, float along with them.


Facebook’s greatest strength—its ability to identify and connect like-minded people—is also a major vulnerability. Over the past month, the company has revealed that Russia-linked accounts purchased thousands of fake political ads on its platform around the 2016 U.S. election. These ads “microtargeted” Americans based on their divisions along political, racial, and religious lines. Some, as CNN recently reported, specifically targeted voters in Michigan and Wisconsin, two of the most heavily contested states.
The apparent goal was to sow distrust among voters, perhaps even shape how they voted.
As an initial response, Facebook announced that it will close the loopholes that allow Russian-backed sources—or any other foreign powers—to open fake accounts. While a productive start, this doesn’t go after the underlying problem that Russian operatives capitalized on: the extreme polarization of Americans on political issues. Wittingly or not, Facebook has taken on a central role in American democracy. Now the company has to decide how proactive it wants to be to become “a force for good,” as Mark Zuckerberg has promised.
One step Facebook could take in this direction: reverse-engineer the very algorithms used by the Russians. Facebook could try an experiment of matching Americans across political lines to help bridge the country’s deep divide.
Key to understanding why the Russian operatives’ efforts worked is looking at the way in which people build social networks online and the value they get from them. In Bowling Alone, the Harvard professor Robert Putnam uses the phrase “social capital” to describe this process, which he explains happens in two ways: “Bonding” is social capital built by connecting within exclusive homogenous groups; “bridging” is social capital built by connecting with inclusive heterogeneous groups. Both are valuable—while bonding offers support and solidarity, bridging helps people expand their perspectives and creates trust across diverse groups.
“Bonding social capital constitutes a kind of sociological superglue,” Putnam writes, “whereas bridging social capital provides a sociological WD-40.”
Facebook is primarily a mechanism for bonding, not bridging. Studies show that in the vast majority of cases, people live in self-made echo chambers on Facebook that reinforce their existing views of the world. You need look no further than the “red feeds” and “blue feeds” on any given issue to see that in general, when people connect on Facebook, they are mostly connecting with others who have similar political beliefs, educational backgrounds, and religious outlooks.
Although bridging is possible—say, when your old high-school friend who stayed local while you flew across the country for college offers to connect with you—the ability to choose your network and “hide,” “unfriend,” or even “block” people with whom you no longer want to engage makes it essentially an exclusive network. Facebook further amplifies this segregation by using data from a user’s social network and activities on the platform to custom-tailor a News Feed that aggregates posts it knows that user wants to see, often reinforcing worldviews. This insularity allowed Russia’s $100,000 investment in “dark ads” to reach roughly 10 million Americans before and after the election in discrete demographic and geographic circles.
Facebook’s emphasis on bonding over bridging also has consequences for how people build trust. The relationship researcher John Gottman has found that successful romantic relationships depend on making frequent deposits in each partner’s “emotional bank account.”  Consistent positive interactions increase levels of trust in the relationship, so that when conflict arises, there are enough “reserves” in place to make a withdrawal, but still leave the relationship in a net-positive place. In fact, Gottman estimates that every relationship needs at least five positive interactions to maintain equilibrium with a single negative interaction.
Applying Gottman’s “bank account” model to social relationships can help explain why it’s difficult to have meaningful disagreements on political issues. Americans today spend an average of six-and-a-half hours each day online, with almost a third of that time on social media. If their social-media diets include relatively insular circles like Facebook, their daily positive interactions are likely occurring more with people they already agree with, and less with people from across groups with different perspectives. In fact, in 2017, odds are that Americans will most likely interact with someone who holds different political views when they’re screaming at them from the other side of a protest line, or inside an angry internet forum.
Without a way to make regular, positive deposits in social relationships that bridge political lines, every civic debate is a withdrawal without social reserves, leaving people perpetually overdrawn.
Some research supports the idea that frequent and meaningful interactions between diverse Facebook users can promote the flow of new ideas across otherwise unconnected groups. Jonny Thaw, a spokesperson for the company, pointed out a 2014 study that looked at how the platform creates the “bridging social capital” described by Putnam. The study, which was conducted by researchers unaffiliated with Facebook, found that “weaker ties” in someone’s network (like a friend of a friend, or someone with whom you would not have other offline connections) offered the platform’s users the most potential for users to expand their worldview, because these connections opened the door to new information and diverse perspectives.
More importantly, however, was that the users who benefited the most from their weak social ties—in terms of expanding their outlook—were those who actively engaged in what the study’s authors call “Facebook Relationship Maintenance Behaviors,” like “responding to questions, congratulating or sympathizing with others, and noting the passing of a meaningful day.”
In other words, simply being connected to Facebook users from different backgrounds isn’t enough to make people open to new perspectives and ideas; users need to actively make deposits in each other’s social bank accounts in order to truly benefit from those diverse connections. The study notes that facilitating bridging among its users “may lie in technical features of the site that lower the cost of maintaining and communicating with a larger network of weak ties.”
This study points to some creative ways that Facebook can promote political bridging among its users—and develop some WD-40 against threats to democracy in the process. Let’s say that Facebook created a new feature called “Friend Swap” for users interested in creating connections with people outside of their political bubble. The company could use its powerful algorithms to match users with someone who, based on their individual preferences and posts, they disagree with politically, but have some things in common with personally. What’s important is that the users don’t engage over political issues, at least until they’ve had time to build some social trust. If you’re a liberal, you might not be so open to being thrown whole-hog into a conservative stranger’s feed and reading their posts from Fox News. But you may find some common ground around, say, rooting for the same sports team, or shared musical tastes or experiences, like being a veteran.
A feature like Friend Swap would selectively share only the posts of each user’s feed in an area they have in common with their political counterpart, and allow them to interact on that topic. After a trial period, the “swapped” posts might include ones on another common interest, and so on, until the users elect, if they eventually choose, to actually be “friends.” By creating connections around common interests or experiences, users would make deposits in each other’s social bank accounts over time. If they do become full-on friends, they would be more likely, at least in theory, to be open to a dialogue on differing viewpoints on political issues from someone they’ve come to trust based on bonding in other areas. Hopefully, at the very least, they could agree to disagree while maintaining their connection, which is still a win in today’s climate.
Of course, Friend Swap won’t be a panacea for political differences. It requires people to view online relationships—with strangers—as being valuable enough to invest their time. And the self-selection of people who opt in to this kind of feature might be the same people who would be more open to different viewpoints anyway. But even as an experiment, Friend Swap would be an opportunity for Facebook to gather data on how it can bridge its red and blue silos over shared values like civility, openness, tolerance, and respect. It would also offer a new way to connect people from politically polarized geographic regions, like the Rust Belt and the coasts.
Trying to “socially engineer” relationships, even for the purposes of political cross-pollination, might go against the grain of a company that has been built upon a principle of fierce neutrality. But Russian operatives’ attempts to use Facebook to disrupt American democracy demonstrates that neutrality no longer seems be an option, if it ever really was one in the first place. On Yom Kippur, the Jewish day of atonement, Zuckerberg acknowledged as much, asking for forgiveness for “the ways [Facebook] was used to divide people rather than bring us together.” Facebook has the talent and the resources to help unite people in defense of democratic values, if it has the will to do it.


It’s just after sunrise in New York City. The sky is bathed in pinks and orange as people walk along a long dock toward a white ship. They board the vessel and it sails out to a launchpad further out in the water, where a spaceship strapped to a giant rocket awaits. After they pile in, the rocket blasts off into the atmosphere. About 39 minutes later, they land halfway around the world, in Shanghai.
This is the scenario imagined by SpaceX founder Elon Musk, who discussed the futurist transport system in a speech in Australia last week about the company’s long-term ambitions. The not-yet-built system—which Musk nicknamed BFR, for “big fucking rocket”—would, someday, ferry passengers from one major city to another. Long-distance trips from Bangkok to Dubai, or from Honolulu to Tokyo, for example, would take about 30 minutes and cost about as much as an economy airline ticket.
(The BFR would also subsume the duties of SpaceX’s current fleet of rockets and spacecraft, like the Falcon 9 and Dragon capsule, by launching satellites into orbit, transporting astronauts and cargo to the International Space Station, and even bringing humans to the moon and Mars.)
The news of the Earth transport system was thrilling for Musk’s fans, for whom a speech from the entrepreneur about space exploration is akin to an Apple launch event. The future is really here, or at least quickly approaching! Imagine setting your smartphone to rocket mode instead of airplane mode. Rocket travel, an animated video of this future seemed to suggest, would be a breeze.
Well, not necessarily. To make a half-hour trip, the BFR would have to travel thousands of miles per hour, with a maximum speed of about 16,700 miles per hour, according to SpaceX. The flight would expose passengers to sensations they don’t usually encounter while traveling, like intense gravitational forces and weightlessness. The spaceship would definitely need to stock barf bags.
Musk explained in a tweet that travelers would experience g-forces between 2 and 3, which means twice or three times their body weight. “Will feel like a mild to moderate amusement park ride on ascent and then smooth, peaceful, and silent in zero gravity for most of the trip until landing,” he said.
“That may not be a very comfortable way to travel,” said Ge-Cheng Zha, a mechanical and aerospace professor at the University of Miami who studies supersonic flight. “Not everyone can take it.”
The ride will be most intense during landing and takeoff. The rapid acceleration and deceleration could lead to motion sickness. So could a quick peek out the window during a particularly twisty maneuver. “There’s a disconnect between the g-force and what the person sees, which can lead to severe motion sickness,” said Andy Feinberg, a geneticist at Johns Hopkins University in Maryland who studies astronaut health (and applied to be an astronaut himself in 1979).
Feinberg has flown aboard NASA’s now-retired zero-gravity plane, which simulates weightlessness by taking a series of dives. The speed of the aircraft matches the speed of the passengers as they fall, which creates the experience of free-falling. Feinberg remembers not everyone on board could handle the shifts.“The NASA people, the astronauts, and I were having the time of our lives,” he said. Everyone else around them was throwing up.
Aside from the flight experience and the discomfort it may bring, there’s a host of other factors Musk and his engineers will need to consider before the BFR becomes reality. While the actual trip may indeed take about a half-hour, preparing and unloading the passengers could take hours. The nature of the travel could increase the time required for security checks, luggage checks, and whatever new safety procedures flight attendants may have to present. (At least some flight attendants, for what it’s worth, seem game for the BFR. While it’s too soon to give a “definitive opinion” on rocket travel, Sara Nelson, the president of the Association of Flight Attendants-CWA International, said flight attendants have the “flexibility to adapt to new conditions.”)
There’s also the question of fuel. Launching the BFR, which will stand 106 meters tall, nearly double the height of the Falcon 9, will require tremendous energy, Zha said. A rocket is much harder to get off the ground than, say, a supersonic plane, he said. The Concorde, a now-retired commercial supersonic airliner that carried passengers from New York to London in under three hours, among other destinations, used about three times as much fuel as a Boeing 747. “The reason we use rockets for space delivery is because there’s no other options,” Zha said. “On Earth, airplanes are way more efficient.”
The transport system will also face questions from the U.S. government and other nations in the BFR’s flight path about the rocket’s safety risks and environmental impact. The rocket’s introduction would require the regulation of an entirely new commercial industry.
Still, many won’t be deterred. The BFR doesn’t exist yet, so the coolness factor outweighs all others. Asked whether he would hitch a ride on the BFR, Feinberg said, “in a second.”


In the crucial early hours after the Las Vegas mass shooting, it happened again: Hoaxes, completely unverified rumors, failed witch hunts, and blatant falsehoods spread across the internet.
But they did not do so by themselves: They used the infrastructure that Google and Facebook and YouTube have built to achieve wide distribution. These companies are the most powerful information gatekeepers that the world has ever known, and yet they refuse to take responsibility for their active role in damaging the quality of information reaching the public.
BuzzFeed’s Ryan Broderick found that Google’s “top stories” results surfaced 4chan forum posts about a man that right-wing amateur sleuths had incorrectly identified as the Las Vegas shooter.
4chan is a known source not just of racism, but hoaxes and deliberate misinformation. In any list a human might make of sites to exclude from being labeled as “news,” 4chan would be near the very top.
Yet, there Google was surfacing 4chan as people desperately searched for information about this wrongly accused man, adding fuel to the fire, amplifying the rumor. This is playing an active role in the spread of bad information, poisoning the news ecosystem.
The problem can be traced back to a change Google made in October 2014 to include non-journalistic sites in the “In the News” box instead of pulling from Google News.
But one might have imagined that not every forum site could be included. The idea that 4chan would be within the universe that Google might scrape is horrifying.
Worse, when I asked Google about this, and indicated why I thought it was a severe problem, they sent back boilerplate.
Unfortunately, early this morning we were briefly surfacing an inaccurate 4chan website in our Search results for a small number of queries. Within hours, the 4chan story was algorithmically replaced by relevant results. This should not have appeared for any queries, and we’ll continue to make algorithmic improvements to prevent this from happening in the future.
It’s no longer good enough to note that something was algorithmically surfaced and then replaced. It’s no longer good enough to shrug off (“briefly,” “for a small number of queries”) the problems in the system simply because it has computers in the decision loop.
After I followed up with Google, they sent a more detailed response, which I cannot directly quote, but can describe. It was primarily an attempt to minimize the mistake Google had made, while acknowledging that they had made a mistake.
4chan results, they said, had not shown up for general searches about Las Vegas, but only for the name of the misidentified shooter. The reason the 4chan forum post showed up was that it was “fresh” and there were relatively few searches for the falsely accused man. Basically, the algorithms controlling what to show didn’t have a lot to go on, and when something new popped up as searches for the name were ramping up, it was happy to slot it as the first result.
The note further explained that what shows up in “In the News” derives from the “authoritativeness” of a site as well as the “freshness” of the content on it. And Google acknowledged they’d made a mistake in this case.
The thing is: This is a predictable problem. In fact, there is already a similar example in the extant record. After the Boston bombings, we saw a very similar “misinformation disaster.”
Gabe Rivera, who runs a tech-news service called Techmeme that uses humans and algorithms to identify important stories, addressed the problem in a tweet. Google, he said, couldn’t be asked to hand-sift all content but “they do have the resources to moderate the head,” i.e., the most important searches.
The truth is that machines need many examples to learn from. That’s something we know from all the current artificial-intelligence research. They’re not good at “one-shot” learning. But humans are very good at dealing with new and unexpected situations. Why are there not more humans inside Google who are tasked with basic information filtering? How can this not be part of the system, given that we know the machines will struggle with rare, breaking-news situations?
Google is too important, and from what I’ve seen reporting on them for 10 years, the company does care about information quality. Even from a pure corporate-trust and brand perspective, wouldn’t it be worth it to have a large enough team to make sure they get these situations right across the globe?
Of course, it is not just Google.
On Facebook, a simple search for “Las Vegas” yields a Group called “Las Vegas Shooting /Massacre,” which sprung up after the shooting and already has more than 5,000 members.
The group is run by Jonathan Lee Riches, who gained notoriety by filing 3,000 frivolous lawsuits while serving a 10 year prison sentence after being convicted for stealing money by impersonating people whose bank credentials had been phished. Now, he calls himself an “investigative journalist” with Infowars, though there is no indication he’s been published on the site, and given that he also lists himself as a former male underwear model at Victoria’s Secret, a former nuclear scientist at Chernobyl, and a former bodyguard at Buckingham Palace, his work history may not be reliable.
The problems with surfacing this man’s group to Facebook users is obvious to literally any human. But to Facebook’s algorithms, it’s just a fast-growing group with an engaged community.
Most people who joined the group looking for information presumably don’t know that the founder is notorious for legal and informational hijinks.
Meanwhile, Kevin Roose of The New York Times pointed out that Facebook’s Trending Stories page was surfacing stories about the shooting from Sputnik, a known source of Russian propaganda. Their statement was, like Google’s, designed to minimize what had happened.
“Our Global Security Operations Center spotted these posts this morning and we have removed them. However, their removal was delayed, allowing them to be screen-captured and circulated online,” a spokesperson responded. “We are working to fix the issue that allowed this to happen in the first place and deeply regret the confusion this caused.”
All across the information landscape, looking for news about the shooting within the dominant platforms delivered horrifying results. “Managing breaking news is an extremely difficult problem but it's incredible that asking the search box of *every major platform* returns raw toxic sewage,” wrote John Hermann, who covers the platforms for The New York Times.
For example, he noted that Google’s conglomerate mate at Alphabet, YouTube, was also surfacing absolutely wild things and no respected news organization.
managing breaking news is an extremely difficult problem but it's incredible that asking the search box of *every major platform* returns raw toxic sewage
As news consumers, we can say this: It does not have to be like this. Imagine a newspaper posting unverified rumors about a shooter from a bunch of readers who had been known to perpetuate hoaxes. There would be hell to pay—and for good reason. The standards of journalism are a set of tools for helping to make sense of chaotic situations, in which bad and good information about an event coexist. These technology companies need to borrow our tools—and hire the people to execute on the principles—or stop saying that they care about the quality of information that they deliver to people.
There’s no hiding behind algorithms anymore. The problems cannot be minimized. The machines have shown they are not up to the task of dealing with rare, breaking news events, and it is unlikely that they will be in the near future. More humans must be added to the decision-making process, and the sooner the better.


In one corner of the internet, World War II is just getting started.
The battle is unfolding at @RealTimeWWII, a Twitter account that “live-tweets” the events of each day of the war, hour by hour, as they happened more than 70 years ago. Here, the conflict that ensnared most of the world’s nations and claimed the lives of millions is broken down into 140-character dispatches. Right now, it’s September 1939, and German bombs are shattering Warsaw. The city is in flames. Its residents are out of food and clean water. “Human wreckage is laid on the table, the surgeon vainly endeavoring to save the lives that are slipping through his hands,” one nurse says of the scene.
@RealTimeWWII will spend the next six years live-tweeting the rest of the war to its nearly 500,000 followers. The account is run by Alwyn Collinson, a digital editor at the Museum of London. Collinson actually spent the last six years tweeting the war, too. His first run started in 2011 and ended last month, and he decided to start all over again in September, writing and tweeting the same dispatches, with some new additions.
“It’s incredible to have hundreds of thousands of people reading and hundreds responding,” Collinson said. “I think it’s a shame just to walk away from that, though perhaps it would have been easier to do so.”
Collinson spends about an hour each day planning, curating, and posting tweets. The preparation for major events—like the German invasion of the Soviet Union, the attack on Pearl Harbor, and D-Day—can take as long as 10 hours. It helps that World War II is one of the most well-studied and documented events in human history; Collinson has at his disposal countless books, newspaper archives, historical records, and other sources. In addition to tweeting the basics, like military operations, Collinson shares quotes from firsthand accounts gleaned from letters and diaries.
“In many ways, it’s like trying to report on the news but years and years ago,” he said. “I’m not just trying to get a sense of who invaded who, which cities fell when, but more of what people thought and how they were trying to make a narrative out of it at the time.”
Collinson was inspired to retrace World War II after watching the events of the Arab Spring play out across social-media platforms in late 2010. Back then, for perhaps the first time, people were using digital means like Twitter and other websites to announce, coordinate, and document widespread political demonstrations in multiple countries. Social media elevated and carried the protests over international borders, reaching anyone with a Twitter handle. The internet made the far-flung events of the movement into something “real and tangible and immediate,” Collinson said.
“I remember being really profoundly moved by the way that you could see history unfolding,” he said. “We didn’t know necessarily how things were going to turn out.”
As @RealTimeWWII’s following grew, users began to reach out to share personal stories, letters, or diaries of family members who lived through the war. Some helpfully corrected Collinson when he tweeted something incorrect, like the wrong name for a military tank in a picture. ( “One of the wonderful advantages of the internet is that people can correct you almost immediately,” he said with a laugh.) Then there were the vitriolic comments. In response to tweets about the Holocaust, Collinson said,“I had neo-Nazis saying, why are you reporting on this Jew news? Get back to real news.”
For the most part, Collinson is tweeting the same kind of content today as he did when he started in 2011. But the atmosphere—of Twitter, of the internet in general—feels different to him. In the last decade, it seems like the discourse on social media has gotten increasingly ugly.
“I suppose now it seems like a less innocent landscape,” Collinson said. “The internet and social media are no longer an escape. It’s just one more way of reflecting, perhaps even in a more disconcerting way, the world that we live in.”
These days, Twitter users are finding more parallels between the events of World War II and current political and economic affairs, particularly in the rise of nationalist sentiment in the United States and Europe. Some people react to the tweets with warnings or lamentations that the world is regressing to early-20th-century ideologies. “Obviously there were terrible events going on [in 2011] as there are terrible events going on now,” Collinson said. “But there was less talk of there being Nazis on the streets of the Western world.”
The account serves as yet another reminder that history is hindsight. Users scrolling through @RealTimeWWII’s feed enjoy an omniscience they cannot possess in their own time. But they are living through history just as much as people during the war did, and perhaps someday their own lives will become a historical narrative told on the internet—or through some technology that doesn’t yet exist.
“It’s reminded us that actually no one lives outside of history,” Collinson said. “We could be living in the calm before some terrible storm. One of the reasons to study history, other than that it’s fascinating, is that it reminds you not to be complacent about your own time and not to think that you can live unaffected by the world.”


Not long ago, Glenn Thrush, a White House reporter for The New York Times, sat down at 7:30 a.m. to plan his day. His schedule and laptop sat arrayed before him. With so much time to spare, he checked Twitter—an easy mistake to make—where he noticed someone saying something vile and ill-considered.
And then, somehow, 90 minutes passed. It was 9 a.m. No planning had gotten done.
“Someone had said something nasty and it had totally hijacked my day,” he said on Thursday. Twitter’s addictiveness, and its mysterious ability to speed the passage of time, were part of the reason that Thrush deactivated his own account on the platform earlier this month—an interesting choice for a White House reporter during the administration of a president who loves to break news on Twitter. On September 19, Thrush posted his last tweet and logged off. At least for now.
Now, he feels more in control of his day, though he lamented that he couldn’t promote his Times colleagues’ work anymore. But there were other workplace benefits to leaving the site. “I’ll just say straight-out, I think my bosses are pretty pleased with my decision to do this,” he said.
Twitter was just one of the topics that Thrush and two other politics reporters covered on Thursday at the Washington Ideas Forum, which is sponsored by the Aspen Institute and The Atlantic. Thrush was joined by Katy Tur, a correspondent for NBC News; and Robert Costa, a national political reporter at The Washington Post and the moderator of  Washington Week on PBS. During the forum, they discussed the many roles the media—social and otherwise—have played in the early Trump era.
All three of the journalists have achieved new recognition in the Trump era—though only Thrush was honored with a regular portrayal on Saturday Night Live. All three have also experienced the topsy-turvy—and, sometimes, inappropriate—assaults of the new administration and its most prominent figure.
Tur was one of the first national reporters tasked to cover Trump full-time. Often, she said, it was just her and a bunch of local reporters at any one campaign event. She came to be one of the first reporters that Trump recognized—so much so that by June 2015, Trump shouted, mid-rally, “Katy, you haven’t even looked up at me once.”
“I yelled back, ‘I’m tweeting what you’re saying!’” Tur said. “And he liked that and moved on.”
The president did not always move on. In a now widely reported moment, Trump shocked Tur by kissing her on the cheek in November 2015 while on an MSNBC set in New Hampshire.  “Before I know what’s happening, his hands are on my shoulders and his lips are on my cheek. My eyes widen. My body freezes. My heart stops,” Tur writes in her new book, Unbelievable.
“F—k,” she thought. “I hope the cameras didn’t see that. My bosses are never going to take me seriously. I didn't have time to duck!”
A few minutes later, Trump mentioned the kiss on-air. “But actually, Katy Tur — what happened? She was so great,” he told Joe Scarborough and Mika Brzezinski. “I just saw her back there. I gave her a big kiss. She was fantastic.”
For Tur, the incident—aside from being inappropriate—signified the president’s ignorance if not active rejection of the courtesies normally expected from presidential candidates (or, for that matter, adult professionals).
Tur and Thrush described Trump as a hybrid figure, a creation and impresario of every kind of media.
“Prior to covering his presidential campaign, my final two interactions with Donald Trump were not taking his phone calls when he was trying to get an exit ramp built [in New York City],” said Thrush.
Like Ed Koch, a former New York mayor, Trump was for decades best described as “unavoidable for comment,” he said. “I think it’s notable that most of the people who cover him right now started at tabloids in New York City.”
The president hasn’t fully given up his old act: He called Costa’s cellphone minutes after an early Obamacare repeal bill failed.
But if his roots are inky, his power is televisual. Trump supporters across the country have mentioned The Apprentice when explaining their support to Tur. “His supporters would say, Donald Trump will know who to hire. And I’d say, how do you know that?” she said. They would reply: “I’ve seen him on The Apprentice.”
In other words, the press—whom Trump has described as “the enemy of the people”—emerged as less like a target of his ire than just another tool to be exploited. “I think it’s way overstated, their hostility to the press,” said Thrush, referring to Trump and his administration.
“It really was, walking into that building, a very hostile environment for the first three to five months. Now,” he said, “it’s settled into the usual trench warfare.”


Donald Trump and Mark Zuckerberg each told partial truths yesterday.
First, Trump tweeted that “Facebook was always anti-Trump.” From all available information, it does seem true that the vast majority of Facebook’s employees did not want Donald Trump elected president of the United States. They are disproportionately young, urban, and socially liberal, living in California’s most left-wing region. Trump lost all these demographic groups.
Zuckerberg, Facebook’s CEO, responded to Trump with a post about the company’s role in the election. “Trump says Facebook is against him,” he wrote. “Liberals say we helped Trump. Both sides are upset about ideas and content they don’t like. That’s what running a platform for all ideas looks like.”
Trump wants Facebook to be seen as having a traditional anti-Trump bias. Mark Zuckerberg wants the service to be seen as neutral. And they’re both wrong.
Zuckerberg’s statement begins with a play right out of the D.C. congressional playbook: The tough-minded, get-things-done pragmatist knows in his heart that if everyone is mad, he must have done something right.
But the sophisticated critiques of Facebook are not about ideas and content that people don’t like, but rather the new structural forces that Facebook has created. News and information flow differently now than they did before Facebook; capturing the human attention that constitutes that flow is Facebook's raison d’être (and value to advertisers). Now that it has done so, Zuckerberg would like to pretend that his software is a pure conduit through which social and political truths can flow.
The conduit must be pure. The platform must be neutral. Because Mark Zuckerberg wants his company’s role in the election to be seen like this: Facebook had a huge effect on voting—and no impact on votes.
Zuckerberg describes Facebook’s central role in the election himself. “More people had a voice in this election than ever before. There were billions of interactions discussing the issues that may have never happened offline. Every topic was discussed, not just what the media covered,” he wrote. “This was the first U.S. election where the internet was a primary way candidates communicated. Every candidate had a Facebook page to communicate directly with tens of millions of followers every day.”
Facebook even registered 2 million people to vote, which Zuckerberg notes was “bigger than the get-out-the-vote efforts of the Trump and Clinton campaigns put together.”
Half apologizing for calling the idea that the spread of misinformation on his platform swung the election “crazy,” he continued, “the data we have has always shown that our broader impact—from giving people a voice to enabling candidates to communicate directly to helping millions of people vote—played a far bigger role in this election.”
But given all that, couldn’t even small structural wrinkles in Facebook have provided more support for one candidate over another? Are we to believe that despite this admittedly enormous impact on 2016, the platform somehow maintained perfect neutrality with respect to the candidates?
One of the foundational documents of the academic field of science and technology studies is a talk given by Melvin Kranzberg, of the Georgia Institute of Technology. In it, he declared six incisive, playful “laws” of technology. The first one is: “Technology is neither good nor bad; nor is it neutral.”
He explains:
“Technology’s interaction with the social ecology is such that technical developments frequently have environmental, social, and human consequences that go far beyond the immediate purposes of the technical devices and practices themselves, and the same technology can have quite different results when introduced into different contexts or under different circumstances.”
And this is the rub with Facebook right now. The technology that has, more or less, created Facebook’s current status as the world’s leading purveyor of information is News Feed.
News Feed had to solve a basic problem: There were always too many posts by people you know and organizations you follow to show them all to you. So, Facebook’s engineers decided to find a way to rank them that wasn’t just chronological. It makes some sense: Why show you four posts from Sports Illustrated before showing you a post from your father?
The News Feed, then, takes many pieces of data about each story and the way that story is playing in your social network to order your personal feed. As a technology, it is one of the most successful products of all time. People spend countless hours on Facebook precisely because News Feed keeps showing them stuff that they want to interact with.
During this time, the actions they take on the platform signal to the News Feed that they’re interested in something. The industry calls this engagement. Reading, watching, sharing, commenting, reposting to your own page: That’s all engagement. Posts that generate a lot of it (within one’s network and beyond) are more likely to show up on your feed.
There are a lot of factors to this, Facebook’s engineers would tell you. But the complexity can’t hide the most basic fact: The goals of News Feed have nothing to do with democracy. They might overlap sometimes. But they are not the same.
Zuckerberg’s note ends saying that he wants Facebook to be a “force for good in democracy.” To recognize that one’s massive platform can do good, however, requires an understanding that it could also do bad. You can’t have one without the other. This sin of omission runs throughout Silicon Valley: “Change the world.” “Have an impact.” These are incomplete phrases that render incomplete thoughts.
If Facebook wants to be a force for good in democracy, it needs to answer some questions. Does maximizing engagement, as it is understood through News Feed’s automated analysis, create structural problems in the information ecosystem? More broadly, do the tools that people use to communicate on Facebook influence what they actually talk about?
Facebook might offer the defense that any changes would reflect equally across partisan lines or that there is no systemic bias that gets baked into the system. But let’s just say that one candidate, in a hypothetical election, was very good at driving engagement on Facebook. Perhaps this candidate was hyperbolic and prone to extreme statements that generated controversy. Perhaps this candidate hit hot-button issues and denigrated opponents personally. Perhaps this candidate used the preexisting fractures among the country’s polity to drive a lot of shares and comments, positive and negative.
The other candidate in this hypothetical election was more measured. The remarks the candidate made were primarily about policy. The candidate tried to calm the passions of political followers. Does anyone doubt that this candidate’s engagement would not be as good?
Now multiply that by all the media that both these candidates generate. Multiply that by the people on Facebook who come to understand that posting an anti-Trump meme gets more engagement than a pro-Clinton meme.
The fake news that ran rampant on Facebook was a symptom of a larger issue. The real problem lies at the very heart of Facebook’s most successful product: Perhaps virality and engagement cannot be the basis for a ubiquitous information service that acts as a “force for good in democracy.”
And if this is true, how much is Facebook willing to change?


Tuesday, Twitter announced that it would test 280-character tweets, a doubling of the 140-character standard on the social network.
If you can still read this while gasping for breath and looking for a place to sit down while you absorb this news, I have something to tell you: You broke it, Twitter’s just trying to fix it.
There was a time when Twitter was 140 characters of text. That was it. That was nice and information-dense, but you wanted to add pictures. You wanted to add more text in one tweet than it was possible to write, so you screenshotted the text of stories or even your own notepad.
You started numbering your tweets, tweetstormin’.
Hell, you wanted to add 14-minute videos. You wanted to add livestreaming video of protests and football games and AM2DM.
Whatever measly restriction Twitter tried to hold onto, for the purity of the product, you just routed around. And now, no one can seriously argue that Twitter is only a place for 140-character bits of text. Like, who are we kidding here? And why do we think that the text-message character limits of the early ’00s somehow magically stumbled onto the platonic ideal of message length for a social network built primarily from small chunks of text?
How many of you even remember the time when you could only send 140 characters in a text message on your smartphone? What keeps texts short, and what will keep most Twitter messages short, is the culture of messaging on a phone, which like all cultures is viscous and will change more slowly than the product. You’ll have time to adjust.
So who cares about a doubling of the character limit? It was arbitrary then and it’s arbitrary now. My guess is you’ll see very little difference in the platform, and maybe the success of the change will give Twitter the confidence to focus on what really matters: the communities that have gathered on the service.
People think that Twitter’s brand is built around the 140-characters. But if you ask me, Twitter’s core identity is contained in the @username. The @ is about the people inside Twitter: people who tweet things they shouldn’t, people who tweet about hurricanes all night, people who love books, people who have rare expertise.
You see it best at conferences, around TV shows, and when there is a major local breaking-news event. It’s not that Twitter, as a whole, becomes awesome during these moments. Sometimes, in fact, the user experience breaks. But no other product on the internet quickly sorts out who the important individuals to follow for a given event are.
Twitter’s value has always been in these little pro-am micro-networks, hived off from the larger feed, where anyone with knowledge, wit, or skills can become central to the perception of a moment. That’s not going to change now.


On Tuesday evening, Twitter announced that it is experimenting with doubling the length of tweets, allowing users to post up to 280 characters per message.
To start, the feature will only be available to a random set of users on the service. But if adopted by the platform as a whole, the change will constitute one of the most fundamental changes to Twitter’s core product in years.
“This is a small change, but a big move for us,” said Jack Dorsey, Twitter’s chief executive officer, in one of the first supersized tweets. “One hundred forty was an arbitrary choice based on the 160-character SMS limit.”
Biz Stone, who cofounded the company and returned to it full-time earlier this year, provided more context in another mega tweet:
Originally, our constraint was 160 (limit of a text) minus username. But we noticed @biz got 1 more than @jack. For fairness, we chose 140. Now texts are unlimited. Also, we realize that 140 isn't fair—there are differences between languages. We're testing the limits. Hello 280!
Twitter’s users responded predominantly with jokes, dread, and pandemonium, which is in line with how they respond to most changes to the service. Tech critics seemed more sanguine. Will Oremus, a reporter at Slate and a reliable defender of Twitter’s leadership, predicted that the change would barely alter the service.
Some users questioned why the company was doing anything other than addressing its ongoing problem with user harassment. In July, Twitter said it had ratcheted up enforcement of its rules, bragging that it now takes down “10 times the number of abusive accounts every day compared to the same time last year.”
Because the big-boy tweet feature has yet to debut at scale—as of Tuesday night, I saw no one outside of Twitter’s leadership cabal posting 280-character tweets—most of the coverage remains speculative. So here are seven questions about the hefty tweets—and a first attempt at answering them.
Why would Twitter do such a thing?
Money. Since it went public in late 2013, Twitter has basically never found a firm financial footing. The number of Americans who log on to Twitter every month stayed flat for years—and then, this summer, it began to decline. (U.S. users have outsize importance here: They are the company’s most lucrative demographic.) For years, Twitter dealt with the crisis of its stalling user numbers by wringing more revenue out of the same number of people by selling better-targeted ads.
Now that project has also slowed. Almost four years after debuting on Wall Street at $44, Twitter’s shares linger at $16. The company has never turned a profit.
Once, analysts hoped Twitter would be another Facebook. But Mark Zuckerberg’s monster soared out of its IPO, and it has now come to gobble one out of every five dollars spent on digital ads in the United States. No one would confuse the companies now.
What do 280 characters have to do with money?
Twitter seems to hope that enlivening the service—and dropping one of its trademark constraints—will bring users back to the service. In fact, it explicitly says as much.
In Twitter’s blog post announcing the change, two of its employees compare American Twitter users to Japanese users. The average English-language tweet is 34 characters, they say; the average Japanese-language tweet is only 15. There are specific reasons for this: As a language, Japanese requires many fewer characters per thought; some nouns can be expressed in only one symbol.
“Our research shows us that the character limit is a major cause of frustration for people tweeting in English, but it is not for those tweeting in Japanese,” they say. “Also, in all [language] markets, when people don’t have to cram their thoughts into 140 characters and actually have some to spare, we see more people tweeting—which is awesome!”
If you are a Twitter executive, board member, or long-suffering investor, that does indeed sound awesome.
Is there any forewarning for this?
There’s some. Critics called for it to double its character limit as early as 2011. But from 2010 to 2015, the company’s former chief executive officer, Dick Costolo, seemed to have little appetite to mess with its core product. (It was always unclear if this was because he revered Twitter or because he was never sure how it worked.)
Jack Dorsey, who helped found Twitter, took over the company later that year. He seemed more confident in messing with it.
In early 2016, for instance, he flirted with the idea of allowing 10,000-character addenda to normal-size tweets. This would have essentially given every tweet an optional embedded blog post, mirroring how users can already attach photos, videos, or a poll to their tweet. (I liked this idea, because it would make screenshots of text more accessible to blind tweeters.)
That idea apparently came to naught, but its reception played into the titanic tweets which debuted on Tuesday.
omg tweets are now long enough for signatures!
best regards,
isaac
--
Isaac Hepworth
Product Manager, Google
+1 303 555 4787
What does this mean for thread culture?
Since the election of Donald Trump (roughly), thousands of progressive Twitter users have strung their isolated, anxious thoughts into extended, panicked manifestos by linking them together into “threads.” The company helped promote this behavior by allowing users to string their own tweets together, forming a list of tweets down the page.
This isn’t a new behavior. Twitter users have been doing it since at least 2010. The venture capitalist Marc Andreessen famously made a habit of it during his 2014 Twitter binge. (Back then, the BuzzFeed tech journalist Charlie Warzel dubbed it “tweetstorming.”) Yet its current resurgence is a defining feature of 2017 Twitter—and a threat, by the way, to Medium’s business model.
If director’s-cut tweets roll out to users en masse, I’d expect little change to the length of the longest threads. But perhaps some of the two- or three-tweet threads will be edited down, instead, into 280-character opuses. Concision will live again.
Will this make Twitter better?
It all depends on what you mean by “better.”
Back in 2015, I wrote that Twitter’s greatest cultural ailment (at least for its English-speaking American users) was the onset of metastasized context collapse. Context collapse is what happens when the audience for any online post becomes unstable and untrustworthy—what happens when you don’t know if an offhand Twitter reply, sent to your friend, will wind up on the front page of Breitbart.
This outbreak of context collapse weakened the good faith of Twitter users, I said: It turned the service, which had flourished as something speech-like, where people could have conversations and test out ideas in public; into something print-like, where someone’s tweets were taken as a lasting statement about their core identity.
Then the 2016 election happened—a mass test of internet users’ good faith if there ever was one. And while it’s hard to envision Twitter ever returning to its speechlike roots, the print-like expectations around the service have only hardened. So too have the incentives around setting clear “in” and “out” groups, especially for the service’s highest-profile users in media and entertainment.
As my friend Charlie Loyd put it, in a mock version of Twitter’s terms of service:
2. Content on the Services
You understand and agree to the following stands of content on the Services. Every morning, you will be presented with one of the worst op-eds, blog posts, or other topical views published in your language of choice, anywhere in the world, on the previous day (the “Take”). If you do not tweet about the Take, you implicitly agree with the Take. If you do tweet about the Take, your criticism will be based on one of the following criticisms: (1) that it is neoliberal; or (2) that it is virtue signaling. If at any time you are not addressing a Take, one will be assigned to you in your mentions by an account with a name like @2ndAmdmtLenin.
It’s hard to see XXL tweets making Twitter more speechlike. I anticipate just about the opposite: The prolix tweets will encourage people to attach disclaimers, footnotes, and other dependent clauses to their messages such that tweets are taken as testaments to political identity. But perhaps that’s a good thing: If Twitter is already print-y, why not make it print-ier? (Or, for that matter, more like Facebook statuses? Which is also what this beta test accomplishes.)
That said, Twitter’s most pressing public-relations problem—as far as I can tell—is not context collapse, but the platform’s reputation for abusive unpleasantness and viral controversy. It can’t fix the latter, and it has a long way to go to sufficiently addressing the former.  I doubt its affiliation with President Trump will much improve the perception, popular among Americans of all stripes, that Twitter simply isn’t very fun anymore.
What does it mean for Twitter’s most (in)famous user?
President Trump has not yet commented on the proposed change. However, the president’s distinct philosophy of tweet threading—he sometimes cuts a sentence off at the end of a tweet, then waits minutes to finish the thought—suggests that he bristles at the 140-character count. In July, for instance, he tweeted:
After consultation with my Generals and military experts, please be advised that the United States Government will not accept or allow......
Then he did not immediately supply a direct object. Unnamed Pentagon officials told BuzzFeed News that for several minutes they worried the president was declaring a military action on North Korea. Nine minutes after the original tweet, Trump finished the thought:
....Transgender individuals to serve in any capacity in the U.S. Military. Our military must be focused on decisive and overwhelming.....
....victory and cannot be burdened with the tremendous medical costs and disruption that transgender in the military would entail. Thank you
However, Trump has also touted his skillful use of Twitter’s concision in the past.
“Thanks—many are saying I’m the best 140-character writer in the world,” he tweeted a few days after the 2012 presidential election. “It’s easy when it’s fun.”
He posted a nearly identical thought less than two years later, in 2014. It remains to be seen whether the same people will praise him at 280 characters.
Can you fit an entire Frank O’Hara poem in a tweet now?
Yes, you can. With thanks to Erik Hinton, a designer at The Outline:
Good news! The new Twitter character limit will accommodate the entirety of Frank O'Hara's very appropriate "Spleen". pic.twitter.com/asolmTQPQG
Alas, not all poets are as concise.


There were six hours during the night of April 10, 2014, when the entire population of Washington State had no 911 service. People who called for help got a busy signal. One Seattle woman dialed 911 at least 37 times while a stranger was trying to break into her house. When he finally crawled into her living room through a window, she picked up a kitchen knife. The man fled.
To hear more feature stories, see our full list or get the Audm iPhone app.
The 911 outage, at the time the largest ever reported, was traced to software running on a server in Englewood, Colorado. Operated by a systems provider named Intrado, the server kept a running counter of how many calls it had routed to 911 dispatchers around the country. Intrado programmers had set a threshold for how high the counter could go. They picked a number in the millions.
Shortly before midnight on April 10, the counter exceeded that number, resulting in chaos. Because the counter was used to generate a unique identifier for each call, new calls were rejected. And because the programmers hadn’t anticipated the problem, they hadn’t created alarms to call attention to it. Nobody knew what was happening. Dispatch centers in Washington, California, Florida, the Carolinas, and Minnesota, serving 11 million Americans, struggled to make sense of reports that callers were getting busy signals. It took until morning to realize that Intrado’s software in Englewood was responsible, and that the fix was to change a single number.
Not long ago, emergency calls were handled locally. Outages were small and easily diagnosed and fixed. The rise of cellphones and the promise of new capabilities—what if you could text 911? or send videos to the dispatcher?—drove the development of a more complex system that relied on the internet. For the first time, there could be such a thing as a national 911 outage. There have now been four in as many years.
It’s been said that software is “eating the world.” More and more, critical systems that were once controlled mechanically, or by people, are coming to depend on code. This was perhaps never clearer than in the summer of 2015, when on a single day, United Airlines grounded its fleet because of a problem with its departure-management system; trading was suspended on the New York Stock Exchange after an upgrade; the front page of The Wall Street Journal’s website crashed; and Seattle’s 911 system went down again, this time because a different router failed. The simultaneous failure of so many software systems smelled at first of a coordinated cyberattack. Almost more frightening was the realization, late in the day, that it was just a coincidence.
“When we had electromechanical systems, we used to be able to test them exhaustively,” says Nancy Leveson, a professor of aeronautics and astronautics at the Massachusetts Institute of Technology who has been studying software safety for 35 years. She became known for her report on the Therac-25, a radiation-therapy machine that killed six patients because of a software error. “We used to be able to think through all the things it could do, all the states it could get into.” The electromechanical interlockings that controlled train movements at railroad crossings, for instance, only had so many configurations; a few sheets of paper could describe the whole system, and you could run physical trains against each configuration to see how it would behave. Once you’d built and tested it, you knew exactly what you were dealing with.
Software is different. Just by editing the text in a file somewhere, the same hunk of silicon can become an autopilot or an inventory-control system. This flexibility is software’s miracle, and its curse. Because it can be changed cheaply, software is constantly changed; and because it’s unmoored from anything physical—a program that is a thousand times more complex than another takes up the same actual space—it tends to grow without bound. “The problem,” Leveson wrote in a book, “is that we are attempting to build systems that are beyond our ability to intellectually manage.”
Our standard framework for thinking about engineering failures—reflected, for instance, in regulations for medical devices—was developed shortly after World War II, before the advent of software, for electromechanical systems. The idea was that you make something reliable by making its parts reliable (say, you build your engine to withstand 40,000 takeoff-and-landing cycles) and by planning for the breakdown of those parts (you have two engines). But software doesn’t break. Intrado’s faulty threshold is not like the faulty rivet that leads to the crash of an airliner. The software did exactly what it was told to do. In fact it did it perfectly. The reason it failed is that it was told to do the wrong thing. Software failures are failures of understanding, and of imagination. Intrado actually had a backup router, which, had it been switched to automatically, would have restored 911 service almost immediately. But, as described in a report to the FCC, “the situation occurred at a point in the application logic that was not designed to perform any automated corrective actions.”
This is the trouble with making things out of code, as opposed to something physical. “The complexity,” as Leveson puts it, “is invisible to the eye.”
The attempts now underway to change how we make software all seem to start with the same premise: Code is too hard to think about. Before trying to understand the attempts themselves, then, it’s worth understanding why this might be: what it is about code that makes it so foreign to the mind, and so unlike anything that came before it.
Technological progress used to change the way the world looked—you could watch the roads getting paved; you could see the skylines rise. Today you can hardly tell when something is remade, because so often it is remade by code. When you press your foot down on your car’s accelerator, for instance, you’re no longer controlling anything directly; there’s no mechanical link from the pedal to the throttle. Instead, you’re issuing a command to a piece of software that decides how much air to give the engine. The car is a computer you can sit inside of. The steering wheel and pedals might as well be keyboard keys.
Like everything else, the car has been computerized to enable new features. When a program is in charge of the throttle and brakes, it can slow you down when you’re too close to another car, or precisely control the fuel injection to help you save on gas. When it controls the steering, it can keep you in your lane as you start to drift, or guide you into a parking space. You couldn’t build these features without code. If you tried, a car might weigh 40,000 pounds, an immovable mass of clockwork.
Software has enabled us to make the most intricate machines that have ever existed. And yet we have hardly noticed, because all of that complexity is packed into tiny silicon chips as millions and millions of lines of code. But just because we can’t see the complexity doesn’t mean that it has gone away.
The programmer, the renowned Dutch computer scientist Edsger Dijkstra wrote in 1988, “has to be able to think in terms of conceptual hierarchies that are much deeper than a single mind ever needed to face before.” Dijkstra meant this as a warning. As programmers eagerly poured software into critical systems, they became, more and more, the linchpins of the built world—and Dijkstra thought they had perhaps overestimated themselves.
What made programming so difficult was that it required you to think like a computer. The strangeness of it was in some sense more vivid in the early days of computing, when code took the form of literal ones and zeros. Anyone looking over a programmer’s shoulder as they pored over line after line like “100001010011” and “000010011110” would have seen just how alienated the programmer was from the actual problems they were trying to solve; it would have been impossible to tell whether they were trying to calculate artillery trajectories or simulate a game of tic-tac-toe. The introduction of programming languages like Fortran and C, which resemble English, and tools, known as “integrated development environments,” or IDEs, that help correct simple mistakes (like Microsoft Word’s grammar checker but for code), obscured, though did little to actually change, this basic alienation—the fact that the programmer didn’t work on a problem directly, but rather spent their days writing out instructions for a machine.
“The problem is that software engineers don’t understand the problem they’re trying to solve, and don’t care to,” says Leveson, the MIT software-safety expert. The reason is that they’re too wrapped up in getting their code to work. “Software engineers like to provide all kinds of tools and stuff for coding errors,” she says, referring to IDEs. “The serious problems that have happened with software have to do with requirements, not coding errors.” When you’re writing code that controls a car’s throttle, for instance, what’s important is the rules about when and how and by how much to open it. But these systems have become so complicated that hardly anyone can keep them straight in their head. “There’s 100 million lines of code in cars now,” Leveson says. “You just cannot anticipate all these things.”
In September 2007, Jean Bookout was driving on the highway with her best friend in a Toyota Camry when the accelerator seemed to get stuck. When she took her foot off the pedal, the car didn’t slow down. She tried the brakes but they seemed to have lost their power. As she swerved toward an off-ramp going 50 miles per hour, she pulled the emergency brake. The car left a skid mark 150 feet long before running into an embankment by the side of the road. The passenger was killed. Bookout woke up in a hospital a month later.
The incident was one of many in a nearly decade-long investigation into claims of so-called unintended acceleration in Toyota cars. Toyota blamed the incidents on poorly designed floor mats, “sticky” pedals, and driver error, but outsiders suspected that faulty software might be responsible. The National Highway Traffic Safety Administration enlisted software experts from NASA to perform an intensive review of Toyota’s code. After nearly 10 months, the NASA team hadn’t found evidence that software was the cause—but said they couldn’t prove it wasn’t.
It was during litigation of the Bookout accident that someone finally found a convincing connection. Michael Barr, an expert witness for the plaintiff, had a team of software experts spend 18 months with the Toyota code, picking up where NASA left off. Barr described what they found as “spaghetti code,” programmer lingo for software that has become a tangled mess. Code turns to spaghetti when it accretes over many years, with feature after feature piling on top of, and being woven around, what’s already there; eventually the code becomes impossible to follow, let alone to test exhaustively for flaws.
Using the same model as the Camry involved in the accident, Barr’s team demonstrated that there were more than 10 million ways for key tasks on the onboard computer to fail, potentially leading to unintended acceleration.* They showed that as little as a single bit flip—a one in the computer’s memory becoming a zero or vice versa—could make a car run out of control. The fail-safe code that Toyota had put in place wasn’t enough to stop it. “You have software watching the software,” Barr testified. “If the software malfunctions and the same program or same app that is crashed is supposed to save the day, it can’t save the day because it is not working.”
Barr’s testimony made the case for the plaintiff, resulting in $3 million in damages for Bookout and her friend’s family. According to The New York Times, it was the first of many similar cases against Toyota to bring to trial problems with the electronic throttle-control system, and the first time Toyota was found responsible by a jury for an accident involving unintended acceleration. The parties decided to settle the case before punitive damages could be awarded. In all, Toyota recalled more than 9 million cars, and paid nearly $3 billion in settlements and fines related to unintended acceleration.
There will be more bad days for software. It's important that we get better at making it, because if we don't, and as software becomes more sophisticated and connected—as it takes control of more critical functions—those days could get worse.
The problem is that programmers are having a hard time keeping up with their own creations. Since the 1980s, the way programmers work and the tools they use have changed remarkably little. There is a small but growing chorus that worries the status quo is unsustainable. “Even very good programmers are struggling to make sense of the systems that they are working with,” says Chris Granger, a software developer who worked as a lead at Microsoft on Visual Studio, an IDE that costs $1,199 a year and is used by nearly a third of all professional programmers. He told me that while he was at Microsoft, he arranged an end-to-end study of Visual Studio, the only one that had ever been done. For a month and a half, he watched behind a one-way mirror as people wrote code. “How do they use tools? How do they think?” he said. “How do they sit at the computer, do they touch the mouse, do they not touch the mouse? All these things that we have dogma around that we haven’t actually tested empirically.”
The findings surprised him. “Visual Studio is one of the single largest pieces of software in the world,” he said. “It’s over 55 million lines of code. And one of the things that I found out in this study is more than 98 percent of it is completely irrelevant. All this work had been put into this thing, but it missed the fundamental problems that people faced. And the biggest one that I took away from it was that basically people are playing computer inside their head.” Programmers were like chess players trying to play with a blindfold on—so much of their mental energy is spent just trying to picture where the pieces are that there’s hardly any left over to think about the game itself.
John Resig had been noticing the same thing among his students. Resig is a celebrated programmer of JavaScript—software he wrote powers over half of all websites—and a tech lead at the online-education site Khan Academy. In early 2012, he had been struggling with the site’s computer-science curriculum. Why was it so hard to learn to program? The essential problem seemed to be that code was so abstract. Writing software was not like making a bridge out of popsicle sticks, where you could see the sticks and touch the glue. To “make” a program, you typed words. When you wanted to change the behavior of the program, be it a game, or a website, or a simulation of physics, what you actually changed was text. So the students who did well—in fact the only ones who survived at all—were those who could step through that text one instruction at a time in their head, thinking the way a computer would, trying to keep track of every intermediate calculation. Resig, like Granger, started to wonder if it had to be that way. Computers had doubled in power every 18 months for the last 40 years. Why hadn’t programming changed?
The fact that the two of them were thinking about the same problem in the same terms, at the same time, was not a coincidence. They had both just seen the same remarkable talk, given to a group of software-engineering students in a Montreal hotel by a computer researcher named Bret Victor. The talk, which went viral when it was posted online in February 2012, seemed to be making two bold claims. The first was that the way we make software is fundamentally broken. The second was that Victor knew how to fix it.
Bret Victor does not like to write code. “It sounds weird,” he says. “When I want to make a thing, especially when I want to create something in software, there’s this initial layer of disgust that I have to push through, where I’m not manipulating the thing that I want to make, I’m writing a bunch of text into a text editor.”
“There’s a pretty strong conviction that that’s the wrong way of doing things.”
Victor has the mien of David Foster Wallace, with a lightning intelligence that lingers beneath a patina of aw-shucks shyness. He is 40 years old, with traces of gray and a thin, undeliberate beard. His voice is gentle, mournful almost, but he wants to share what’s in his head, and when he gets on a roll he’ll seem to skip syllables, as though outrunning his own vocal machinery.
Though he runs a lab that studies the future of computing, he seems less interested in technology per se than in the minds of the people who use it. Like any good toolmaker, he has a way of looking at the world that is equal parts technical and humane. He graduated top of his class at the California Institute of Technology for electrical engineering, and then went on, after grad school at the University of California, Berkeley, to work at a company that develops music synthesizers. It was a problem perfectly matched to his dual personality: He could spend as much time thinking about the way a performer makes music with a keyboard—the way it becomes an extension of their hands—as he could thinking about the mathematics of digital signal processing.
By the time he gave the talk that made his name, the one that Resig and Granger saw in early 2012, Victor had finally landed upon the principle that seemed to thread through all of his work. (He actually called the talk “Inventing on Principle.”) The principle was this: “Creators need an immediate connection to what they’re creating.” The problem with programming was that it violated the principle. That’s why software systems were so hard to think about, and so rife with bugs: The programmer, staring at a page of text, was abstracted from whatever it was they were actually making.
“Our current conception of what a computer program is,” he said, is “derived straight from Fortran and ALGOL in the late ’50s. Those languages were designed for punch cards.” That code now takes the form of letters on a screen in a language like C or Java (derivatives of Fortran and ALGOL), instead of a stack of cards with holes in it, doesn’t make it any less dead, any less indirect.
There is an analogy to word processing. It used to be that all you could see in a program for writing documents was the text itself, and to change the layout or font or margins, you had to write special “control codes,” or commands that would tell the computer that, for instance, “this part of the text should be in italics.” The trouble was that you couldn’t see the effect of those codes until you printed the document. It was hard to predict what you were going to get. You had to imagine how the codes were going to be interpreted by the computer—that is, you had to play computer in your head.
Then WYSIWYG (pronounced “wizzywig”) came along. It stood for “What You See Is What You Get.” When you marked a passage as being in italics, the letters tilted right there on the screen. If you wanted to change the margin, you could drag a ruler at the top of the screen—and see the effect of that change. The document thereby came to feel like something real, something you could poke and prod at. Just by looking you could tell if you’d done something wrong. Control of a sophisticated system—the document’s layout and formatting engine—was made accessible to anyone who could click around on a page.
Victor’s point was that programming itself should be like that. For him, the idea that people were doing important work, like designing adaptive cruise-control systems or trying to understand cancer, by staring at a text editor, was appalling. And it was the proper job of programmers to ensure that someday they wouldn’t have to.
There was precedent enough to suggest that this wasn’t a crazy idea. Photoshop, for instance, puts powerful image-processing algorithms in the hands of people who might not even know what an algorithm is. It’s a complicated piece of software, but complicated in the way a good synth is complicated, with knobs and buttons and sliders that the user learns to play like an instrument. Squarespace, a company that is perhaps best known for advertising aggressively on podcasts, makes a tool that lets users build websites by pointing and clicking, instead of by writing code in HTML and CSS. It is powerful enough to do work that once would have been done by a professional web designer.
But those were just a handful of examples. The overwhelming reality was that when someone wanted to do something interesting with a computer, they had to write code. Victor, who is something of an idealist, saw this not so much as an opportunity but as a moral failing of programmers at large. His talk was a call to arms.
At the heart of it was a series of demos that tried to show just how primitive the available tools were for various problems—circuit design, computer animation, debugging algorithms—and what better ones might look like. His demos were virtuosic. The one that captured everyone’s imagination was, ironically enough, the one that on its face was the most trivial. It showed a split screen with a game that looked like Mario on one side and the code that controlled it on the other. As Victor changed the code, things in the game world changed: He decreased one number, the strength of gravity, and the Mario character floated; he increased another, the player’s speed, and Mario raced across the screen.
Suppose you wanted to design a level where Mario, jumping and bouncing off of a turtle, would just make it into a small passageway. Game programmers were used to solving this kind of problem in two stages: First, you stared at your code—the code controlling how high Mario jumped, how fast he ran, how bouncy the turtle’s back was—and made some changes to it in your text editor, using your imagination to predict what effect they’d have. Then, you’d replay the game to see what actually happened.
Victor wanted something more immediate. “If you have a process in time,” he said, referring to Mario’s path through the level, “and you want to see changes immediately, you have to map time to space.” He hit a button that showed not just where Mario was right now, but where he would be at every moment in the future: a curve of shadow Marios stretching off into the far distance. What’s more, this projected path was reactive: When Victor changed the game’s parameters, now controlled by a quick drag of the mouse, the path’s shape changed. It was like having a god’s-eye view of the game. The whole problem had been reduced to playing with different parameters, as if adjusting levels on a stereo receiver, until you got Mario to thread the needle. With the right interface, it was almost as if you weren’t working with code at all; you were manipulating the game’s behavior directly.
When the audience first saw this in action, they literally gasped. They knew they weren’t looking at a kid’s game, but rather the future of their industry. Most software involved behavior that unfolded, in complex ways, over time, and Victor had shown that if you were imaginative enough, you could develop ways to see that behavior and change it, as if playing with it in your hands. One programmer who saw the talk wrote later: “Suddenly all of my tools feel obsolete.”
When John Resig saw the “Inventing on Principle” talk, he scrapped his plans for the Khan Academy programming curriculum. He wanted the site’s programming exercises to work just like Victor’s demos. On the left-hand side you’d have the code, and on the right, the running program: a picture or game or simulation. If you changed the code, it’d instantly change the picture. “In an environment that is truly responsive,” Resig wrote about the approach, “you can completely change the model of how a student learns ... [They] can now immediately see the result and intuit how underlying systems inherently work without ever following an explicit explanation.” Khan Academy has become perhaps the largest computer-programming class in the world, with a million students, on average, actively using the program each month.
Chris Granger, who had worked at Microsoft on Visual Studio, was likewise inspired. Within days of seeing a video of Victor’s talk, in January of 2012, he built a prototype of a new programming environment. Its key capability was that it would give you instant feedback on your program’s behavior. You’d see what your system was doing right next to the code that controlled it. It was like taking off a blindfold. Granger called the project “Light Table.”
In April of 2012, he sought funding for Light Table on Kickstarter. In programming circles, it was a sensation. Within a month, the project raised more than $200,000. The ideas spread. The notion of liveness, of being able to see data flowing through your program instantly, made its way into flagship programming tools offered by Google and Apple. The default language for making new iPhone and Mac apps, called Swift, was developed by Apple from the ground up to support an environment, called Playgrounds, that was directly inspired by Light Table.
But seeing the impact that his talk ended up having, Bret Victor was disillusioned. “A lot of those things seemed like misinterpretations of what I was saying,” he said later. He knew something was wrong when people began to invite him to conferences to talk about programming tools. “Everyone thought I was interested in programming environments,” he said. Really he was interested in how people see and understand systems—as he puts it, in the “visual representation of dynamic behavior.” Although code had increasingly become the tool of choice for creating dynamic behavior, it remained one of the worst tools for understanding it. The point of “Inventing on Principle” was to show that you could mitigate that problem by making the connection between a system’s behavior and its code immediate.
In a pair of later talks, “Stop Drawing Dead Fish” and “Drawing Dynamic Visualizations,” Victor went one further. He demoed two programs he’d built—the first for animators, the second for scientists trying to visualize their data—each of which took a process that used to involve writing lots of custom code and reduced it to playing around in a WYSIWYG interface. Victor suggested that the same trick could be pulled for nearly every problem where code was being written today. “I’m not sure that programming has to exist at all,” he told me. “Or at least software developers.” In his mind, a software developer’s proper role was to create tools that removed the need for software developers. Only then would people with the most urgent computational problems be able to grasp those problems directly, without the intermediate muck of code.
Of course, to do that, you’d have to get programmers themselves on board. In a recent essay, Victor implored professional software developers to stop pouring their talent into tools for building apps like Snapchat and Uber. “The inconveniences of daily life are not the significant problems,” he wrote. Instead, they should focus on scientists and engineers—as he put it to me, “these people that are doing work that actually matters, and critically matters, and using really, really bad tools.” Exciting work of this sort, in particular a class of tools for “model-based design,” was already underway, he wrote, and had been for years, but most programmers knew nothing about it.
“If you really look hard at all the industrial goods that you’ve got out there, that you’re using, that companies are using, the only non-industrial stuff that you have inside this is the code.” Eric Bantégnie is the founder of Esterel Technologies (now owned by ANSYS), a French company that makes tools for building safety-critical software. Like Victor, Bantégnie doesn’t think engineers should develop large systems by typing millions of lines of code into an IDE. “Nobody would build a car by hand,” he says. “Code is still, in many places, handicraft. When you’re crafting manually 10,000 lines of code, that’s okay. But you have systems that have 30 million lines of code, like an Airbus, or 100 million lines of code, like your Tesla or high-end cars—that’s becoming very, very complicated.”
Bantégnie’s company is one of the pioneers in the industrial use of model-based design, in which you no longer write code directly. Instead, you create a kind of flowchart that describes the rules your program should follow (the “model”), and the computer generates code for you based on those rules. If you were making the control system for an elevator, for instance, one rule might be that when the door is open, and someone presses the button for the lobby, you should close the door and start moving the car. In a model-based design tool, you’d represent this rule with a small diagram, as though drawing the logic out on a whiteboard, made of boxes that represent different states—like “door open,” “moving,” and “door closed”—and lines that define how you can get from one state to the other. The diagrams make the system’s rules obvious: Just by looking, you can see that the only way to get the elevator moving is to close the door, or that the only way to get the door open is to stop.
It’s not quite Photoshop. The beauty of Photoshop, of course, is that the picture you’re manipulating on the screen is the final product. In model-based design, by contrast, the picture on your screen is more like a blueprint. Still, making software this way is qualitatively different than traditional programming. In traditional programming, your task is to take complex rules and translate them into code; most of your energy is spent doing the translating, rather than thinking about the rules themselves. In the model-based approach, all you have is the rules. So that’s what you spend your time thinking about. It’s a way of focusing less on the machine and more on the problem you’re trying to get it to solve.
“Typically the main problem with software coding—and I’m a coder myself,” Bantégnie says, “is not the skills of the coders. The people know how to code. The problem is what to code. Because most of the requirements are kind of natural language, ambiguous, and a requirement is never extremely precise, it’s often understood differently by the guy who’s supposed to code.”
On this view, software becomes unruly because the media for describing what software should do—conversations, prose descriptions, drawings on a sheet of paper—are too different from the media describing what software does do, namely, code itself. Too much is lost going from one to the other. The idea behind model-based design is to close the gap. The very same model is used both by system designers to express what they want and by the computer to automatically generate code.
Of course, for this approach to succeed, much of the work has to be done well before the project even begins. Someone first has to build a tool for developing models that are natural for people—that feel just like the notes and drawings they’d make on their own—while still being unambiguous enough for a computer to understand. They have to make a program that turns these models into real code. And finally they have to prove that the generated code will always do what it’s supposed to. “We have benefited from fortunately 20 years of initial background work,” Bantégnie says.
Esterel Technologies, which was acquired by ANSYS in 2012, grew out of research begun in the 1980s by the French nuclear and aerospace industries, who worried that as safety-critical code ballooned in complexity, it was getting harder and harder to keep it free of bugs. “I started in 1988,” says Emmanuel Ledinot, the Head of Scientific Studies for Dassault Aviation, a French manufacturer of fighter jets and business aircraft. “At the time, I was working on military avionics systems. And the people in charge of integrating the systems, and debugging them, had noticed that the number of bugs was increasing.” The 80s had seen a surge in the number of onboard computers on planes. Instead of a single flight computer, there were now dozens, each responsible for highly specialized tasks related to control, navigation, and communications. Coordinating these systems to fly the plane as data poured in from sensors and as pilots entered commands required a symphony of perfectly timed reactions. “The handling of these hundreds of and even thousands of possible events in the right order, at the right time,” Ledinot says, “was diagnosed as the main cause of the bug inflation.”
Ledinot decided that writing such convoluted code by hand was no longer sustainable. It was too hard to understand what it was doing, and almost impossible to verify that it would work correctly. He went looking for something new. “You must understand that to change tools is extremely expensive in a process like this,” he said in a talk. “You don’t take this type of decision unless your back is against the wall.”
He began collaborating with Gerard Berry, a computer scientist at INRIA, the French computing-research center, on a tool called Esterel—a portmanteau of the French for “real-time.” The idea behind Esterel was that while traditional programming languages might be good for describing simple procedures that happened in a predetermined order—like a recipe—if you tried to use them in systems where lots of events could happen at nearly any time, in nearly any order—like in the cockpit of a plane—you inevitably got a mess. And a mess in control software was dangerous. In a paper, Berry went as far as to predict that “low-level programming techniques will not remain acceptable for large safety-critical programs, since they make behavior understanding and analysis almost impracticable.”
Esterel was designed to make the computer handle this complexity for you. That was the promise of the model-based approach: Instead of writing normal programming code, you created a model of the system’s behavior—in this case, a model focused on how individual events should be handled, how to prioritize events, which events depended on which others, and so on. The model becomes the detailed blueprint that the computer would use to do the actual programming.
Ledinot and Berry worked for nearly 10 years to get Esterel to the point where it could be used in production. “It was in 2002 that we had the first operational software-modeling environment with automatic code generation,” Ledinot told me, “and the first embedded module in Rafale, the combat aircraft.” Today, the ANSYS SCADE product family (for “safety-critical application development environment”) is used to generate code by companies in the aerospace and defense industries, in nuclear power plants, transit systems, heavy industry, and medical devices. “My initial dream was to have SCADE-generated code in every plane in the world,” Bantégnie, the founder of Esterel Technologies, says, “and we’re not very far off from that objective.” Nearly all safety-critical code on the Airbus A380, including the system controlling the plane’s flight surfaces, was generated with ANSYS SCADE products.
Part of the draw for customers, especially in aviation, is that while it is possible to build highly reliable software by hand, it can be a Herculean effort. Ravi Shivappa, the VP of group software engineering at Meggitt PLC, an ANSYS customer which builds components for airplanes, like pneumatic fire detectors for engines, explains that traditional projects begin with a massive requirements document in English, which specifies everything the software should do. (A requirement might be something like, “When the pressure in this section rises above a threshold, open the safety valve, unless the manual-override switch is turned on.”) The problem with describing the requirements this way is that when you implement them in code, you have to painstakingly check that each one is satisfied. And when the customer changes the requirements, the code has to be changed, too, and tested extensively to make sure that nothing else was broken in the process.
The cost is compounded by exacting regulatory standards. The FAA is fanatical about software safety. The agency mandates that every requirement for a piece of safety-critical software be traceable to the lines of code that implement it, and vice versa. So every time a line of code changes, it must be retraced to the corresponding requirement in the design document, and you must be able to demonstrate that the code actually satisfies the requirement. The idea is that if something goes wrong, you’re able to figure out why; the practice brings order and accountability to large codebases. But, Shivappa says, “it’s a very labor-intensive process.” He estimates that before they used model-based design, on a two-year-long project only two to three months was spent writing code—the rest was spent working on the documentation.
As Bantégnie explains, the beauty of having a computer turn your requirements into code, rather than a human, is that you can be sure—in fact you can mathematically prove—that the generated code actually satisfies those requirements. Much of the benefit of the model-based approach comes from being able to add requirements on the fly while still ensuring that existing ones are met; with every change, the computer can verify that your program still works. You’re free to tweak your blueprint without fear of introducing new bugs. Your code is, in FAA parlance, “correct by construction.”
Still, most software, even in the safety-obsessed world of aviation, is made the old-fashioned way, with engineers writing their requirements in prose and programmers coding them up in a programming language like C. As Bret Victor made clear in his essay, model-based design is relatively unusual. “A lot of people in the FAA think code generation is magic, and hence call for greater scrutiny,” Shivappa told me.
Most programmers feel the same way. They like code. At least they understand it. Tools that write your code for you and verify its correctness using the mathematics of “finite-state machines” and “recurrent systems” sound esoteric and hard to use, if not just too good to be true.
It is a pattern that has played itself out before. Whenever programming has taken a step away from the writing of literal ones and zeros, the loudest objections have come from programmers. Margaret Hamilton, a celebrated software engineer on the Apollo missions—in fact the coiner of the phrase “software engineering”—told me that during her first year at the Draper lab at MIT, in 1964, she remembers a meeting where one faction was fighting the other about transitioning away from “some very low machine language,” as close to ones and zeros as you could get, to “assembly language.” “The people at the lowest level were fighting to keep it. And the arguments were so similar: ‘Well how do we know assembly language is going to do it right?’”
“Guys on one side, their faces got red, and they started screaming,” she said. She said she was “amazed how emotional they got.”
Emmanuel Ledinot, of Dassault Aviation, pointed out that when assembly language was itself phased out in favor of the programming languages still popular today, like C, it was the assembly programmers who were skeptical this time. No wonder, he said, that “people are not so easily transitioning to model-based software development: They perceive it as another opportunity to lose control, even more than they have already.”
The bias against model-based design, sometimes known as model-driven engineering, or MDE, is in fact so ingrained that according to a recent paper, “Some even argue that there is a stronger need to investigate people’s perception of MDE than to research new MDE technologies.”
Which sounds almost like a joke, but for proponents of the model-based approach, it’s an important point: We already know how to make complex software reliable, but in so many places, we’re choosing not to. Why?
In 2011, Chris Newcombe had been working at Amazon for almost seven years, and had risen to be a principal engineer. He had worked on some of the company’s most critical systems, including the retail-product catalog and the infrastructure that managed every Kindle device in the world. He was a leader on the highly prized Amazon Web Services team, which maintains cloud servers for some of the web’s biggest properties, like Netflix, Pinterest, and Reddit. Before Amazon, he’d helped build the backbone of Steam, the world’s largest online-gaming service. He is one of those engineers whose work quietly keeps the internet running. The products he’d worked on were considered massive successes. But all he could think about was that buried deep in the designs of those systems were disasters waiting to happen.
“Human intuition is poor at estimating the true probability of supposedly ‘extremely rare’ combinations of events in systems operating at a scale of millions of requests per second,” he wrote in a paper. “That human fallibility means that some of the more subtle, dangerous bugs turn out to be errors in design; the code faithfully implements the intended design, but the design fails to correctly handle a particular ‘rare’ scenario.”
Newcombe was convinced that the algorithms behind truly critical systems—systems storing a significant portion of the web’s data, for instance—ought to be not just good, but perfect. A single subtle bug could be catastrophic. But he knew how hard bugs were to find, especially as an algorithm grew more complex. You could do all the testing you wanted and you’d never find them all.
This is why he was so intrigued when, in the appendix of a paper he’d been reading, he came across a strange mixture of math and code—or what looked like code—that described an algorithm in something called “TLA+.” The surprising part was that this description was said to be mathematically precise: An algorithm written in TLA+ could in principle be proven correct. In practice, it allowed you to create a realistic model of your problem and test it not just thoroughly, but exhaustively. This was exactly what he’d been looking for: a language for writing perfect algorithms.
TLA+, which stands for “Temporal Logic of Actions,” is similar in spirit to model-based design: It’s a language for writing down the requirements—TLA+ calls them “specifications”—of computer programs. These specifications can then be completely verified by a computer. That is, before you write any code, you write a concise outline of your program’s logic, along with the constraints you need it to satisfy (say, if you were programming an ATM, a constraint might be that you can never withdraw the same money twice from your checking account). TLA+ then exhaustively checks that your logic does, in fact, satisfy those constraints. If not, it will show you exactly how they could be violated.
The language was invented by Leslie Lamport, a Turing Award–winning computer scientist. With a big white beard and scruffy white hair, and kind eyes behind large glasses, Lamport looks like he might be one of the friendlier professors at the American Hogwarts. Now at Microsoft Research, he is known as one of the pioneers of the theory of “distributed systems,” which describes any computer system made of multiple parts that communicate with each other. Lamport’s work laid the foundation for many of the systems that power the modern web.
For Lamport, a major reason today’s software is so full of bugs is that programmers jump straight into writing code. “Architects draw detailed plans before a brick is laid or a nail is hammered,” he wrote in an article. “But few programmers write even a rough sketch of what their programs will do before they start coding.” Programmers are drawn to the nitty-gritty of coding because code is what makes programs go; spending time on anything else can seem like a distraction. And there is a patient joy, a meditative kind of satisfaction, to be had from puzzling out the micro-mechanics of code. But code, Lamport argues, was never meant to be a medium for thought. “It really does constrain your ability to think when you’re thinking in terms of a programming language,” he says. Code makes you miss the forest for the trees: It draws your attention to the working of individual pieces, rather than to the bigger picture of how your program fits together, or what it’s supposed to do—and whether it actually does what you think. This is why Lamport created TLA+. As with model-based design, TLA+ draws your focus to the high-level structure of a system, its essential logic, rather than to the code that implements it.
Newcombe and his colleagues at Amazon would go on to use TLA+ to find subtle, critical bugs in major systems, including bugs in the core algorithms behind S3, regarded as perhaps the most reliable storage engine in the world. It is now used widely at the company. In the tiny universe of people who had ever used TLA+, their success was not so unusual. An intern at Microsoft used TLA+ to catch a bug that could have caused every Xbox in the world to crash after four hours of use. Engineers at the European Space Agency used it to rewrite, with 10 times less code, the operating system of a probe that was the first to ever land softly on a comet. Intel uses it regularly to verify its chips.
But TLA+ occupies just a small, far corner of the mainstream, if it can be said to take up any space there at all. Even to a seasoned engineer like Newcombe, the language read at first as bizarre and esoteric—a zoo of symbols. For Lamport, this is a failure of education. Though programming was born in mathematics, it has since largely been divorced from it. Most programmers aren’t very fluent in the kind of math—logic and set theory, mostly—that you need to work with TLA+. “Very few programmers—and including very few teachers of programming—understand the very basic concepts and how they’re applied in practice. And they seem to think that all they need is code,” Lamport says. “The idea that there’s some higher level than the code in which you need to be able to think precisely, and that mathematics actually allows you to think precisely about it, is just completely foreign. Because they never learned it.”
Lamport sees this failure to think mathematically about what they’re doing as the problem of modern software development in a nutshell: The stakes keep rising, but programmers aren’t stepping up—they haven’t developed the chops required to handle increasingly complex problems. “In the 15th century,” he said, “people used to build cathedrals without knowing calculus, and nowadays I don’t think you’d allow anyone to build a cathedral without knowing calculus. And I would hope that after some suitably long period of time, people won’t be allowed to write programs if they don’t understand these simple things.”
Newcombe isn’t so sure that it’s the programmer who is to blame. “I’ve heard from Leslie that he thinks programmers are afraid of math. I’ve found that programmers aren’t aware—or don’t believe—that math can help them handle complexity. Complexity is the biggest challenge for programmers.” The real problem in getting people to use TLA+, he said, was convincing them it wouldn’t be a waste of their time. Programmers, as a species, are relentlessly pragmatic. Tools like TLA+ reek of the ivory tower. When programmers encounter “formal methods” (so called because they involve mathematical, “formally” precise descriptions of programs), their deep-seated instinct is to recoil.
Most programmers who took computer science in college have briefly encountered formal methods. Usually they’re demonstrated on something trivial, like a program that counts up from zero; the student’s job is to mathematically prove that the program does, in fact, count up from zero.
“I needed to change people’s perceptions on what formal methods were,” Newcombe told me. Even Lamport himself didn’t seem to fully grasp this point: Formal methods had an image problem. And the way to fix it wasn’t to implore programmers to change—it was to change yourself. Newcombe realized that to bring tools like TLA+ to the programming mainstream, you had to start speaking their language.
For one thing, he said that when he was introducing colleagues at Amazon to TLA+ he would avoid telling them what it stood for, because he was afraid the name made it seem unnecessarily forbidding: “Temporal Logic of Actions” has exactly the kind of highfalutin ring to it that plays well in academia, but puts off most practicing programmers. He tried also not to use the terms “formal,” “verification,” or “proof,” which reminded programmers of tedious classroom exercises. Instead, he presented TLA+ as a new kind of “pseudocode,” a stepping-stone to real code that allowed you to exhaustively test your algorithms—and that got you thinking precisely early on in the design process. “Engineers think in terms of debugging rather than ‘verification,’” he wrote, so he titled his internal talk on the subject to fellow Amazon engineers “Debugging Designs.” Rather than bemoan the fact that programmers see the world in code, Newcombe embraced it. He knew he’d lose them otherwise. “I’ve had a bunch of people say, ‘Now I get it,’” Newcombe says.
He has since left Amazon for Oracle, where he’s been able to convince his new colleagues to give TLA+ a try. For him, using these tools is now a matter of responsibility. “We need to get better at this,” he said.
“I’m self-taught, been coding since I was nine, so my instincts were to start coding. That was my only—that was my way of thinking: You’d sketch something, try something, you’d organically evolve it.” In his view, this is what many programmers today still do. “They google, and they look on Stack Overflow” (a popular website where programmers answer each other’s technical questions) “and they get snippets of code to solve their tactical concern in this little function, and they glue it together, and iterate.”
“And that’s completely fine until you run smack into a real problem.”
In the summer of 2015, a pair of American security researchers, Charlie Miller and Chris Valasek, convinced that car manufacturers weren’t taking software flaws seriously enough, demonstrated that a 2014 Jeep Cherokee could be remotely controlled by hackers. They took advantage of the fact that the car’s entertainment system, which has a cellular connection (so that, for instance, you can start your car with your iPhone), was connected to more central systems, like the one that controls the windshield wipers, steering, acceleration, and brakes (so that, for instance, you can see guidelines on the rearview screen that respond as you turn the wheel). As proof of their attack, which they developed on nights and weekends, they hacked into Miller’s car while a journalist was driving it on the highway, and made it go haywire; the journalist, who knew what was coming, panicked when they cut the engines, forcing him to a slow crawl on a stretch of road with no shoulder to escape to.
Although they didn’t actually create one, they showed that it was possible to write a clever piece of software, a “vehicle worm,” that would use the onboard computer of a hacked Jeep Cherokee to scan for and hack others; had they wanted to, they could have had simultaneous access to a nationwide fleet of vulnerable cars and SUVs. (There were at least five Fiat Chrysler models affected, including the Jeep Cherokee.) One day they could have told them all to, say, suddenly veer left or cut the engines at high speed.
“We need to think about software differently,” Valasek told me. Car companies have long assembled their final product from parts made by hundreds of different suppliers. But where those parts were once purely mechanical, they now, as often as not, come with millions of lines of code. And while some of this code—for adaptive cruise control, for auto braking and lane assist—has indeed made cars safer (“The safety features on my Jeep have already saved me countless times,” says Miller), it has also created a level of complexity that is entirely new. And it has made possible a new kind of failure.
“There are lots of bugs in cars,” Gerard Berry, the French researcher behind Esterel, said in a talk. “It’s not like avionics—in avionics it’s taken very seriously. And it’s admitted that software is different from mechanics.” The automotive industry is perhaps among those that haven’t yet realized they are actually in the software business.
“We don’t in the automaker industry have a regulator for software safety that knows what it’s doing,” says Michael Barr, the software expert who testified in the Toyota case. NHTSA, he says, “has only limited software expertise. They’ve come at this from a mechanical history.” The same regulatory pressures that have made model-based design and code generation attractive to the aviation industry have been slower to come to car manufacturing. Emmanuel Ledinot, of Dassault Aviation, speculates that there might be economic reasons for the difference, too. Automakers simply can’t afford to increase the price of a component by even a few cents, since it is multiplied so many millionfold; the computers embedded in cars therefore have to be slimmed down to the bare minimum, with little room to run code that hasn’t been hand-tuned to be as lean as possible. “Introducing model-based software development was, I think, for the last decade, too costly for them.”
One suspects the incentives are changing. “I think the autonomous car might push them,” Ledinot told me—“ISO 26262 and the autonomous car might slowly push them to adopt this kind of approach on critical parts.” (ISO 26262 is a safety standard for cars published in 2011.) Barr said much the same thing: In the world of the self-driving car, software can’t be an afterthought. It can’t be built like today’s airline-reservation systems or 911 systems or stock-trading systems. Code will be put in charge of hundreds of millions of lives on the road and it has to work. That is no small task.
“Computing is fundamentally invisible,” Gerard Berry said in his talk. “When your tires are flat, you look at your tires, they are flat. When your software is broken, you look at your software, you see nothing.”
“So that’s a big problem.”
* This article originally stated that there were 10 million ways for the Toyota Camry to cause unintended acceleration. We regret the error.


Perhaps the grimmest aspect of the ongoing emergency in Puerto Rico is that the knowledge of the aftermath of Hurricane Maria—including 10 deaths, the devastation of entire swathes of the island, and dire shortages of food, water, and fuel—come from the teaspoonfuls of information that have dribbled out of the island. Most places don’t have power, and won’t for weeks, if not months. Less than 300 of the island’s 1,600 cellphone towers are functional, 85 percent of all above-ground cable and phone infrastructure was knocked out, and the more remote areas that make up the majority of Puerto Rico’s landmass have mostly gone dark, with brief calls from satellite phones providing sporadic updates. There’s no way to know for certain how bad the crisis is, and that dearth of information in turn has become part of the crisis itself.
For the millions of people in the Puerto Rican diaspora, both on the island and on the mainland, the lack of information is one of the most disruptive parts of the disaster. For people living on the island, it’s often impossible to get word out as conditions deteriorate, to know when aid might be coming, or to coordinate the delivery and access to life-saving services. Their relatives scattered across the United States and elsewhere have been left entirely in the dark as to the status of family members. While there’s no shortage of stories on the long-term implications of Hurricanes Irma and Maria on Puerto Rico and its existing humanitarian problems, at the moment many Puerto Ricans haven’t yet settled a more basic question: Who made it through the storm and its aftermath?
On the mainland, Puerto Ricans are still waiting for any news about loved ones on the island. For Alexandra Gates, a graduate student at the University of Chicago, communication with extended family in Puerto Rico has been limited. “We have literally gotten two text messages and one or two very spotty phone calls from them,” Gates says. “All they were saying is 'hey we're alive,’ and that's it. Then it all goes dark again.” Since most wi-fi on the island is down, the WhatsApp groups that the family usually uses to keep tabs seamlessly with people on both sides of the Caribbean have mostly gone dark, and family members have had to rely on pilgrimages to working cell towers in order to send brief status updates by text.
For cousins in or near the capital of San Juan, this is an inconvenient, but not terribly difficult proposition. For cousins further out in more remote areas, updating family means using limited gasoline supplies to take long trips across washed out roads, mud, and debris in order to stand in line outside a working tower.
But in more remote areas, even those options aren’t available. The tiny, rural island municipality of Vieques, which used to house a Navy munitions test facility, only had tenuous infrastructure connections to the larger island before it suffered a direct hit from Maria. Now, according to residents, the only power on Vieques is supplied by generators, which must themselves be resupplied regularly with gasoline from the main island, which itself might face gasoline supply problems in the near future. Power isn’t expected to be restored for months, and most telephone lines and trees on the island are down. Cell service hasn’t been working, and only a handful of satellite phones—including some owned and operated by FEMA officials—are available to provide updates to the authorities or to families. According to Steven Mueller, a resident of Vieques currently working out of Washington, D.C., “they're entirely dependent on one power line and one water line that we have to the main island.”
Still, residents of Vieques are working to get around those barriers. The day after Hurricane Maria hit, Vieques resident Brittany Roush began using Facebook to coordinate communications and aid. “This effort started with the Facebook page Vieques Peeps that most residents use to connect with each other,” Roush says. “The morning after the storm, there were a few hours where people had lost contact and frantic posts were starting to pile up.” Roush and friends who were not on Vieques when it was hit began taking calls and posts about family members and working to provide up-to-date information.
Those efforts evolved into the ViequesLove project, which Roush runs along with Mueller, her husband, and which has raised over $200,000 via GoFundMe. The group has worked to maintain a list of people verified as safe on Vieques by communicating with the few residents with satellite phones, having coordinated supply runs with government officials, and on Sunday and Monday managing to charter a plane to drop off more supplies on the island. Those flights helped resupply the hospital, where doctors and nurses are currently operating in tents, relying on a gasoline stockpile that can keep generators going for five days.
Although FEMA and the Red Cross are present on Vieques, as they are across Puerto Rico, with federal assets often slow to arrive and difficult to manage, social media and efforts by concerned citizens have been vital to keeping lines of communication open and coordinating relief. The “Puerto Rico Maria Updates” Facebook page boasts over 150,000 members, and has developed in a matter of days into perhaps the quickest real-time news service about developments in Puerto Rico, as well as one of the primary ways for Puerto Ricans on the mainland or in different municipalities to find out about relatives elsewhere.
Patricia Pichardo, who was born and raised in Puerto Rico, but lives and works in Atlanta, Georgia, started the Puerto Rico Maria Updates group the morning after Maria made landfall. “Wednesday morning when they were hit, I was expecting for communications to be lost, but not right away,” Pichardo says. “But the news outlets were only communicating with very broad information,” with very little in the way of news about individual towns. Pichardo was looking for news about her sister, Carolina, in Caguas, which is located about 20 miles inland from San Juan.
In her search for news about her sister and relative in Caguas, Pichardo came across posts from family friends on Facebook verifying that they were safe. She realized that a large-enough moderated Facebook group could amplify the sparse news from outlying municipalities, and could help connect Puerto Ricans with family and with resources.  “I said maybe if I can get enough friends and enough friends' friends to join, then maybe I can reach a critical mass where I'm able to hear more inside news than just by browsing my newsfeed alone,” Pichardo says. So on Wednesday, she created posts exhorting people on the island and on the mainland to join, and paid to sponsor the posts. By the evening the group boasted almost a thousand people. At its most rapid growth rate, the page added 1,000 new members an hour, and generated hundreds of new posts and dozens of documents detailing logistics, supplies, and safe lists.
The Puerto Rico Maria Updates page has become such an indispensable source of knowledge to people seeking any sense of what’s happening on the ground that it’s become a key source for a tool from Univision News that provides municipality-by-municipality updates. That tool is powered by a mixture of official news reports and social media chatter, much of which comes from Pichardo’s group.
The tool’s usefulness is reliant on the quality of information from the Facebook group, which now features an extensive cast of moderators who regularly call out and ban false reports and posts that don’t meet their strict guidelines for quality. Moderators upvote seemingly reliable content, use hundreds of volunteers to scour posts and discern whether information is corroborated internally or with news reports, block trolls and unauthorized fundraisers, and even issue corrections or warnings on faulty posts. All of those activities are directed by what Pichardo describes as “self-organization,” with people volunteering for fact-checking duties. “The group itself has a very strong identity and a very strong desire for real information,” Pichardo says. “But we’re not sleeping very much.”
The collection of efforts launched by activists on the island and family members on the mainland are similar in spirit to the deployment of the “Cajun Navy” in Houston after Hurricane Harvey. Then, residents used WhatsApp and the Talkie app, along with dozens of hours of work from unpaid volunteer dispatchers, to triage relief efforts in real time, get boats and trucks to people the most in need, and provide information to victims, their families, and authorities. In that example, the efforts of private citizens proved perhaps just as valuable as and more trusted than those of first responders and federal entities.
For Puerto Rico, where migration to the mainland is common, and where a recession and economic crisis have forced a wave of millions to move in the past few years in search of greater job prospects, political status, or health care, the connection is deeper. Crowdfunding and crowdsourcing simply mirror existing familial and community lines that have been maintained even as families are spread across the continental United States. And those lines are more important in Puerto Rico, which was singled out over Texas and Florida on Monday by President Trump as being “in deep trouble,” and where official aid and federal authorities have been criticized for their slow response and lack of communication.
“We are doing things everywhere we are, and we're doing it because maybe our aspirations go beyond what the island can provide us in terms of infrastructure,” Pichardo says. “We're everywhere. We're no longer all in Puerto Rico. But not because we don't love them and not because we are disconnected.”


Many questions remain about the ads purchased by Russian-linked accounts during the 2016 presidential election.
Earlier this month, the company announced that Russian-linked accounts had purchased $100,000 worth of advertising.
The scale of this advertising buy is mysterious. In an election where billions of dollars were spent, why even bother to spend $100,000? It seems like a drop in the bucket, but also more than nothing. For comparison, in 2015 and 2016, all campaigns directly paid Facebook a collective $11,313,483.59 across all races, according to Federal Election Commission numbers. The Trump campaign paid Facebook $261,685 directly for ads. But those numbers are only lower bounds for the amount of money spent on Facebook because many campaigns pay consultants, who then purchase ads on their behalf. (For example, Cambridge Analytica, which worked with the Cruz and then Trump campaigns, took in $15.4 million during the cycle, including $5 million in one payment from the Trump campaign on September 1.)
So, the Russian ad buy is a significant Facebook purchase, but not one that seems scaled to the ambition of interfering with a national U.S. election.
That could be because: 1) Not all the ads have been discovered, so the $100,000 is a significant undercount. 2) That was the right number, and the ads worked to aid distribution of disinformation. 3) The ads were part of a message-testing protocol to improve the reach of posts posted natively by other accounts. Think of it as a real-time focus group to test for the most viral content and framing. 4) That $100,000 was a test that didn’t work well, so it didn’t get more resources. 5) That $100,000 was merely a calling card, spent primarily to cause trouble for Facebook and the election system.
Let’s walk through these branching possibilities for what this advertising buy could mean.
We don’t know much about how Facebook conducted its investigation. We do know that they repeatedly denied there was Russian influence during the election, and then copped to it in early September.
A Washington Post article fleshed out a few details, including that President Obama personally spoke with Mark Zuckerberg after the election to get him to take the misinformation campaign seriously.
The problem appears to have been that Facebook’s spam- and fraud-tuned machine-learning systems could not see any differences between the “legitimate” speech of Americans discussing the election and the work of Russian operatives.
Here’s the description of the process that eventually found the ad purchases:
Instead of searching through impossibly large batches of data, Facebook decided to focus on a subset of political ads. Technicians then searched for “indicators” that would link those ads to Russia. To narrow down the search further, Facebook zeroed in on a Russian entity known as the Internet Research Agency, which had been publicly identified as a troll farm. “They worked backward,” a U.S. official said of the process at Facebook.
I take this to mean that they identified known Internet Research Agency trolls, looked at the ads they posted, and then looked for similar ads being run, liked, or shared by other accounts.
Why this would have taken several months is unclear. Journalist Adrian Chen was able to build out a network of Russian operative–run pages without any of the data that Facebook has. Given that the story he wrote ran in The New York Times Magazine in 2015, you’d think that particular agency would have been the first place Facebook would have looked.
That could be one reason a Congressional investigator told the Washington Post that Facebook had only hit “the tip of the iceberg.”
But that’s only one possibility. The ads could have done exactly what the Russians intended, even at this limited scale, as part of a broader information campaign.
Some context: Facebook ads can do several different things. They can promote a piece of existing content somewhere on the internet. They can be used to try to drive “likes” to a page. They can be used to get people to watch a video.
With the right (salacious/truthy/fake) material, even a little money can go a long way. The Daily Beast had a Facebook ad specialist calculate how far $100,000 worth of Facebook spending would go and came up with a range of 23-70 million people, depending on how they were targeted.
Vice News talked with the owner of a right-wing Facebook page who uses Facebook ads to juice conservative content. After spending $34,100, the man controlled pages with 1.8 million likes. With that distribution base, he was able to push out content that could, on occasion, do serious numbers. “With a few advertising dollars, one April video ... received more than 27 million views and over 450,000 shares, spreading so pervasively into the conservative media universe that Donald Trump’s official Facebook page shared it two days later,” Vice wrote.
So maybe that’s it. The ads were simply a smallish part of growing the distribution network for disinformation and propaganda.
Looking back at Adrian Chen’s reporting on the Russian troll farm known most commonly as the Internet Research Agency, there’s a mix of skill and blundering. The Agency was smart enough to set up Chen with a neo-Nazi, surreptitiously photograph it, get stories written about the encounter, and then promote those via social media. But they also struggled to find English speakers who could write with proper grammar. The Agency could orchestrate a very complicated hoax about a chemical plant, but also let a known activist and journalist slip inside the company as new hires. The Agency was playing in international geopolitics, perhaps funded by a billionaire oligarch, but Chen reported the agency’s rumored budget back in 2015 was a mere $400,000 a month, or $4.8 million a year.
Perhaps the best mental model is simply a digital-advertising agency. In that case, there are some other intriguing possibilities.
Regular digital agencies (and media companies) routinely use Facebook ad buys to test whether stories and their attached “packaging” will fly on the social network. You run a bunch of different variations and find the one that the most people share. If the Internet Research Agency is basically a small digital agency, it would be quite reasonable that there was a small testing budget to see what content the operatives should push. In this case, the buys wouldn’t be about direct distribution of content—they aren’t trying to drive clicks or page likes—but merely to learn about what messages work.
And there’s a variation on these two scenarios, too. It could be that only $100,000 got spent simply because the ads were ineffective. Facebook itself has a case study on the reelection bid of Senator Pat Toomey that showed substantial increases in “voter intent” for key demographic groups. But the Toomey campaign spent $2.8 million on digital strategy.
That said, there is certainly reasonable doubt that even millions of dollars of Facebook spending could change the outcome of even a state in the U.S. presidential election. And perhaps the digital agency came to the conclusion that its budget was better spent elsewhere. Or maybe one group within the Internet Research Agency began buying ads—we do know the place is obsessed with metrics—to make itself look better to superiors for some period of time.
But it seems possible, from Chen’s description, that this was just a small thing for the Agency, which never gained institutional support.
And the last possibility is that the Internet Research Agency wanted to make a buy that it knew would get Facebook in trouble with the government once it was revealed. Think of it as corporate kompromat. Surely the Internet Research Agency would know that buying Facebook ads would look bad for Facebook, not to mention sowing the discord that seems to have been the primary motivation for the information campaign.
Some of these questions will be solved by simply seeing the ads. If they were testing the same content with different headlines or a bunch of different videos or posts, that’d tell us something about their operation. If the messaging ended up outside the ads, that might tell us something. In all cases, seeing the ads will be a major part of deciding what the known world of Russian influence was up to, which seems important for other social networks and political campaigns to defend themselves against future operations.
Maybe all we’ll see is bungling and a scattershot, silly approach. That would be useful and interesting information, too.
In any case, the Russian effort remained hidden in plain sight, which could be due to sophistication. Or it could be that having a nonfinancial motivation essentially served as an exploit for Facebook’s security systems, which are tuned to fighting fraud.
“Various groups regularly attempt to use such techniques to further financial goals, and Facebook continues to innovate in this area to detect such inauthentic activity,” wrote Chief Security Officer Alex Stamos and two Facebook coauthors in a white paper on information operations. “The area of information operations does provide a unique challenge, however, in that those sponsoring such operations are often not constrained by per-unit economic realities in the same way as spammers and click fraudsters, which increases the complexity of deterrence.”
The bottom line is that Facebook was not prepared for the threat. And I highly doubt that we won’t see a lot more come out about Russian operations on Facebook.


On Friday, more than 3 million London commuters learned that they would soon have fewer options for the commute home, after the city’s transport authority announced Uber will no longer be allowed to operate within the sprawling capital’s city limits.
Uber Did What!?
“TfL has concluded that Uber London Limited is not fit and proper to hold a private hire operator license,” Transport for London said in a statement, citing a range of concerns with the ride-sharing app, from its history of reporting serious criminal offenses to its policies on obtaining background checks for its drivers. Uber’s current operating license expires in eight days, after which the app will no longer be permitted to operate within the city. TfL, however, said Uber would have three weeks to appeal the decision, during which time the company may continue to operate.
The leader of London’s Licensed Taxi Drivers’ Association praised the transport authority’s decision, as did London Mayor Sadiq Khan, who noted that “providing an innovative service must not be at the expense of customer safety and security.” Tom Elvidge, Uber London’s general manager, accused the city of attempting to restrict consumer choice. “Not only will this decision deprive you of the choice of a convenient way of getting about town,” Elvidge said in an email to customers, “it will also put more than 40,000 licensed drivers who rely on our app out of work.” The email did not make mention of the safety concerns raised by the city.
Overall, the move will affect millions. Since Uber first made its London debut in 2012, some 3.5 million have downloaded the app—many of them seeking a cheaper alternative to the traditional black cabs, which can be as much as twice as costly. In the five-year period Uber has operated in London, its fleet of cars has nearly doubled the number of traditional cabs.
Though rivalries between Uber and the traditional taxi companies where they operate are hardly new (they’ve been well documented in Brazil, Canada, and China), Uber’s relationship to London—and the iconic cabs that roam its streets—is in a different category. As The New York Times reported in July, the clash is less about “the disruptive power of an app, or a new business model, than about the disruption of Britain. London’s cabbie wars echo the culture wars that fueled Britain’s vote last summer to leave the European Union—and that have brutally flared up again in recent weeks: immigrant versus native, old versus new, global versus national.”
Part of this divide can be seen in the demographic makeup of those who drive for both services. According to 2017 statistics by TfL, 68 percent of London’s taxi drivers identified as white and British, compared to only 6 percent of private-hire drivers, under which Uber qualifies. Other factors the Times noted, such as those who regard Uber as emblematic of the threat of globalization to the hundreds of complaints Uber drivers have reported involving offensive or racist abuse from other cab drivers, make this division even more stark.
The rift between Uber and London’s cabs may not be resolved anytime soon. Since TfL announced its decision, more than 300,000 people have signed an online petition announcing their opposition. Uber, meanwhile, has said it will immediately challenge the decision.


In the last week, one of Facebook’s greatest business strengths became a source of tremendous embarrassment to the company. Its famed money-making engine—which ceaselessly converts user data and content into advertising dollars, all underpinned by algorithmic plumbing—was found to have some glaring issues.
Specifically, as ProPublica revealed Thursday, advertisers could target self-described anti-Semites as an audience for their ads. Facebook’s algorithmic ad tool allowed buyers to target users who publicly entered phrases like “Jew hater,” “How to burn the Jews,” or “History of ‘why Jews ruin the world’” into their Facebook profile as their educational background or professional interest.
Facebook apologized for the feature last week, pulling every algorithmically defined ad-targeting category. On Wednesday afternoon, Sheryl Sandberg, the company’s chief operating officer, outlined further steps.
Facebook would be tightening its “enforcement processes” to make sure that content in violation of its community standards could not be used to target ads, Sandberg said in a public note on her profile. The company would also make it easier for users to report offensive or abusive ads directly to corporate workers.
Facebook’s community standards, which have been in effect for years, already prohibit “anything that directly attacks people based on their race, ethnicity, national origin, religious affiliation, sexual orientation, sex, gender or gender identity, or disabilities or diseases.”
The company also reactivated more than 5,000 audience-targeting demographics after checking them individually. “After manually reviewing existing targeting options, we are reinstating the roughly 5,000 most commonly used targeting terms—such as ‘nurse,’ ‘teacher,’ or ‘dentistry,’” Sandberg said.
She also suggested that the company would manually vet future target-audience terms in the future.
This final change—that Facebook should check the audience categories that underpin its central business—was one of the main reforms that experts called for after last week.
“We don’t need to be too awed by this problem,” Aaron Rieke, a technologist at the civil-rights firm Upturn, told me last week, about the anti-Semitic targeting. “You have a finite list of categories, many of which were generated automatically. Take a look, and see what matches up with your community standards and the values of the company. Facebook can monitor the things it does that make it money.”
Other suggested changes—like a public clearinghouse of all ad categories available to purchasers—were not embraced by the company.
Sandberg, writing on the eve of Rosh Hashanah, the Jewish new year, took an unusually personal and contrite tone in her note of apology. “The fact that hateful terms were even offered as options was totally inappropriate and a fail on our part,” she said:
Seeing those words made me disgusted and disappointed—disgusted by these sentiments and disappointed that our systems allowed this. Hate has no place on Facebook—and as a Jew, as a mother, and as a human being, I know the damage that can come from hate. [...]
We never intended or anticipated this functionality being used this way—and that is on us. And we did not find it ourselves—and that is also on us.
Like so many of Facebook’s recent struggles, the whole story turns on the intersection of automation, algorithms, and users acting in bad faith. It also turns on the company’s consistent inability to imagine the worst applications of its software.
Facebook, like many other tech companies, allows advertisers to “self-serve” their own ad purchases. There’s little human interference with—or oversight of—this process: A buyer can write a post or upload a video, select a target audience, and take out an ad with no humans otherwise involved. Many of the systems that maintain Facebook’s plumbing are essentially algorithmic like this, too.
This automation has come back to bite the company twice in the past week—at both the targeting and the purchasing ends of its pipeline. In addition to the anti-Semitic problem with the targeting end, Facebook also appears to have allowed Russian shell firms to buy political ads, attempting to influence the 2016 election.
Facebook may be able to plug some of the simplest holes in its targeting software. But the Russia-purchased ads—and the surprise with which Facebook discovered the anti-Semitic targeting in the first place—suggests that it is facing an arduous, important, and possibly Sisyphean task in trying to understand the darkest corners of its own business.


Decades after the National Science Foundation took over the old McMurdo naval base in Antarctica, and five years after design work began, there are renderings of the plan for a new McMurdo.
The OZ Architecture design will consolidate the current 105 buildings into six large, insulated, modern buildings. The new McMurdo is of this moment: It looks like an Apple Store.
Perched on volcanic gravel near the Ross Ice Shelf, McMurdo is the largest community in Antarctica with over 1,200 residents during summer and about a quarter as many during the winter. Many are scientists, but the support staff includes workers from all the trades—plumbers, carpenters, HVAC specialists, IT folks.
The old McMurdo was constructed for the International Geophysical Year of 1957 by a special group from the U.S. Naval Mobile Construction Battalion called “Task Force 43.” It was part of Operation Deep Freeze, the overarching U.S. military movement into Antarctica. Film from the time shows what a brutal task it was. Tractors dragged sleds laden with construction materials bumping over sea ice. It was bleak.
The most salient fact from the construction process is that McMurdo was never intended as a permanent settlement. And yet, there it is, sitting by the volcano, pressed into service by the “beakers,” as the Navy guys called the scientists taking over.
As one might imagine, there are problems with adapting a transient military facility into a permanent scientific research base. In 2012, a Blue Ribbon Panel catalogued all the facilities’ needs in Antarctica. That led to the Antarctic Infrastructure Modernization for Science, which selected OZ Architecture to do a master plan for the site.
“This is a career highlight to be challenged to create a community that is new and remote and in a hostile environment where you want to provide well-being,” said Rick Petersen, a principal at OZ. “This is the main logistical hub to support really groundbreaking science that affects the whole globe.”
Petersen said the design was an attempt to maximize energy efficiency and increase the logistical logic of the site while also providing for the mental and physical needs of the people at the base.
The walls have an R value, a measurement of insulation, of 72, five to seven times what you probably have in your house. The windows are triple-pane and the best insulated (or “highest performance”) out there. That should reduce the use of diesel generators on site to some degree.
The buildings, like many Antarctic structures such as the newish Amundsen-Scott South Pole Station and the British Halley VI, are lifted off the ground, even though (unlike most others) they aren’t designed to be moved. The situation at McMurdo is different: There is “a matrix of ice” in the ground, which they don’t want to warm with heat from the buildings. So, OZ designed insulated boxes lofted above the permafrost. Skirts around the bottom of the building prevent ice and snow from building up underneath them. (Petersen and Halley VI architect Hugh Broughton know and consult with each other.)
Logistics is a core consideration of the new McMurdo plan. Right now, warehouses and facilities that need the cargo in them are not well-placed. “There are 22 different warehouses scattered about the community, so there is a challenge in getting equipment or food from one to the other,” Petersen told me. “Things are spread out. It requires pickup trucks and vehicles to move things from one warehouse to another.”
Just to carry out the basics of camp life, like making food, requires schlepping to a warehouse and back to the production facility. In the new design, there are interior routes to key locations, although there are also ways to pop outside to escape the relentlessness of confinement.
The current design sounds great. It was built with input from years of listening to experienced personnel. But Petersen knows that they can’t get everything right, so they designed the buildings to be adaptable to changing needs and circumstances.
“The different work groups adapted the current buildings not only to optimize how they work, but also to reflect their culture,” Petersen said. “There are basically neighborhoods” that correspond to the type of workers who spend time there, from science outfitters to carpenters.
“We have to understand that the buildings we’re creating today and building in the next few years are going to outlast the people and programs that will first be using them,” he said.
The new buildings will be prefabricated and shipped to the site in containers. Construction will begin in 2019 and should take about five years.


Take a video of a birthday cake’s candles sparkling in an Instagram story, then tap the sticker button. Near the top of the list you’ll see a slice of birthday cake.
It’s a little thing. This simple trick is not breathtaking nor magical. But it is the beginning of something transformative. Smartphones already changed how most people take pictures. The latest Silicon Valley quest is to reimagine what a camera is, applying the recent progress in artificial intelligence to allow your phone to read the physical world as easily as Google read the web.
With 2 billion users, Facebook has reorganized the teams responsible for coding the camera software in Instagram, Facebook, and Messenger into a new unit it calls “AI Camera.” The group started last year with a single person. Now, it has grown to 60 people. That includes Rick Szeliski and Michael Cohen who worked on Photosynth (among other things) at Microsoft. The AI Camera team also can draw on the expertise of top neural-network researchers like Yann LeCun and Yangqing Jia in other parts of the company.
The AI Camera team is responsible for giving the cameras inside these apps an understanding of what you’re pointing them at. In the near future, your camera will understand its location, recognize the people in the frame, and be able to seamlessly augment the reality you see.
Right now, the team’s work has shipped in small ways, like the birthday sticker trick. But that is just the beginning of a development program that wants to transform the way you use the camera on your phone.
AI Camera combines many of the most important technological developments of the last several decades: neural networks, robotics, camera systems, and social-network data. This underlying basket of technologies—more adjacent to each other than in “a stack,” as software developers might conceive it—are converging into the smartphone's ability to take and display pictures.
Perhaps that seems absurd. But the human desire to capture, understand, and share images of the physical world has proven to be nearly insatiable, which is why this is the one domain where Facebook, Apple, Google, Samsung, Snapchat, and Microsoft directly compete.
Facebook’s work mirrors what’s happening at the other tech giants. Snap calls itself a camera company, and its realization of “lenses” are the best embodiment of augmented reality outside of Pokémon Go. At Google’s developer conference in May, Sundar Pichai showed off Google Lens, software that can detect what a camera is seeing and do something with that information, from entering a password to identifying a flower.
Prodded by Snap, the tech giants have begun to piece together what can be accomplished with the whole imaging and display system that a smartphone is. Every millisecond a phone’s camera is engaged is a moment when data can be captured, processed, understood, and looped back to the user for viewing.
“We’re basically looking at what pieces of technology we need to build amazing augmented-reality products,” said John Barnett, product manager on the AI Camera team.
Imagine, he said, a persistent, shareable social layer on the physical world, a spatialized Facebook that’s escaped the feed.
“Everyone got so excited about Pokémon Go when it was just one thing. What if there are 1,000 things like that?” Barnett asked. “All these layers of information that are spatially situated and relevant to what you care about.”
This is a radically different notion from the Facebook we’ve come to know, which, even though it made the leap from the desktop to “mobile,” rarely engages with the physical space where your hand clutches the phone.
“In the existing Facebook structure, we’re giving you everything that’s happening right now in the world, collapsing space to give you a a slice of time,” said Barnett. “This is talking about collapsing time to give you this piece of space.”
Facebook would take on two modes: The News Feed, in the company’s terms, would show you what you care about now, and the spatial Facebook would tell you what’s happening here. One could read from, and write to, the world. Your world, at least.
* * *
On one of the many decks at Facebook’s Menlo Park campus, overlooking the mudflats of the south Bay, there’s a nondescript corner. Pipes run along it. A surveillance camera sits on the east-facing wall. To the naked eye, there’s nothing to distinguish it from the hundreds of others that help form Facebook’s gargantuan ark.
However, pull out a phone loaded with an app Facebook has in development and point it at the wall, and you get a beautiful piece of art, created primarily by Heather Day, a San Francisco artist. It made a brief appearance during Mark Zuckerberg’s keynote at the company’s F8 conference.

Brilliant blues, cyans, teals drip from the pipes, pooling away from the wall. It’s cool, this thing hanging in the air.
Put the app away, open it back up and point it at the corner again, and the art is back. Move around it, move through it, and the ghostly remains of Day’s paint strokes and pours remain there. What if there were thousands of things like this all over the world? Next to burrito recommendations and Strava segment records and pictures of your friends, mugging for the camera, in situ.
This is one vision for augmented reality, the name for this layering of digital information on top of imagery of the physical world. AR has gotten a big push in recent months by Apple’s announcement of ARKit, a framework for developers to enable AR in apps. They’ve been showing it off, and Google recently announced a similar (though not as widely lauded) set of tools called ARCore.
No matter what, AR is a ridiculously complicated task for a smartphone. Alvaro Collet is a computer-vision Ph.D. from Carnegie Mellon University who came to AI Camera from Microsoft. He’s standing next to me looking at the wall. “This is actually a pretty challenging scene because it is very plain,” Collet tells me.
The basic task mirrors what robots have had to do for decades. Researchers call it SLAM (simultaneous localization and mapping).
The theory and practice of SLAM were developed over the past 30 years by robotics researchers like SRI’s Randall Smith and Peter Cheeseman, the University of Sydney’s Hugh Durrant-Whyte, Sebastian Thrun, and Carnegie Mellon’s Martial Hebert, who was Collet’s advisor. Most of these people were working on real robots, largely autonomous vehicles loaded up with all kinds of sensors. But as smartphones began to roll out, researchers realized that their systems might be able to reach hundreds of millions of people, not a few dozen.
The problem of SLAM is that you need to build a map of the world in which to place the robot (or phone), but the position of the robot (or phone) and the world are both uncertain.
“If you had all the features of the world already in 3-D, it would be very easy to place the position of the camera. And, conversely, if you had all the camera positions, it’s very easy to create the 3-D map of the world,” Collet said. “The problem with SLAM is that when you start, you don’t have a 3-D map and you don’t know where the camera is. That’s the simultaneous part.”
There are many ways to go about the problem that are encoded in different algorithms. Each has tradeoffs. Some provide excellent precision, but are computationally expensive. Others might consider the images from a sensor less extensively, but work quickly and without much computing.
Facebook finds itself building across both iOS and Android, which introduces many challenges. Facebook’s advantage, though, is its tremendous scale, 2 billion users and counting. But to use that scale, Facebook must make AR work on all kinds of crappy phones, not just Pixel 2s and Samsung Galaxy Note 8s and iPhone Xs. And that means that they actually deploy multiple algorithms to do SLAM. For lower-end phones, they do rougher, faster calculations. Higher-end phones get better performance because they can handle the processing.
Down on the low end, the AI Camera team must try to account for a bunch of mostly invisible hardware problems. Inside the phone, there is a camera, but there’s also an inertial-measurement unit, which they can use to tell how the phone is moving. The IMU contains gyroscopes and accelerometers. And all of these components, on low-end devices, have to be calibrated. Their clocks must be synchronized. And each device, because of lower manufacturing quality, might show more variance than one iPhone to the next.
Once all that electronic work has been done, and the phone knows roughly where it is, and the geometry of the scene, the next layer of technology gets piled on top: deep neural networks. The “neural” part means that this kind of software is “trained,” not programmed with traditional rules. After being shown large amounts of labeled data, the neural network can label new data based on what it has seen. The deep part refers to the neural network’s number of layers, which correspond to the complexity of features in a dataset.
Over the past five-ish years, this type of machine-learning system has transformed the way image recognition, among other things, is done. If you’ve ever used Google Photos to find pictures of business cards or mountains or people, you’ve made use of a deep neural network’s power.
Imagine, though, the next step: Instead of merely recognizing objects in artifacts, the phone can recognize objects live within the model of a scene that the device has already built. That’s only become possible in the last year.
“For the first time, you can run SLAM and deep networks on a cellphone,” Collet said. “We have two big teams: SLAM geometric teams and the other is deep nets. And the goal is these two things are going to combine.”
That’s the only way you get to augmented reality of the kind that Facebook imagines. Then they’d just have to get people to populate all the layers of spatial information.
“One thing we really are about is giving everyday users—maybe a year from now, maybe two years from now—the ability to recreate that Heather Day scene with just tools you have on your phone,” Collet said.
Anyone with a Facebook account could create media and fix it to a spot in the world. There will be food recommendations and wedding photos and paintings dripping in the air. A globe of ghostly art and burrito spots.
***
But we do know one thing from the history of every social platform: People will make their own uses of the tools. They’ll find new, unforeseen uses and abuses. There will be unintended consequences to spatializing Facebook.
Some of these could be predictable. There is already spatial information out there, just not displayed in the way AI Camera imagines or running through Facebook. Yelp, for example, has struggled with troll reviewers. (Here’s a nice list of some common variants.) Restaurants have been struggling to deal with the digital signs that lovers and haters affix to their doors for a decade now.
Another cautionary example comes from Pokémon Go. Omari Akil wrote a post describing his experience playing the game as a black man. He spent more time worrying about whether other people would find him suspicious—and bring him into contact with police—than he did actually engaging with the app. “When my brain started combining the complexity of being black in America with the real-world proposal of wandering and exploration that is designed into the game play of Pokémon Go, there was only one conclusion,” he wrote. “I might die if I keep playing.”
The realities of race and gender in America, which already play out in ugly ways across the internet, will be amplified by the physicality of augmented reality. Not everyone will be able to access the same spaces with the same ease.
In 2016, Waze rolled out a high-crime alert in Brazil to let people navigate around “bad neighborhoods.” Microsoft ran into trouble for a similar 2012 patent that got termed the “avoid ghetto” feature.
Even in more benign examples, the imperfect fit of spatial information on top of space can cause problems. Near my home in North Oakland, navigation apps lead many people to make a dangerous left just past an overpass and before a much larger intersection that the urban plan and human driving intuition both discourage. Nearly every time I drive past the intersection, that left is causing problems in the traffic flow up Claremont Avenue.
It’s not that Facebook can or should be expected to fix all of trolling or American antiblackness or the complexities of layering digital on physical. But as they develop this world, they can build with these problems in mind.
There’s even an analogy within the AI Camera project. Collet, the computer-vision specialist, was describing all the work that they have to do to make their systems work with the weird and wild world of phones across the globe. The calibrations, the algorithms, the fault tolerance of the systems.
“If you don’t think about them from the start, it’s very hard, once you have a system to say, ‘Oh, maybe we should tolerate this better,’” he said.
And as it goes with the reality of physical components, so it should be with the reality of the ethical and behavioral aspects of augmented reality. It’s gonna be more work to consider the misuses and biases in the system, but considering those things now will make the system more robust later.
If the AI Camera team succeeds, they will open up a new and basically infinite space on top of the land. The open question is what that will do the places under this new digital layer.


In 1804, a young naturalist named John James Audubon tied silver threads to the legs of the eastern phoebes, tiny white-and-brown songbirds, that lived in a nest near his home near Philadelphia. The birds soon flew away for the winter. The following spring, two returned with threads still attached. The experiment marked the first recorded use of bird banding in America, a technique for studying migration patterns.
More than two centuries later, technology, particularly the wonder of GPS, has turned silver threads into tags, sensors, and other devices capable of tracking all kinds of species around the world. Today, researchers get text-message alerts from collars worn by elephants in Kenya. They can stick tags on the shells of turtles or attach sensors to the fur of seals that transmit information with the help of satellites. They can even glue tiny barcodes to the backs of carpenter ants.
These are just a few of the projects described in Where the Animals Go, a book by geographer James Cheshire and designer Oliver Uberti, out this week in the United States. Cheshire and Uberti spoke to dozens of scientists tracking animals, from owls and elk to pythons and hyenas, and turned their data into a collection of 50 beautiful maps.
The maps show the paths the animals take as they cross desert, forest, ice, and ocean to feed, breed, and survive. The maps reveal what Audubon couldn’t see when he tied his silver thread to the birds: a journey. Some are especially quirky, as in the case of the seagulls who made daily trips to a city in France that was 40 miles away from their breeding colony. When researchers visited the site to investigate, they found the gulls feasting on discarded food outside of a potato-chip factory.
I spoke with Uberti and Cheshire about animal-tracking technology and the strange places it takes us. Our conversation has been edited for length and clarity.
Marina Koren: So your book introduces us to a fairly new era of tracking animals using technology. Can you tell me about the era that we’ve left behind? How did humans track animals before they could stick GPS tags on them?
Oliver Uberti: Until fairly recently, tracking involves looking for footprints, looking for fallen feathers, broken branches, droppings—any sign that an animal has passed through. Then around the past couple centuries, you start to get people like John James Audubon, who are tying threads to the legs of songbirds to prove that they’re actually returning to the same place every year. And then in the 20th century, you start to get people doing some real DIY tracking where they’re attaching cameras to pigeons or radio transmitters to a duck. But it’s only really been in the past 20 to 30 years that GPS has come on the scene, and then after that the miniaturization of computing power and the internet and satellites—it’s literally just exploded what you can do in a really tiny device.
James Cheshire: We’ve really transitioned from monitoring animals as a resource to be exploited to monitoring them as individuals within a species that we can collect a myriad of data from. Prior to the animal-tracking revolution, one of the biggest data sets collected on whales came from where we killed them, from whaling ship logs. There’s now sensors that they’ve got on whales that collect more data points than the sum total of the data collected in the previous five decades or more of research.
Koren: What kind of technology do we use today?
Uberti: There’s really no “one size fits all.” Scientists tailor tracking tags to the species and to the study, and that all depends on the environment that the animal lives in. GPS doesn’t penetrate underwater, so if you’re tracking a marine animal like a shark or a turtle, you need to have a device that can transmit when the animal comes to the surface and gets a brief, sometimes split-second window to shoot up to a satellite and transmit. Or you need a device that can release off the animal and float to the surface and then transmit. If you’re talking about songbirds, there’s protections in place to make sure they’re not taking tiny little songbirds that weigh only a few grams and saddling them with a giant computer. You talk about tracking plankton—you’re not even using a computer. You’re setting up an environment in an aquarium, you turn off all the lights, and you inject the plankton with a fluorescent particle like they use to track cancer in some medical technologies. And in the darkness, the individual plankton fluoresce and, by recording that with cameras, [scientists] can watch the illuminated animals move up and down the water column in response to UV light.
[At the Save the Elephants organization,] they can get text messages if they think elephants are in danger, if they’re moving too slowly, which is often a result of being shot by poachers or herders who’ve been scared because the livestock are coming close to these elephants and they don’t know what to do. If the animal is moving slower than it’s expected to move, the GPS tags notice and their accelerometers inside notice. It’s much like the way Google Maps can track different traffic patterns on the road by how fast they’re expecting you to be moving. If the elephant moves below a threshold, then it sends out an alert and Save the Elephants can dispatch rangers and law enforcement to check on the animal immediately.
Koren: How did you go about visualizing the immense data sets from the sicentists you talked to?
Uberti: When the data comes out raw from the tags, it’s an immense hairball. Tracks of many, many mountain lions, for instance, are all tangled on top of each other. To really tell any story of what the animals experienced or what the scientists are investigating, we try to turn big data into small data, and highlight one or two individual animals and walk you through or swim you through or fly you through what that animal experiences. It was a lot of editing down, taking out extraneous tracks, and really zeroing in on the lives of a few individuals.
Cheshire: Getting the right base-mapping information, all of the contextual stuff that goes around the tracks—that was a huge amount of effort, actually. It was probably more work than dealing with the actual animal data itself because we needed to make sure that what we were showing on those maps was relevant to that particular animal. We wanted to create maps that conveyed some of the environmental conditions. For example, when we mapped snowy owls over the Great Lakes, the satellite imagery we used for that roughly comes from the same time as when the owls themselves were flying over them, so you could see the same ice floes that the owls are seeing.
Koren: What was it like to look at a completed map, see a bunch of squiggles, and know that that’s an animal going about its life?
Uberti: This all goes back to an elephant named Annie, who I first mapped years ago. I saw where her GPS tracker stopped recording in [southeastern Chad], where she and a number of her companions had been killed by poachers. Mapping her moving through her environment, waiting till nighttime to cross the roads, to avoid interaction with humans, and then just watching her life stop on a map—that was the first time a map had connected me to an individual animal. It’s easy to think of animals when you hear about them, when you watch nature specials on TV, as furry robots that are just kind of preprogrammed, moving about the earth, doing what they as a species are assigned to do. And when you look at these tracks, you see in each individual path animals making decisions that are unique to them—what they like to eat, where they like to go.
Koren: The maps in the book really make clear how animal migrations transcend borders, whether it’s of national parks or nations. Has your idea of borders changed after seeing just how irrelevant they are for animals?
Uberti: It confirmed my own assumptions that borders don’t exist in the natural world. A great example of that goes back to the first GPS tracking study in Africa, where Cynthia Moss was doing research on elephants in Amboseli [in Kenya]. Some of her elephants were leaving and crossing the border into Tanzania into wildlife hunting ground and were being shot. When they confronted the Tanzanian government and the hunters about it, the hunters said, oh, no, these are our elephants in Tanzania, they’re not yours from Kenya. So Moss got Iain Douglas-Hamilton, the founder of Save the Elephants, and he put two [sensors] on two elephants, and sure enough, one of them crossed over the border into Tanzania. Here you had this undeniable proof that animals don’t follow human borders.
Border walls and pipelines make headlines, but far more insidious in their effect on animal behavior are freeways and fences. Mountain lions in Los Angeles are completely marooned on genetic islands in the Santa Monica and the Santa Ana Mountains because of this web of freeways, many of them eight, 10 lanes long, that the animal can’t get across. And if they can’t get an influx of DNA from animals from outside their population, they’re doomed by inbreeding.
Cheshire: It’s certainly true that animals don’t care about national borders, but the national borders themselves and the geopolitics around them still have big impacts on animals. We have a story about a wolf named Slavc who walked across Europe and starts in Slovenia and finishes in Italy and goes via Austria. Each of those countries had different laws and different attitudes toward large carnivores and whether they could be hunted. So for the researcher who was tracking Slavc’s progress, it was a real roller coaster, because as soon as Slavc stepped over the Austrian border, the chances of him getting hunted were bigger because there are laws there that mean you can shoot stray dogs. An animal doesn’t have to show a passport to cross from one country to another, but the impacts, when they do, can still have a big difference on their life chances.
Koren: What does the future of animal-tracking technology look like?
Cheshire: Stuff is going to get smaller and more powerful and cheaper to deploy. But the real interesting stuff is going to be what the researchers are able to do with the data when they get back to their labs, and start crunching through these big sets of numbers they’ve got, and see if they can look at the health of an ecosystem almost in real time. Researchers are now looking into how animals interact with one another, with predators, with prey. It’s those interactions that dictate how successful an animal is—are they getting enough food, are they able to hunt, are they able to breed? [Researchers] are beginning to tag enough animals within an ecosystem to see how those interactions are playing out.
Koren: Has working on this book changed the way you look at animals in your everyday life, even just a squirrel on the street?
Cheshire: For sure. There’s a tree in my backyard that has songbirds in it that come and go. Before, I didn’t pay them much attention, but now I’m wondering how long they’re going to hang around for, where they’re going to go, where they’ve been.
Uberti: The Wallace Stevens stanza that we excerpt kind of sums it up for me. “When the blackbird flew out of sight, it marked the edge of one of many circles.” Working on this book just made me think that way about all the animals I see in my day, whether it was a roebuck that jumped past me while hiking in Slovenia, to a lizard that skirted out from underneath a trash can here in Los Angeles, or the brown widow spider I saw last night outside of my flowerpots in my backyard. So much is happening outside of what we can see for ourselves.


Updated at 8:04 a.m. ET
More than 200 people are dead after a magnitude 7.1 earthquake struck Mexico on Tuesday, the anniversary of the 1985 quake that killed at least 5,000 people. It was the second major earthquake in the region this month. Just 12 days ago, Mexico City’s earthquake-warning sirens blared ahead of an 8.1-magnitude temblor—and many Mexicans reacted as they are by now accustomed to when the alarm went off Tuesday.
The warning system, which has been in place since the early 1990s, is linked to a complementary smartphone app used by millions of Mexicans.
The U.S. Geological Survey said the quake’s epicenter was five miles southeast of Atencingo, in central Puebla state, at a depth of 32 miles. Details of Tuesday’s quake are still emerging, but videos and images posted on social media showed fleeing people, collapsed buildings, and smoke emerging from structures. The mayor of Mexico City, where the quake was also felt, said many people are trapped in the rubble.
In an odd coincidence, the quake struck just hours after Mexicans participated in earthquake drills to mark the anniversary of the 1985 Mexico City earthquake. It was also the second powerful quake to strike this month: an 8.1-magnitude temblor that hit Chiapas on September 7 killed 98 people. Mexico has a rash of major quakes in its recent history. Tuesday’s was the 14th earthquake greater than 7.0 magnitude in Mexico since 1985. None of the quakes since that period have had the same devastating impact as the one more than three decades ago, even if several of them, including the one on September 7, were at least as powerful, if not more so. One reason for this: improved building codes.
The codes are strictly enforced in Mexico City—which is particularly vulnerable to the impact of faraway quakes because it’s built on a dried-out lake bed—and other major cities. As the Christian Science Monitor noted in 2003, after a deadly quake killed 25 people:
Millions of dollars have been spent to reinforce centuries-old structures across the city, and any new construction must comply with strict quake protection measures.
Teams of rescue workers, police, hospital staff, and firemen practice earthquake simulations throughout the year so they can respond immediately.
City workers from the subway system to the power grid are assigned specific checklists they must work through the instant any seismic activity occurs.
They are understanding orders to repair any damage immediately, even in the event that telecommunications are down and they can not reach their superiors.
Meanwhile, a state-of-the-art alert system beams emergency messages to the capital every time major seismic activity is detected along coastal fault lines. Warning sirens then sound, giving city residents up to a minute's warning that a quake is coming.
Perhaps most important, massive public-awareness campaigns have trained Mexico City residents how to react when they hear the sirens: Get outside quickly and stand clear of any structure or power cables that could fall.
Many of those systems put in place have proved invaluable in the years since the 1985 quake. For instance, during the September 7 quake, the early-warning system, called SASMEX, which relies on sensors installed along Mexico’s western coast, gave residents more than a minute’s warning of the impending quake, likely saving lives. That system has been in place since the early 1990s; a complementary smartphone app is used by millions of Mexicans.
This sort of early-warning system is particularly useful outside the major cities where building codes may not be as well enforced. But as CityLab pointed out, poorer states like Chiapas, where 16 people were killed in the September 7 quake, aren’t part of the system. (The CityLab piece also noted that the SASMEX system needs an overhaul.)
The southern state of Oaxaca bore the brunt of the earlier September quake, and the damage in the town of Ixtaltepec prompted one state official to tell The New York Times: “We have no idea how we are going to rebuild the entire town.” There have been warnings for years about this looser enforcement of building codes in some parts of the country. The Guardian pointed out in 2015:
Yet, in the peripheral zones, beyond the tourist’s reach, informal settlements continue to proliferate. There, codes and regulations are irrelevant. Residents build with whatever materials they can find and afford, and they continue to struggle to gain access to basic services such as water and electricity. For all the improvements of the last 30 years, roughly 60% of the city is made up of these unregulated, informal and vulnerable zones.
The newspaper continued: “The growth of these settlements over the last three decades attests to the persistence of poverty and inequality in Mexico City—and as a result, its vulnerability to its next big earthquake.” This month’s two quakes, the accompany devastation, and deaths illustrate just how vulnerable it is.


In the beginning, it wasn’t the heat, but the humidity. In 1902, the workers at Sackett & Wilhelms Lithographing & Printing Company in New York City were fed up with the muggy summer air, which kept morphing their paper and ruining their prints. To fix the problem, they needed a humidity-control system. The challenge fell to a young engineer named Willis Carrier. He devised a system to circulate air over coils that were cooled by compressed ammonia. The machine worked beautifully, alleviating the humidity and allowing New York’s lithographers to print without fear of sweaty pages and runny ink.
But Carrier had a bigger idea. He recognized that a weather-making device to control humidity had even more potential to control heat. He went on to mass-manufacture the first modern air-conditioning unit at the Carrier Corporation (yes, that Carrier Corporation), which is still one of the largest HVAC manufacturers in the world. Air-conditioning went on to change far more than modern printing—it shaped global productivity, migration, and even politics.
The story of air-conditioning—and 49 other breakthroughs—is the subject of a new book, Fifty Inventions That Shaped the Modern Economy, by the economist and Financial Times columnist Tim Harford. I spoke to him recently about some of the book’s biggest ideas, and the following conversation has been edited for clarity and brevity.
Derek Thompson: Humans wanted to keep cool long before Carrier’s invention. But it’s sort of pathetic how we tried to do it. You describe the early 19th-century business of New England companies shipping large carved ice cubes insulated with sawdust around the country. New England literally exported ice the way Georgia exports peaches. There were even shortages during mild winters—“ice famines.”
Tim Harford: It was really hard to cool things! Before the invention of air-conditioning, you had to take something that was very cold and move it to places that were hot. And there were fascinating problems. For example, when the bodies of water that supplied the ice, like lakes, started getting polluted, the pollutants would be trapped in the pieces of ice. When they melted at their destination, it filled the air with unpleasant smells.
Thompson: Truly, thank God for Willis Carrier. The global effects of air-conditioning that you describe are mind-blowing. Air-conditioning transformed cities’ skylines, allowing for tall glassy skyscrapers that didn’t broil people in the top floors. It transformed demographics, allowing for migration in the U.S. to the Sun Belt, to Atlanta and Phoenix. By allowing politically conservative retirees to move south and west, you quote the author Steven Johnson saying that air-conditioning elected Ronald Reagan.
Harford: Yes, and it’s key to have a global perspective, too. This didn’t just reshape America. Air-conditioning reshaped the world. Places like Singapore and Shanghai are miserable when they’re hot and humid, but today they are global metropolises. There are studies saying that human productivity peaks around 70 degrees. That means that air-conditioning made us more productive, but also, by creating density in Singapore, it allows people to work longer and keep making the world a rich place. There is also the dark side of air-conditioning. You cool the temperature inside, but these units are energy-hungry, and they contribute to global warming.
Thompson: The first invention in the book is the plow, which facilitated the agricultural revolution. You write that “agricultural abundance creates rulers and the ruled, masters and servants, and inequality of wealth unheard of in hunter-gatherer societies.”
The second invention in the book is the gramophone, which allowed the most popular singers to reverberate in houses around the world, turning local stars into global superstars. That, too, creates inequality of wealth unheard of in pre-gramophone societies. Do you think technology inherently creates superstar effects and inequality?
Harford: There are two questions in there. First, does tech always increase inequality? Second, can politics offset that?
First, no. The famous example of technology displacing workers is the mechanized loom, famously smashed by the Luddites. These machines made their owners rich. But I think it’s possible that those looms actually decreased income inequality, because the new looms could be used by lower-skilled workers, who earned more money. On the other hand, I do think that much technology today is biased toward people who already have skills, which exacerbates winner-take-all effects.
But the winners of technology are often determined by politics and law. Bill Gates’s wealth could not exist without the U.S. property system that protected his software from being copied. Political choices have made him one of the richest people in the world.
Can politics offset inequality, too? Yes, of course. The welfare state can offset it.
Thompson: In fact, “the welfare state” is an invention that you discuss in the book, which is an impressively expansive definition of invention. The founders of the modern welfare state, in your telling, were much more interested in warfare than welfare. Otto Van Bismarck needed a dedicated army when he created pensions. To suppress his socialist rivals and secure popular support, Mussolini offered a fleet of social-welfare programs, including paid vacations and infant care. It seems to me that political necessity was the mother of political invention.
Harford: The welfare state was designed to strengthen the role of—it’s right there in the name—the state. It was initially designed to make the state more powerful and make people rely on the state.
More generally, “necessity as the mother of invention” is an intriguing cliché. Sometimes it’s not true. Take an invention like public-key cryptography. Here you had mathematicians tinkering around with prime numbers for a long time, in what might have seemed like useless work. Then some computer scientists realized that they could use the research behind prime numbers to encrypt messages. But it was unimaginable where the prime-number research would inevitably go.
But then, take barbed wire. Here, necessity was clearly the mother of invention. Americans could see for years that there was a burning need for a cheap material, which didn’t require a lot of wood, to fence in large areas in the western United States.
Thompson: In 1875, barbed wire was marketed as “The Greatest Discovery of the Age.” After the President Lincoln opened up the West by signing the Homestead Act of 1862, the American West was difficult to settle because settlers couldn’t enforce the boundaries of their property. You explain that barbed wire created the modern definition of private property.
Harford: Barbed wire allowed farmers after the Homestead Act to enforce their property rights. In any business, if you can’t practically enforce property rights, it doesn’t matter what the law says. You see the importance of enforcement in the music industry. The law says that the people who create music own the rights to it. But property rights meant something very different in the streaming world, when people could easily illegally download songs. Digital rights management was an attempt to create barbed wire for digital products. It tried to solve a “fencing” problem in music.
Thompson: Do you have a favorite invention in the book—or perhaps, an invention that you considered most underrated?
Harford: My favorite invention was paper. It didn’t occur to me at first. People said to me, “You must put the printing press in the book. It revolutionized the way we think; it led to the Reformation and rise of Europe.” But without paper, the press isn’t very useful. Yes, you can print on animal skin. But 5,000 copies of a book will require a quarter of a million sheep. Paper is what makes the printing press economically feasible. It’s an extremely underrated invention. Also, it has other practical uses, like toilet paper.
Thompson: As you write, the first example of paper money in world history was developed by merchants in Sichuan, China, around 1000 AD. It was an IOU. It wasn’t supposed to be a formal government currency. But the promise of value is one of the definitions of money, and an IOU is the promise of money. So people started trading their IOUs rather than carry around iron coins. They accidentally invented a private currency.
Harford: The paper money was a tradable IOU, which had been separated from the original debt. And if the debt can be traded, that’s what money is.
Thompson: It’s a beautiful lesson in the definition of money. And you write that when the Chinese government got wind of this, they tried to control the supply of IOU paper.
Harford: It wasn’t so much that the government wanted to control the money—they wanted to monopolize it. Here’s why. Let’s say you have an IOU from me. You go down to the corner store. You say, “Well, I could write you an IOU, or I could give you this IOU from Tim, who is a really good guy.” The corner store might say, “Actually, I trust Tim more than Derek anyway, so pass it over.” So now, my debt—Tim’s IOU—is circulating as money.
Here’s the magic: Tim’s IOU can be traded, over and over and over again, just like you traded it at the corner store. But nobody is ever coming back to me for the money. It’s more useful if it’s never cashed in. It’s pure profit. So, of course the government wants to be involved in that!
Thompson: In writing this book, did you learn anything about where invention comes from?
Harford: This is not a book about where ideas come from, or how inventions happen. Some of the inventions in this book come from private-sector geniuses, and there is stuff that is funded by the state. The biggest pattern is that it’s hard to find a pattern.


Facebook’s fact-checking efforts are on the rocks. Five months after the social-media giant debuted a third-party tool to stop the spread of dubious news stories on its platform, some of its fact-checker partners have begun expressing frustration that the company won’t share data on whether or not the program has been effective.
In the absence of that official data, a study by Yale researchers made waves last week by suggesting that flagging a post as “disputed” makes readers just a slim 3.7 percent less likely to believe its claim. Among Trump supporters and young people, the fact-checking program could even backfire: Those respondents were more likely to believe unflagged posts after they saw flags on others.* That concern was echoed earlier this year by the actor James Woods, who tweeted that a disputed tag on Facebook was the “best endorsement a story could have.”
The study—as well as ongoing revelations about how Russian troll farms might have used Facebook ads to meddle with the U.S. presidential election—has been stirring up the debate about whether and how social-media companies ought to police misinformation and propaganda on their platforms. Facebook claims that its efforts are working, and criticized the Yale researchers’ methodology, but a growing body of scholarship shows how difficult fact-checking has become online. With roots in old-fashioned cognitive biases that are amplified by social-media echo chambers, the problem is revealing itself to be extraordinarily difficult to fight at an institutional level.
Take Walter Quattrociocchi, a computer scientist at the University of Venice who has published a torrent of research over the past few years that examines how Facebook users consume information and self-segregate into online communities. In one recent paper, Quattrociocchi’s team looked at five years’ worth of Facebook posts, along with likes and comments, from a group of 413 public pages. These pages ranged from science-themed fare like “ScienceDaily” to ominously-titled conspiracy pages like “I Don’t Trust The Government.”
What Quattrociocchi found may have deep implications for the future of online fact-checking. Facebook users who cluster around conspiracy-related content tend to interact only with material that affirms their preexisting worldview, but in the rare cases when they do come into contact with dissenting information that attempts to debunk conspiracy theories—in the form of public posts by science-related pages—the conspiracy theorists become more, rather than less, likely to interact with conspiracy-related content in the future. In fact, conspiracy theorists who never interact with dissenting viewpoints are almost twice as likely as those who do to eventually drift away from conspiracy-themed content.
In other words, attempting to correct wrongheaded beliefs on Facebook appears to accomplish the precise opposite. Instead of alerting readers to the post’s factual inaccuracy, it entrenches them further in their erroneous beliefs. That’s not the same as studying the effect of a “disputed” tag on an article’s virality—only Facebook has access to that information—but it appears to be a good proxy.
Quattrociocchi doesn’t equivocate about his own feelings. He calls any promise that fact-checking can stomp out the spread of misinformation on social media a “hoax” and “bullshit.”
Though this issue predates the 2016 presidential election, the problem came into focus during that time. There were those Macedonian teens who discovered they could make a quick buck by publishing fictitious news reports designed to outrage conservative Americans. There was the rise of fringe media outlets like Infowars, whose figurehead Alex Jones has refused to retract conspiracy theories about how the Sandy Hook shooting was staged by paid actors. Donald Trump acknowledged what was already coming to be called “fake news” during the campaign by appropriating the term as a diss he still often lobs at CNN and The New York Times in the wake of unfriendly reports.
Before the internet, Quattrociocchi says, information had to make it past various gatekeepers before it could be widely disseminated. Those sentinels were often flawed, but they tended to filter out the most outrageous misinformation. Now, he suspects, information propagates via the same mechanisms as selfies or memes, leading to a crisis of authority.
None of that puts Facebook in an easy position. In the wake of the 2016 election, the platform faced a wave of criticism for having allowed misinformation to go unchecked on its platform during the campaign. Mark Zuckerberg, the company’s CEO, initially went on the defensive, but eventually acquiesced. In response to criticism of the new fact-checking program’s implementation, the company argues that flagging posts is only one part of a larger effort to fight misinformation on the platform. (Facebook declined to provide any further information on the record.)
Regardless, the difficulty of online fact-checking presents a grave challenge to public discourse. On the open web, biases can lead to vast, ingrown communities that reinforce preposterous beliefs. Kate Starbird, a researcher at the University of Washington, set out to study how useful, accurate information about safety and breaking news spreads on Twitter during bombings and mass shootings. But she started to notice an unnerving trend: As each disaster unfolded, a group of fringe accounts would start to promulgate paranoid theories about how the tragedy had been a “false flag,” carried out by the government or some other shadowy cabal.
Earlier this year, she published a provocative paper arguing that these strange networks of conspiracy posters are deeply connected to white nationalism, the alt-right, and the associated media ecosystem of sites like Infowars.
“We have ideas about how we make sense of the world,” Starbird said. “Our current information environment makes us newly vulnerable to things like filter bubbles and purposeful manipulation. People who understand how we think will try to influence us with ads and propaganda.”
Starbird did point out that not all findings are as somber as Quattrociocchi’s. One bright spot, for example, is that users may be more receptive to fact-checking if it comes from a friend—though that sort of engagement can be exhausting in these fraught times.
Starbird’s work has led her to reevaluate her outlook on the role of social media in the world. “When I started doing research about social media, I was very optimistic about its role in crisis response,” Starbird said. “But after the last year and a half, I’ve become increasingly depressed. I have a much more negative outlook.”
* This article originally stated that young people and Trump supporters were more likely to believe flagged posts than unflagged posts. We regret the error.


Nine months after Donald Trump won the presidency by unexpectedly swinging key states in the upper Midwest by slim margins, Facebook’s role in the 2016 election is still not clear.
Just in the last week, Facebook’s advertising has come under new scrutiny. Friday evening, The Wall Street Journal reported and CNN confirmed that special prosecutor Robert Mueller served the company with a search warrant to gather information on Russia-linked accounts that Facebook said purchased $150,000 worth of ads on the platform.
Earlier in the week, ProPublica found that anti-Semitic terms could be used to target advertising, prompting Facebook to scramble to patch up its advertising-keyword system.
Both incidents highlighted a basic question: What should Facebook know about the automated systems that it uses to make money?
Facebook’s advertising platform allows anyone, really, to purchase advertising targeted at particular populations. That could be people who recently got engaged, graduates of San Jose State, Red Sox fans, or as ProPublica discovered, bigots.
Facebook’s response to the ProPublica investigation was that few people actually targeted ads to people who listed themselves as “Jew haters.” It was a capability that was merely latent in the system.
And with the Russian ad buy, $150,000 of ad purchases is a drop in the bucket for Facebook, which had $8.8 billion of revenue in the fourth quarter of 2016 alone.
But American elections are not “Facebook-scale.” They can be swung by thousands of people. And within a massive automated system like Facebook’s, it was possible to stash a targeted disinformation campaign around the 2016 election.
The great irony of Facebook is that a system built to connect people makes high-margin advertising dollars by doing away not just with the middleman, but with any man or woman. There doesn’t have to be any human between an advertiser and the people who are targeted.
In the past, salespeople and production teams would have provided a check on the advertising that ran. Now, the people are gone, but the algorithmic systems that Facebook has created are not yet up to the task.
And not just with the advertising ecosystem. Earlier this year, a New York Times Magazine story asked, more or less, whether Facebook’s News Feed was, on balance, bad for American democracy. The aperture of the critique keeps widening, too: a new Times story highlights that governments all over the world are passing laws to regulate Facebook and other internet platforms.
The new zeitgeist has forced Facebook’s leadership to accept some responsibility for the way it shapes the political information people receive.
“Giving everyone a voice has historically been a very positive force for public discourse because it increases the diversity of ideas shared,” Mark Zuckerberg himself admitted. “But the past year has also shown it may fragment our shared sense of reality.”
That followed the burst of “fake news ” in the original sense of the coinage around the election: straight-up hoaxes perpetrated by a variety of actors like the viral story that the Pope endorsed Donald Trump. (He did not.)
The real problem of “fake news” wasn’t merely that it could work on Facebook’s system, but that the incentives of News Feed seemed to make fake news perform substantially better than real news. URLs filled with nonsense aren’t the problem, but the actual distribution system that Facebook’s News Feed represents. If lies, hoaxes, or “truthiness” are so effective, that does not portend good things for factual accounts. The system, whatever else it might be, does not seem to be as neutral as Facebook would like it to be.
It’s possible that the Russian ad buy will end up connected to this larger complex of problems. The way that many media organizations use Facebook ads is to test their content, not to distribute it. It could be that the ads were merely a good way to mark certain kinds of users or find the most viral, divisive content.
Both kinds of knowledge could have multiplied the impact of any kind of disinformation campaign.
One thing seems clear: Facebook will end up before Congress in one way or another. Before congresspeople, demurrals won’t cut it. Facebook has to know itself better, even if that means hiring a lot more people. Facebook wants these things to be artificial-intelligence problems, but counterintelligence may be the more relevant field right now.


Wiffle ball is a variant of baseball played with a plastic perforated ball. Eight three-quarter-inch, oblong holes cover half the ball’s surface area, while the other hemisphere is uninterrupted. Originally designed to relieve the arm of a young baseball pitcher (the son of its inventor, David N. Mullany), the ball achieves a curving trajectory without requiring the pitcher to impart spin or hurl at top speed. Each ball is packaged with instructions for how to release it in order to achieve various effects—with the perforations up for a straight ball, toward the pitcher’s thumb for a curve, and toward the outer fingers for a slider.
The inventor’s grandsons still run the family enterprise, with a product unchanged since its 1953 launch. Their dad, the pitcher for whom the ball was designed, told The Atlantic in 2002 that the Mullany family believed cutting the holes might create a “weight imbalance” that would cause the ball to curve. To this day, the company insists, “we don’t know exactly why it works—it just does!”
That folksy answer is charming, but a scientific one can foster even greater admiration for this curious ball and the sport that makes use of it.
* * *
Before Wiffle ball, uncertainty over the baseball curveball motivated investigation from both the media and the scientific community. Life magazine commissioned photographic studies of curve balls in 1941, to determine whether the phenomenon was real or an optical illusion. The magazine’s editors concluded it was illusory, enraging pitchers of the era.
In 1953, Igor Sikorsky, the inventor of the first viable helicopter, performed experiments demonstrating that a spinning sphere did indeed experience lateral deflection (that is, “curve”). Although Sikorsky didn’t publish the data, his collaborators did share a summary of findings—among them, that four-seam pitches curved more than two-seam. By 1959, the University of Notre Dame’s F.N.M. Brown produced gorgeous wind-tunnel images of smoke streamlines showing the way a ball with backspin deflected its wake. And that same year, the National Institute of Standards and Technology engineer Lyman Briggs published his own study, concluding that a baseball could indeed arc up to 17.5 inches on its way to the plate. The spinning ball lowered air pressures on one hemisphere, pulling the ball in that direction.
And so, physicists confirmed that a curveball really does curve. But even so, the batter’s perception is different. At the plate, a pitch appears to “break”—jumping or dropping suddenly, rather than smoothly arcing. The neuroscientist Arthur Shapiro has shown that this optical illusion may be due to the way our visual system processes information.
That’s for baseballs, which are made from rubber or cork wrapped in yarn and leather. What about Wiffle balls?
Wiffle balls wouldn’t be possible without the ubiquity of plastic. In postwar America, lab-synthesized plastics flooded consumer markets once they were no longer needed for wartime duties in mortar fuses, parachutes, soldiers’ service-issued combs, aircraft components, or in the Teflon containers used for the Manhattan Project’s most volatile gases. The first Wiffle-ball prototypes were made by cutting holes into the plastic packaging for Coty perfume. Today’s mass-produced Wiffle balls begin life as polyethylene pellets, melted and injection-molded into hemispheres that are then pressure-sealed together.
The asymmetric flow field caused by the Wiffle-ball holes might yield the same result as does the effect on a spinning baseball: a trajectory that curves or bends in the direction of the resulting pressure force. Still, whether the ball tends to curve toward, or away from, those holes is a matter of some contention, actively debated in Wiffle chat rooms and on the field.
Robert Adair, a Yale physicist and the author of The Physics of Baseball, has speculated that the holes, like the stitching on a baseball, accelerate turbulence on the perforated side of the Wiffle ball. Faster airflow may lower the pressure and cause the ball to move toward the holes. However, the Brooklyn College professor Peter Brancazio has countered that scuffing a Wiffle ball “essentially takes the holes out of the equation.” If the smooth, unperforated side of the ball were sufficiently roughened, it might disturb the air more than the holes, reversing the pressure asymmetry and causing the ball to curve away from the holes.
* * *
My own study of baseball aerodynamics began as a trap meant to lure my undergraduate students toward the beauty of fluid mechanics. We skewered some baseballs and Wiffle balls and used a wind tunnel to measure the forces—lift, drag, and side or lateral forces—as functions of things like the airspeed and spin rate. For the Wiffle ball, we also varied the orientation of the ball with respect to the airflow, making our own version of the manufacturer’s pitching instructions.
With perforations on either side of the ball, we found that the Wiffle balls experienced a lateral force that generally acted to push the ball toward the position of the holes. Things got more complicated when the perforations were on the upstream portion of the ball. As shown in the first image below, fog traces the airflow over a ball with the holes facing the flow, with a symmetric wake pattern suggesting that if we untethered the ball it would fly straight. The second image shows the flow over a ball with its holes facing up, and a wake that is deflected upward, meaning that the ball is experiencing a downward force.
Unlike a baseball, air can flow through a Wiffle ball. Our results suggested that some airflow is captured within the ball, and that this captured air creates a “trapped vortex” effect that also induces a force on the ball. This effect can either compete with or complement the asymmetric pressure distribution outside the ball due to the perforations. So, we measured the airflow inside the balls in the wind tunnel, and also performed computational fluid-dynamics calculations to confirm this (below).
Which effect—internal or external flow—dominates the ball trajectory depends on the pitching speed, and how much spin the pitcher puts on the ball. Especially at moderate speeds, nuances such as ball scuffing can be decisive.
Publishing a study of Wiffle-ball aerodynamics reveals just how many people care about Wiffle ball, and how deeply. I learned about Wiffle leagues full of adult players, and a tournament in which pitchers hurl lightweight Wiffle balls at 80 to 90 miles per hour, with wicked curves and drops. This isn’t just tweens trying to avoid elbow injuries or broken windows; Wiffle ball is serious. Players sent me modified Wiffle balls with handwritten notes describing the effects the alterations had on their pitches. Some of the notes implied (or insisted on) a challenge: See if your fancy wind tunnel can figure this out.
There’s a hot-rod culture at work in the customization of Wiffle balls. It’s legal in many adult leagues, too. Scuffing will rough up the surface, creating a locally turbulent airflow and a reduced pressure that pulls the ball toward the scuff. Some players use a knife to change the shape of the Wiffle perforations, demonstrating their intuition that the airflow into the ball might be important. Indeed, this alteration can reroute or accelerate the entering air, changing the ball’s eventual trajectory due to the trapped vortices inside the ball. Blocking some of the perforations will have similar effects.
Pitching with top, back, or sidespin—as a former baseball pitcher is likely to do when taking up Wiffle ball later in life—makes things even more interesting. And the unhittable knuckleball resulting from a Wiffle ball lobbed with its perforations toward the batter is a natural effect of the uneasy balancing act being performed by these competing effects of internal and external aerodynamics.
The Mullany family and their company have kept the ball itself above the fray of modifications, never varying the features of the product or marketing any “air effects” kits. Players have full ownership of their modifications, and the company’s lack of involvement lends a sense that the player scuffing or knifing a ball is a bit of an outlaw, rebelling against the rigidity of baseball and its regulations.
* * *
The Wiffle ball was designed to increase access to the thrills of baseball. It made trick pitches available to younger players, and also made the game accessible to adults whose ball-playing glory days are now behind them, offering an experience similar to baseball without the risk of rotator-cuff injury. The Wiffle Ball company has cultivated this all-American, family-run story with their “it just works, who could possibly explain it?” mystique. There is innocent nostalgia baked into the game. And yet, understanding how the ball works its magic takes none of that folk glory away. Instead, it increases respect for the sport, by showing how much is possible within it.
This article appears courtesy of Object Lessons.


Facebook lives and dies by its algorithms. They decide the order of posts in your News Feed, the ads you see when you open the app, and which which news topics are trending. Algorithms make its vast platform possible, and Facebook can often seem to trust them completely—or at least thoughtlessly.
On Thursday, a pitfall of that approach became clear. ProPublica revealed that people who buy ads on Facebook can choose to target them at self-described anti-Semites. The company’s ad-targeting tool allows companies to show ads specifically to people who use the language “Jew hater” or “How to burn Jews” on their profile.
Should Facebook Ads Be Regulated Like TV Commercials?
The number of people in those groups alone isn’t large enough to target for an ad, as Facebook will only let advertisers focus on groups of several thousand people. The platform’s ad-selling portal reported, at most, 2,200 self-interested “Jew haters.”
But the anti-Semitic categories can be lumped up into larger groups, which can be targeted. When ProPublica first tried buying an ad, the service suggested that it add “Second Amendment,” presumably because self-described “Jew haters” also expressed an interest in that law. ProPublica declined its suggestion, ultimately targeting fans of Germany’s National Democratic Party, an ultranationalist organization often described as neo-Nazi.
ProPublica does not argue that Facebook actually set up an anti-Semitic demographic that can be targeted by advertising. Rather, it suggests that algorithms—which developed the list of targetable demographics—saw enough people self-describing as “Jew hater” to lump them into a single group.
“Facebook does not know what its own algorithm is doing,” said Susan Benesch, a faculty associate at the Berkman Klein Center for Internet and Society and the director of the Dangerous Speech Project. “It is not the case that somebody at Facebook is sitting in a dark room cackling, and said, ‘Let’s encourage people to sell Nazi ads to Nazi sympathizers.’”
She continued: “In some ways, that would be much easier to correct, because there would be some intention, some bad person doing this on purpose somewhere. The algorithm doesn’t have a bad intention.”
“It’s not surprising, sadly, that people are expressing hateful views on Facebook, but it was surprising to see that hate reflected back in the categories that Facebook uses to sell ads,” said Aaron Rieke, a technologist at Upturn, a civil-rights and technology consulting firm based in Washington, D.C.
He suggested that human oversight may be the best mechanism to avoid this kind of mistake. “We don’t need to be too awed by this problem,” he told me: Even if its list of algorithmic categories is tens of thousands of entries long, it would take relatively little time for a couple paid, full-time employees to go through it.
“This is not a situation where you’re asking Facebook to monitor the ocean of user content,” he said. “This is saying: Okay, you have a finite list of categories, many of which were generated automatically. Take a look, and see what matches up with your community standards and the values of the company. Facebook can monitor the things it does that make it money.”
After ProPublica published its story, Facebook said that it had removed the offending ad categories. “We know we have more work to do, so we're also building new guardrails in our product and review processes to prevent other issues like this from happening in the future,” Rob Leathern, a product-management director at Facebook, said in a statement.
He also reiterated that Facebook does not allow hate speech. “Our community standards strictly prohibit attacking people based on their protected characteristics, including religion, and we prohibit advertisers from discriminating against people based on religion and other attributes.”
Last October, another ProPublica investigation revealed that Facebook allowed landlords and other housing providers to omit certain races when selling advertising, a practice that violates the Fair Housing Act.
To Jonathan Zittrain, a professor of law at Harvard University, that story suggests the entire way that tech companies currently sell ads online might need an overhaul. “For categories with tiny audiences, with titles drawn from data that Facebook users themselves enter—such as education and interests—it may amount to a tree falling in a forest that no one hears,” he said. “But should anything more than negligible ad traffic begin with categories, there’s a responsibility to see what the platform is supporting.”
He continued:
More generally, there should be a public clearinghouse of ad categories —and ads—searchable by anyone interested in what’s being shown to audiences large and small. Unscrupulous lenders ought not to be able to exquisitely target vulnerable people at their greatest moments of weakness with no one else ever knowing, and real accountability takes place through public channels, not only through adding another detachment of “ad checkers” internal to the company.
Indeed, Facebook might not be alone in permitting unscrupulous ads to get through. The journalist Josh Benton demonstrated on Thursday that many of the same anti-Semitic keywords used by Facebook can also be used to buy ads on Google.
For Facebook, this entire episode comes as the platform comes under unprecedented political scrutiny. Last week, Facebook admitted to selling $100,000 in internet ads to a Kremlin-connected troll farm during last year’s U.S. presidential election. The ads, which the company did not previously reveal, could have been seen by tens of millions of Americans.
On Wednesday, Facebook announced new rules governing what kinds of content can and can’t be monetized on its platform, prohibiting any violent, pornographic, or drug- or alcohol-related content. The guidelines followed the U.S.-wide debut of a video tab in its mobile app.
Zittrain said the episode reminded him of the T-shirt vendor Solid Gold Bomb. In 2011, that firm used software to auto-generate more than 10 million possible T-shirts, almost all variations on a couple themes. Shirts were only produced when someone actually ordered one. But after the Amazon listing for one shirt, which read “KEEP CALM AND RAPE A LOT,” went viral in 2013, Amazon shut down the company’s accounts, and it saw its business collapse.
His point: Algorithms can seem ingenious at making money, or making T-shirts, or doing any task, until they suddenly don’t. But Facebook is more important than T-shirts: After all, the average American spends 50 minutes of their time there every day. Facebook’s algorithms do more than make the platform possible. They also serve as the country’s daily school, town crier, and newspaper editor. With great scale comes great responsibility.


Suddenly, everything is a computer. Phones, of course, and televisions. Also toasters and door locks, baby monitors and juicers, doorbells and gas grills. Even faucets. Even garden hoses. Even fidget spinners. Supposedly “smart” gadgets are everywhere, spreading the gospel of computation to everyday objects.
It’s enough to make the mundane seem new—for a time anyway. But quickly, doubts arise. Nobody really needs smartphone-operated bike locks or propane tanks. And they certainly don’t need gadgets that are less trustworthy than the “dumb” ones they replace, a sin many smart devices commit. But people do seem to want them—and in increasing numbers. There are now billions of connected devices, representing a market that might reach $250 billion in value by 2020.
Why? One answer is that consumers buy what is on offer, and manufacturers are eager to turn their dumb devices smart. Doing so allows them more revenue, more control, and more opportunity for planned obsolescence. It also creates a secondary market for data collected by means of these devices. Roomba, for example, hopes to deduce floor plans from the movement of its robotic home vacuums so that it can sell them as business intelligence.
But market coercion isn’t a sufficient explanation. More so, the computational aspects of ordinary things have become goals unto themselves, rather than just a means to an end. As it spreads from desktops and back-offices to pockets, cameras, cars, and door locks, the affection people have with computers transfers onto other, even more ordinary objects. And the more people love using computers for everything, the more life feels incomplete unless it takes place inside them.
* * *
A while back, I wrote about a device called GasWatch, a propane-tank scale that connects to a smartphone app. It promises to avert the threat of cookouts ruined by depleted gas tanks.
When seeing devices like this one, I used to be struck by how ridiculous they seemed, and how little their creators and customers appeared to notice, or care. Why use a computer to keep tabs on propane levels when a cheap gauge would suffice?
But now that internet-connected devices and services are increasingly the norm, ridicule seems toothless. Connected toasters promise to help people “toast smarter.” Smartphone-connected bike locks vow to “Eliminate the hassle and frustration of lost keys and forgotten combinations,” at the low price of just $149.99. There’s Nest, the smart thermostat made by the former designer of the iPod and later bought by Google for $3.2 billion. The company also makes home security cameras, which connect to the network to transmit video to their owners’ smartphones. Once self-contained, gizmos like baby monitors now boast internet access as an essential benefit.
The trend has spread faster than I expected. Several years ago, a stylish hotel I stayed at boasted that its keycards would soon be made obsolete by smartphones. Today, even the most humdrum Hampton Inn room can be opened with Hilton’s app. Home versions are available, too. One even keeps analytics on how long doors have been locked—data I didn’t realize I might ever need.
These devices pose numerous problems. Cost is one. Like a cheap propane gauge, a traditional bike lock is a commodity. It can be had for $10 to $15, a tenth of the price of Nokē’s connected version. Security and privacy are others. The CIA was rumored to have a back door into Samsung TVs for spying. Disturbed people have been caught speaking to children over hacked baby monitors. A botnet commandeered thousands of poorly secured internet-of-things devices to launch a massive distributed denial-of-service attack against the domain-name system.
Reliability plagues internet-connected gadgets, too. When the network is down, or the app’s service isn’t reachable, or some other software behavior gets in the way, the products often cease to function properly—or at all.
Take doorbells. An ordinary doorbell closes a circuit that activates an electromagnet, which moves a piston to sound a bell. A smart doorbell called Ring replaces the button with a box containing a motion sensor and camera. Nice idea. But according to some users, Ring sometimes fails to sound the bell, or does so after a substantial delay, or even absent any visitor, like a poltergeist. This sort of thing is so common that there’s a popular Twitter account, Internet of Shit, which catalogs connected gadgets’ shortcomings.
As the technology critic Nicholas Carr recently wisecracked, these are not the robots we were promised. Flying cars, robotic homes, and faster-than-light travel still haven’t arrived. Meanwhile, newer dreams of what’s to come predict that humans and machines might meld, either through biohacking or simulated consciousness. That future also feels very far away—and perhaps impossible. Its remoteness might lessen the fear of an AI apocalypse, but it also obscures a certain truth about machines’ role in humankind’s destiny: Computers already are predominant, human life already plays out mostly within them, and people are satisfied with the results.
* * *
The chasm between the ordinary and extraordinary uses of computers started almost 70 years ago, when Alan Turing proposed a gimmick that accidentally helped found the field of artificial intelligence. Turing guessed that machines would become most compelling when they became convincing companions, which is essentially what today’s smartphones (and smart toasters) do. But computer scientists missed the point by contorting Turing’s thought experiment into a challenge to simulate or replace the human mind.
In his 1950 paper, Turing described a party game, which he called the imitation game. Two people, a man and a woman, would go behind closed doors, and another person outside would ask questions in an attempt to guess which one was which. Turing then imagined a version in which one of the players behind the door is a human and the other a machine, like a computer. The computer passes the test if the human interlocutor can’t tell which is which. As it institutionalized, the Turing test, as it is known, has come to focus on computer characters—the precursors of the chatbots now popular on Twitter and Facebook Messenger. There’s even an annual competition for them. Some still cite the test as a legitimate way to validate machine intelligence.
But Turing never claimed that machines could think, let alone that they might equal the human mind. Rather, he surmised that machines might be able to exhibit convincing behavior. For Turing, that involves a machine’s ability to pass as something else. As computer science progressed, “passing” the Turing test came to imply success as if on a licensure exam, rather than accurately portraying a role.
That misinterpretation might have marked the end of Turing’s vision of computers as convincing machines. But he also baked persuasion into the design of computer hardware itself. In 1936, Turing proposed a conceptual machine that manipulates symbols on a strip of tape according to a finite series of rules. The machine positions a head that can read and write symbols on discrete cells of the tape. Each symbol corresponds with an instruction, like writing or erasing, which the machine executes before moving to another cell on the tape.
The design, known as the universal Turing machine, became an influential model for computer processing. After a series of revisions by John von Neumann and others, it evolved into the stored-programming technique—a computer that keeps its program instructions as well as its data in memory.
In the history of computing, the Turing machine is usually considered an innovation independent from the Turing test. But they’re connected. General computation entails a machine’s ability to simulate any Turing machine (computer scientists call this feat Turing completeness). A Turing machine, and therefore a computer, is a machine that pretends to be another machine.
Think about the computing systems you use every day. All of them represent attempts to simulate something else. Like how Turing’s original thinking machine strived to pass as a man or woman, a computer tries to pass, in a way, as another thing. As a calculator, for example, or a ledger, or a typewriter, or a telephone, or a camera, or a storefront, or a café.
After a while, successful simulated machines displace and overtake the machines they originally imitated. The word processor is no longer just a simulated typewriter or secretary, but a first-order tool for producing written materials of all kinds. Eventually, if they thrive, simulated machines become just machines.
Today, computation overall is doing this. There’s not much work and play left that computers don’t handle. And so, the computer is splitting from its origins as a means of symbol manipulation for productive and creative ends, and becoming an activity in its own right. Today, people don’t seek out computers in order to get things done; they do the things that let them use computers.
* * *
When the use of computers decouples from its ends and becomes a way of life, goals and problems only seem valid when they can be addressed and solved by computational systems. Internet-of-things gadgets offer one example of that new ideal. Another can be found in how Silicon Valley technology companies conceive of their products and services in the first place.
Take abusive behavior on a social networks as an example. Earlier this year, Chris Moody, Twitter’s vice president of data strategy, admitted, “We have had some abuse on the platform.” Moody cited stopping abuse as the company’s first priority, and then added, “But it’s a very, very hard challenge.” To address it, Twitter resolved to deploy IBM’s Watson AI to scan for hate speech. Google has a similar effort. One of its labs has developed Perspective, an “API that uses machine learning to spot abuse and harassment online.”
Sometimes tech firms will make efforts like these a matter of business viability—the search for “scalable” solutions to products and services. When I asked Twitter about Moody’s comments, a spokesperson told me that the company uses a combination of computational and human systems when reviewing safety content, but they couldn’t share many specifics.
It sounds promising, but the results are mixed. Twitter claims it’s getting better at fighting abuse, but it still seems to ignore even the worst cases. And Google’s Perspective can be fooled by simple typos and negations.
Even though it’s in these companies’ immediate business interest to solve abuse problems now, making online spaces safer is assumed to require a computational answer. Human content moderation is notoriously hard, admittedly. And the volume of content is so high, a matter Twitter emphasizes, that computational systems are needed to manage and operate them.
But perhaps managing abuse is “a very, very hard challenge” largely on account of that assumption. The very idea of a global network of people able to interact with one another anonymously precludes efficient means of human intervention. Twitter’s answer assumes that their service, which is almost entirely automated by apps and servers, is perfectly fine—they just need to find the right method of computational management to build atop it. If computer automation is assumed to be the best or only answer, then of course only engineering solutions seem viable.
Ultimately, it’s the same reason the GasWatch user wouldn’t choose a cheap, analog gauge to manage cookout planning. Why would anyone ever choose a solution that doesn’t involve computers, when computers are available? Propane tanks and bike locks are still edge cases, but ordinary digital services work similarly: The services people seek out are the ones that allow them to use computers to do things—from finding information to hailing a cab to ordering takeout. This is a feat of aesthetics as much as it is one of business. People choose computers as intermediaries for the sensual delight of using computers, not just as practical, efficient means for solving problems.
That’s how to understand the purpose of all those seemingly purposeless or broken services, apps, and internet-of-things devices: They place a computer where one was previously missing. They transform worldly experiences into experiences of computing. Instead of machines trying to convince humans that they are people, machines now hope to convince humans that they are really computers. It’s the Turing test flipped on its head.
* * *
There’s a name for that, as it happens: the “reverse Turing test.” CAPTCHAs, those codes in online forms that filter out automated bots, are reverse Turing tests in which the computer judges whether a user is a human. There are also reverse Turing tests in which people try to guess which actor is a human among a group of computers.
These works use the Turing test as an experience in its own right, rather than a measure of intelligence. That precedent dates to the early days of computing. One of the most famous examples of imitation game-style chatterbots is Joseph Weizenbaum’s 1966 program ELIZA. The program acts like a Rogerian therapist—a kind of psychotherapy built on posing clients’ questions back to them. It’s an easy pattern to model, even in the mid-1960s (“What would it mean to you if you got a new line printer?” ELIZA responds heroically to a user pretending to be an IBM 370 mainframe), but it hardly counts as intelligence, artificial or otherwise. The Turing test works best when everyone knows the interlocutor is a computer but delights in that fact anyway.
“Being a computer” means something different today than in 1950, when Turing proposed the imitation game. Contra the technical prerequisites of artificial intelligence, acting like a computer often involves little more than moving bits of data around, or acting as a controller or actuator. Grill as computer, bike lock as computer, television as computer. An intermediary.
Take Uber. The ride-hailing giant’s main business success comes from end-running labor and livery policy. But its aesthetic success comes from allowing people to hail cars by means of smartphones. Not having to talk to anyone on the phone is a part of this appeal. But so is seeing the car approach on a handheld, digital map. Likewise, to those who embrace them, autonomous vehicles appeal not only because they might release people from the burden and danger of driving, but also because they make cars more like computers. Of course, computers have helped cars run for years. But self-driving cars turn vehicles into machines people know are run by computers.
Or consider doorbells once more. Forget Ring, the doorbell has already retired in favor of the computer. When my kids’ friends visit, they just text a request to come open the door. The doorbell has become computerized without even being connected to an app or to the internet. Call it “disruption” if you must, but doorbells and cars and taxis hardly vanish in the process. Instead, they just get moved inside of computers, where they can produce new affections.
One such affection is the pleasure of connectivity. You don’t want to be offline. Why would you want your toaster or doorbell to suffer the same fate? Today, computational absorption is an ideal. The ultimate dream is to be online all the time, or at least connected to a computational machine of some kind.
This is not where anyone thought computing would end up. Early dystopic scenarios cautioned that the computer could become a bureaucrat or a fascist, reducing human behavior to the predetermined capacities of a dumb machine. Or else, that obsessive computer use would be deadening, sucking humans into narcotic detachment.
Those fears persist to some extent, partly because they have been somewhat realized. But they have also been inverted. Being away from them now feels deadening, rather than being attached to them without end. And thus, the actions computers take become self-referential: to turn more and more things into computers to prolong that connection.
* * *
This new cyberpunk dystopia is more Stepford Wives, less William Gibson. Everything continues as it was before, but people treat reality as if it were in a computer.
When seen in this light, all the issues of contemporary technology culture—corporate data aggregation, privacy, what I’ve previously called hyperemployment (the invisible, free labor people donate to Facebook and Google and others)—these are not exploitations anymore, but just the outcomes people have chosen, whether through deliberation or accident.
Among futurists, the promise (or threat) of computer revolution has often been pegged to massive advances in artificial intelligence. The philosopher Nick Bostrom has a name for AI beyond human capacity: “superintelligence.” Once superintelligence is achieved, humanity is either one step away from rescuing itself from the drudgery of work forever, or it’s one step away from annihilation via robot apocalypse. Another take, advocated by the philosopher of mind David Chalmers and the computer scientist Ray Kurzweil, is the “singularity,” the idea that with a sufficient processing power, computers will be able to simulate human minds. If this were the case, people could upload their consciousness into machines and, in theory, live forever—at least for a certain definition of living. Kurzweil now works at Google, which operates a division devoted to human immortality.
Some even believe that superintelligence is a technology of the past rather than the future. Over millions of years, a computer simulation of sufficient size and complexity might have been developed, encompassing the entirety of what Earthly humans call the universe. The simulation hypothesis, as this theory is known, is of a piece with many ancient takes on the possibility that reality is an illusion.
But the real present status of intelligent machines is both humdrum and more powerful than any future robot apocalypse. Turing is often called the father of AI, but he only implied that machines might become compelling enough to inspire interaction. That hardly counts as intelligence, artificial or real. It’s also far easier to achieve. Computers already have persuaded people to move their lives inside of them. The machines didn’t need to make people immortal, or promise to serve their every whim, or to threaten to destroy them absent assent. They just needed to become a sufficient part of everything human beings do such that they can’t—or won’t—imagine doing those things without them.
There’s some tragedy in this future. And it’s not that people might fail to plan for the robot apocalypse, or that they might die instead of uploading. The real threat of computers isn’t that they might overtake and destroy humanity with their future power and intelligence. It’s that they might remain just as ordinary and impotent as they are today, and yet overtake us anyway.


Last week, Facebook disclosed to congressional investigators that it sold $100,000 worth of advertisements to a troll farm connected to the Kremlin surrounding the U.S. presidential election. These advertisements, which targeted voters with divisive political content, added even more evidence of Russia’s attempts to meddle with the election. But they also contributed to a larger conversation about free speech in an era where social-media posts replace political pamphlets and the public square has increasingly moved into cyberspace.
Tech giants like Facebook are largely left to decide for themselves how to arbitrate what is said on their platforms: what speech is permissible or not, whether to flag propaganda or not, and how to regulate advertisements. While, across mediums, it is illegal for foreigners to financially influence U.S. elections, last week’s news also means a new reckoning with the specific responsibility of tech companies to regulate the ads they sell and the content they host.
Legal and tech experts have growing concerns about individual private companies’ massive power over public discourse on the internet. The ability of any single entity to significantly arbitrate speech has previously belonged to the government alone. Now, Facebook, with a small number of its peers, has assumed much of the responsibility for regulating this influential realm online.
Jay Stanley, a privacy expert at the American Civil Liberties Union, sees danger in steps toward censorship on social media. “We would ideally like to see companies that provide a forum in which people communicate with each other to be free-speech zones, especially companies that play important roles in our national discourse,” he said. “Once companies go down the path of engaging in censorship, line-drawing decisions are often impossible, inconsistent, capricious, or downright silly.”
But Andrew McLaughlin, the cofounder of Higher Ground Labs, a company that invests in technology to help progressive candidates, believes that platforms should suppress propaganda in ad space. “Despite their best intentions, tech companies have built systems that are so open to manipulation by bots and trolls and other techniques that they effectively reward propaganda,” he says. “Failing to tackle that problem means ceding the terrain to fraudsters, fake-news pushers, and other kinds of propagandists, who easily gain the upper hand.”
Susan Benesch, a faculty associate at Harvard’s Berkman Klein Center for Internet and Society and the founding director of the Dangerous Speech Project, likewise falls in this camp. “If you deceive people consistently and on a large scale, you are probably damaging their willingness to engage as citizens in our democracy,” she says. She believes that the public should continue to pressure tech companies to create some mechanism for oversight as to what content is taken offline.
Facebook’s ad policy already prohibits some forms of messaging, such as use of politically or socially controversial material for commercial benefits. And on Wednesday, Facebook announced new guidelines for monetized content—including new steps to verify the authenticity of buyers, which could deter trolls and bots. One of its provisions is a warning against making money off some forms of deception: Users who “share clickbait or sensationalism, or post misinformation and false news, may be ineligible or may lose their eligibility to monetize.”
As far as non-ad content, tech companies censor certain disagreeable speech as well. Facebook, Twitter, and YouTube have removed ISIS-linked propaganda and accounts from their platforms. Following The Daily Stormer’s inflammatory coverage of Charlottesville, Google and the web-hosting company GoDaddy refused to provide service to the neo-Nazi website. Meanwhile, the internet company Cloudflare revoked the site’s DDoS-attack protections. And the chat app Discord banned other alt-right groups.
Eric Goldman, a codirector of Santa Clara University’s High-Tech Law Institute, sees these cases as “inextricably linked” to the recent controversy over Facebook advertisements. “On the one hand, I’m excited when I see social-media companies and other online services being thoughtful about what kind of content they want on their services,” he said. “On the other hand, whenever we see online services tinkering with political ads, we have the risk they might be adding their own biases into the mix.”
Experts seem to agree on two principles with respect to free speech and tech company arbitration. First, social-media companies, like other private publishers and unlike the government, are not bound by the First Amendment—meaning that they have discretion over what kind of speech is allowed on their platforms. And second, it is dangerous for the government to play a role in censoring content and advertisements on social media—beyond requiring companies to ban illegal activity from their platforms.
Further, many experts believe that mandating the disclosure of political-advertisement sources on Facebook, like the requirement for television or radio to identify ad sponsors on air, could help solve the problem. “Even if nothing else is done, it should be possible to require that political advertisers on Facebook embed the financing information in the ad, and it should be possible for Facebook to archive a copy of the ad with state elections officials or the FEC,” says Philip Howard, the director of research at the Oxford Internet Institute.
Senators Mark Warner and Martin Heinrich have suggested that social-media ads should be regulated like TV ads. And Warner, who vice-chairs the Senate Intelligence Committee, is calling for Facebook and Twitter to testify about Russian election meddling, as he sees the current landscape of social media and political campaigns as “the Wild West.”
Of course, with any new disclosure requirements, defining the realm of political speech for Facebook and other online entities won’t be simple. Most of the Russian ads disclosed last week were aimed at increasing the divisiveness across contested issues in America rather than overtly endorsing a candidate. Similarly, lawmakers would need to define the extent (if any) to which digital platforms should be required to investigate the underlying identities of its ad buyers.
Facebook, for its part, is “always exploring ways to make our ad policies more effective and transparent,” a press representative claims.
The story of Russian Facebook advertisements also fuels the debate about the responsibilities of American companies that serve international populations. For example, India has more active Facebook users than the United States. Chinmayi Arun, an assistant professor at the National Law University in Delhi, sees the need for additional mechanisms to both flag harmful content and to contest unreasonable censorship by platforms.
It remains unclear if Facebook will adopt this or other measures. Morgan Weiland, an attorney and scholar affiliated with Stanford Law School’s Center for Internet and Society, believes that along with previous problematic content—like untrue news and hoax videos—last week’s revelations may help force tech companies to identify their values and define their role in the public discourse.
“Right now they are operating in an arena where they have some, but very few, legal responsibilities,” she says. “We are going to keep seeing examples of this kind, and at some point the jig is going to be up and the regulators are going to act. Will the tech companies be the first movers and decide what they [themselves] will get to say, or will the regulators be the ones that move first?”
Neil Richards, a law professor at Washington University in St. Louis, likens the present moment to a Facebook relationship status: “It’s complicated,” he says. “The way these advertising models subject all of us and our attention to advertising, a kind of mental pollution, is unprecedented in human history. The decisions we make regarding the way that tech companies are a market for advertising affect, in a very real sense, what kind of digital democracy we are going to build.”


Customers who shell out $999 for an iPhone X when it comes out in November will have a new party trick in their pockets: They’ll be able to unlock the phone with nothing more than a quick glance at the screen. When they look away, it will lock up again.
When new features like this one, which Apple is calling Face ID, make it easier to unlock a phone, they save time; Apple says iPhone users unlock their phones an average of 80 times a day. But make it too easy to get into a phone and people start to get nervous.
Apple executives emphasized the security of Face ID as they announced the latest generation of iPhones on Tuesday. Once a phone learns to recognize its owner, using a 3-D map made up of more than 30,000 infrared dots projected onto a person’s face, it’ll only unlock itself when it senses that the owner is looking straight at the device. The map can’t be fooled by photos or masks, said Phil Schiller, Apple’s senior vice president for marketing, at Tuesday’s event, and it’s stored locally in a secure part of the device. Locking the phone just requires closing your eyes or glancing away, so waving a phone in front of its sleeping owner won’t unlock it.
Even if Face ID is advanced enough to keep pranksters out, many wondered Tuesday if it would actually make it easier for police to get in. Could officers force someone they’ve arrested to look into their phone to unlock it?
That’s a question with no easy answer. Technologists had a similar question a few years ago when smartphones started rolling out fingerprint readers: Could cops make someone scan their thumbprint to unlock their phone? The answer, it turned out, is ... maybe. In several cases since 2014, state and federal judges have signed search warrants that compelled fingerprint unlocks. The Fifth Amendment protects people from having to give up information that could incriminate them, like a password or PIN code. But a thumbprint isn’t something you know, which would be protected by the Constitution; it’s something you are. Like DNA or your handwriting, physical attributes are usually considered outside the boundaries of Fifth Amendment protections.
Despite the judges’ decisions, some legal scholars disagree with the idea that the government should be able to use a search warrant to force people to unlock phones secured with biometric authentication, which relies on physical characteristics. The fact that a physical attribute is used for unlocking a device changes how it should be treated under law, the argument goes.
“When you put your fingerprint on the phone, you’re actually communicating something,” Albert Gidari, the director of privacy at Stanford University’s Center for Internet and Society, told me last year. “You’re saying, ‘Hi, it’s me. Please open up.’” That communication should be protected under the Fifth Amendment, just like a password, he said—and the same would hold for any other way of unlocking your phone using physical characteristics, including facial recognition.
Whether law enforcement could legally get someone to unlock their phone with their face will remain an open question until it’s been litigated. Presumably, this uncertainty wouldn’t be the deciding factor for most people buying the new phone, especially those who are already comfortable using their fingerprints to unlock their devices. (It’s far more likely the deciding factor would be the phone’s nearly four-digit price tag.)
In fact, the most meaningful changes in Apple’s digital security won’t arrive with its new flagship phone, but with iOS 11, the company’s new mobile operating system, which will be available for free next week on newer iPhone models.
With this upgrade, users will be able to click the power button five times fast to enter an emergency mode, with options to make an SOS call or display a medical ID. It also disables the fingerprint reader, immediately requiring a password to get back into the phone. (In the past, disabling Touch ID required restarting the phone, or waiting 48 hours.) It’s not clear yet whether emergency mode will also disable Face ID, but it seems likely that it would. And since constitutional protection for passwords is set in stone, those five clicks would quickly move an iPhone user out of the gray area of biometrics.
A second change, which was discovered last week by ElcomSoft, a Russian cybersecurity company, adds a simple step when a person connects their device to a computer. Before, the phone only required a user to unlock it—which could be done with a fingerprint scan—and then tap “trust” to establish a connection. Now, the process requires the user to enter the phone’s PIN or passcode, again avoiding the legal gray area.
This extra layer of protection may be especially useful at the U.S. border, writes Nicholas Weaver, a computer-security researcher at the International Computer Science Institute in Berkeley, in Lawfare. Taking advantage of a broad exception at the border to the Fourth Amendment, which protects against warrantless searches, border agents regularly look through passengers’ devices as they enter or even exit the country. Often, they copy the entire contents of a device to a computer using forensic software, where they can scan it thoroughly for keywords or contacts.
But at the border, the Fifth Amendment also isn’t always honored the way it would be elsewhere. Agents have shown that they’re more than willing to ask passengers for their phone PINs, which would nullify the protection of iOS 11’s extra password prompt.


Look, it’s true: People gather in Apple Stores. Kids play games. Design nerds fondle iPad Pros. Befuddled people seek refuge at the Genius Bar.
The stores have good vibes. Everything is clean. There are no sounds of commerce. No clanging till. No specials on an aisle. No mechanical belt sliding products toward a beeping scanner. People will tell you they like your new shoes. I love Apple Stores.
But there is one problem with calling an Apple Store an Apple “Town Square”—which the company announced it’s now doing at Tuesday’s iPhone event. Namely, the Apple Store is a store and not a town square.
Furthermore, they are truly explicitly not a public space. They are the opposite of a public space.
Since the 19th century, stores have served as gathering places for people. American department stores had cafés and gardens and all sorts of places for people to hang out. But they would have never had the audacity to confuse themselves with town squares. The nice stuff was just a way of bringing customers to the store to purchase goods.
And most surreally, a dominant problem for democracy at this moment is that truly public space doesn’t exist on the internet you access through your phone.
Internet platforms, as John Herrman has argued, merely masquerade as democratic spaces. But they are not. They are private, as private as an Apple Store.
In adopting the faux democratic language of Facebook and Twitter, Apple has made the perfect physical metaphor for the largely ineffable problem the internet poses to democracy.
Maybe that will make people realize how absurd it is to expect fundamentally commercial entities to build community or to serve liberal democracy or to make your voice heard or to act as an agora or whatever else.
These are businesses. They sell stuff. People buy it. That’s great.
Bringing these democratic ideas inside private enterprises seems nice, but it warps the very idea of “the public.” Who is excluded from the Apple Town Square that should have equal access to the soapbox?


For two decades now, Apple has been fighting a battle between attention and disregard. In 1997, when Steve Jobs returned as interim CEO, the company was a struggling maker of personal computers with limited market share. Then came the iMac, a Mac computer people finally wanted to own again. Then the iPod, which transformed the company into a maker of high-design personal electronics and accessories. Then, of course, the iPhone, which made Apple the most valuable company in the world—and changed forever the way people live, work, and play.
Apple’s latest plans, announced on Tuesday at its Cupertino headquarters in a new theater bearing Jobs’s name, suggest that the company has entered a new phase. The iPhone has become so popular that it’s almost hard to notice, like the air people breathe. So now Apple has a new job: to make the iPhone just as important when it’s old and banal as it was when new and extraordinary.
* * *
As Apple’s success has magnified, so have expectations for the company. Investors want assurances of continued growth, which has become a concern as the smartphone market has matured. Now everyone has one, and sales growth is stagnating.
For years, both the market and the public have been waiting on Apple’s response. The assumption has been that this would come in the form of a new product (or products). The iPad was the first candidate. Then the Apple Watch. Both have been successful, but not nearly as successful as the iPhone. Expectations have remained transfixed on new, more dramatic future products, among them the mythical Apple Car, whose future is uncertain.
This approach—lining up one new, killer product after another—seems almost impossible, even for Apple. But the company’s latest announcement points toward a new way of culturing attention, one that’s much more subtle than just getting people to buy or rent a glass rectangle year after year.
Attention is a strange thing. It’s often thought that the way to retain power or influence is to hold onto people’s attention—to keep it active, front and center. That’s how iPhone rose to prominence, after all: by ripping a hole in popular understanding of mobile telephony and introducing a totally new paradigm.
But over time, active attention recedes into the background. It has to. Extraordinary events, products, and ideas cannot survive as wild curiosities. They must be made ordinary. Such is the fate of every influential media form, from the electric light to the automobile to the refrigerator to the television to the smartphone.
Media’s true power comes from this habituation. When everyone relies on electricity. When everyone unloads a dishwasher. When everyone commutes by personal automobile. When everyone connects and reads and works and plays on a smartphone.
* * *
Apple’s announcement revealed two new approaches to manage its fall into habituation. One is technical and one is social.
The first approach changes the way an iPhone turns on. Apple’s new, flagship smartphone, the iPhone X, has an OLED display from bezel to bezel, supplanting the home button from the device’s front. To replace TouchID, the fingerprint sensor that provided security for device use and payment, Apple has introduced a new facial recognition technology called Face ID.
Its technical implementation is impressive, using a front-facing camera and dot projector to map and model an iPhone user’s facial features. Apple claims that it's secure and reliable, with one in a million odds of being cracked.
But to invoke Face ID to unlock a phone, the user must look at the device, so that the phone can see its owner’s eyes. This has a convenient side effect: People must pay deliberate attention to their iPhones again. No longer can one fail to notice the fact of the iPhone as a device mediating life in the background, even if it is about to be put to that use. First, the user must acknowledge it in the foreground. iPhone, see me. Recognize me, as I recognize you doing so.
It’s a small thing—a setup, perhaps, for something yet to come. But temporarily, at least, it reactivates the iPhone as a thing that demands and receives active, rather than just passive, attention.
The second approach is far stranger. Angela Ahrendts, Apple’s senior vice president of retail, announced that the company would rebrand its retail locations. Instead of stores, they would become “town squares, because they are gathering places.” She’s creating “plazas” in Apple’s largest stores, adding “boardrooms” for entrepreneurs, and recasting aisles as “avenues,” which are “like shop windows around a town square.” Apple’s Fifth Avenue store in Manhattan will become such a town square, as will a new one in the Carnegie Library in Washington D.C.—a controversial use of an architectural landmark that once served as a real public sphere rather than one remixed out via capitalism.
Whether successful or not, Apple’s town-square plan acknowledges that the company’s devices have had a substantial impact on the public sphere—and not always a good one. Access to information, people, services, and ideas now route frequently—sometimes exclusively—through rectangles. We know that the apps and services that provide that material, including Twitter and Facebook, have had a profound and disturbing effect on political reality.
Apple’s new, if deranged, interest in civics appears to respond to this reality. Apple town squares make physical and tangible a reality otherwise left immaterial: that screens have turned public spaces into private ones, and that those private spaces rumble with quiet threat.
* * *
In Tuesday’s address, Apple’s CEO, Tim Cook, said something so trite as to pass by without notice. “It’s truly amazing how much iPhone impacts the world each and every day,” he noted as he prepared to introduce the new iPhone models. “Our intention with iPhone has always been to create something so powerful, so immersive, and so magical that the hardware virtually disappears.”
For years, lines like this were understood as matters of usability. The Macintosh was a personal computer so intuitive anyone could use it. Later, the iPod and iPhone followed suit. But Cook is also talking about another kind of disappearance: the vanishing act that media perform when they become mature, mass media.
The future of Apple isn’t in tablets or watches or even cars. It’s in how well, or how poorly, it manages global life run by its smartphones. And how willingly the public lets it.


The iPhone is the single most successful consumer product of all time. It’s generated $762 billion of revenue for Apple, the most valuable company listed on the American stock exchanges. It has made Apple, more or less, the iPhone company.
And the model that the iPhone established—phone-on-glass, apps-on-phone—is “eating the world.” All of which which makes Tuesday’s upcoming announcement of the next generation of the phone an important event for the technology industry, generally.
But it’s an even bigger deal for Apple at this particular moment. Looking back at all the quarters since the company launched the iPhone in the fall of 2007, it is clear that this set of phones, this announcement, will determine if Apple can return to major unit and revenue growth or if Apple’s tremendous run building an already huge and profitable user base is over.
With the benefit of hindsight, it’s possible to discern several clear quantitative periods for the iPhone business. Here they are in a chart of the number of iPhone units sold each quarter since launch (note that Apple, like many companies, begins its fiscal year in October, so the extremely important holiday season falls into Q1):
First, there was the launch phase, in which Apple was selling just single-digit millions of phones, but its growth rates were phenomenal. Let’s call this The Launch Hyper Growth phase. It lasted from the third quarter of 2007 until the first quarter of 2010. Comparing quarters with the year prior (e.g. Q3 2009 vs. Q3 2008), the company posted annual growth of between 470 percent and 8,280 percent, despite supply problems.
Then, for three full years, Apple entered the The Great Scaling epoch. During this time, some notable things happened; Steve Jobs died in what Apple would call the first quarter of 2012. But nothing stopped the upward movement of the iPhone. Apple went from selling 8 to 9 million phones each quarter to selling between 26 and 37 million phones. The average year-on-year revenue growth through this period was 87.3 percent.
Next was the blowout first quarter of fiscal year 2013. Apple sold 47.79 million iPhones. And for two years, the company struggled to show the kind of growth in revenue or unit sales that it had in the previous three years. They were getting huge, and it made sense to many industry analysts that the “law of large numbers” would drop their growth rates considerably. It was The First Plateau. For these two years, Apple’s iPhone revenue averaged a year-on-year increase of a much more modest 14.1 percent.
Then came 2015. The Annus Mirabilis, or miraculous year. It started with selling nearly 75 million units in the first quarter and continued with huge unit and revenue leaps over the previous year. After many quarters of slowing growth, Apple blew the doors out with its new large-screen phones and they almost quadrupled their average year-on-year quarterly revenue growth to 51.7 percent.
In context, it has to be seen as one of the greatest corporate performances the world has ever seen. It was truly shocking.
Now, there’s the fiscal year 2016 and 2017 so far. Having posted such enormous numbers in The Annus Mirabilis, Apple has found itself unable to show growth without some exciting new species of phone. In the second quarter of 2016, revenue and unit sales fell on a year-over-year basis for the first time. Across the board, the average unit and revenue growth fell by 4.4 percent and 6.2 percent, respectively.
If you’re looking at the chart above and wondering how anyone could be worried about Apple, take a look at the chart below showing revenue growth rate.
Now the question is: As we roll the clock forward, will The No-Growth Era become simply The Second Plateau, or has Apple reached the edge of what’s possible for its high-price, high-margin mobile phone?
It’s all riding on this next generation of iPhones. Which could make Tuesday’s keynote/marketing speech a much more interesting affair than the last several Apple events.


Hurricane Irma slammed the west coast of Florida on Sunday, making landfall first in the Keys and then at Marco Island, 15 miles south of Naples. Since then, it’s been making its way northward, visiting destruction on the state as it weakens.
As the storm progressed through Florida, it knocked out the lights all over the state. In a press conference Monday morning, Eric Silagy, the president of the state’s largest electric utility, Florida Power and Light, estimated that more than half the state is without power. That’s more than 10 million people, which dwarfs the number who lost electricity during Hurricane Sandy, which had been the record holder for hurricane-related power problems with 6.2 million affected.
Florida Power and Light is the nation’s third-largest utility and provides power to 4.9 million homes and businesses. Early Monday morning, 4.4 million of those customers had lost power, some multiple times, as the utility restored service and then it was knocked out again. “We’ve had over 5 million outages across our territory. That is unprecedented,” Silagy said. “We’ve never had that many outages. I don’t think any utility across the country has. It is, by far, the largest in the history of our company.”
Already, the company has restored 1 million connections, though some only temporarily.
On Sunday, the utility’s VP of communications, Rob Gould, told ABC that residents on the east coast could expect a standard post-storm restoration timeline, but that the west coast’s electrical grid would need a “wholesale rebuild.”
“This is going to be a very, very lengthy restoration, arguably the longest and most complex in U.S. history,” Gould said.
That task will begin very soon. The company plans to have 16,000 people, including thousands from other utilities, working out on the lines.
The restoration of power to western Florida will be a test of the resilience of Florida Light and Power’s vaunted smart-grid infrastructure. The utility says it has invested over $3 billion in making its grid “stronger, smarter, and more storm-resilient.”
It was standing with FPL’s CEO that President Obama announced $3.4 billion in smart-grid grants through the Department of Energy as part of the stimulus package, and when the utility finished its smart-grid installation in 2013, it was lauded as smart-grid technology’s coming-of-age moment.
In other words, FPL’s grid was about the best the country could have brought to the table. And now, apparently, Irma has laid waste to at least a large chunk of that system.
What could a “wholesale rebuild” mean?
An electric grid is a complex technical system. There are power plants that feed electricity onto the grid and there are consumer loads that take the power. In between, there is connective tissue that allows for long-distance transmission of power as well as for the local distribution of that electricity: high-voltage transmission lines, substations, transformers, and regular local power lines.
Generally speaking, power outages usually happen toward the edges of the network, when local power lines get snapped or their poles are felled.
FPL’s statement indicates that the west-coast grid has sustained damage beyond the standard downed power lines. Pieces of the system’s core have been compromised. However, given how early they are in the effort to bring power back, they haven’t had a chance to inspect all their facilities.
“We haven’t seen structural damage,” the utility’s president Silagy said Monday morning, “but I am sure we will see some.”
This happened to several utilities during Hurricane Sandy in 2013—which, until today, had caused more people to lose power than any other in history. In a deep postmortem, Greentech Media detailed some of the carnage inflicted on one utility, Public Service Enterprise Group. Sandy had damaged 16 substations, one-third of its transmission circuits, multiple power plants, and thousands of distribution lines and poles. Two million of their 2.2 million customers lost service.
By November 3, a few days after Sandy struck, 65 percent of PSEG’s customers had power. A week later, 98 percent of service had been restored.
Given that the aftermath of Sandy is one of the parables of the electric-utility industry, it’s fair to say that Gould means that the restoration project on the western coast of Florida will take longer and be harder, despite FPL’s state-of-the-art grid.
FPL operates 6,926 miles of transmission lines and 605 transmission, subtransmission, and distribution substations. Major ones are shown on the map below, which highlights that most of FPL’s infrastructure is on the eastern seaboard. The red lines are key power-import links, which run into Georgia and tap the Southern Company’s power plants. Completed in 1982, they substantially upgraded peninsular’s Florida’s electrical reliability.
This Homeland Security map of substations shows the relative density of electrical infrastructure within the bottom half of the state.
FPL’s storm preparations are a matter of the public record. Each year, they submit a report to the Florida Public Service Commission detailing their distribution reliability and storm preparedness. Two things are clear from reading this document: First, FPL’s overall reliability was excellent both in absolute terms and relative to other utilities in the state. Second, the utility learned from the northeastern utilities’ experience with Sandy. For example, it installed flood monitors in 223 substations, which are supposed to help protect critical components, though how well they worked during Irma is unknown. They’ve also been investing hundreds of millions a year into hardening the system near key locations like police, fire, and sewage stations. And they established an initiative to coordinate more closely with local governments, an area that became a flashpoint after Sandy.
As the storm continues northward and the restoration effort begins in earnest, FPL’s ability to bounce back will become a test of how resilient the smart-grid and post-Sandy systems really are.
Despite the huge numbers of people who have lost power, the early results have portend a faster restoration of service. Silagy, in particular, noted that the substation flood monitors seem to have performed well.
“Frankly those flood monitors [at substations] saved 3 or 4 days of work and millions of dollars worth of equipment that would have had to be be replaced rather than simply re-energized,” the FPL president told reporters at the Monday press conference.


The U.S. oil industry pumps more than 3 billion barrels of crude per year. Oil crosses continents in pipelines like the Keystone, which moves 1.3 million barrels per day. It travels between them on tanker ships, the largest of which can carry 3.7 million barrels. When oil leaks, the disaster is quantified in barrels spilled—more than 250,000 from the Exxon Valdez, and at least 3 million from Deepwater Horizon. When oil sells, it is priced per barrel, and when it burns, its energy output is measured in “barrel-of-oil equivalents” (5.8 × 106 BTUs). The world of oil is a world of barrels.
And yet less and less of the oil trade requires actual barrels. In the movies they make good historical set pieces and symbols of future apocalypse. But there aren’t any barrels in the Dakota Access Pipeline. No barrels rolled off the Exxon Valdez. And the oil that spewed from Deepwater Horizon never had a chance to reach a barrel in the first place.
So why do people still talk about barrels when they talk about oil? Because the oil barrel became a concept rather than a physical thing.
* * *
The oil barrel almost didn’t survive the 19th century. Oil companies hated them: Barrels were leaky, costly, and cumbersome. But they were also necessary. When the first Pennsylvania wells began gushing in the late 1850s, prospectors scrambled to catch the erupting crude. Any container would do—whiskey or ale jugs, salt or turpentine vats. The best option was an old one: the casks still used today to age wines and whiskies. They dated to the Romans and Celts, who designed them to replace clay pots for moving wine and olive oil. In the first Pennsylvania oil fields, demand for those barrels rose so quickly that at times their price exceeded the value of oil itself.
The first oil barrels held between 31.5 and 45 gallons, but Pennsylvania producers settled on a common standard by the late 1860s. They based their new system on another old-world model. In 1482, King Edward IV had moved to eliminate shady dealing in the English herring industry by imposing a 42-gallon standard on shipping containers. Oil companies promised similar market consistency with an added bonus. They would sell oil in 40-gallon units, but buyers would also get “an allowance of two gallons” as a measure of good faith. The measurement stuck.
The barrel requirement was hard to solve. John D. Rockefeller fought the battle on multiple fronts. If Standard Oil had to use barrels, it would do it on the cheap. Barrel manufacturing became part of the oil monopoly—one tentacle of Rockefeller’s octopoid oil empire, as an editorial cartoon of the era cast his business. To cut prices, Standard cut trees, and acres of oaks became stacks of barrels. By the 20th century, Standard had developed steel containers that eliminated the need for trees (and Rockefeller made another fortune selling iron ore to steel plants). But the basic problems remained: The barrels were still hard to move, and Standard’s mass-produced steel ones had worse seals, meaning more leaks.
Standard and its competitors wanted to eliminate the barrels altogether. They developed railway tanks to replace barrel-filled boxcars and sent horse-drawn kegs to distribute kerosene to local retailers. Water transport was desirable but inefficient, because the barrels were so heavy (and leaky). But in the early 1870s, small tanker boats made shipment by river possible without barrels. By the end of the decade, Ludvig Nobel (brother of Alfred, the Nobel Prize founder) had developed a tanker called the Zoroaster to ship Russian oil across the Caspian Sea. And in the 1890s, similar ships sent Standard Oil all around the world.
These years also witnessed the emergence of the pipeline system. If oil could get from the well to the refinery in a single container, the barrels and the teamsters who moved them could be eliminated. From the earliest days of drilling, short pipelines built using wooden ducts and gravity offered one approach—at least if they didn’t leak, which they usually did. Soon, iron pipes and pumps made longer lines viable. In 1865, the first commercial line ran five miles from Pithole City, Pennsylvania, to the railroad terminal in Oil Creek. In 1881, Standard connected the Pennsylvania oil fields to New York City, and over the next decade it created similar lines to link fields in Ohio with refineries in Chicago. By the 1890s, longer lines and larger pipes linked new fields from Oklahoma and the Gulf Coast to Illinois. The Keystone XL and Dakota Access Pipeline are just the latest versions of the same saga—the dream of an oil world without barrels.
* * *
The barrels finally changed in 1905, when Nellie Bly patented the steel drum still used today. Bly designed her barrels to carry more oil, 55 gallons, and to leak less. Her invention also caused the barrel’s meaning to separate from its physical reality. Despite adoption of the new, larger barrels, the old 42-gallon quantity remained the industry unit of measure for a “barrel”—as it still does. The barrels people used to hold oil were no longer the barrels they used to talk about it. A barrel became a quantity of oil futures sold, or crude spilled, or latent energy stored.
The discrepancy between the barrel’s physical reality and its meaning in industry parlance didn’t do anything to solve the barrel problem. Bly had improved the barrels, but no barrels at all would still have been better for the oil business. Even better would be symbolic barrels that existed in name only, used to measure production and sales, but hidden from sight. To present a good public face, the barrels that did exist were painted in corporate colors and logos. Like advertisements, promotional films, and glossy service stations, colorful containers like Standard’s “holy blue barrels,” as the muckraker Ida Tarbell called them, sold as pleasant and diverting an image as possible.
Talking about barrels that weren’t real made perfect sense. They were part of how the industry presented its contribution, measured in the number of barrels produced, to the economy, national security, and the everyday happiness of a nation of drivers. Could there ever be too many “barrels” in the Strategic Petroleum Reserve?
By the 1950s, the spread of pipelines, rail tanks, and tanker trucks meant that less and less oil needed to move in barrels. They found alternate use—in ports, for instance, where they transported raw materials and carried fuel for ships. But they also piled up in suburban junkyards as evidence of their looming obsolescence.
As waste objects, the barrels found new lives in the art world. As a young artist working in Paris, Christo Javacheff would drag them from the junkyard to his studio, where he learned to turn industrial objects into modern art by wrapping and stacking them. In 1962, less than a year after the construction of the Berlin Wall, he and his partner Jeanne-Claude used barrels to barricade Paris’s narrowest street. They called it the “Iron Curtain.” Their barrels—once icons of the global oil trade—became symbols of a world divided.
As objects, oil barrels have had many fates in the last half-century. Hobos and hippies turned the discarded ones into fireplaces. Trinidadian drummers turned them into steelpans. And a DIY manual for making your own barrel-bottom fire pit, trash can, planter, or kegerator is just clicks away.
For the oil industry, the 19th-century barrel problem has taken on new forms. In places like Nigeria, for instance, they support a thriving black market. Siphoned from leaky pipelines, this illicit oil moves in barrels again. Meanwhile scientists continue to search for better ways to move the stuff, perhaps even in a future without pipelines.
In authorized settings, the oil barrel continues to find uses in the industrial hinterlands. The last time I saw an oil barrel was in a Tuscan marble mine. I was touring the quarries in Carrara, home of the famed white marble that gave us Michelangelo’s “David”—along with countless kitchen countertops and bank facades. Near the end of the tour, something caught my eye. Tucked off in a corner was a pile of oil barrels, dented and dirty but with the brand names still visible. It was one of those unusual places where oil barrels remain more than just words.
The oil barrel tells a story of struggle between what industries need and what they want. Oil companies needed barrels, but they didn’t want them. They were dirty, wasteful objects. When the industry got rid of them, it became a little less dirty, and a little less wasteful. But mostly it became easier to trade the mess and the waste for the image the industry wanted: a world of energy, vitality, and security.
This article appears courtesy of Object Lessons.


Modern cargo ships are huge, slow machines that ply the world’s oceans, delivering fuel, raw materials, and products to power the economy.
Sailing the seas is dangerous. Thanks to safer ships and weather forecasting, it’s not as dangerous as it used to be, but rough seas can sink a modern ship, as we saw when the freighter El Faro sank during an Atlantic storm in October 2015, killing all 33 seafarers aboard.
This September, Irma has been beating its way across the Atlantic Ocean toward the edge of the Caribbean Sea. And cargo ships have been powering away from a broad swath of ocean to avoid the storm. In this case, many ships are seeking shelter on the west side of the islands that mark the eastern boundary of the Caribbean. Their specific routes are chosen by captains with the aid of company headquarters, usually relying on specialized forecasters who work with shippers.
MarineTraffic is a service that tracks the movement of ships using Automatic Identification System (AIS) beacons on board. They sent me images of ship positions from early on September 4th through September 5th. I was able to roughly match those frames with wind-track data provided by the National Hurricane Center, so you can see what it is that the ships are avoiding.
In the animation below, the maroon represents the route that hurricane-speed winds took. The yellow is the envelope of tropical-storm-level winds.
The ship colors represent different types of vessels: Tankers are red, cargo ships green. Pleasure craft are pink.
Already, ships are returning to routes that are behind the storm and clearing out of the areas that remain in the path. This map shows MarineTraffic data overlaid with a roughly contemporaneous image of Irma from the GOES satellite on Thursday.
As you can see, many ships are still hiding from the storm behind the islands, waiting for it to pass by on its way to making landfall in Florida.


Consumer data breaches have become so frequent, the anger and worry once associated with them has turned to apathy. So when Equifax revealed late Thursday that a breach exposed personal data, including social-security numbers, for 143 million Americans, public shock was diluted by resignation.
There are reasons for the increased prevalence and severity of these breaches. More data is being collected and stored, for one, as more people use more connected services. Corporate cybersecurity policy is lax, for another, and sensitive data isn’t sufficiently protected. Websites and apps, which are demanded by consumers as much as they serve the interests of corporations, expose paths to data that should be better firewalled. Software development has become easy and popular, making security an afterthought, and software engineering has failed to adopt the attitude of civil service that might treat security as a first-order design problem. And hacking and data theft have risen in popularity and benefit, both as an illicit business affair and as a new kind of cold warfare.
People have started to experience data loss and theft in a new way. Breaches have settled into a kind of modern malaise, akin to traffic or errands. They are so frequent and so massive that the whole process has become a routine.
Online data, like usernames and passwords, have been leaked and hacked with such frequency and in such great quantities (a hacker stole more than a billion Yahoo! email accounts in 2013), that savvy people treat their credentials as violated in advance. Breaches of more sensitive data, like bank, social-security, address, and health or employment records, have also become common. Home Depot, Target, Sony, Anthem, the U.S. Office of Personnel Management, and other recent violations felt shocking and violating at first, but over time that sensation has waned. With over half of the entire U.S. adult population potentially exposed by the Equifax breach, what’s left to do but shrug and sigh? I’ve got so many stacked-up subscriptions to credit-monitoring services from previous consumer breaches, adding another one would be superfluous.
Most organizations affected by hacks and leaks have treated the matter with great seriousness and care, understanding that their reputations were on the line. But whether intentionally or not, Equifax appears to have leaned into the new malaise, treating this massive breach with the bureaucratic apathy one might expect from a big, faceless credit-reporting agency—a company everyone must use, but no one chooses to.
The announcement of the breach, which came after hours on Thursday, offered the first sign of indifference. Media outlets, including The Atlantic, rushed to cover the matter, but details were slim. When my colleague Gillian White contacted them, Equifax offered no further comment beyond the materials they had published on an informational website. Other outlets experienced similar silence.
Those websites confused the matter more than they clarified it. The company had launched a new domain, equifaxsecurity2017.com, to communicate about the breach. That site appeared, to some users, like a phishing effort. Given the option to assuage concern, why set up a new domain that would only instill more of it? Once inside, this sensation only amplified. The site offers a tool to “determine if your personal information may have been impacted by this incident,” but accessing it requires submitting a last name and the last six digits of a social-security number. That’s a lot of data to hand over to anyone, especially an organization that has just demonstrated that it cannot be trusted with it.
Once submitted, the website either confirms no impact, or it offers an ambiguous response, inviting the supposedly impacted person to sign up for credit-monitoring services from TrustedID Premier, an Equifax subsidiary. Even that task cannot be performed immediately; the user is presented with a date on which the process can continue. The website also warns that no further notice will be provided to the user. It recommends marking your calendar. Even those who were not affected, according to Equifax’s confusing tool, are invited to sign up for TrustedID, making the whole affair feel like a grotesque marketing campaign.
In press coverage and on social media, some have speculated that submission of the personal information requires the individual to agree to Equifax terms of service that mandate arbitration in the case of dispute. If true, such an agreement would prohibit affected parties from suing Equifax, including via class-action lawsuits. But even this ambiguity seems unclear. TrustedID Premier’s terms of use do require agreeing to arbitration to use the service, but TrustedID’s services are separate from Equifax’s. The terms page itself is identical to the one that appears on TrustedID’s stand-alone website, although it was updated the day before the breach was made public, suggesting that the company buttoned up in anticipation.
Ultimately, not only is it unclear if one must agree to arbitration for access to the free credit-monitoring services—it’s also uncertain if consumers even learn the fact and details of their breached data without signing up for TrustedID, with or without agreeing to arbitration with Equifax. The whole affair is permeated with unknowable rules, some of which feel like traps.
In the end, the truth of the Equifax breach—who was affected, and how, and what the company will do to help, and what the terms of such assistance entail—might not be the most important lesson from this incident. More than anything, it suggests that a corner has been turned in corporate consumer data responsibility. Like severe weather, breaches have become so frequent and severe that they can begin receding from prominence. No matter their grievous effects, Equifax’s response suggests that fatalism might replace responsibility, planning, and foresight. This is just what happens now.


Updated at 8:15 p.m.
On Thursday, Equifax, one of three major credit reporting agencies, revealed that highly sensitive personal and financial information for around 143 million U.S. consumers was compromised in a cybersecurity breach that began in late spring. There are only around 125 million households in the U.S.
Two Major Credit Reporting Agencies Have Been Lying to Consumers
According to the company’s statement, the cybersecurity breach started in May of this year and continued until it was discovered on July 29. While criminals did not appear to have accessed what Equifax describes as “core consumer or commercial credit reporting databases,” which help in the generating of credit scores, some pretty important personal information was accessed. According to the company, criminals were able to access the social security numbers, birth dates, and addresses for a massive—but as yet unspecified—number of U.S. consumers. The hack also included credit card numbers for more than 200,000 Americans and documentation related to disputes, which contain personal and identifying information, for some 180,000 Americans. On top of that, financial disclosures show that three top Equifax executives sold $1.8 million worth of company stock in the days after the breach was discovered, according to Bloomberg.
"This is clearly a disappointing event for our company, and one that strikes at the heart of who we are and what we do. I apologize to consumers and our business customers for the concern and frustration this causes," said the company’s CEO Richard F. Smith in a statement. Equifax declined to comment further.
As Sarah Jeong has written before for The Atlantic, new technologies have resurfaced old problems related to the collection—and protection—of financial data. The circumstances in present times are reminiscent of the ones that precipitated the creation of the Fair Credit Reporting Act of 1970, she explains. But even with rules in place about how to separate and collect financial data from individuals, the transition to digital has brought those problems back, and they haven’t yet been satisfactorily addressed. Add to that the ongoing challenge of securing important information online—one that just about every organization faces—and the ability to harm the public in the course of normal operations for businesses built to collect and create crucial, personal, highly sensitive data becomes enormous.
This breach comes on the heels of a recent finding by the Consumer Financial Protection Bureau, the government agency responsible for monitoring and regulating the financial industry, that Equifax had been deceiving American consumers, signing them up for costly products without their knowledge, misrepresenting credit scores, and violating the Fair Credit Reporting Act. At the start of 2017, Equifax, along with another credit reporting agency, Transunion, were ordered to pay $23 million in fines and restitution by the CFPB. The credit-reporting industry is controlled largely by three companies: Equifax, Experian, and Transunion. Their culling and dissemination of financial data is what allows—or prevents—people from being able to buy or rent houses, get auto loans, have credit cards, and a host of other everyday necessities.
The Equifax breach, in its size, duration, and scope, is more than an unfortunate mishap. Part of the tragedy in all of this is that, those whose information has been compromised never asked to have their information collected in the first place—all major credit reporting agencies receive data directly from a host of financial companies, such as banks and credit card companies, in order to build credit reports. For Americans who want to protect their personal financial information, there is no way, in our current system, to do so.


“Toxicity is a relative thing.” So Arkema executive Richard Rennard described the noxious fumes emanating from a plant that had been flooded by Hurricane Harvey last week. Locals had claimed that the plant, which is dangerously close to residential areas, had caught fire—with some containers possibly exploding—and sent potentially poisonous chemicals across the area. Rennard and other Arkema officials vehemently denied those claims, claiming that the “pops” residents heard were not explosions, and that the chemical fumes leaking from the plant were “noxious,” but not necessarily poisonous.
A new lawsuit filed in a Harris County district court not only directly contradicts those claims from Arkema, but paints a much more harrowing picture of the facility’s meltdown following the flood. The suit alleges that a series of explosions on August 31 spread dangerous fumes to a perimeter 1.5 miles around the plant, where it incapacitated police officers charged with maintaining that perimeter, and then even overwhelmed medical professionals responding to their calls. The suit—filed by some of those first responders who say they were made ill by the fumes—also alleges that a series of negligent decisions by Arkema and operators at the plant led directly to those explosions, and to planned explosions on September 3 that it claims spread contaminated material into the surrounding neighborhoods.
The Crosby, Texas, facility, which French-owned Arkema operates, manufactured organic peroxides for the creation of plastics. Hurricane Harvey, which made landfall in Texas on August 25 and stalled over the gulf region of the state for the week after, dumped up to 40 inches of rain on the Crosby plant. Its main power source went out on August 27, and backup generators failed on August 28. Since organic peroxides need refrigeration to avoid ignition, Arkema feared explosions were imminent. On August 30, concerned about such explosions, the Harris County fire marshal evacuated people within that 1.5 mile perimeter, citing “a potential for a chemical reaction leading to a fire at the facility, which could produce a large amount of black smoke.”
The lawsuit alleges that at least some of that buildup was avoidable. The plaintiffs cite the frequency of flooding in Houston—which has seen disastrous floods in each of the past three years—as a reason for Arkema officials to have been better prepared for the flood. According to the plaintiffs, Arkema “never heeded the warnings and ignored the foreseeable consequences of failing to prepare.” Those preparations might have included the adequate provision of backup refrigeration services in the case of loss of power, and the evacuation of critical personnel.
On the day of the fires, the plaintiffs allege that first responders were required to secure the perimeter and wait for the “inevitable” explosions that would come. According to the suit, the fumes immediately sickened police officers, then sickened emergency medical personnel who were called to tend to them, and left several individuals hospitalized. Further, the suit claims that those responders had been put in the position to fall ill—and suffer potential unknown future health issues—because of the misrepresentations of Arkema executives, who have maintained that the substances leaking into the air and exploding were not toxic.
Just days after the last planned explosions at the Crosby facility, residents near the plant began moving back into their homes. But the lawsuit by the first responders has some implications for them as well. Residents have been wary of Arkema’s assurances that the plants fumes weren’t anything more than unpleasant, and of claims that other forms of chemical contamination hadn’t affected their homes. If during discovery—during which the plaintiffs have sought to obtain a restraining order on Arkema from tampering with evidence—and the course of the trial, the plaintiffs find that the chemicals first responders were exposed to were more dangerous than advertised, and that negligence on behalf of the company led to bodily harm, then residents of homes within and even outside the explosion may have similar concerns about exposure.
The suit, in which plaintiffs are seeking damages over $1 million, is one of a few proceedings that will probe the nature and genesis of the Crosby plant explosion. In a statement on August 31, the U.S. Chemical Safety Board announced it would launch an investigation into the Arkema processes and protocols as soon as “the emergency response activities have been completed and the facility is deemed safe for entry.”
Additionally, the Environmental Protection Agency, which apparently hasn’t inspected the Arkema facility in 14 years, is working with the Texas Commission on Environmental Quality to monitor the site. On September 6, TCEQ announced “an open investigation into the Arkema incident that will include an evaluation of any impacts due to the fires at the site.”


On my final night on Powder Mountain in Eden, Utah, I joined a yoga class in a window-walled lodge with dream catchers dangling from the rafters. The class’s attendees were blissed out from spending several days in late February skiing. As we stretched, a man wearing gray athleisure gave us all high fives before unfurling his own mat. It was unclear what we were being congratulated for—perhaps our luck. Whatever our stories, we’d landed here, in the snow globe that is the Wasatch Mountain Range.
Lying on my back, I could see the area’s surrounding mountains and the lights inside a dense thicket of town house–style condos—all that seemed to stand between the lodge and Paradise. Literally, Eden is a 58-mile drive south of the town of Paradise, Utah, making Powder Mountain some kind of mecca.
Since the 1970s, the mountain’s pilgrims have been skiers, drawn to its unpretentiousness and unbeatable conditions. Today, it’s also a Zion for a different kind of seeker. Construction began this summer on a public mountain town that will straddle a 10,000-acre site between three skiing bowls. In 2013, Powder Mountain was purchased by Summit, a company—or, perhaps more accurately, a collective—founded in 2008 by five 20-something friends who want to “catalyze entrepreneurship” and “create global change.”
With $40 million raised from its “members”—mainly tech entrepreneurs recruited through days-long events on the mountain, on cruise ships, or, this fall, in downtown Los Angeles—Summit plans to build 500 single-family houses, a village for amenities, and a home for the organization’s nonprofit arm, Summit Institute. By 2022, Summit says much of the village will be operational and occupied, with full build-out occurring over the next 20 years.
Despite having some of the trappings of an exclusive retreat for start-ups, Summit Powder Mountain is not a gated community or a resort. Its founders say they are creating a year-round community for innovators, thought leaders, artists, scientists, and others to solve the world’s most pressing challenges, from environmental catastrophe to access to basic medical care.
They plan to do this, in large part, by physical design. Mapped out over years by a coterie of prestigious architects and planners, Summit adheres to a logical grid and strict aesthetic guidelines specifically meant to avoid a dissolution into a ski resort of McMansions and Gucci outposts. If you attract the right people, the founders envision, solutions to global problems will come on a shared chairlift ride up the mountain, or during a fireside chat between strangers.
While Summit doesn’t like to associate itself with the word “utopia,” its turn to architecture to reflect and contain its ideals puts it squarely in line with the hundred or so utopian societies, secular and religious, that were founded in the United States in the 19th century, and also the hippie communes of the 1960s and ’70s. Many of these societies failed because of change—in religion, politics, technology, war. But when they did succeed—as Summit hopes to—it often had a lot to do with how successfully their physical world matched their spiritual and intellectual goals.
* * *
I flew out to Powder Mountain during one of Summit’s weekend retreats, where 150 or so attendees paid $1,500 for skiing, farm-to-table meals, guest speakers, and entertainment. The retreats are aimed mostly at entrepreneurs, though the organization says anyone “kind and open-hearted” and “doing innovative work regardless of your discipline” can attend.
That’s a tricky line to toe when also offering a rarefied experience. At a communal dinner on my first night, guests gathered at one of the mountain’s original ’70s-era lodges. The banquet tables were set with tea lights, and we shared braised beef from a cow that had grazed the mountain over the previous summer. A spoken-word poet asked everyone to shout our names at the same time.
Later, while most people were skiing, I traveled between three lodges on the mountain, one of which is a yurt-like structure called Sky Lodge on the very top. Whiteout conditions prevented me from looking out over the deck and better envisioning Summit’s snaking site plan, so I worked on my laptop instead, sipping something called Saffron Elixir and listening to the deaf-blind advocate Haben Girma discuss her disability-rights work. When lunch was served, Summiters clambered in on their boots. “This is next-level, man,” I overheard one of them say. (Summit has values—and it has perks.)
One neighbor, as Summit calls future residents of the town, is Bryan Meehan, the CEO of Blue Bottle Coffee. “There’s a lot of amazing progress happening in California in technology,” says Meehan, who lives in Marin, “but sometimes we don’t pay attention to the impacts of progress socially and environmentally. The conversation can be too focused on how beautiful this bottle of wine is, or ‘have you seen my new Tesla?’” At Summit, the discussions might be about poetry or regenerative agriculture. Pass out your business card and you’ll get the cold shoulder from the Summit community.
Christiana Moss, a cofounder of an architecture firm involved in design and planning on the mountain, refers to Summit’s founders as “compassionate capitalists”—businessmen with a heart, who want Powder Mountain to seed new companies, philanthropic endeavors, and creativity. “You have young people with revolutionary ideas hanging out with people with a lot of money who can make their ideas happen,” Moss says of Summit’s design. If it works like its supporters hope it will, Powder Mountain will become a new center for thought leadership, and one that isn’t located on either of the coasts: a new American campus.
* * *
As Chris Jennings writes in Paradise Now: The Story of American Utopianism, the experimental communities that were born in the 19th century “intended to catalyze a global revolution by building a working prototype of the ideal society.” The Shakers built more than 20 communities in the United States and are known for their distinct architectural style, with its identical sets of doors, gates, and stairways—everything in perfect symmetry and broom-swept starkness. Others, like Brook Farm, inspired by transcendentalism and founded in the 1840s in West Roxbury, Massachusetts, attempted to construct a single building to house their entire community and bind it together spiritually.
In the 1960s and ’70s, rather than create communities meant to transform the world, a new wave of DIY counterculturists escaped it. They constructed camps like Drop City, established in 1965 by four art students and filmmakers who bought seven acres of land in southeastern Colorado and erected Buckminster Fuller–esque dome structures made of junkyard scraps. By 1977, Drop City had been abandoned, but not before a period of intense creativity ensued.
One of Summit’s inspirations is Aspen, Colorado, as the Aspen Institute’s founder Walter Paepcke originally conceived it. In the late 1940s, Paepcke began buying up property in the defunct mining town with the intent to make it not just a vacation retreat, but “a Kulturstaat, a civilized state organized around culture and thriving on it,” writes James Sloan Allen in The Romance of Commerce and Culture: Capitalism, Modernism, and the Chicago-Aspen Crusade for Cultural Reform. By the 1950s, Paepcke had succeeded in opening a ski resort, the Institute, the Aspen Music Festival and School, and the International Design Conference. But Greg Mauro, a venture capitalist who purchased Powder Mountain with the other founders, believes the opulence of Aspen of today would be anathema to Paepcke.
Sensitive to potential criticism that Summit Powder Mountain could become a wealth ghetto, the leadership is baking equal opportunity into the design of the town: subsidizing artists-in-residence, building cabins and hostels, adding low-income housing, offering fellowships to artists and entrepreneurs, and creating programming for the town of Eden’s 600 residents. It would be a fake town otherwise, says Mauro. As a managing partner of Learn Capital, a venture-capital firm focused on educational technology, he hopes to bring schools to the mountain, too, aiming to grow the permanent population one or two percentage points per year.
In part, Summit’s owners are banking on a 125-page book of design guidelines to discourage potential residents who would want to steer the town toward luxury. Private houses designed by architects such as Brian MacKay-Lyons (who, for years, created his own utopian experiment on his family’s farm, calling it the Ghost Architectural Laboratory) and Todd Saunders (whose work, like MacKay-Lyons’s, often feels like a dramatic extension of its surroundings) must adhere to the guidelines, which dictate what can and can’t be done on the mountain, down to a list of acceptable shrubs, houses capped at 4,500 square feet, and a “modern mountain” style.
* * *
No one at Summit mentioned Salt Lake City in any of our discussions, but the Mormon-founded city is only an hour’s drive south of Eden, and it’s an obvious example of urban planning and architecture as physical embodiments of ideals—and one that survives, in an evolved form, today. Just as Summit’s founders knew that Powder Mountain was their future home as soon as Mauro introduced them to it, Brigham Young supposedly declared “this is the place” about Salt Lake City in 1847, kicking off nearly a century of impressive architecture, industry, and growth for Mormons and non-Mormons alike.
Other utopias can look alluring on paper, but would have been dystopian in practice. Frank Lloyd Wright’s Broadacre City, an “anti-city” plan the architect introduced in 1932, laid out a gridded suburbia that isolated families within their own one-acre plots of land, and made the automobile central to survival. With no true center, Broadacre City would have siloed the programs that create a sense of community: schools, public transportation, retail, recreation.
Summit, too, has risks. The project has been criticized by some Eden residents who are wary about what is being built and how it will affect their town. The organization recently clashed with some people over water rights; “Summit Sucks Water” signs still dot some lawns. (Echoing Mauro, Jeff Rosenthal, another of Summit’s cofounders, says that if the town of Eden doesn’t accept the mountain town, he’ll deem the project a failure.)
There’s also the issue of diversity—of great importance to Summit. While Summit’s attendance is around 55 percent men and 45 percent women, the organization was founded by young, white men. For some members, that translates into a tangible feeling of a boys’ club. Personally and professionally, Summit has been a boon for Rebecca Clyde, a cofounder of a digital-marketing agency and a chatbot startup: She’s met friends and investors, and has considered buying property on Powder Mountain. But she chuckled when she attended a session about the development plans on Powder Mountain. “There were three men up there on this panel, and they were talking about why they loved that mountain: because the land has a feminine energy and the village and concept is trying to harness it,” she recalls. “Where are all the women who are going to bring all this feminine energy to life?”
The French philosopher Michel Foucault once wrote that utopias “present society itself in a perfected form, or else society turned upside down, but in any case these utopias are fundamentally unreal spaces.” Summit has money, time, sensitive architects, and a mission of goodwill—perhaps all the raw ingredients needed to actually succeed in creating a paradisiacal mix of leisure, thought leadership, philanthropy, and education.
Still, the intentional communities in the United States have tended to fall short of their initial feverish inspiration, or else survived in isolation from the rest of the world. Summit’s brand of utopianism is not that of the 19th century: Its community isn’t interested in multiplying Powder Mountains across the globe, nor is it cut off from the world in an outcropping of geodesic domes in the desert. As an idea, Summit Powder Mountain is some next-gen hybrid. Time will tell if it can become a real place—and not just another resort town.


In an effort to understand the politics of tech’s wealthy elites, Stanford researchers surveyed 600 technology-company founders, asking them about different policies.
The caricature of Silicon Valley people, which we’ve pilloried before, are that they are simple libertarians. The new survey continues to deconstruct this silly portrait. Fewer tech founders (24 percent) than Republicans (63 percent) or Democrats (44 percent) generally agree with a simple statement of libertarian philosophy: “I would like to live in a society where government does nothing except provide national defense and police protection, so that people could be left alone to earn whatever they could.”
But tech founder politics are different from either political party in unusual ways. Tech founders are developing a new, strange kind of politics that’s opposed to regulation, but for redistribution through taxation.
The New York Times has a really excellent visualization of the core ideological findings:
Look at the big bubble in the yellow tech founder column: Don’t regulate and do redistribute. None of the other large groups they surveyed showed a comparable interest in this political philosophy.
The “do not regulate” category was formed from responses to questions about regulating Uber, how the gig economy should be structured, whether it is too hard to fire workers, and the general proposition of whether “government regulation of business does more harm than good,” as well as specific questions about regulating drones, self-driving cars, and internet companies.
For example, 80 percent of tech founders think economic inequality is fine if it means the economy grows faster and 75 percent of tech founders think labor unions should lose influence. “They look like Republican donors when we ask them these questions,” said David Broockman, a Stanford Graduate School of Business professor who coauthored the study with fellow academic Neil Malhotra and journalist Greg Ferenstein.
And yet, when the researchers asked the tech founders about taxation and redistribution policies, they expressed major support for things like “universal healthcare, even if it means raising taxes,” increases in spending on the poor, and taxes on high-income individuals.
If tech founders had their way, government regulation might not stop you from financially falling through market action, but it’d bounce you back up. This is government not as a shaper of markets, but as a market-failure compensator. It’s not quite a “social safety net,” but maybe it’s a social trampoline.
This in part explains the interest of Silicon Valley in a universal basic income, which would distribute a cash stipend that’s enough to cover the basic necessities of life. While the nominal recent interest in UBI stems from fears over job displacement by robots, the UBI solution squares perfectly with the underlying ideology of tech founders. This is a group of people who don’t want government or organized labor to check their power over workers or restrict the disruptions their products may have in existing industries. But their liberal social politics lead them to wanting some kind of societal cushion for capitalism.
Which is one reason my colleague Ian Bogost suggested this name for the tech-founder political philosophy: “Trust me.”


Facebook claims it can reach more young people in the United States with advertisements than are actually alive, according to a new report.
Last week, a trade magazine in Australia looked at the “reach” statistics that Facebook gave for that country, and found that the company estimated it could reach more young people than Australia’s census.
Now, the firm Pivotal Research has done the same thing for America. “Through Facebook’s Ads Manager we can see that Facebook claims a potential reach within the United States of 41 million 18- to 24-year-olds, 60 million 25- to 34-year-olds, and 61 million 35- to 49-year-olds,” they wrote. “By contrast, U.S. Census data indicates that last year there were a total of 31 million 18- to 24-year-olds, 45 million 25- to 34-year-olds, and 61 million 35- to 49-year-olds.”
That’s an overstatement of 10 million people among 18- to 24-year-olds, and 15 million among 25- to 34-year-olds.
A Facebook spokesperson told The Wall Street Journal that the reach estimates “are not designed to match population or census estimates. We are always working to improve our estimates.”
Which, sure. Except that you’d probably want your estimates to match reality as closely as possible, and showing 30 percent more people than could possibly exist seems like a problem.
The larger context is that Facebook has been eating ever larger chunks of the digital-advertising market, soaking up ad dollars in large part because of the scale of the company’s user base (a representative example of coverage: “Facebook’s Scale Casts a Shadow Over Publishers”).
If just some of that scale is not real, that appears significant, even if, as the Journal notes, “the impact of the discrepancy isn’t clear” for advertisers—at least not yet. (Several advertising associations did not immediately respond to requests for comment.)
It’s easy to see how such estimates could go wrong. People create more than one account. They switch devices. They lie about their ages.
But that strikes at the heart of the other big lure of Facebook. This is the company that’s supposed to have the most and best user data. They’re supposed to have solved the problem of matching up real people with online personae. That was the point of a huge London Review of Books essay on the company. “Facebook, in fact, is the biggest surveillance-based enterprise in the history of mankind,” writes the journalist John Lanchester.
And yet, on Instagram, which shares data with Facebook and uses its parent company’s back-end advertising infrastructure, the company’s algorithms continue to believe that I live in Washington, D.C., and occasionally serve me local advertising to that effect.
This makes some sense, given that The Atlantic is headquartered in D.C. But I only lived there for 18 months, and I left in 2011.
Granted, I’m just one person, and anecdotes aren’t data. But I’ve been posting pictures from and receiving bills in Oakland for six years. Shouldn’t the “biggest surveillance-based enterprise in the history of mankind” know that?
Maybe these demographic notes matter far less than it’s thought for advertising—age, location, that kind of thing. Because the truth is, I’ve even purchased hundreds of dollars worth of sneakers and clothes through Instagram ads. They do understand what I want to buy, even if they don’t know where I live.


The weight of water can deform the Earth’s crust, if there’s enough of it. And we can measure that change with the ultraprecise global-positioning satellites humans have launched into orbit.
On Monday, Chris Milliner of the Jet Propulsion Laboratory tweeted a simple map visualizing data from the Nevada Geodetic Laboratory. It showed that the GPS data from special stations around Houston detected that the whole area had been pushed down roughly two centimeters by the weight of the water that fell during Hurricane Harvey.
GPS data show #Harveyflood was so large it flexed Earth's crust, pushing #Houston down by ~2 cm! #EarthScience #HurricaneHarvey #txflood pic.twitter.com/88lNScJBq9
Why this would happen is simpler than you might think. A gallon of water weighs about 8.34 pounds. And by one estimate, Harvey dropped 33 trillion gallons of water across the area it hit. So that’s roughly 275 trillion pounds.
And it turns out that scientists have measured the effects of loading a bunch of water onto land many times. For example, a 2012 study of the Himalayas detected a seasonal flux in the height of the mountains as water fell, and then ran off those mountains into Asia’s rivers.
A 2017 study in Science found that the Sierra Nevada exhibits “vertical surface displacement [with] peak-to-peak amplitudes” of 0.5 to one centimeter. More water is more mass. Less water is less mass. And the crust underneath that water responds to the changes.
One could see Harvey’s deposition of water as a fast-action version of what happens in the Earth’s mountain ranges each year.
There are some caveats, Milliner later explained. Some of the change could come from the soil underneath the GPS stations becoming compacted by the water’s weight. But because some stations located on bedrock also experienced the depression, he believed that the key mechanism was crust deformation. It’s also possible, he elaborated, that some areas outside Houston were pushed up by the way the water squished the Houston crust.
Perhaps a hurricane seems a match for the crust of the Earth, in mental scale. But humans can also have this sort of effect (even without calculating how much of Harvey’s water was due to climate change). Huge dams can impound water on a scale of Hurricane Harvey’s rains. The Hoover Dam and Three Gorges Dam both created reservoirs with a capacity of roughly 10 trillion gallons of water.
The effect of the filling of the Three Gorges Dam has experienced substantial attention from Chinese researchers, in part because there was a marked increase in small earthquakes in the region as the reservoir was filled. That’s not a concern in Houston, which is not a seismically active region.
It also remains to be seen how quickly the Earth rebounds in Houston, given that the water is rushing back to the sea, rather than sitting on mountaintops as glaciers or in reservoirs as hydrologic storage. And one assumes there’s an interesting paper in that for a geodetic researcher.
And there are (at least) two more ways humans are changing the crust of the Earth. Because the climate is warming, there’s less ice—and therefore less mass—attached to the world’s mountaintops. “The Earth, behaving like an elastic body, uplifts in a response to the load loss,” the Himalayan researchers write.
Meanwhile, in Houston, the ground had already been sinking—scientists call it subsidence—because humans have pumped the groundwater out of the aquifers under the city.
With each passing year, the combined and uncoordinated efforts of humanity force the mountains a little higher and the flats a bit lower.


In 2009, with little attention from abroad, the government of India launched a new identification program that has gone on to become the largest biometric database in the world. The program, known as Aadhaar, has collected the names, addresses, phone numbers—and perhaps more significantly, fingerprints, photographs, and iris scans—of more than 1 billion people. In the process, Aadhaar has taken on a role in virtually all parts of day-to-day life in India, from schools to hospitals to banks, and has opened up pathways to a kind of large-scale data collection that has never existed before.
The Indian government views Aadhaar as a key solution for a myriad number of societal challenges, but critics see it as a step toward a surveillance state. Now, the Aadhaar experiment faces a significant threat from the Indian Supreme Court—one that may prove to be existential.
In late August, the Supreme Court issued a unanimous decision that found, for the first time, a fundamental right to privacy in the Indian Constitution. The decision has been widely celebrated by Aadhaar’s opponents, who believe that the program is in conflict with the newly enshrined right. Soon, the Supreme Court will direct its attention to this very issue, and if they find that Aadhaar violates privacy rights, it will be up to lawmakers to rethink the entire program. But if the Supreme Court rules that the program is constitutional, then Aadhaar, already staggering in scope and ambition, will continue to grow.
When the Indian government first launched Aadhaar, it saw an opportunity to harness the country’s burgeoning technology sector to reduce corruption and streamline the delivery of government services. Prior to the advent of Aadhaar, the government has said it was plagued by problems managing its welfare programs, and lost millions of dollars each year as Indian residents either inserted fake names or their own names multiple times into the system in order to withdraw more than their fair share of benefits. With Aadhaar, the practice of accessing benefits became a simple matter of touching a fingerprint scanner. If the fingerprint matches the one on file, the benefit can be approved and administered. When it is working well, the process is comparable to unlocking an iPhone, and ensures that government benefits go only to the people who qualify.
Open to all Indian residents, Aadhaar was optional at first and associated with only a handful of government subsidies, including those for food and liquefied petroleum gas for cooking. It was targeted at those who needed help the most, particularly rural villagers who lacked official forms of identification, and were therefore unable to open bank accounts or access welfare programs in the past.
But over time, mission creep set in. Under the leadership of Nandan Nilekani, the cofounder of the outsourcing firm Infosys (whom Jon Stewart once welcomed as his new overlord on The Daily Show), Aadhaar was used as a way to apply data-driven improvements to a wide range of government and private-sector services. Aadhaar was soon linked to so many activities that it has now become almost impossible to live in India without enrolling. Participation in the program is a requirement, or will be soon, for filing taxes, opening bank accounts, receiving school lunch in the state of Uttar Pradesh, purchasing railway tickets online, accessing some public Wi-Fi, participating in the state of Karnataka’s universal health-care coverage, and benefiting from a wide range of welfare programs. The Indian Member of Parliament Jairam Ramesh has sarcastically described the program as “compulsorily mandatorily voluntary.”
The government authority responsible for administering Aadhaar declined to comment, but in an op-ed for The Indian Express, its CEO, Ajay Bhushan Pandey, wrote that the program has saved the government approximately $8 billion in the past two years alone (the World Bank has estimated that the figure is closer to $1 billion per year). Pandey says that the program has succeeded in improving the government’s capacity for reaching and serving people directly.
For many of the communities Aadhaar was designed to help—particularly the poor and the underserved—the technology hasn’t lived up to the sunny rhetoric, however. In a country with inconsistent internet outside of its large cities, remote towns struggle to get online to authenticate peoples’ fingerprints with the central database. Some enrollees insist that their satellite-internet access works only on cloudy days; others say it functions best when it’s sunny.
According to an analysis of government data by Reetika Khera, a professor of economics at the Indian Institute of Technology in Delhi, millions of people have missed out on government benefits because of Aadhaar. In some cases, that’s because those who are elderly or disabled are unable to walk to the distribution sites to verify their identities. Others, who do manual labor, find that their fingerprints are too weathered from years of physical exertion to scan correctly, and so are denied their food rations.
Nikhil Dey, one of the founders of the grassroots organization Mazdoor Kisan Shakti Sangathan, also studied the government’s data. He found that approximately 1 million people in the state of Rajasthan had been unfairly dropped from the government lists for food subsidies due to Aadhaar, and more than 3 million were unable to collect their designated grain allocations. In one district alone, Dey says, 1,350 out of approximately 2,900 people marked “dead” or “duplicate” were actually neither, but lost access to their pensions anyway.
Despite these implementation challenges, the scariest parts about the program for privacy advocates are its ubiquity and lax security. According to the technology engineer Anand Venkatanarayanan, when biometric information is used to access a service via Aadhaar, such as purchasing a new cell phone, the service provider receives that person’s demographic data (name, address, phone number), and the government receives the metadata—specifically, the date and time of the transaction, the form of identification used, and the company with which the transaction was carried out. That information can paint a fuzzy but intimate long-term picture of a person’s life, and raises concerns about both government surveillance and private-sector abuse.
There is already ample evidence of misuse. High-profile examples from the past several months have dominated news cycles: 210 government agencies published full names, addresses, and Aadhaar numbers of welfare beneficiaries; 120 million users’ Aadhaar information appears to have been leaked from the telecommunications company Reliance Jio (the company claimed the data was inauthentic); bank-account and Aadhaar details of more than 100 million people were disclosed through certain open-government portals; the government’s e-hospital database was hacked to access confidential Aadhaar information.
These disclosures may be most damaging for those who are already vulnerable. Apar Gupta, a lawyer on the team that challenged Aadhaar before the Supreme Court, is particularly concerned about many Dalits (previously the “untouchables” in the caste system) and migrant laborers who work as manual scavengers, entering sewers without protection to clean them by hand. It’s a dangerous occupation with a high fatality rate, and it can also bring immense social stigma. Gupta worries that Aadhaar will permanently stigmatize these individuals by allowing future employers, schools, banks, and new acquaintances to view their database information and judge them based on their socioeconomic standing. Social mobility in India could become even more difficult. So could hiding a pregnancy or a gender-reassignment surgery, or failing the eighth grade. In many of the objections raised about Aadhaar, there’s a kernel of fear that the program could turn a person’s identity into a prison.
The August 24 Supreme Court ruling seemed to address these concerns, making the case that privacy is essential for an individual to function in society. “Privacy ensures that a human being can lead a life of dignity by securing the inner recesses of the human personality from unwanted intrusion,” Justice Dhananjaya Y. Chandrachud wrote. In arriving at its decision, the Supreme Court rejected two previous decisions from the 1950s and 1960s that denied a right to privacy, and instead framed privacy as a “primordial” right that must be understood in the context of an interconnected world. The justices further emphasized the point by referencing international jurisprudence about privacy from the United States, Canada, South Africa, and the European Union.
That didn’t surprise Mishi Choudhary, the legal director of the Software Freedom Law Center, who noted that “we’re at a stage where technology is sweeping the planet in almost the same way. A lot of countries are looking to each other for guidance on how to adapt their jurisprudence to suit the current world.”
Over the last few years, Russia, Tunisia, Morocco, and Algeria have all expressed interest in the Aadhaar program, and according to reports, representatives from Tanzania, Afghanistan, and Bangladesh recently visited India to learn more about implementing an Aadhaar system of their own. As the Supreme Court once again prepares for hearings about Aadhaar, the world will be paying close attention.


A series of satellite images provides an expected yet still breathtaking look at the flooding caused by Hurricane Harvey in Texas.
Taken by DigitalGlobe, the primary private provider of spy-satellite imagery to the U.S. government, they add to the stockpile of data that illustrate the storm’s record-setting rainfall.
The most striking combination of images, at the top of this story, shows a piece of Simonton, Texas, completely swallowed by brown flood waters.
At times, as in the images of Wharton below, the mud from the flood reveals the underlying topology of the land, showing how the bayou might flow without human intervention.
Other images show flooding in Angleton, Brookshire, Holiday Lakes, and Rosenberg.



Benjamin Franklin was attracted to electricity. Given its similar color, crackle, and configuration, he suspected that lightning itself was electricity. Noting that a pointed metal needle could draw electricity from a charged metal sphere, Franklin became convinced that a metal rod could coax lightning from the sky. Why? So it would strike the rod instead of buildings or passersby.
As legend has it, Franklin hopped on a horse in 1752 with key-adorned kite in hand, determined to prove his conviction. The two pranced about under stormy skies until the charged-filled atmosphere energized the key and confirmed his suspicions.
More than two-and-a-half centuries later, lighting rods persist—as decorative architectural pieces, as vestiges of the past, and as mitigators of lightning’s power.
* * *
Franklin later extended his lightning-rod idea to ships, including British warships, which were eventually outfitted with anchor chains that stretched from the top of their wooden masts to the sea. They aimed to dissipate electrical energy so the masts would stay intact if lightning struck. Soon, lightning rods were widely adopted in the northeastern United States, and elsewhere during the mid-1700s.
But not without resistance from some quarters, including the clergy. In fact, the Reverend Thomas Prince, pastor of Boston’s Old South Church, asserted that the Cape Ann earthquake of 1755 could be attributed to the ubiquitous placement of lightning rods in New England, especially in Boston. Centered off the coast of what’s now Massachusetts, the earthquake, Reverend Prince seemed to imply, was no accident given man’s unwise attempts to deflect the hand of God.
Today, Franklin’s lightning rods are known by many names: air terminals, finials, lightning conductors, or strike-termination devices among them. To me, calling them strike-termination devices makes it sound as if once a lightning bolt strikes the rods, the danger is averted. Instead, the rods, typically a half-inch in diameter, are connected to a metal cable hidden within the building or structure. The diameter of both the rod and cable vary depending on the height of the building and the type of metal. In general, the higher the building, the heavier the rods and cables. No matter the size, the cables make their way down to Earth, where they are anchored. Grounded, the lightning rod dissipates the lightning strike’s energy.
Without this seemingly simple system, damage to a structure can range from a minor insult to a complete loss. Parker M. Willard Jr. has seen just that. “We see a lot of damage from indirect strikes that come in through the utility lines,” he says. “The average insurance claim is $7,400, and I’ve seen some in excess of $700,000.”
Willard is the co-owner of Boston Lightning Rod Company, along with his father, Parker M. Willard Sr. Willard Jr.’s great-great-grandfather, Henry Willard, founded the company, which is based in Dedham, Massachusetts, 144 years ago. Now 40, Willard Jr. started working for Boston Lightning Rod when he was 16. The lighting-rod industry is “family oriented,” according to Willard. Multigenerational, really. “We’re one of the oldest [lightning rod companies] in the United States,” he tells me. “It’s not unusual to go to trade seminars and meet the next generation. There are a lot of lightning-protection families out there.”
When it comes to lightning and its stupendous energy, the bottom line, says Willard, is that lighting rods, when installed correctly, provide an effective path to ground for electrical energy, thus mitigating or avoiding damage to buildings. Especially when surge protection for incoming telecommunications, electric lines, and the internet is added to the setup.
“A lot of times people will put lightning rods up on their home or business and think they’re protected, but the structure can take an indirect strike to a utility line or a transformer outside the structure and the lightning rod is defenseless against that kind of strike,” Willard says. That’s why surge protection for telecommunications and cable have become a bigger and bigger part of his business: “Twenty years ago, people had a telephone, a TV, and an electrical line. Now they have high-end electronics, stuff that’s highly susceptible to any kind of electrical surge. A lightning rod system protects against a direct strike. Surge protection protects against an indirect strike.”
* * *
In fact, most structures need more than one lighting rod, Willard explains. The rods should be spaced across the structure’s main ridge, with a maximum distance of 20 feet between them. The average house needs three or four rods, and ideally other rods should be placed on prominent points such as chimneys and dormers. All are interconnected with that all-important cable that runs to ground.
“The idea is that when lightning strikes your house, or any building, you want it to strike in a safe place that takes the current down to the ground without damaging your house, your television set, or you,” says Joseph Dwyer, a professor of physics at the University of New Hampshire. “You can’t prevent the lightning from striking, but if it’s going to strike a certain area, it gives it a safe place to do it.”
In the past, people favored what are known as Franklin air terminals—rods that come to a point at the top. Willard says that until recently they were used almost exclusively in the United States, because their points were believed to conduct electricity more effectively. Now, most newly installed lightning-protection systems use blunt, or rounded, rods. Studies have shown that blunt terminals work just as well as their pointy brethren, if not better. Plus, they’re safer to install—no risk of impalement.
“Safety is always a concern, and you can’t be afraid of heights in our business,” Willard says. He and his crew have worked on some of the tallest buildings in Boston, including the landmark Prudential tower in the heart of the city. Lightning rods or no, Dwyer says humans should have a healthy fear of lightning, especially when outside. “There’s no safe place outside during a thunderstorm,” he says.
That’s because most lightning never leaves the storm. Often, it never strikes the ground, traveling upward and branching out a bit in the sky. It’s the lightning that travels down toward the Earth—cloud-to-ground lightning, as it’s called—that can threaten life and property. “If you ever see a nasty-looking cloud up there, you might think, ‘I’m okay, maybe there’s no lighting,’” says Dwyer. “But it could be that that storm has been making lightning for a while. It just hasn’t bothered to send one down to the ground. The lightning you see first may be just the tip of the iceberg.”
Dwyer describes lightning as “a really big spark that is measured on the scale of miles or kilometers and is about as wide as a human finger.” It can travel 100 miles through a storm by breaking down the air in front of it and transferring charges.
What’s more, lightning doesn’t travel in a smooth line; it zigzags. “It will go maybe 50 yards, and then pause as if it has run out of steam, then it will suddenly leap forward another 50 yards, sometimes in a new direction, sometimes it will branch,” Dwyer says.
Lightning is erratic, so people try to impose order—even if that order leads to more disorder in the form of myths. As it happens, lightning does strike in the same place twice—and sometimes more. Take the Empire State Building, for instance. The New York City art-deco landmark is struck nearly 100 times a year.
* * *
Some people think that lightning rods actively draw lighting. That, too, is a myth; they help dissipate electrical energy if it forms. But get this: Lightning rods did attract attention from European fashionistas in the late 18th century. According to the Lubbock Morning Avalanche’s May 13, 1933, edition, ladies of the haute mode were wearing lightning rods attached to their hats, hats known as chapeau paratonnerre. The so-called rods, the paper said, “consisted of a woven metal ribbon which encircled the hat and terminated in a long silver cord trailing on the ground.”
Because they are so prominent, real lighting rods have also inspired decoration. I appreciate good design, so I decided to search for lighting rods with fine form. I came across some striking designs: an antique copper lightning rod with a starburst tip, a vintage lightning rod with a wind indicator from Maryville, Missouri, and a vintage lightning rod in ornate hammered aluminum with a cobalt-blue ball and a roof mount. Prices ranged from $49.99 to $145.
All lovely, but it was the cobalt-blue ball that caught my eye. To my disappointment, it was made from hard plastic, not glass. Some people say glass balls were added to lightning rods so people would know if a structure had been hit should they find the ball shattered following a storm. Others say this is not so. No matter, the glass ball has become a decorative feature.
According to Willard, nowadays the glass balls are purely ornamental. He sees them on old barns in Maine, New Hampshire, and Massachusetts. “They don’t explode, and they don’t glow,” he says. “Lightning passes harmlessly right through them. They’ve returned over the years because some people want them as an architectural point.”
I asked Willard if he has installed lightning rods on his home. “That’s only a recent development,” he confided. “I was just doing the siding and the roof on my house before I put the lightning rods on.” After a roofing job, homeowners need to have someone qualified reinstall the lightning rods. They may look simple to install to the untrained eye, but they have idiosyncrasies, and if installed incorrectly they can be dangerous.
That’s why I’ve decided to purchase a single lightning rod, a dramatic one, and install it in my living room, so I can enjoy its sculptural qualities absent the storms. I suspect it will make for a striking piece of art.
This article appears courtesy of Object Lessons.


On Friday, Muslims around the world will buy a sacrificial animal, have it slaughtered in accordance with Islamic law, and divide up the meat between their family, their relatives, and the poor. The ritual slaughter is among the central traditions of a joyous feast called Eid al-Adha, the larger of the two annual festivals in Islam, which marks the end of the annual hajj, or pilgrimage to Mecca. In Islamic tradition, the ritual slaughter honors Abraham’s willingness to sacrifice his son in the name of God, although God at the last moment gave Abraham a ram to sacrifice instead.
In some countries like Pakistan, Yemen, and Afghanistan, celebrants will head to bustling outdoor markets to buy a choice animal—usually a sheep, but people also sacrifice goats, cows, and even camels—directly from the farmer who raised it, and have it slaughtered on the spot. But elsewhere in the Muslim world, the supply chain isn’t so straightforward. Many countries in the Middle East import the majority of their Eid livestock from places like Somalia, India, Romania, and as far as Australia. Some 12,500 Syrian sheep will even be airlifted from Lebanon to Qatar, owing to an ongoing political standoff between Qatar and several Gulf countries led by Saudi Arabia, which has shut down the only roads into Qatar.
In Dubai, if you fancy a nice Australian steer, there’s no need to block off an afternoon to find one and have it slaughtered: An app from a Kuwaiti company called al-Mawashi (“livestock” in Arabic) lets you buy as many as you like online, for pickup at the “smart” abattoir of your choice, already slaughtered in accordance with Muslim law and cut up to order.
What Does It Mean to Be a ‘Secular Muslim’?
Outside the Eid rush, al-Mawashi will even deliver to your door. In a 30-second ad spot for the service, a portly kid tumbles down the stairs in an ostentatious, gold-trimmed home, imploring his mother to cook him some meat. Busy on the phone, she sends him to the kitchen to ask the South Asian cook, who declares that there’s no meat at all. He calls for his father, who sends him to Raju, the Indian driver, who tells him to buzz off. Down at the soccer field, he’s struck with inspiration when he sees an al-Mawashi delivery truck. Moments later, a delivery man hands his mother a box of meat at the door. “Who ordered this?” she wonders aloud, until the kid bounds up with a sly smile. The whole transaction is a far cry from Abraham’s sacrifice in the desert.
An app for reserving a sacrificial animal for Eid isn’t out of place in the sea of programs that have sprouted up to help the smartphone-toting Muslim go about his or her religious duties. Apps abound that calculate prayer times and send push notifications when they’re about to start, like a modern call to prayer. Many include a compass that points toward Mecca, to show Muslims which way to face as they pray. (Even Google got into the qibla-finding game this summer.) An app called Zabihah is like a Yelp for Halal food—except its desktop version has been around six years longer than Yelp itself. And hundreds of Koran apps help users memorize the holy book with translations, audio recordings, and games.
Social networks, forums, and messaging apps allow Muslims to connect with one another, and share information about Islam with curious non-Muslims. The internet is also a popular destination for believers searching for guidance from Islamic scholars about the finer points of living religiously in the modern world. Can the Koran be read on a smartphone or laptop? Yes, says a fatwa, or religious decree, from the United Arab Emirates—but reading it in print and memorizing it is best. Can a recorded recitation of a Koranic verse be used as a cellphone ringtone? No, said a panel of clerics in India in 2009, because the ringtone may be cut off before the verse is over, which would be a sin. What’s more, it would be inappropriate for the recitation to be heard by an individual in a bathroom.
These innumerable “new media” fatwas, which touch on topics large and small, have garnered some pushback from the religious old guard, some of whom see them as a threat to their authority. The proliferation of new technology also poses ethical conundrums for Muslims, who debate how it should and shouldn’t be allowed to impact their lifestyle and religion.
In a 2005 lecture titled “Islam, Muslims, and Modern Technology,” Seyyed Hossein Nasr, a celebrated Iranian scholar of Islam, explores Muslims’ relationship with Western technology. “We cannot be naïve and think it is simply neutral,” Nasr said, pointing to technology’s deleterious effects on health and mental well-being. Nasr isn’t calling for cutting out television and internet and cellphones entirely; he advocates instead for casting a critical eye on Western technology to discern its value and objectives. He emphasizes tradition: Mass production shouldn’t be allowed to take over traditionally handmade goods like Persian carpets and Indian saris, and Islamic architecture and urban design should be taught alongside their Western equivalents in the Muslim world.
Amana Raquib, a professor of philosophy and religion at the Institute of Business Administration in Karachi, Pakistan, argues for the development of an Islamic ethical framework for evaluating technology. She proposes a handful of ways to apply Islamic values to modern technology in a 2016 paper submitted to a conference called “Islamic Perspectives on Science and Technology.” Technology that promotes greed and selfishness doesn’t align with Islamic values, she writes; nor does technology that could “enhance” humans (such as gene therapy), because it contradicts our nature as divinely created beings.
Apps that order up religious sacrifices, however, would seem to be a prime example of technology imbued with Islamic values. Dubai’s ordering service may seem newfangled, but Muslims have been arranging Eid al-Adha sacrifices remotely for a long time. Numerous services allow Muslims to offer their ritual sacrifice to a family in need abroad, and some have had online order forms since the internet was young.
“These are interesting ideas because they take out some of the traditional processes,” said Gary Bunt, a professor of Islamic Studies at University of Wales Trinity Saint David. “Now it’s another product on your phone.” But for big, busy families, he said, it’s a natural progression.
Even as technology alters every facet of daily life, the core practices of Eid al-Adha haven’t changed dramatically over the years, said Hilal Khashan, a political-science professor at the American University of Beirut. The first morning of the four-day celebration involves an early visit to the mosque for prayer. Muslims make family visits, trade presents, cook traditional dishes and sweets, and slaughter and share a sacrificial animal, if they can afford to do so.
The most tangible encroachment of modernity may be an uptick in buzzing smartphone notifications around the holidays. “In the past, people would send postcards and letters,” Khashan said. “Now, everybody just exchanges holiday wishes on WhatsApp.”


Washington, D.C., woke up to a humdinger of a story today, a flash portrait that shows the relationship between money, power, and ideas—and highlights the potential for intellectual corruption that has accompanied the flood of Big Tech money into the capital.
The New York Times reported that the New America Foundation, the digital-savvy center-left think tank, might have pushed out Barry Lynn, a ferocious and influential critic of “platform monopolies” like, for example, Alphabet (née Google). After Google was hit with a 2.42 billion–euro fine by the European Commission in June, Lynn posted a congratulatory note to the regulators and a call for action by American anti-trust officials.
New America, meanwhile, has received more than $20 million since its founding in 1999 from Alphabet companies and the foundation established by Eric Schmidt. Schmidt, currently executive chairman of Alphabet, also previously served as chairman of New America’s board.
Before Lynn began his Open Markets program, New America had been closely, deeply associated with technology- and market-friendly Silicon Valley progressivism.
Open Markets took a completely different tack, “researching and reporting on the political and economic dangers posed by monopolization,” largely by technology companies. And they were good at it. They were gaining adherents with influential essays by Lynn, Lina Khan, and Matt Stoller. They’d gotten the ears of Senators like Elizabeth Warren and even one of Silicon Valley’s own representatives, Ro Khanna. Some have given them credit for the anti-monopoly plank in the Democrats’ platform. (New America also has two Atlantic people on its board: the Atlantic Media chairman David Bradley and The Atlantic national correspondent Jim Fallows, neither of whom contributed to this story. Fallows was the chairman of the New America board from its founding until 2008, when Schmidt took over. New America’s CEO, Anne-Marie Slaughter, has written for this magazine. As have Lynn, Matt Stoller, and Phillip Longman of the Open Markets team. And we’ve cited Open Markets work repeatedly in our coverage.)
Soon, Open Markets—and its staff of 10—will be spun out of New America and will stand on its own.
Those are the well-established public facts. What happened next is what’s up for debate.
The Times article says that Schmidt saw the June note Lynn posted to the New America website and “communicated his displeasure” to Slaughter. The statement came down, then went back up without explanation. A few days later, Slaughter asked to meet with Lynn, and—somehow—it was decided his group would leave New America. She then sent him an email that he shared with the Times.
The Times wrote that the email said “the decision was ‘in no way based on the content of your work,’” but but also noted that “Slaughter accused Lynn of ‘imperiling the institution as a whole.’”
While the Times leaves out the context of the “imperiling” remark, a plain reading of the excerpts—and the fact that the email was sent at all—suggests a leader panicked about the loss of the institution’s most important constellation of funders.
Shortly after the story broke, Slaughter said on Twitter, “This story is false. New America will issue statement shortly. We are proud of Open Markets work.”
Slaughter’s longer statement emphasized that the spin-out was occasioned by Lynn’s “repeated refusal to adhere to New America’s standards of openness and institutional collegiality,” which is how educated people say, “He was an asshole.” (Neither Slaughter nor Lynn immediately replied to my request for an interview Wednesday. Nor did a representative for New America.)
Update: New America has released the full text of three emails from Slaughter to Lynn. A charitable reading of Slaughter’s position is that it was not Lynn’s work itself—indeed she offers to keep his team on doing their same research—but his behavior around the presentation of his work. Slaughter tells Lynn that he’s being pushed out not “based on any response from Google” but because of a “pattern” of “telling me one thing and doing another.”
In the Times, Google disputed that it had anything to do with Open Markets leaving New America.
If it all sounds like an inside-baseball Washington-power story, that’s because it is. But that doesn’t make it unimportant. This is one way that the future of the nation’s economy is being decided.
Whether Alphabet, Amazon, and others are treated as special entities, regular old companies, or monopolists is a key factor in determining what the internet will look like in the coming years.
All of which makes the undisputed facts of the case enough to tell the story. Maybe Eric Schmidt sent Anne-Marie Slaughter an email or a text message saying, “WTF?” with a link to Lynn’s post. Or even just an email with the link. But even if he didn’t, is anyone to believe that the institutional apparatus of New America would not be aware that one of its units was directly targeting the business model of a major funder and allied entity?
Working with just the dumb facts of the money involved, one might imagine that Lynn’s time at New America would come to a close, sooner or later.
The scale of Silicon Valley money and Washington money are so different that the introduction of the former into the latter is almost comical. You’ve got companies amassing tens of billions of dollars in cash with mechanics that are linked to particular regulatory and tax regimes. And those regimes are held up by people who measure donations in the tens or hundreds of thousands.
This is a key component of what Robert Reich (a decade ago) called supercapitalism. Many times the most efficient way to make money is to change the rules governing how that money can be made.
“Supercapitalism has not stopped at the artificial boundary separating economics from politics. The goal of the modern corporation—goaded by consumers and investors—is to do whatever is necessary to gain competitive advantage,” Reich wrote. “That includes entering any battleground where such gains can be made. Washington—and other capital cities around the world where public policies are devised—has become a competitive battleground because public policies often help some companies or industries while putting rivals at a comparative disadvantage.”
And one of the best ways to change (or freeze) policy making and regulation is to change the conversations that people are having in Washington, D.C., through funding research at think tanks.
If Alphabet were Monsanto or General Motors or Pfizer, most people probably wouldn’t even feign surprise. But for many years, just the word Google conferred a special halo on otherwise anodyne projects. They were the standard-bearers for the future and were afforded a generosity of intention that no other company received. But no matter the color of its logo, Alphabet’s money is green, too, and it exerts the same power as anyone else’s.
The details of how this all happened are significant for the individuals involved. But for the country at large, outside Washington policy circles, the essence of the story will remain the same: At some number of millions, even the invisible, digital kind of money takes on a tremendous weight. No matter where it falls, or even could potentially fall, it shapes the structures of power.
Which might just be the point that the Open Markets program has been trying to make.


In the wake of Hurricane Harvey, the nation has once again seen a Gulf Coast city flooded, its residents in peril. A new book of essays, due to be published next January, Environmental Disaster in the Gulf South, provides important social context for the many “natural” disasters that have plagued the region for 200 years.
The book’s editor, Cindy Ermus, a professor at the University of Lethbridge, argues in her introduction that the Gulf South shares underlying characteristics that connect its disasters across time and space.
“The Gulf South, and the Gulf Coast in particular, is bound together by much more than geography or the shared experience of risk and vulnerability to wind, water, erosion, and biological exchanges,” she writes. “More fundamentally, the environment has helped define the region’s identity and largely determined its history, its social fabric, and its economy.”
The cities of the Gulf Coast exist where they do because the ocean and rivers provide economic opportunities and scenic landscapes. But those same waters threaten to undermine the growing cities' safety during storms. Ermus cites the geographer Kent Mathewson, who maintained that the “workings and movements of streams, soils, seas, storms, and continental substrata may ultimately play a larger and more immediate role on this region than almost anywhere else on earth.”
But the environmental processes are only one component of the region’s vulnerability to disaster.
In these acute crises, we see inspiring stories over and over: Communities come together. People risk their lives to save their neighbors as well as strangers. It is heartwarming. It reinforces everything that we want to believe about ourselves as Americans, as Texans, as Floridians, as Louisianans.
But examining why these moments of heroism become necessary tells a darker story about America. People don’t just find themselves in places vulnerable to flooding. They are pushed there by racial injustice, economic inequality, and short-term, profit-driven development practices. The long-term decay of the nation’s infrastructure is a direct result of policy decisions that politicians and communities make time and again. The Gulf Coast is an extreme example of this, a laboratory for what happens when you combine lax planning policies, aging flood-control mechanisms, and a geography that channels storms from the warm (and warming) waters of the Gulf into the cities that line it.
Wetland cities sitting on or near a gulf that generates some of the fiercest storms on earth are becoming more vulnerable to the “natural” hazards they’ve long battled. Their development is booming, but in the process the cities have torn out the wetlands, paved over the prairies, and built an economy (and politics) around the carbon-heavy oil and gas industries. The cities grow. Local disaster managers do the best they can to prepare. And then they wait, hoping the circulation of wind and water does not bring the worst case to them. But it will, eventually, and everyone knows this, except when they manage to forget it. This is a slow tragedy in innumerable acts.
“It is hard to escape the conclusion that there is something exceptional—exceptionally dangerous, that is—about the South’s approach to the sea,” writes Ted Steinberg, a historian and author of Acts of God: The Unnatural History of Natural Disaster in America, writes in the book’s final chapter
I talked with the editor of Environmental Disaster in the Gulf South, Cindy Ermus, about what the book could teach us about Houston and Harvey, disaster management, and racial inequality.
Alexis Madrigal: So your academic field is the history of disasters?
Cindy Ermus: I looked at disasters and crises throughout graduate school. My manuscript looks at a plague in 1720 in southern France. And I edited a volume about disasters in the Gulf South.
Madrigal: Do you see through-lines in this long history of disaster management from back then to now?
Ermus: There are certainly a lot of parallels in disaster management, the ways that societies and governments come together or don’t come together to manage a crisis or prevent a crisis before it happens. I argue that prior to the 18th century, disaster management was more local. It depended more on the responses and reactions of municipal authorities rather than a capital of an emerging nation-state. And what we see emerging then is the centralization of disaster management. It’s coming out of these capitals, or to use the U.S. term, federal bodies, like FEMA or international ones like the UN.
Madrigal: Thinking about the Cajun Navy or the people organizing themselves on Twitter, do you think we’re seeing a move away from that central government control?
Ermus: I would say that there does appear to be a kind of coming back to the community in terms of mitigating these crises. In the 18th and 19th centuries, communities begin to rely more and more on the state, whether it is for welfare, public health, or assistance in terms of disaster.
One of the things that has come with increased awareness of the role of governments and corporations in disasters—the neoliberalization of disaster responses—is a sense of distrust.
The community is thinking: We can’t rely on the state right now. We can’t wait around for Bush—in the case of Katrina—to finally respond and send in assistance. So, let’s mobilize ourselves. Maybe the state isn’t doing what it should. I do wonder. As a historian, all of this is very ongoing.
Madrigal: What is one concept from your field that you wish people out there knew?
Ermus: A natural disaster implies that human activity doesn’t have anything to do with it. One phrase that we use in disaster studies in place of “natural disaster” is a “natural hazard.” It’s not a disaster until it affects a human population and the degree to which it affects that population is oftentimes a human problem.
Why did Houston pave over their floodplains—knowing or potentially knowing the fact that they would flood if a major storm hit?
Madrigal: But even the concept of a quote-unquote “natural hazard” seems to have undergone a weirding because of climate change. It’s clearly a very difficult task to disentangle precisely or even roughly how much of the rain that fell from Harvey was linked to global warming and how much would have happened under “natural” conditions. (One climate scientist estimated about 30 percent.)
Ermus: To use Harvey as an example, the degree to which climate change is at fault should be clear. It should be clear that’s not the main cause of the storm. Climate change is not solely responsible for the storm. These kinds of storms have happened. What climate change might have influenced and very much did influence is the strength of the storm. We know that the Gulf of Mexico serves as the brewing pot, if you will. It’s this creator of storms. The water in the Gulf of Mexico remained warm year-round. The average sea-surface temperature never fell below 73 degrees, even in winter. Climate change is undeniably taking place. And it is influencing storms like Harvey.
So how natural is a natural hazard even, given the effects of climate change? That’s something we’ll be talking about a lot.
Madrigal: Where did your interest in the Gulf South come from?
Ermus: I grew up in the Gulf South. I experienced Hurricane Andrew [in 1992]. I do 18th-century [history] but I find myself always applying things from the 18th century to today and vice versa.
Madrigal: Just in the last 10 or 15 years, it seems like the Gulf has been hit with several region-altering environmental disasters from acute ones like Katrina, Ike, and the Deepwater Horizon oil spill to the slow-burn of the oxygen-deprived dead zone and loss of coastal wetlands. Did you do the book because it’s such a disaster-prone region?
Ermus: It may not be the only capital of disaster but it is one among many. It has hundreds of years of experience with disasters and mitigation. And looking at this area will help us learn from the past as we look into a future where there will be more and more crises like this.
Madrigal: Aside from vulnerability to the same kinds of weather events, what unites the region?
Ermus: Infrastructure. I should mention Ted Steinberg’s afterword to Environmental Disaster in the Gulf South sums things up by looking at the slow disaster of infrastructure in Florida. In the case of infrastructure, this negligence takes decades and eventually comes to a head when Hurricane Katrina or Harvey makes landfall.
For example, infrastructure is what made Katrina the kind of disaster that it was. Decisions had been made on the ground. Things that could have easily been done and were instead neglected in terms of the construction of levees and the decisions of the Army Corps of Engineers in previous decades. Then there is the disappearance of the wetlands on the coast of Louisiana, which were disposed of to increase traffic to the Port of New Orleans. The wetlands are nature’s way of serving as protection against big storms.
Madrigal: How do the region’s politics and long history of racial inequality play into disaster management?
Ermus: Florida was the first state that I’m aware of to ban the use of the phrase “climate change.” The denial of climate change is going to significantly slow the progress of any infrastructure development or decisions that could help prevent crises like this.
In terms of the social history of the region, there are several chapters in the volume that address the environmental injustice that has this long history in the region. For instance, Andy Horowitz, a professor at Tulane University, looks at the 1900 Galveston storm in Texas as a “part of the ongoing disaster of racial terror in Texas at the turn of the 20th century.”
Christopher Church, a professor at University of Nevada, Reno, looks at hurricanes throughout the 20th century to see how lower socioeconomic communities are almost always the most vulnerable to disaster because the most affordable part of any city is usually the part that’s most vulnerable to disaster. In some ways, really, all of the chapters touch on this.
Madrigal: Taking the long view, do you think the way we look at disasters has changed?
Ermus: The way that we view disasters and crises has changed. Disasters were viewed as acts of god. Divine punishment. In the 18th century we start to get away from that. We see an exponential increase in people looking at geological and “natural”—that’s in quotations—causes. And today, increasingly, we see them as the result of human activity.
Even in 1755, during the Lisbon earthquake, Jean-Jacques Rousseau cites human activity for creating a disaster. It’s kind of an evolution of the way we view disasters from acts of god to natural occurrences to human events.
Madrigal: What does focusing on the humans allow us to see?
Ermus: Andy Horowitz, one of the contributors to my volume, recently tweeted something to the effect of: As you observe what’s going on in Houston, you observe the effects of this storm, ask yourself what human decisions contributed to these ill effects. And that’s the bottom line. We’re seeing what’s going on with these storms. We’re seeing that these effects are becoming worse and affecting more people than ever before. So, what can we do differently? Are we gonna do nothing? At what point are we going to decide as a society that climate change is real and that infrastructure matters?


The fallout from the rainiest storm in (at least) Texas history continues. No one has ever seen a storm dump this much water over so wide an area. Or as the National Weather Center’s prediction team put it, “The breadth and intensity of this rainfall are beyond anything experienced before.” As the situation on the ground continues to evolve, we’ve compiled a short list of significant resources, and helpful stories, to stay abreast of what’s happening. It will be updated.
We’ve gathered up the Twitter folks mentioned by name here (and several others) into this list.
Weather
The driving force for the event is, of course, the storm. As in many cases, the National Weather Service’s various feeds are essential. The NWS websites are a bit inscrutable, so the Twitter feeds are a good way to access their updates:
Two meteorologists have started up their own site, Space City Weather, for “hype-free forecasts.” They provide very in-depth analysis. For example, they have done an excellent job providing historical perspective on the storms that have flooded Houston:
Two other meteorologists have been heavily involved in filtering and amplifying significant information about the storm: Eric Holthaus and Brian McNoldy.
For a local perspective, Jeff Lindner is a meteorologist for the Harris County Flood-Control District, which manages the area’s flood mitigation infrastructure. He’s been tweeting, too.
Officials
The Harris County Flood-Control District manages the different pieces of infrastructure that try to protect Houston from flooding. They take a more traditional news approach, releasing press releases here. Their in-depth reports serve as excellent backgrounders:
The Harris County Sheriff’s Office has been tweeting extensively, including Sheriff Ed Gonzalez himself.
The City of Houston has an active Twitter feed including live broadcasts of some news conferences. Mayor Sylvester Turner, too.
Media
The Houston Chronicle has a large team of reporters on the ground with local knowledge. Many are tweeting in addition to filing stories. The investigative reporter Susan Carroll has been doing an excellent job of highlighting coverage. They’ve also, through time, created some of the best contextual explainers, including these two, which are the best journalistic accounts of the city’s flood history:
ProPublica, The Texas Tribune, and Reveal put together a remarkable explainer about the the previous two years’ worth of flooding. It’s essential background:
The reporters from that story are worth following: Neena Satija, Kiah Collier, and Al Shaw.
The alt-weekly Houston Press is also doing a very good job on the ground. Dianna Wray’s explainer on the (troubled, in-need-of-upgrades) dams holding a ton of rainwater is an excellent starting point for their coverage:
Community Organizations
Texas Monthly has compiled a list of organizations that can help people on the ground. And the Greater Houston Community Foundation, which works with and vets nonprofits, has set up a Harvey fund.
Scholars and Writers
Cindy Ermus is a history professor at the University of Lethbridge. She has a book coming out next January about environmental disaster in the Gulf South. She tweets.
Scott Knowles is a disaster historian at Drexel University. He also tweets: “In American disaster we ask responders and emergency managers to save lives and to fix generations of poverty and deferred maintenance. Impossible.”
Rebecca Solnit is not a scholar, per se, but her work on the 1906 San Francisco earthquake, A Paradise Built in Hell, is a rich (and heartening) exploration of what people do under the worst possible conditions. Here’s an excerpt in The New York Times.


Floods cause greater property damage and more deaths than tornadoes or hurricanes. And Houston’s flood is truly a disaster of biblical proportions: The sky unloaded 9 trillion gallons of water on the city within two days, and much more might fall before Harvey dissipates, producing as much as 60 inches of rain.
Pictures of Harvey’s runoff are harrowing, with interstates turned to sturdy and mature rivers. From Katrina to Sandy, Rita to Tōhoku, it’s easier to imagine the flooding caused by storm surges wrought by hurricanes and tsunamis. In these cases, the flooding problem appears to be caused by water breaching shores, seawalls, or levees. Those examples reinforce the idea that flooding is a problem of keeping water out—either through fortunate avoidance or engineering foresight.
But the impact of flooding, particularly in densely developed areas like cities, is far more constant than a massive, natural disaster like Harvey exposes. The reason cities flood isn’t because the water comes in, not exactly. It’s because the pavement of civilization forces the water to get back out again.
* * *
There are different kinds of floods. There’s the storm surge from hurricanes, the runoff from snowmelt, the inundation of riverbanks. But all these examples cast flooding as an occasional foe out to damage human civilization. In truth, flooding happens constantly, in small and large quantities, every time precipitation falls to earth. People just don’t tend to notice it until it reaches the proportions of disaster.
Under normal circumstances, rain or snowfall soaks back into the earth after falling. It gets absorbed by grasslands, by parks, by residential lawns, by anywhere the soil is exposed. Two factors can impede that absorption. One is large quantities of rain in a short period of time. The ground becomes inundated, and the water spreads out in accordance with the topography. The second is covering over the ground so it cannot soak up water in the first place. And that’s exactly what cities do—they transform the land into developed civilization.
Roads, parking lots, sidewalks, and other pavements, along with asphalt, concrete, brick, stone, and other building materials, combine to create impervious surfaces that resist the natural absorption of water. In most of the United States, about 75 percent of its land area, less than 1 percent of the land is hardscape. In cities, up to 40 percent is impervious.
The natural system is very good at accepting rainfall. But when water hits pavement, it creates runoff immediately. That water has to go somewhere. So it flows wherever the grade takes it. To account for that runoff, people engineer systems to move the water away from where it is originally deposited, or to house it in situ, or even to reuse it. This process—the policy, planning, engineering, implementation, and maintenance of urban water systems—is called stormwater management.
According to my Georgia Institute of Technology colleague Bruce Stiftel, who is chair of the school of city and regional planning and an expert in environmental and water policy governance, stormwater management usually entails channeling water away from impervious surfaces and the structures built atop them. In other words, cities are built on the assumption that the water that would have been absorbed back into the land they occupy can be transported away instead.
Like bridges or skyscrapers designed to bear certain loads, stormwater management systems are conceived within the limits of expected behavior—such as rainfall or riverbank overrun events that might happen every 10 or 25 years. When these intervals are exceeded, and the infrastructure can’t handle the rate and volume of water, flooding is the result.
Houston poses both a typical and an unusual situation for stormwater management. The city is enormous, stretching out over 600 square miles. It’s an epitome of the urban sprawl characterized by American exurbanism, where available land made development easy at the edges. Unlike New Orleans, Houston is well above sea level, so flooding risk from storm surge inundation is low. Instead, it’s rainfall that poses the biggest threat.
A series of slow-moving rivers, called bayous, provide natural drainage for the area. To account for the certainty of flooding, Houston has built drainage channels, sewers, outfalls, on- and off-road ditches, and detention ponds to hold or move water away from local areas. When they fill, the roadways provide overrun. The dramatic images from Houston that show wide, interstate freeways transformed into rivers look like the cause of the disaster, but they are also its solution, if not an ideal one. This is also why evacuating Houston, a metropolitan area of 6.5 million people, would have been a terrible idea. This is a city run by cars, and sending its residents to sit in gridlock on the thoroughfares and freeways designed to become rivers during flooding would have doomed them to death by water.
* * *
Accounting for a 100-year, 500-year, or “million-year” flood, as some are calling Harvey’s aftermath, is difficult and costly. Stiftel confirms that it’s almost impossible to design for these “maximal probable flood events,” as planners call them. Instead, the hope is to design communities such that when they flood, they can withstand the ill effects and support effective evacuations to keep people safe. “The Houston event seems like an illustration that we haven’t figured it out,” Stiftel says.
Many planners contend that impervious surface itself is the problem. The more of it there is, the less absorption takes place and the more runoff has to be managed. Reducing development, then, is one of the best ways to manage urban flooding. The problem is, urban development hasn’t slowed in the last half-century. Cities have only become more desirable, spreading outward over the plentiful land available in the United States.
The National Flood Insurance Program, established in 1968, offered one attempt at a compromise. It was meant to protect and indemnify people without creating economic catastrophe. Instead of avoiding the floodplain, insurance allowed people to build within it, within management constraints recommended by FEMA. In theory, flood-hazard mitigation hoped to direct development away from flood-prone areas through the disincentives of risk insurance and regulatory complexity.
Since then, attitudes have changed. For one part, initial avoidance of floodplains created desirable targets for development, especially in the middle of cities. But for another, Stiftel tells me that attitudes about development in floodplains have changed, too. “It’s more about living with water than it is about discouraging development in areas prone to risk.”
Sometimes “living with water” means sidestepping the consequences. Developers working in flood zones might not care what happens after they sell a property. That’s where governmental oversight is supposed to take over. Some are more strict than others. After the global financial crisis of 2008, for example, degraded local economies sometimes spurred relaxed land-use policy in exchange for new tax bases, particularly commercial ones.
In other cases, floodplains have been managed through redevelopment that reduces impervious surfaces. Natural ground cover, permeable or semi-permeable pavers, and vegetation that supports the movement of water offer examples. These efforts dovetail with urban redevelopment efforts that privilege mixed-use and green space, associated with both new urbanism and gentrification. Recreation lands, conservation lands and easements, dry washes, and other approaches attempt to counterbalance pavement when possible. Stiftel cites China’s “sponge cities” as a dramatic example—a government-funded effort to engineer new, permeable materials to anticipate and mitigate the flooding common to that nation.
* * *
But Thomas Debo, an emeritus professor of city planning at Georgia Tech who also wrote a popular textbook on stormwater management, takes issue with pavement reduction as a viable cure for urban flooding. “We focus too much on impervious surface and not enough on the conveyance of water,” he tells me. Even when reduced in quantity, the water still ends up in in pipes and concrete channels, speeding fast toward larger channels. “It’s like taking an aspirin to cure an ailment,” he scoffs. Houston’s flooding demonstrates the impact.
Instead, Debo advocates that urban design mimic rural hydrology as much as possible. Reducing impervious surface and improving water conveyance has a role to play, but the most important step in sparing cities from flooding is to reduce the velocity of water when it is channelized, so that it doesn’t deluge other sites. And then to stop moving water away from buildings and structures entirely, and to start finding new uses for it in place.
That can be done by collecting water into cisterns for processing and reuse—in some cases, Debo explains, the result can even save money by reducing the need to rely on utility-provided water. Adding vegetation, reclaiming stormwater, and building local conveyance systems for delivery of this water offer more promising solutions.
Though retired from Georgia Tech, Debo still consults on the campus’s local stormwater management efforts. In one case, the institute took a soccer field and made it into an infiltration basin. Water permeates the field, where it is channeled into pipes and then into local cisterns.
In Houston’s case, catastrophic floods have been anticipated for some time. The combination of climate change, which produces more intense and unpredictable storms, and aggressive development made an event like this week’s almost inevitable. The Association of State Floodplain Managers has called for a national flood risk-management strategy, and the Houston Chronicle has called flood control the city’s “most pressing infrastructure need.” A lack of funding is often blamed, and relaxed FEMA regulations under the Trump Administration won’t help either.
But for Debo and others, waiting for a holistic, centralized approach to stormwater management is a pipe dream anyway. Just as limiting impervious surface is not the solution to urban stormwater management, so government-run, singular infrastructure might not be either. “It’s much more difficult, and a much bigger picture,” Debo insists to me. “There is no silver bullet for stormwater management.”
* * *
One problem is that people care about flooding, because it’s dramatic and catastrophic. They don’t care about stormwater management, which is where the real issue lies. Even if it takes weeks or months, after Harvey subsides, public interest will decay too. Debo notes that traffic policy is an easier urban planning problem for ordinary folk, because it happens every day.
So does stormwater—it just isn’t treated that way. Instead of looking for holistic answers, site-specific ones must be pursued instead. Rather than putting a straight channel through a subdivision, for example, Debo suggests designing one to meander through it, to decrease the velocity of the water as it exits.
The hardest part of managing urban flooding is reconciling it with Americans’ insistence that they can and should be able to live, work, and play anywhere. Waterborne transit was a key driver of urban development, and it’s inevitable that cities have grown where flooding is prevalent. But there are some regions that just shouldn’t become cities. “Parts of Houston in the floodway, parts of New Orleans submerged during Katrina, parts of Florida—these places never should have been developed in the first place,” Debo concludes. Add sea-level rise and climate-change superstorms, and something has to give.
Debo is not optimistic about resisting the urge toward development. “I don’t think any of it’s going to happen,” he concedes. “Until we get people in Congress and in the White House who care about the environment, it’s just going to get worse and worse.”
Even so, there’s reason for optimism. If good stormwater management means good, site-specific design, then ordinary people have a role to play, too. Residential homeowners who install a new cement patio or driveway might not even realize that they are channeling water down-grade to their neighbors, or overwhelming a local storm drain. Citizens can also influence stormwater issues within their municipalities. Many folks know that they have a local city council and school board, but local planning, zoning, and urban design agencies also hold regular public meetings—unfortunately, most people only participate in this aspect of local governance when they have an axe to grind. For the average American concerned with the deluge, the best answer is to replace an occasional, morbid curiosity with flooding with a more sophisticated, long-term interest in stormwater management.


Texas continues to battle the ravages of Hurricane Harvey, which has dumped more than 30 inches of rain across the greater Houston area. The record-setting rains have caused devastating floods for the third straight year in the area, following the Tax Day Flood of 2016 and the Memorial Day Flood of 2015.
Houston keeps flooding, but local authorities have long tried to mitigate the risks of living in the Bayou City. Houston is located inside Harris County, and after a series of floods in the 1930s, the county created a flood-control district to regionally manage the area’s flood-preparedness system. Over the years, the Harris County Flood-Control District worked with the Army Corps of Engineers to make the entire city of Houston into a hydraulic machine that can direct water into a series of bayous, canals, and reservoirs, then on into the Gulf of Mexico. There are now 2,500 miles of channels for moving water and a total of $4 billion of flood-mitigation infrastructure.
As bad as things are in Houston right now, they’d be much worse without all this infrastructure in the ground. So, to keep these mitigation measures working, the Harris County Flood Control District spends roughly $100 million per year.
That sounds like a lot, but Houston is now a metropolitan powerhouse. The fourth-largest city in the country, it’s also the primary home of the nation’s oil and gas industry. The area’s real GDP is approaching half a trillion dollars a year.
And consider what some other agencies spend. The South Florida Water Management District spends $664 million per year on its operations. The California Department of Water Resources, which operates the state’s dams and canals, spends more than $500 million per year on “public safety and prevention of damages.”
Or compare to road-infrastructure spending: In Houston itself, the public-works budget is $2 billion this year. In California, the state spends $1.9 billion on highway maintenance alone.
Infrastructure is expensive, which is one reason that even an additional $1 trillion would only be a down payment on what the United States needs to maintain its functioning basic systems. But it’s expensive because it serves the needs of whole populations. Multiply anything by 350 million (or 7 million or 1 million) and you get a pretty big number.
In 2016, the Harris County Water District’s former chief estimated that $26 billion would be necessary to really harden the city against flooding. That would be done by, among other things, widening existing channels to increase the throughput of the water-moving machine.
At the same time, however, the city has continued to pave away the land’s natural capacity to absorb rainwater. One study found that over the last four decades, rainfall had increased 26 percent in the Brays Bayou watershed, but “runoff has skyrocketed by 204 percent.”
The city’s human inhabitants must race to protect themselves from ... themselves. The system was designed for a much smaller city with fewer people and much less runoff.
The system was also designed for a Houston climate that featured less extreme rains. The Texas state climatologist John Nielsen-Gammon has shown that rains have gotten more extreme in the city. “Heavy precipitation of any particular magnitude are twice as likely to fall in the Bayou City today as they were in the early 20th century,” the Houston Chronicle reported.
And though improvements to planning and mitigation would be actual improvements, it’s not clear that even a system with a $26 billion upgrade—or more—could have handled what Harvey brought to the area. The 2016 Tax Day Flood dumped 240 billion gallons of water on the county. Already, Harvey has sent 734 billion gallons to the ground in the same area; according to Flood District calculations, there have been 23.7 inches of rainfall on average across the whole county in the 48 hours since the storm made landfall.
Tropical Storm Allison, which struck in 2001, also brought tremendous rainfall, but only a tiny portion of the county crossed 25 inches of precipitation over the five days of that event.
Harvey will be remembered for the breadth of its rainfall, a burden which shows the cracks in Houston’s infrastructure. To survive on the Gulf Coast in the coming century, the city and its surrounding county are going to have to make the kinds of huge investments in flood mitigation that their forbears did after previous disasters.


As torrential rain continued and floods swelled in Texas this weekend, thousands of people trapped in Harvey’s path started calling 911 and other emergency lines for help. When they couldn’t get through, some started tweeting.
“Please help I’m stranded with my kids I need help fast,” a user wrote to the Houston Police’s account on Twitter.
“Our apartments are surrounded with water like an island we need rescue,” a woman pleaded.
“Please help us she a new born,” a woman wrote above photo of a sleeping baby.
Dozens of pleas like this spread across Twitter and other social-media networks on Sunday as Harvey continued its assault in southeastern Texas. Users included their street addresses, mostly in Houston, which received 25 inches of rain in the last two days. Some took to Twitter because they said they couldn’t get through to 911 and other helplines. Officials in Houston said 911 operators received more than 56,000 calls between Saturday and Sunday nights, a span of time that usually gets 8,000 calls. Emergency services received nearly 6,000 calls for high-water rescues, and more than 1,000 people have been rescued, officials said. In a tweet on Sunday, Houston Police asked anyone with a boat who can help to call the department, providing a phone number that was immediately swarmed with calls.
Social media provided stranded people with a digital megaphone that could be carried far beyond Harvey’s path. Many other users joined the digital search-and-rescue operation and retweeted the voices of the stranded, hoping to buoy their pleas in fast-moving feeds. People offered words of comfort and replied with numbers or Twitter handles for local and national emergency services. Friends and family tweeted on their loved one’s behalf. “Anyone in NE #Houston able to rescue my Aunt? Just had surgery & home filling up w/ water,” a man pleaded. Conversations between flooding victims and concerned Twitter users, some nowhere near Houston, unfolded in real-time as the victims shared updates about water rising around them.
A crowdsourcing rescue effort appeared in the form of @HarveyRescue, a “remote volunteer logging requests for rescues seen on social media,” according to the account’s bio. The account started a Google spreadsheet Sunday and encouraged people to fill in requests for help. By Sunday night, they’d set up a more formal Google form, asking people for their names, phone numbers, and addresses, as well as information about the number of people and pets stranded and whether they had any urgent medical needs. The spreadsheet no longer appears to be visible to users, but on Monday morning there were about 400 requests for rescue. “Mother has 2 small kids & handicap son.” “Family w/ water pouring into home.” “One person is paralyzed.” “5-week-old baby.” About 40 were listed as “resolved.”
Some calls for help in Houston eventually resulted in good news. “Rescued! Thank you one and all!” one woman wrote Sunday night, after sharing an address for three stranded women in their 70s. Others continued to broadcast updates, while some fell silent.
Ed Gonzalez, the sheriff for Harris County, one of the most affected areas, joined in, responding to users seeking help and asking for updates on their situations. But other officials, seeing pleas pile up in their comments, advised people against using social media to register their requests. “Please do not use the HPD social media accounts for rescue requests,” Houston Police tweeted, telling people to 911 for life-threatening emergencies and 311 for help leaving their homes. The U.S. Coast Guard urged the same, tweeting, “Do not report distress on social media.” If phone lines are busy, “please keep trying.”
The use of social media during natural disasters like Harvey has become the norm in the last decade, allowing people to broadcast their safety or peril beyond the confines of a catastrophe. When phone networks crashed during the earthquake and tsunami in Japan in 2011, many people turned to Twitter, Facebook, Skype, and Mixi, a social-networking website in the country. When Manila, the capital of the Philippines, flooded after heavy rains in 2012, residents circulated a Google spreadsheet and crowdsourced aid. When a 7.8-magnitude earthquake shook Nepal in 2015, people around the world combed through Twitter and Facebook posts from the disaster zone and collected reports for help to help guide first responders and charities. More than 8.5 million people in Nepal used Facebook’s Safety Check feature, launched a year earlier, to say they were safe.
The calls for help in Texas, online and offline, will likely grow as Harvey continues its slow churn this week, producing effects that the National Weather Service described in a tweet as “unknown & beyond anything experienced.” Harvey struck the state’s Gulf Coast late Saturday as a Category 4 hurricane and weakened to a tropical storm as it moved inland, but has unleashed nonstop rains, flooding whole neighborhoods and turning roads and highways into rivers that rise by the hour. The storm has resulted in at least five deaths and the displacement of thousands. The National Hurricane Center said Sunday night Texas could see between 15 to 25 inches of rain through Friday, with some parts getting as much as 50 inches—the amount Houston usually sees in an entire year.



It's been ten years since the iPhone came out, and now the first generation to grow up with smartphones is coming of age. Jean Twenge, a psychologist who has studied generational behaviors, has found troubling signals that these devices seem to be taking a visible toll on the mental health of post-Millennials. In the September 2017 issue of The Atlantic, Twenge shares her findings in a story adapted from her new book, iGen: Why Today’s Super-Connected Kids Are Growing Up Less Rebellious, More Tolerant, Less Happy—and Completely Unprepared for Adulthood—and What That Means for the Rest of Us.
In this episode, we talk with Twenge about her findings, hear from a few members of the post-Millennial generation about their relationships with their devices, and discuss what the research means for parents.
Links:


Physicians call it the 5,000-hour problem. If you have a common chronic condition such as cardiovascular disease or diabetes, the expert in charge of your health for almost all of your 5,000 waking hours annually is—you. And, frankly, you won’t always make the best choices.
“The behavior changes that are necessary to address chronic disease are much more in your hands than in the doctor’s,” points out Stacey Chang, executive director of the Design Institute for Health at Dell Medical School in Austin, Texas. “To cede that control to the doctor sometimes is actually counterproductive.”
With that in mind, a rapidly evolving set of new digital health tools is angling to help patients engage better with their own care. Wearable health monitors already on the market help to track heart rate, footsteps, or blood glucose levels; sophisticated home health sensors can report on weight and blood pressure; and phone apps can present key feedback and maybe even offer personalized advice.
The only problem: It has thus far proved very difficult to know what really works.
Indeed, despite a veritable avalanche of “digital health” products, from Fitbits to telehealth heart sensors, and despite floods of data flowing both to the people who use them and to their physicians—and even despite clear evidence that many doctors very much want these new gadgets to work—there is still precious little clinical data proving that they are providing major patient benefits or delivering more cost-effective care.
As with everything else in health care, the reasons for this gap are many, but one major factor is the difficulty of getting patients to embrace new forms of self-monitoring and self-care beyond the novelty period—and to a point where such technology might actually make a difference.
Many early attempts to truly test the efficacy of such digital technologies have shown them to be a flop in clinical trials—in large part because participants drop out. An analysis of five health apps built with Apple iPhone software, for example, found that only about one-eighth of participants, or less, were still hanging in after 10 weeks. Another recent study out of Singapore found that about 200 people outfitted with fitness trackers showed no better health outcomes than a similar control group after a year. And when Cedars-Sinai Medical Center in Los Angeles invited about 66,000 patients registered on its portal to share data from their fitness trackers, less than 1 percent did so, according to a paper published last year in the journal PLOS One, part of the open-access Public Library of Science.
“If you build it,” quipped Brennan Spiegel, an internal-medicine physician and head of the Cedars-Sinai Center for Outcomes Research and Education, “they will not come.”
That’s not entirely surprising, according to Kevin Volpp, director of the University of Pennsylvania’s Center for Health Incentives and Behavioral Economics. “Studies that have looked at ongoing engagement with devices have found that basically people stop using devices that are given to them over time,” he said. “That’s one of the central challenges in efforts to improve health through technology and digital devices. While there have been enormous advances in technology, there’s still a lot of work to be done with the science of habit formation.”
Some clinicians hope that simply doing a better job of educating patients on these new technologies—and on their personal role in using them to improve their own health—can help bridge this gap, but others suggest that won’t be enough. “It’s not a very potent way of changing behavior,” said David Asch, executive director of the University of Pennsylvania Perelman School of Medicine’s Center for Health Care Innovation. Most people on a diet, after all, know what they shouldn’t eat.
One example is a “digital therapeutics” service from Omada Health of San Francisco that targets people at high risk of developing cardiovascular disease or diabetes. In one study, among 1,121 overweight or obese seniors, participants averaged a 6.8 percent reduction in body weight within 26 weeks, and 89 percent of them completed 9 of 16 core lessons—high marks of achievement in this field.
“It’s a very high-tech and high-touch program,” said Sean Duffy, Omada’s co-founder and chief executive. “We mail people a welcome kit that contains a digital scale that has a cellphone chip in it so it’s preregistered to you as a participant. You step on the scale, there’s a beep and the data is in the system.” The system also forms patient-support groups based on location and demographics, and those groups are then paired with health coaches. With patient data flowing continually to Omada, the health coaches can be alerted if someone starts backsliding.
That flow of patient data also allows the service to act as a continuous learning system, with the company’s data scientists studying ways to optimize and personalize interventions to improve behaviors, Duffy added.
Another app, developed by researchers at Johns Hopkins Bayview Medical Center in Baltimore, tackles a very different problem in cardiovascular health: assisting patients recovering from a heart attack.
Traditionally, hospitals saddle patients with a pile of discharge-instruction papers just before they are wheeled down to their cars—“when they couldn’t care less about what we’re giving them,” said Francoise Marvel, an internal medicine physician at Bayview. That failure to communicate can add to patients’ confusion about how to care for themselves during the crucial early days of recovery—one main reason that about a fifth of heart attack patients nationally are rehospitalized within a month after discharge.
The Johns Hopkins “Corrie” app aims to address that by integrating educational videos, medication information, medical appointment tracking, and other crucial aids in an easy-to-use mobile app. Marvel says that early results from a first clinical study of 50 patients have been promising: None were rehospitalized in the first month.
The Corrie app taps not only smartphones and smartwatches, but smart timing. “We can actually enroll patients who are six or seven hours out of having a stent placed in the ICU,” Marvel said. “We’re giving it to patients when they have the time to spend watching the videos, asking questions about their medications, and following their blood pressure and their vital signs—which they will likely need to be taking when they go home.
“We’re getting them to buy in and learn the skills,” she added, “while they care the most.”
Family members can also follow along with a companion Corrie app. “If the adult daughter knows that her dad’s not taking his blood pressure medicine every day,” Marvel said, “she’ll rain down on him in a way that no doctor or no nurse follow-up call could ever compete with.”
* * *
Still, for all the enthusiasm surrounding these technologies, limited independent real-world testing makes it difficult to categorically say what works and what doesn’t—though researchers do suggest that one thing is abundantly clear: Patient acceptance of high-tech self-care and monitoring devices cannot be taken for granted.
“We have to think about how we intervene on behaviors in really different ways, ones that are much more consumer-like,” said Chang. Social networks such as Facebook, for instance, can provide much more pervasive, much more consistent touchpoints than traditional health-care practices and can shift behaviors in striking ways, he says.
In chronic disease, people lack instant gratification for, say, dropping that slice of pizza and eating their damn broccoli. Individual patients need to find personally meaningful ways to motivate themselves, like picturing themselves playing actively with their grandchildren, Chang said. A trained human health coach can help them build this motivational framework, but it remains a real challenge for a mobile app.
Research in behavioral economics can help to optimize more concrete healthy-habit incentives offered by employers or health plans, Volpp said. A clinical study published last year in the Annals of Internal Medicine, for example, either paid participants $1.40 for each day they took 7,000 steps, or gave them a virtual bank account holding 30 days’ worth of those payments—or $42—and then subtracted $1.40 for each day they didn’t walk 7,000 steps. The second method worked appreciably better.
Overall, as our understanding of how to apply digital health technologies matures, the importance of active patient engagement can only grow. “The conventional model is that we can use technology to bring patients closer to doctors,” Asch said. “I think that’s wrong. We have a shortage of doctors, and doctors are one of the most expensive ways of delivering health care.”
“A better and more sustainable solution,” he added, “is to learn how to be less reliant on your doctor and still get good outcomes.”
This article appears courtesy of Undark Magazine.


Playing cards are known and used the world over—and almost every corner of the globe has laid claim to their invention. The Chinese assert the longest pedigree for card playing (the “game of leaves” was played as early as the 9th century). The French avow their standardization of the carte à jouer and its ancestor, the tarot. And the British allege the earliest mention of a card game in any authenticated register.
Today, the public might know how to play blackjack or bridge, but few stop to consider that a deck of cards is a marvel of engineering, design, and history. Cards have served as amusing pastimes, high-stakes gambles, tools of occult practice, magic tricks, and mathematical probability models—even, at times, as currency and as a medium for secret messages.
In the process, decks of cards reveal peculiarities of their origins. Card names, colors, emblems, and designs change according to their provenance and the whims of card players themselves. These graphic tablets aren’t just toys, or tools. They are cultural imprints that reveal popular custom.
* * *
The birthplace of ordinary playing cards is shrouded in obscurity and conjecture, but—like gunpowder or tea or porcelain—they almost certainly have Eastern origins. “Scholars and historians are divided on the exact origins of playing cards,” explains Gejus Van Diggele, the chairman of the International Playing-Card Society, or IPCS, in London. “But they generally agree that cards spread from East to West.”
Scrolls from China’s Tang Dynasty mention a game of paper tiles (though these more closely resembled modern dominoes than cards), and experts consider this the first written documentation of card playing. A handful of European literary references in the late 14th century point to the sudden arrival of a “Saracen’s game,” suggesting that cards came not from China but from Arabia. Yet another hypothesis argues that nomads brought fortune-telling cards with them from India, assigning an even longer antiquity to card playing. Either way, commercial opportunities likely enabled card playing’s transmission between the Far East and Europe, as printing technology sped their production across borders.
In medieval Europe, card games occasioned drinking, gambling, and a host of other vices that drew cheats and charlatans to the table. Card playing became so widespread and disruptive that authorities banned it. In his book The Game of Tarot, the historian Michael Dummett explains that a 1377 ordinance forbade card games on workdays in Paris. Similar bans were enacted throughout Europe as preachers sought to regulate card playing, convinced that “the Devil’s picture book” led only to a life of depravity.
Everybody played cards: kings and dukes, clerics, friars and noblewomen, prostitutes, sailors, prisoners. But the gamblers were responsible for some of the most notable features of modern decks.
Today’s 52-card deck preserves the four original French suits of centuries ago: clubs (♣), diamonds (♦), hearts (♥), and spades (♠). These graphic symbols, or “pips,” bear little resemblance to the items they represent, but they were much easier to copy than more lavish motifs. Historically, pips were highly variable, giving way to different sets of symbols rooted in geography and culture. From stars and birds to goblets and sorcerers, pips bore symbolic meaning, much like the trump cards of older tarot decks. Unlike tarot, however, pips were surely meant as diversion instead of divination. Even so, these cards preserved much of the iconography that had fascinated 16th-century Europe: astronomy, alchemy, mysticism, and history.
Some historians have suggested that suits in a deck were meant to represent the four classes of Medieval society. Cups and chalices (modern hearts) might have stood for the clergy; swords (spades) for the nobility or the military; coins (diamonds) for the merchants; and batons (clubs) for peasants. But the disparity in pips from one deck to the next resists such pat categorization. Bells, for example, were found in early German “hunting cards.” These pips would have been a more fitting symbol of German nobility than spades, because bells were often attached to the jesses of a hawk in falconry, a sport reserved for the Rhineland’s wealthiest. Diamonds, by contrast, could have represented the upper class in French decks, as paving stones used in the chancels of churches were diamond shaped, and such stones marked the graves of the aristocratic dead.
But how to account for the use of clover, acorns, leaves, pikes, shields, coins, roses, and countless other imagery? “This is part of the folklore of the subject,” Paul Bostock, an IPCS council member, tells me. “I don’t believe the early cards were so logically planned.” A more likely explanation for suit marks, he says, is that they were commissioned by wealthy families. The choice of pips is thus partly a reflection of noblemen’s tastes and interests.
* * *
While pips were highly variable, courtesan cards—called “face cards” today—have remained largely unchanged for centuries. British and French decks, for example, always feature the same four legendary kings: Charles, David, Caesar, and Alexander the Great. Bostock notes that queens have not enjoyed similar reverence. Pallas, Judith, Rachel, and Argine variously ruled each of the four suits, with frequent interruption. As the Spanish adopted playing cards, they replaced queens with mounted knights or caballeros. And the Germans excluded queens entirely from their decks, dividing face cards into könig (king), obermann (upper man), and untermann (lower man)—today’s Jacks. The French reintroduced the queen, while the British were so fond of theirs they instituted the “British Rule,” a variation that swaps the values of the king and queen cards if the reigning monarch of England is a woman.
The ace rose to prominence in 1765, according to the IPCS. That was the year England began to tax sales of playing cards. The ace was stamped to indicate that the tax had been paid, and forging an ace was a crime punishable by death. To this day, the ace is boldly designed to stand out.
The king of hearts offers another curiosity: The only king without a mustache, he appears to be killing himself by means of a sword to the head. The explanation for the “suicide-king” is less dramatic. As printing spurred rapid reproduction of decks, the integrity of the original artwork declined. When printing blocks wore out, Paul Bostock explains, card makers would create new sets by copying either the blocks or the cards. This process amplified previous errors. Eventually, the far edge of our poor king’s sword disappeared.
Hand craftsmanship and high taxation made each deck of playing cards an investment. As such, cards became a feast for the eye. Fanciful, highly specialized decks offered artists a chance to design a kind of collectible, visual essay. Playing-card manufacturers produced decks meant for other uses beyond simple card playing, including instruction, propaganda, and advertising. Perhaps because they were so prized, cards were often repurposed: as invitations, entrance tickets, obituary notes, wedding announcements, music scores, invoices—even as notes between lovers or from mothers who had abandoned their babies. In this way, the humble playing card sometimes becomes an important historical document, one that offers both scholars and amateur collectors a window into the past.
While collectors favored ornate designs, gamblers insisted on standard, symmetrical cards, because any variety or gimmickry served to distract from the game. For nearly 500 years, the backs of cards were plain. But in the early 19th century, Thomas De La Rue & Company, a British stationer and printer, introduced lithographic designs such as dots, stars, and other simple prints to the backs of playing cards. The innovation offered advantages. Plain backs easily pick up smudges, which “mark” the cards and make them useless to gamblers. By contrast, pattern-backed cards can withstand wear and tear without betraying a cardholder’s secrets.
Years later, Bostock tells me, card makers added corner indices (numbers and letters), which told the cardholder the numerical value of any card and its suit. This simple innovation, patented during the Civil War, was revolutionary: Indices allowed players to hold their cards in one hand, tightly fanned. A furtive glance offered the skilled gambler a quick tally of his holdings, that he might bid or fold or raise the ante, all the while broadcasting the most resolute of poker faces.
Standard decks normally contain two extra “wild” cards, each depicting a traditional court jester that can be used to trump any natural card. Jokers first appeared in printed American decks in 1867, and by 1880, British card makers had followed suit, as it were. Curiously, few games employ them. For this reason, perhaps, the Joker is the only card that lacks a standard, industry-wide design. He appears by turns the wily trickster, the seducer, the wicked imp—a true calling card for the debauchery and pleasure that is card playing’s promise.
This article appears courtesy of Object Lessons.


On Tuesday, one Robert Lee was punished for the actions of another. When ESPN decided to remove the sports broadcaster Robert Lee from covering the first University of Virginia football game of the season, it was, per the network’s own statement “simply because of the coincidence of his name.” UVA, and its home of Charlottesville, have been embroiled in turmoil over the removal of a statue of Robert E. Lee, and the network thought it best to not evoke the obvious comparisons.
According to a statement from an ESPN executive shared by the journalist Yashar Ali, the move was made to protect Lee from potential “memes and jokes” and Lee himself was involved in the decision.
Regardless, all of this happened because, presumably, Lee’s parents liked the sound of the name Robert.
Both Robert and Lee are extremely common names. According to the website HowManyofMe.com, which searches a database of U.S. Census data, there are 5,128,282 Roberts in the United States, 731,046 people with the last name Lee, and a whopping 11,518 Robert Lees.
“I assume there’s a reason that even Robert E. Lee, the original general, went by his initial,” says Laura Wattenberg, the creator of BabyNameWizard.com, who researches names. “He’s never referred to without that initial.”
Surely some of them were named explicitly for Robert E. Lee, but many—probably most—were not. Wattenberg says that there used to be many people named for General Lee, but nowadays, “homage names are just an endangered species.” If someone chooses to go by the full “Robert E. Lee,” you might reasonably presume that they are trying to play up the Confederate connection, Wattenberg says. But the sports broadcaster Robert Lee is Asian American, and “one knows that broadcaster is not from a family proud of its Confederate ancestry,” she says.
Lee is the 22nd most common last name in the United States, according to the U.S. Census Bureau, and the people who share it are a fairly diverse group. White people make up 40.1 percent of Lees, 37.8 percent are Asian or Pacific Islander, 17.4 percent are black, 1.3 percent are Hispanic, and 1 percent are Native American.
One Robert Lee, who lives in San Francisco and works as a business analyst, didn’t fully understand the significance his name holds in the United States until he went to college. He lived in Hong Kong until he was 18, and then went to Brown University.
Before that, “I knew that [Robert E. Lee] was a general, but I didn’t really know anything about the Civil War or American history at all,” he says. But when people he met at college would casually mention the connection, he asked them about it and learned that way. He says people ask him often what his middle initial is.
“At the beginning I was confused as to why people were asking that question all the time until somebody finally told me,” he says.
In his sophomore year, he became the director of an a cappella group. “I had really high standards and I was used to a strict musical environment,” he says, “so I got the nickname ‘The General.’” He says he didn’t mind it at the time because he didn’t know much about Robert E. Lee. “But after that year I started moving away from it,” he says.
Sharing a name with someone famous (or infamous) means that person will already cast a bit of a shadow over your name. But unlike the 699 Jennifer Lawrences in the United States who are probably just tired of jokey Hunger Games references, the Robert Lees have the larger shadow of America’s fraught history of racism, which endures to the present.
Names shape how we move through the world; they are a shorthand that is used to refer to a person in all their complex humanity. “Robert E. Lee” is shorthand not only for the man himself, but for the Confederacy and all the connotations that carries. It is a way of referring to all the people who served under him who fought to maintain slavery; it is a way of referring to the false alternative history which some would like to preserve, in which Lee was a hero, the Confederacy a romantic lost cause, and the Civil War fought over “states’ rights.” His name has become inextricably linked to the movement to remove statues not only of Lee, but other Confederate figures—and to the white supremacists who violently opposed this removal in Charlottesville.
It might seem silly to think that simply sharing a name with a figure like Lee could have real consequences, but it did for Robert Lee the sports broadcaster. Even without that middle “E.”
“I think ironically ESPN did more harm to the name than the statue did,” Wattenberg says. “Because ESPN just sort of fired a shot across the bow saying that the name Robert Lee—in all of its forms—is now linked to the general and I suspect that a lot of Bob Lees are cursing them right now.”
When I wrote earlier this year about interviewing people who shared my name, Wattenberg told me that names alone really do shape how other people see you. “Your name forms this little pocket of identity around you,” she said. And sometimes, we carry things in that pocket that someone else placed there.
Robert Lee from San Francisco says lately “I have become more aware of the name when I introduce myself. Saying the name out loud, it brings a bunch of different conflicting emotions.”
But “I do like my name,” he says. “I’m named after my godfather, and the last name Lee also runs in the family and represents a lot. I think the importance of your name is really up to you.”


In a corner of Alphabet’s campus, there is a team working on a piece of software that may be the key to self-driving cars. No journalist has ever seen it in action until now. They call it Carcraft, after the popular game World of Warcraft.
The software’s creator, a shaggy-haired, baby-faced young engineer named James Stout, is sitting next to me in the headphones-on quiet of the open-plan office. On the screen is a virtual representation of a roundabout. To human eyes, it is not much to look at: a simple line drawing rendered onto a road-textured background. We see a self-driving Chrysler Pacifica at medium resolution and a simple wireframe box indicating the presence of another vehicle.
Months ago, a self-driving car team encountered a roundabout like this in Texas. The speed and complexity of the situation flummoxed the car, so they decided to build a look-alike strip of physical pavement at a test facility. And what I’m looking at is the third step in the learning process: the digitization of the real-world driving. Here, a single real-world driving maneuver—like one car cutting off the other on a roundabout—can be amplified into thousands of simulated scenarios that probe the edges of the car’s capabilities.
Listen to the audio version of this article:Feature stories, read aloud: download the Audm app for your iPhone.
Scenarios like this form the base for the company’s powerful simulation apparatus. “The vast majority of work done—new feature work—is motivated by stuff seen in simulation,” Stout tells me. This is the tool that’s accelerated the development of autonomous vehicles at Waymo, which Alphabet (née Google) spun out of its “moon-shot” research wing, X, in December of 2016.
If Waymo can deliver fully autonomous vehicles in the next few years, Carcraft should be remembered as a virtual world that had an outsized role in reshaping the actual world on which it is based.
Originally developed as a way to “play back” scenes that the cars experienced while driving on public roads, Carcraft, and simulation generally, have taken on an ever-larger role within the self-driving program.
At any time, there are now 25,000 virtual self-driving cars making their way through fully modeled versions of Austin, Mountain View, and Phoenix, as well as test-track scenarios. Waymo might simulate driving down a particularly tricky road hundreds of thousands of times in a single day. Collectively, they now drive 8 million miles per day in the virtual world. In 2016, they logged 2.5 billion virtual miles versus a little over 3 million miles by Google’s IRL self-driving cars that run on public roads. And crucially, the virtual miles focus on what Waymo people invariably call “interesting” miles in which they might learn something new. These are not boring highway commuter miles.
The simulations are part of an intricate process that Waymo has developed. They’ve tightly interwoven the millions of miles their cars have traveled on public roads with a “structured testing” program they conduct at a secret base in the Central Valley they call Castle.
Waymo has never unveiled this system before. The miles they drive on regular roads show them areas where they need extra practice. They carve the spaces they need into the earth at Castle, which lets them run thousands of different scenarios in situ. And in both kinds of real-world testing, their cars capture enough data to create full digital recreations at any point in the future. In that virtual space, they can unhitch from the limits of real life and create thousands of variations of any single scenario, and then run a digital car through all of them. As the driving software improves, it’s downloaded back into the physical cars, which can drive more and harder miles, and the loop begins again.
To get to Castle, you drive east from San Francisco Bay and south on 99, the Central Valley highway that runs south to Fresno. Cornfields abut subdevelopments; the horizon disappears behind agricultural haze. It’s 30 degrees hotter than San Francisco and so flat that the grade of this “earthen sea,” as John McPhee called it, can only be measured with lasers. You exit near the small town of Atwater, once the home of the Castle Air Force Base, which used to employ 6,000 people to service the B-52 program. Now, it’s on the northern edge of the small Merced metro area, where unemployment broke 20 percent in the early 2010s, and still rarely dips below 10 percent. Forty percent of the people around here speak Spanish. We cross some railroad tracks and swing onto the 1,621 acres of the old base, which now hosts everything from Merced County Animal Control to the U.S. Penitentiary, Atwater.
The directions in my phone are not pointed to an address, but a set of GPS coordinates. We proceed along a tall opaque green fence until Google Maps tells us to stop. There’s nothing to indicate that there’s even a gate. It just looks like another section of fence, but my Waymo host is confident. And sure enough: A security guard appears and slips out a widening crack in the fence to check our credentials.
The fence parts and we drive into a bustling little campus. Young people in shorts and hats walk to and fro. There are portable buildings, domed garages, and—in the parking lot of the main building—self-driving cars. This is a place where there are several types of autonomous vehicle: the Lexus models that you’re most likely to see on public roads, the Priuses that they’ve retired, and the new Chrysler Pacifica minivans.
The self-driving cars are easy to pick out. They’re studded with sensors. The most prominent are the laser scanners (usually called LIDARs) on the tops of the cars. But the Pacificas also have smaller beer-can-sized LIDARs spinning near their side mirrors. And they have radars at the back which look disturbingly like white Shrek ears.
When a car’s sensors are engaged, even while parked, the spinning LIDARs make an odd sound. It’s somewhere between a whine and a whomp, unpleasant only because it’s so novel that my ears can’t filter it out like the rest of the car noises that I’ve grown up with.
There is one even more special car parked across the street from the main building. All over it, there are X’s of different sizes applied in red duct tape. That’s the Level Four car. The levels are Society of Automotive Engineers designations for the amount of autonomy that the car has. Most of what we hear about on the roads is Level One or Level Two, meant to allow for smart cruise control on highways. But the red-X car is a whole other animal. Not only is it fully autonomous, but it cannot be driven by the humans inside it, so they don’t want to get it mixed up with their other cars.
As we pull into the parking lot, there are whiffs of Manhattan Project, of scientific outpost, of tech startup. Inside the main building, a classroom-sized portable, I meet the motive force behind this remarkable place. Her name is Steph Villegas.
Villegas wears a long, fitted white collared shirt, artfully torn jeans, and gray knit sneakers, every bit as fashionable as her pre-Google job at the San Francisco boutique Azalea might suggest. She grew up in the East Bay suburbs on the other side of the hills from Berkeley and was a fine-arts major at University of California, Berkeley before finding her way into the self-driving car program in 2011.
“You were a driver?” I ask.
“Always a driver,” Villegas says.
She spent countless hours going up and down 101 and 280, the highways that lead between San Francisco and Mountain View. Like the rest of the drivers, she came to develop a feel for how the cars performed on the open road. And this came to be seen as an important kind of knowledge within the self-driving program. They developed an intuition about what might be hard for the cars. “Doing some testing on newer software and having a bit of tenure on the team, I began to think about ways that we could potentially challenge the system,” she tells me.
So, Villegas and some engineers began to cook up and stage rare scenarios that might allow them to test new behaviors in a controlled way. They started to commandeer the parking lot across from Shoreline Amphitheater, stationing people at all the entrances to make sure only approved Googlers were there.
“That’s where it started,” she says. “It was me and a few drivers every week. We’d come up with a group of things that we wanted to test, get our supplies in a truck, and drive the truck down to the lot and run the tests.”
These became the first structured tests in the self-driving program. It turns out that the hard part is not really the what-if-a-zombie-is-eating-a-person-in-the-road scenarios people dream up, but proceeding confidently and reliably like a human driver within the endless variation of normal traffic.
Villegas started gathering props from wherever she could find them: dummies, cones, fake plants, kids’ toys, skateboards, tricycles, dolls, balls, doodads. All of them went into the prop stash. (Eventually, the props were stored in a tent, and now at Castle, in a whole storage unit.)
But there were problems. They wanted to drive faster and use streetlights and stop signs. And the concert season at Shoreline Amphitheater regularly threw kinks in their plans. “It was like, ‘Well, Metallica is coming, so we’re gonna have to hit the road,’” she says.
They needed a base, a secret base. And that’s what Castle provided. They signed a lease and started to build out their dream fake city. “We made conscious decisions in designing to make residential streets, expressway-style streets, cul-de-sacs, parking lots, things like that,” she says, “so we’d have a representative concentration of features that we could drive around.”
We walk from the main trailer office to her car. She hands me a map as we pull away to travel the site. “Like at Disneyland, so you can follow along,” she says. The map has been meticulously constructed. In one corner, there is a Vegas-style sign that says, “Welcome to Fabulous Castle, California.” The different sections of the campus even have their own naming conventions. In the piece we’re traveling through, each road is named after a famous car (DeLorean, Bullitt) or after a car (e.g., Barbaro) from the original Prius fleet in the early days of the program.
We pass by a cluster of pinkish buildings, the old military dormitories, one of which has been renovated: That’s where the Waymo people sleep when they can’t make it back to the Bay. Other than that, there are no buildings in the testing area. It is truly a city for robotic cars: All that matters is what’s on and directly abutting the asphalt.
As a human, it feels like a video-game level without the non-player characters. It’s uncanny to pass from boulevards to neighborhood-ish streets with cement driveways to suburban intersections, minus the buildings we associate with these places. I keep catching glimpses of roads I feel like I’ve traveled.
We pull up to a large, two-lane roundabout. In the center, there is a circle of white fencing. “This roundabout was specifically installed after we experienced a multilane roundabout in Austin, Texas,” Villegas says. “We initially had a single-lane roundabout and were like, ‘Oh, we’ve got it. We’ve got it covered.’ And then we encountered a multi-lane and were like, ‘Horse of a different color! Thanks, Texas.’ So, we installed this bad boy.”
We stop as Villegas gazes at one piece of the new addition: Two car lanes and a bike lane run past parallel parking abutting a grass patch. “I was really keen on installing something with parallel parking along it. Something like this happens in suburban downtowns. Walnut Creek. Mountain View. Palo Alto,” she says. “People are coming out of storefronts or a park. People are walking between cars, maybe crossing the street carrying stuff.” The lane was like a shard of her own memory that she’s embedded in the earth in asphalt and concrete, which will make its way into a more abstract form, an improved ability for a robot to handle her home terrain.
She drives me back to the main office and we hop into a self-driving van, one of the Chrysler Pacificas. Our “left-seat” driver is Brandon Cain. His “right-seat” co-driver in the passenger seat will track the car’s performance on a laptop using software called XView.
And then there are the test assistants, who they call “foxes,” a sobriquet that evolved from the word “faux.” They drive cars, create traffic, act as pedestrians, ride bikes, hold stop signs. They are actors, more or less, whose audience is the car.
The first test we’re gonna do is a “simple pass and cut-in,” but at high speed, which in this context means 45 miles per hour. We set up going straight on a wide road they call Autobahn.
After the fox cuts us off, the Waymo car will brake and the team will check a key data point: our deceleration. They are trying to generate scenarios that cause the car to have to brake hard. How hard? Somewhere between a “rats, not gonna make the light” hard stop and “my armpits started involuntarily sweating and my phone flew onto the floor” really hard stop.
Let me say something ridiculous: This is not my first trip in a self-driving vehicle. In the past, I’ve taken two different autonomous rides: first, in one of the Lexus SUVs, which drove me through the streets of Mountain View, and second, in Google’s cute little Firefly, which bopped around the roof of a Google building. They were both unremarkable rides, which was the point.
But, this is different. These are two fast-moving cars, one of which is supposed to cut us off with a move that will be, to use the Waymo term of art, “spicy.”
It’s time to go. Cain gets us moving and with a little chime, the car says, “Autodriving.” The other car approaches and cuts us off like a Porsche driver trying to beat us to an exit. We brake hard and fast and smooth. I’m impressed.
Then they check the deceleration numbers and realize that we had not braked nearly hard enough. We have to do it again. And again. And again. The other car cuts us off at different angles and with different approaches. They call this getting “coverage.”
We go through three other tests: high-speed merges, encountering a car that’s backing out of a driveway while a third blocks the autonomous vehicle’s view, and smoothly rolling to a stop when pedestrians toss a basketball into our path. Each is impressive in its own way, but that cut-off test is the one that sticks with me.
As we line up for another run, Cain shifts in his seat. “Have you ever seen Pacific Rim?” Cain asks me. You know the Guillermo del Toro movie where the guys get synced up with huge robot suits to battle monsters. “I’m trying to get in sync with the car. We share some thoughts.”
I ask Cain to explain what he actually means by syncing with the car. “I’m trying to adjust to the weight difference of people in the car,” he says. “Being in the car a lot, I can feel what the car is doing—it sounds weird, but—with my butt. I kinda know what it wants to do.”
Far from the haze and heat of Castle, there is Google’s comfy headquarters in Mountain View. I’ve come to visit Waymo’s engineers, who are technically housed inside X, which you may know as Google X, the long-term, high-risk research wing of the company. In 2015, when Google restructured itself into a conglomerate called Alphabet, X dropped the Google from its name (their website is literally X.company). A year after the big restructuring, X/Alphabet decided to “graduate” the autonomous vehicle program into its own company as it had done with several other projects before, and that company is Waymo. Waymo is like Google’s child, once removed, or something.
So, Waymo’s offices are still inside the mother ship, though, like two cliques slowly sorting themselves out, the Waymo people all sit together now, I’m told.
The X/Waymo building is large and airy. There are prototypes of Project Wing’s flying drones hanging around. I catch a bit of the cute little Firefly car the company built. (“There’s something sweet about something you build yourself,” Villegas had said back at Castle. “But they had no A/C, so I don’t miss them.”)
Up from the cafeteria, tucked in a corner of a wing, is the Waymo simulation cluster. Here, everyone seems to have Carcraft and XView on their screens. Polygons on black backgrounds abound. These are the people creating the virtual worlds that Waymo’s cars drive through.
Waiting for me is James Stout, Carcraft’s creator. He’s never gotten to speak publicly about his project and his enthusiasm spills out. Carcraft is his child.
“I was just browsing through job posts and I saw that the self-driving car team was hiring,” he says. “I couldn’t believe that they just had a job posting up.” He got on the team and immediately started building the tool that now powers 8 million virtual miles per day.
Back then, they primarily used the tool to see what their cars would have done in tricky situations in which human drivers have taken over control of the car. And they started making scenarios from these moments. “It quickly became clear that this was a really useful thing and we could build a lot out of this,” Stout says. The spatial extent of Carcraft’s capabilities grew to include whole cities, the number of cars grew into a huge virtual fleet.
Stout brings in Elena Kolarov, the head of what they call their “scenario maintenance” team to run the controls. She’s got two screens in front of her. On the right, she has up XView, the screen that shows what the car is “seeing.” The car uses cameras, radar, and laser scanning to identify objects in its field of view—and it represents them in the software as little wireframe shapes, outlines of the real world.
Green lines run out from the shapes to show the possible ways the car anticipates the objects could move. At the bottom, there is an image strip that displays what the regular (i.e., visible-light) cameras on the car captured. Kolarov can also turn on the data returned by the laser scanner (LIDAR), which is displayed in orange and purple points.
We see a playback of a real merge on the roundabout at Castle. Kolarov switches into a simulated version. It looks the same, but it’s no longer a data log but a new situation the car has to solve. The only difference is that at the top of the XView screen it says “Simulation” in big red letters. Stout says that they had to add that in because people were confusing simulation for reality.
They load up another scenario. This one is in Phoenix. Kolarov zooms out to show the model they have of the city. For the whole place, they’ve got “where all the lanes are, which lanes lead into other lanes, where stop signs are, where traffic lights are, where curbs are, where the center of the lane is, sort of everything you need to know,” Stout says.
We zoom back in on a single four-way stop somewhere near Phoenix. Then Kolarov starts dropping in synthetic cars and pedestrians and cyclists.
With a hot key press, the objects on the screen begin to move. Cars act like cars, driving in their lanes, turning. Cyclists act like cyclists. Their logic has been modeled from the millions of miles of public-road driving the team has done. Underneath it all, there is that hyper-detailed map of the world and models for the physics of the different agents in the scene. They have modeled both the rubber and the road.
Not surprisingly, the hardest thing to simulate is the behavior of the other people. It’s like the old parental saw: “I’m not worried about you driving. I’m worried about the other people on the road.”
“Our cars see the world. They understand the world. And then for anything that is a dynamic actor in the environment—a car, a pedestrian, a cyclist, a motorcycle—our cars understand intent. It’s not enough to just track a thing through a space. You have to understand what it is doing,” Dmitri Dolgov, Waymo’s vice president of engineering, tells me. “This is a key problem in building a capable and safe self-driving car. And that sort of modeling, that sort of understanding of the behaviors of other participants in the world, is very similar to this task of modeling them in simulation.”
There is one key difference: In the real world, they have to take in fresh, real-time data about the environment and convert it into an understanding of the scene, which they then navigate. But now, after years of work on the program, they feel confident that they can do that because they’ve run “a bunch of tests that show that we can recognize a wide variety of pedestrians,” Stout says.
So, for most simulations, they skip that object-recognition step. Instead of feeding the car raw data it has to identify as a pedestrian, they simply tell the car: A pedestrian is here.
At the four-way stop, Kolarov is making things harder for the self-driving car. She hits V, a hot key for vehicle, and a new object appears in Carcraft. Then she mouses over to a drop-down menu on the righthand side, which has a bunch of different vehicle types, including my favorite: bird_squirrel.
The different objects can be told to follow the logic Waymo has modeled for them or the Carcraft scenario builder can program them to move in a precise way, in order to test specific behaviors. “There’s a nice spectrum between having control of a scenario and just dropping stuff in and letting them go,” Stout says.
Once they have the basic structure of a scenario, they can test all the important variations it contains. So, imagine, for a four-way stop, you might want to test the arrival times of the various cars and pedestrians and bicyclists, how long they stop for, how fast they are moving, and whatever else. They simply put in reasonable ranges for those values and then the software creates and runs all the combinations of those scenarios.
They call it “fuzzing,” and in this case, there are 800 scenarios generated by this four-way stop. It creates a beautiful, lacy chart—and engineers can go in and see how different combinations of variables change the path that the car would decide to take.
The problem really becomes analyzing all these scenarios and simulations to find the interesting data that can guide engineers to be able to drive better. The first step might just be: Does the car get stuck? If it does, that’s an interesting scenario to work on.
Here we see a video that shows exactly such a situation. It’s a complex four-way stop that occurred in real life in Mountain View. As the car went to make a left, a bicycle approached, causing the car to stop in the road. Engineers took that class of problem and reworked the software to yield correctly. What the video shows is the real situation and then the simulation running atop it. As the two situations diverge, you’ll see the simulated car keep driving and then a dashed box appear with the label “shadow_vehicle_pose.” That dashed box shows what happened in real life. To Waymo people, this is the clearest visualization of progress.
But they don’t just have to look for when the car gets stuck. They might want to look for too-long decision times or braking profiles outside the right range. Anything that engineers are working on learning or tuning, they will simulate looking for problems.
Both Stout and the Waymo software lead Dolgov stressed that there were three core facets to simulation. One, they drive a lot more miles than would be possible with a physical fleet—and experience is good. Two, those miles focus on the interesting and still-difficult interactions for the cars rather than boring miles. And three, the development cycles for the software can be much, much faster.
“That iteration cycle is tremendously important to us and all the work we’ve done on simulation allows us to shrink it dramatically,” Dolgov told me. “The cycle that would take us weeks in the early days of the program now is on the order of minutes.”
Well, I asked him, what about oil slicks on the road? Or blown tires, weird birds, sinkhole-sized potholes, general craziness. Did they simulate those? Dolgov was sanguine. He said, sure, they could, but “how high do you push the fidelity of the simulator along that axis? Maybe some of those problems you get better value or you get confirmation of your simulator by running a bunch of tests in the physical world.” (See: Castle.)
The power of the virtual worlds of Carcraft is not that they are a beautiful, perfect, photorealistic renderings of the real world. The power is that they mirror the real world in the ways that are significant to the self-driving car and allow it to get billions more miles than physical testing would allow. For the driving software running the simulation, it is not like making decisions out there in the real world. It is the same as making decisions out there in the real world.
And it’s working. The California DMV requires that companies report the miles that they’ve driven autonomously each year along with disengagements that test drivers make. Not only has Waymo driven three orders of magnitude more miles than anyone else, but their number of disengagements have fallen quickly.
Waymo drove 635,868 autonomous miles from December 2015 to November 2016. In all those miles, they only disengaged 124 times, for an average of about once every 5,000 miles, or 0.20 disengagements per 1,000 miles. The previous year, they drove 424,331 autonomous miles and had 272 disengagements, for an average of once every 890 miles, or 0.80 disengagements per 1,000 miles.
While everyone takes pains to note that these are not exactly apples-to-apples numbers, let’s be real here: These are the best comparisons we’ve got and in California, at least, everybody else drove about 20,000 miles. Combined.

The tack that Waymo has taken is not surprising to outside experts. “Right now, you can almost measure the sophistication of an autonomy team—a drone team, a car team—by how seriously they take simulation,” said Chris Dixon, a venture capitalist at Andreessen Horowitz who led the firm’s investment in the simulation company Improbable. “And Waymo is at the very top, the most sophisticated.”
I asked Allstate Insurance’s head of innovation, Sunil Chintakindi, about Waymo’s program. “Without a robust simulation infrastructure, there is no way you can build [higher levels of autonomy into vehicles].” he said. “And I would not engage in conversation with anyone who thinks otherwise.”
Other self-driving car researchers are also pursuing similar paths. Huei Peng is the director of Mcity, the University of Michigan’s autonomous- and connected- vehicle lab. Peng said that any system that works for self driving cars will be “a combination of more than 99 percent simulation plus some carefully designed structured testing plus some on-road testing.”
He and a graduate student proposed a system for interweaving road miles with simulation to rapidly accelerate testing. It’s not unlike what Waymo has executed. “So what we are arguing is just cut off the boring part of driving and focus on the interesting part,” Peng said. “And that can let you accelerate hundreds of times: A thousand miles becomes a million miles.”
What is surprising is the scale, organization, and intensity of Waymo’s project. I described the structured testing that Google had done to Peng, including the 20,000 scenarios that had made it into simulation from the structured testing team at Castle. But he misheard me and began to say, “Those 2,000 scenarios are impressive,”—when I cut in and corrected him—“It was 20,000 scenarios.” He paused. “20,000,” he said, thinking it over. “That’s impressive.”
And in reality, those 20,000 scenarios only represent a fraction of the total scenarios that Waymo has tested. They’re just what’s been created from structured tests. They have even more scenarios than that derived from public driving and imagination.
“They are doing really well,” Peng said. “They are far ahead of everyone else in terms of Level Four,” using the jargon shorthand for full autonomy in a car.
But Peng also presented the position of the traditional automakers. He said that they are trying to do something fundamentally different. Instead of aiming for the full autonomy moon shot, they are trying to add driver-assistance technologies, “make a little money,” and then step forward toward full autonomy. It’s not fair to compare Waymo, which has the resources and corporate freedom to put a $70,000 laser range finder on top of a car, with an automaker like Chevy that might see $40,000 as its price ceiling for mass-market adoption.
“GM, Ford, Toyota, and others are saying ‘Let me reduce the number of crashes and fatalities and increase safety for the mass market.’ Their target is totally different,” Peng said. “We need to think about the millions of vehicles, not just a few thousand.”
And even just within the race for full autonomy, Waymo now has more challengers than it used to, Tesla in particular. Chris Gerdes is the director of the Center for Automotive Research at Stanford. Eighteen months ago, he told my colleague Adrienne LaFrance that Waymo “has much greater insight into the depth of the problems and how close we are [to solving them] than anyone else.” When I asked him last week if he still thought that was true, he said that “a lot has changed.”
“Auto manufacturers such as Ford and GM have deployed their own vehicles and built on-road data sets,” he said. “Tesla has now amassed an extraordinary amount of data from Autopilot deployment, learning how the system operates in exactly the conditions its customers experience. Their ability to test algorithms on board in a silent mode and their rapidly expanding base of vehicles combine to form an amazing testbed.”
In the realm of simulation, Gerdes said that he had seen multiple competitors with substantial programs. “I am sure there is quite a range of simulation capabilities but I have seen a number of things that look solid,” he said. “Waymo no longer looks so unique in this respect. They certainly jumped out to an early lead but there are now a lot of groups looking at similar approaches. So it is now more of a question of who can do this best.”
This is not a low-stakes demonstration of a neural network’s “brain-like” capacities. This is making a massive leap forward in artificial intelligence, even for a company inside Alphabet, which has been aggressive in adopting AI. This is not Google Photos, where a mistake doesn’t mean much. This is a system that will live and interact in the human world completely autonomously. It will understand our rules, communicate its desires, be legible to our eyes and minds.
Waymo seems like it has driving as a technical skill—the speed and direction parts of it—down. It is driving as a human social activity that they’re working on now. What is it to drive “normally,” not just “legally”? And how does one teach an artificial intelligence what that means?
It turns out that building this kind of artificial intelligence does not simply require endless data and engineering prowess. Those are necessary, but not sufficient. Instead, building this AI requires humans to sync with the cars, understanding the world as they do. As much as anyone can, the drivers out at Castle know what it is to be one of these cars, to see and make decisions like them. Maybe that goes both ways, too: The deeper humans understand the cars, the deeper the cars understand humans.
A memory of a roundabout in Austin becomes a piece of Castle becomes a self-driving car data log becomes a Carcraft scenario becomes a web of simulations becomes new software that finally heads back out on a physical self-driving car to that roundabout in Texas.
Even within the polygon abstraction of the simulation the AI uses to know the world, there are traces of human dreams, fragments of recollections, feelings of drivers. And these components are not mistakes or a human stain to be scrubbed off, but necessary pieces of the system that could revolutionize transportation, cities, and damn near everything else.


If you use Instagram, you have seen an exhortation from a friend or colleague to check out some article or event. These calls to action inevitably end, “Link in Bio.”  
That’s shorthand, of course, for the single link that Instagram allows users to drop into their profiles. Because other links can’t be added to posts, that single link is an endorsement: It must be the one URL in the world that you are willing to attach to yourself at that moment.
For years, I’ve wondered why Instagram doesn’t allow links elsewhere. It would be so simple. (I reached out to Instagram for comment, but they didn’t respond.) The Ringer’s Alyssa Bereznak dressed down the company in a post titled, appropriately, “‘Link in Bio’ Is the Worst Thing About Instagram.”
“A network that hosts millions of people won’t let them do something that is second nature for digital natives. So its users have concocted their own clunky loophole to get around the problem,” Bereznak writes. “It’s as if there were a permanent snowstorm in a city, and the mayor refused to clear the sidewalks. Inevitably, pedestrians would just stomp out their own inelegant roundabout paths to navigate the dirty, urine-filled slush.”
Other writers have called Link in Bio “dreaded,” “clumsy,” and “clunky at best.”
And yet, Instagram crushes on, adding users by the hundred millions. The reason is simple: People, like myself, like Instagram. It is a plain like, uncomplicated. In 2015, when my colleague Rob Meyer wrote the definitive post about liking Instagram, “I Like Instagram,” he laid out its excellent lack of features:
It is a silly, idiosyncratic piece of software, but so simple. It says: Here is a picture. Here is a picture of a weird bird my friend saw. Here is a picture of my friend celebrating Eid with her brother. Here is a picture of an acquaintance flying over the city where I used to live.
With every photo, I have two options. I can scroll by, or I can say “I saw this and liked it.” Either way, then I scroll some more. It is a place to look at pictures and, maybe, video. It does not do much else. It doesn’t need to. It is so simple as to be almost serene.
It was true back then! The app’s simplicity seemed to be its heart. But many things have changed about Instagram in the intervening two years. There are now stories, daring you to step into their circles. The formerly chronological feed has been Facebooked. Even back then, Instagram had already added private messaging to its basic function of picture posting.
But the basic feeling people have about Instagram remains the same. They might be annoyed by certain aspects of the app, but they still like it, basically, and in the flat way that Meyer captured perfectly. “Oh, that was nice” is something one could imagine saying after looking at Instagram.
Not so for Facebook or Twitter. On those platforms, I feel like I’ve been snookered into emoting. The reason for this, I would argue, is simple: Instagram has never become a full participant in the web. By refusing to allow hyperlinks, it has maintained a distinct space on the internet. Twitter and Facebook expanded to become a messy, permeable front end for the whole of the web (even as they try to claw ever more video minutes/ads into their players).
And what is on the web right now is, more or less, politics. The Trump era has meant that Americans are talking about politics perpetually, endlessly, circuitously, directly, boringly, excitedly. Given the circumstances and stakes, that’s not necessarily a bad thing. But as “the conversation” goes, so follows the content, bucketing arguments, relating to your feelings, connecting popular culture with political theater. I don’t know anyone who isn’t exhausted, at least some of the time.
Then there is Instagram, where the documentation that life goes on doesn’t feel out of place. Like a recipe book written in 1944, Instagram declares: Still gotta eat! Like sunset pics from 1968: Still the world turns!
You can trace this right back to Link in Bio. Mobile apps are supposed to let the users do what they want, what they demand. Their paths of desire must be seamless and easy. Friction is the enemy.
But what if friction is necessary for the long-term health of these social systems? What if the platforms sometimes need to do the thing that generates lower short-run “engagement”? What if social networks now need dampening, not amplification?
The outright denial of user desire is also a good reminder of what these spaces actually are. The New York Times’ John Herrman calls the big platforms a “commercial simulation” of freedom. Instagram does not pretend to be part of the public sphere. It is not the natural home of #theresistance. It’s a place for the Sunday’s-best version of your personal life to have space on the internet.
In recent months, Instagram has experimented with letting verified users link out to the web from inside their stories. It solves the Link-in-Bio problem like that. But once the web starts to creep in, will its exhausting dynamics follow?


After Charlottesville, white supremacists’ physical and digital presences—and the tactics used to combat them—are under renewed scrutiny.
There have been attempts, most prominently by Logan Smith, who runs the Twitter account Yes, You’re Racist, to tack real names and identities onto the pictures of people who showed up to rally for white supremacy in the city. Some people have called this “doxxing,” referring to the practice of figuring out the identity of an anonymous or pseudonymous person online and publishing their IRL details, usually to spur harassment.
The technique has been used online for decades. It gained widespread notoriety during the Gamergate saga, when a variety of unsavory far-right-wing figures doxxed women in the video-game media, but people all over the political spectrum have done their fair share. That’s led a lot of reasonable people to declare the tactic verboten, more or less, no matter the target. It’s just beyond the pale.
What Smith is doing, however, is not doxxing as it has been understood. His targets showed up at a public rally. They made no attempt to hide their identities.
The merging of the digital and physical worlds does make things more complicated. One can imagine that the rally participants did not expect national scrutiny before the event. They showed up with a few hundred people and have ended up in pictures seen by tens of millions online. This is a variation on what academics have called “context collapse.” Smith is doing that collapsing, making white supremacists accountable for their views and speech in a greater portion of their lives.
At the same time, their outing digitally opens them up to the dynamics of online vigilantism: jeers, threats, and more.
“If you’re going to go out in public and advocate for Nazi ideas, you have to be prepared for people to say, ‘You’re a terrible person,’” said Sasha Costanza-Chock, a Massachusetts Institute of Technology associate professor of civic media. “I don’t think there is much defensible in saying we shouldn’t do that. We might want to preserve the term ‘doxxing’ having a specific meaning, but identifying and mapping extreme-right networks—we should agree that’s reasonable to do.”
Smith’s actions are certainly on the same continuum as doxxing, but not precisely in the same spot, and Costanza-Chock asked, what else is on that continuum? Perhaps the Southern Poverty Law Center’s research into hate groups (or Political Research Associates’ report on the rise of the “alt-right”)? Maybe a local reporter digging into a government official who is secretly in the Ku Klux Klan?
When people say “the solution to hate speech is more speech”—might this fit the bill as the “more speech” part? It’s imperfect, sometimes painful, but ultimately part of the universe of speech that free-speech advocates hope will result in a better society.
Maybe.
But there are established rules and norms, as well as (some) accountability processes for journalists and researchers, which don’t tend to exist among doxxers.
And online searches like this are very prone to error. This sort of thing has been happening for years—and the internet sleuths, as local news stories love to tag them, have often gone awry. Smith has misidentified some of the people in the photos, with predictably bad consequences. A poor tissue engineer at the University of Arkansas got randomly dragged into the mess.
Lucas Wright and Susan Benesch, of the Dangerous Speech Project, write on the project’s blog that searches like this will always encounter similar problems, by malfeasance or actual bad actors feeding false information into the system. “These false positives are inevitable since the strategy relies on imperfect information—yet can seriously disrupt a target’s life,” they write. “As it becomes easier to forge realistic fake videos, such errors will only become more common.”
Costanza-Chock argued that some guide should exist for people wishing to engage in Nazi doxxing that would instruct them on how to do so as responsibly as possible and offer other routes to grapple with their online accounts.
But Wright and Benesch reject the tactic more completely, at least from the perspective of trying to de-radicalize white supremacists. “Disrupting their lives—getting them fired from their jobs, disowned by their parents, or dogpiled with threats on Twitter—may give a satisfying jolt of schadenfreude, but it also cuts them off from the remaining moderating forces in their lives,” they write. “When that happens, they will not learn to love; they will only commit further to the dangerous communities that are willing to embrace them.”
And that may be true. There may be a tradeoff between raising the costs of doing white supremacy out there in the world—and increasing the radicalization of those who Smith and others identify.


A young girl stands alongside a wood-canvas canoe. She wraps the tails of a long, leather strap around one of the canoe’s horizontal struts in preparation for a two-mile portage to the next lake. She swings the 100-pound, waterlogged canoe up over her head in a single fluid motion, resting it upside down on her shoulders, one end pointed toward the sky. Then she pulls the leather strap behind her hairline like a bandanna, adjusts her stance so the weight of the canoe is channeled smoothly down her neck and spine, and starts walking the rocky trail.
Methods of “head carry,” or weighting a load on top of the head, are standard across the developing world. The best-known images of the technique show African women trekking miles to and from water sources. Despite 10-gallon buckets balanced precariously above them, their backs are straight as a rod and their chins lifted. Often, heavy loads aren’t only carried on the heads, but suspended from it by a strap, called a tumpline. Light, internal-frame backpacks have largely replaced tumplines among modern folk who carry loads outdoors. But when done properly, head carry can be safer, more efficient, and more functional than supposedly better, newer technologies.
* * *
Looking for the inventor of head-carry devices and techniques is like asking who invented shoes. No single origin story exists for a piece of leather, fabric, or rope that is knotted, looped, or buckled around a load and worn across the top of the head. The tumpline precedes even the backpack. It has been used across every populated continent. Congolese women use tumplines to carry charcoal and firewood. Sherpas, perhaps the most famous tumpline users today, have been known to eschew modern packs even when they are offered, preferring the simple strap when carrying gear in the Himalayas. They call tumplines namlo and carry up to their own body weight in baskets called doko.
The English word “tumpline” is thought to be a shortening of the Algonquian words mattump or metump, and it entered the Western lexicon alongside trade. Fur-trapping voyageurs and coureurs de bois learned the method from their Native allies in the 17th and early 18th centuries. It was the only feasible way to carry large bundles of furs and gear through the dense forests between navigable bodies of water in what is now Canada and the northernmost United States.
In 1882, the French Canadian shoemaker Camille Poirier brought the tumpline to the North American masses when he created the Duluth pack. This pack kept the familiar shape of the backpack, with shoulder straps and a fixed-volume storage compartment, but added a tumpline attachment that allowed for the pack to be weighted on the shoulders, the neck, or both. The Duluth pack quickly became a classic; it is still manufactured in Duluth, Minnesota, and imitated by outdoor outfitters around the world.
By the early 20th century, the tumpline’s influence had spread from outfitters to the military. During World War I, in the trenches of the Somme, Staff Captain F.R. Phelan formed the 11th Canadian Brigade Tumpline Company after showing how much manpower and time could be saved if resupplies of the muddy trenches were conducted by tumpline. Phelan had learned the technique while hunting and fishing in the wilderness of Quebec, mirroring the journeys and challenges of the voyageurs.
Phelan’s men were issued an oiled leather tumpline with two long tails. The tails could be tied around larger loads than would fit in a backpack or in the hands. It also could be worn without interfering with their helmets. The men were trained in knots and proper posture, and they slowly increased the weight of their loads until they were carrying twice what had previously been possible, and with equal or greater efficiency. The tumpline made the dangerous process of resupplying the trenches faster and safer.
By 1944, tumpline companies had become standard across the Canadian ground forces. A Popular Mechanics article from that year shows Canadian soldiers using tumplines to carry medical supplies, machine guns, and even to pull sled-style stretchers designed for moving casualties while under fire during World War II.
* * *
In the second half of the 20th century, the rise of performance outdoor gear threatened to extinguish the tumpline outside of traditional communities. Backpacks are not a modern creation, but the internal-frame pack, invented by Greg Lowe in 1967, was a watershed moment in modern outdoorsmanship. Tumplines had never been ubiquitous on packs, but the creation of a less bulky pack that comfortably loaded weight on the hips rather than the shoulders made it easy to justify removing tumpline attachments from backpack designs.
Advances in padding made packs more comfortable. Improved textiles increased waterproofness. And better design made them more space-efficient. In the process, backpacks became commodities—mass-produced, but also one-size-fits-most. By contrast, tumplines are precision tools that need to be adjusted carefully to fit each user. Wearers need to be trained in proper posture and technique. They must carefully ease into heavier loads as they build up muscles in their necks and backs. Backpacks can be treated as precision tools as well, but the barrier to entry is much lower. Shoulder straps are simple and intuitive; they slip right on. Hip belts are equally self-explanatory, and the flaws of the modern backpack have mostly been overlooked in favor of convenience and fashion.
This state of affairs makes Yvon Chouinard an outlier. He is the founder of Patagonia, a company that makes some of the fanciest gear in the outdoor sports world. And even though he built an empire on $900 parkas and $500 sleeping bags, Chouinard still swears by the tumpline. His company sells a simple nylon version of the product for under $20.
Chouinard took up the tumpline in 1968, after sustaining a neck injury in the jungles of Colombia that resulted in severe recurring back pain. A decade later, during an expedition to Nepal, he saw that the porters were carrying twice as much as the climbers with much simpler gear. Chouinard began training with the tumpline, and it proved an effective a solution. To this day, the founder of a company some critics have nicknamed “Patagucci” makes use of an ancient tool, claiming that he’d never go back to using a conventional pack without adding on a tumpline.
Despite the advocacy of industry leaders like Chouinard, the proven efficacy by some of the world’s strongest athletes and Sherpas, and hundreds of years of documented use, the most common argument against tumplines is that they aren’t safe. The claim is not without merit, but impatience is more to blame than intrinsic risk. If a runner were suddenly to run barefoot on a dirt trail, they’d likely get injured. Feet accustomed to structured cocoons of cushioned safety do not take well to the shock of having to work in new ways. Likewise, hoisting 50 pounds atop an unstrengthened spine without training guidance can also lead to harm.
Numerous studies show that tumplines and other head-carry techniques are more metabolically efficient and physically healthy than the supposedly high-tech successors that fill today’s gear shops. African women have been found to carry loads of up to 60 percent of their body weight on their heads more economically than army recruits with a backpack of an equivalent weight. Nepalese porters with a tumpline have been found to be 60 percent faster and 39 percent more powerful than their clients carrying modern packs.
In 2007, the Outside magazine contributor Eric Hansen wrote about how he had tested out the economy of the tumpline for himself. After convincing a team of Nepalese porters to let him into their ranks, he was fitted with a rope tumpline and loaded up. The experience wasn’t fun or comfortable, but it did question prevailing norms in the world of outdoor sports. Whereas a client might struggle with a 55-pound load, that was the absolute minimum that a porter would carry for the same distance.
* * *
Back in Northern Ontario, a group of kids from Keewaydin sets out on a canoe trip. Their camp is committed to preserving “the old ways”: They are portaging a month’s worth of gear, food, and the canoes needed to carry it all by tumpline. It’s not just for the sake of nostalgia or grit-building, either. The old technology makes efficient use of the limited space inside of a canoe, for one thing. For another, it’s more logical than the complicated contraptions people have dreamt up for portaging boats. There are rolling carts, backpack-style frames, and yokes—but a simple strap still suffices.
The wilderness-canoe guide and writer Cliff Jacobson recommends that backpackers and canoe-trippers take a piece from each tradition—a hip belt for added support when going downhill, shoulder straps for stability, and a tumpline modification for uphill climbs. Like Chouinard, he claims that he’d never own a pack without a tumpline. “Everest Sherpas use tumplines,” he writes, “but Americans still insist on backpacks with hip belts. You tell me: Who’s the professional?”
With simple technologies staging heroic comebacks, maybe the tumpline will enjoy a 21st-century resurgence. In the meantime, native communities and passionate advocates who choose function over fashion and long-term safety over short-term comfort keep this traditional method of carrying things on the head from going extinct.
This article appears courtesy of Object Lessons.


Last night, Tucker Carlson took on the subject of slavery on his Fox News show. Slavery is evil, he noted. However, slavery permeated the ancient world, he said, as reflected in the on-screen graphics.
On Twitter, recent University of Toronto English Ph.D. graduate Anthony Oliveira noted, “Here's Tucker Carlson right now on Fox making the *exact* pro-slavery case (bad but status-quo and well-precedented) made 160 years ago.”
It sounds like a particular variety of Twitter gallows humor, not meant to be taken quite seriously. But it is not a joke.
This precise series of ostensible mitigating factors around the institution of American slavery were, in fact, advanced by pro-slavery forces through the 19th century. And it got me wondering: Given that The Atlantic was founded as an abolitionist magazine before the Civil War, might there be an article or two that might address Carlson’s warmed-over proto-Confederate arguments?
And indeed, there are.
Take Carlson’s bullet point, “Until 150 years ago, slavery was rule.”
Well, yes. Slavery was legal in some American states. But how did this happen, especially when other countries began abolishing slavery early in the 19th century? In our second issue, Edmund Quincy put his pen to “Where Will It End?” And he doesn’t mess around. Slavers had power because they went on bloody conquests to open up new territory for slavery.
The baleful influence thus ever shed by Slavery on our national history and our public men has not yet spent its malignant forces. It has, indeed, reached a height which a few years ago it was thought the wildest fanaticism to predict; but its fatal power will not be stayed in the mid-sweep of its career ... Slavery presiding in the Cabinet, seated on the Supreme Bench, absolute in the halls of Congress—no man can say what shape its next aggression may not take to itself. A direct attack on the freedom of the press and the liberty of speech at the North, where alone either exists, were no more incredible than the later insolences of its tyranny ... The rehabilitation of the African slave-trade is seriously proposed and will be furiously urged, and nothing can hinder its accomplishment but its interference with the domestic manufactures of the breeding Slave States ... Mighty events are at hand, even at the door; and the mission of them all will be to fix Slavery firmly and forever on the throne of this nation.
Indeed, in the early days of The Atlantic, the violent battle over whether Kansas would become a slave state raged. In the “Kansas Usurpation,” from Issue 4, our author details the endless skulduggery that slavers perpetrated “to force the evils of slavery upon a people who cannot and will not endure them.”
And how about the idea that ancient peoples also held slaves? The Atlantic didn’t address Greek slaveholding, but takes on their admirers, the Romans. In a piece called “Spartacus,” published in Issue 3 in 1858, the author explicitly differentiates the Roman version of slavery from the American.
“Fowell Buxton has happily translated [the Roman motto], ‘They murdered all who resisted and enslaved the rest.’ But it was as slaveholders that the Romans most clearly exhibited their impartiality,” the piece states. “They were above those miserable subterfuges that are so common with Americans. They made slaves of all, of the high as well as the low—of Thracians, as well as Sardinians, of Greeks and of Syrians as readily as of Scythians and Cappadocians.”
With ever-increasing rigor from colonial times, the American system explicitly made only people with African ancestry subject to chattel slavery, i.e. they were the only people whose children were born enslaved and who would die enslaved, absent an extraordinary circumstance. American slavery was different.
To be clear, this isn’t just about Carlson. My target is the implicit idea that American slavery was not historically, distinctly terrible. It was. There is no parallel. While other countries—and states within the Union—were banning slavery, the South was intensifying slavery in several different ways.
First, the ideological and theological interpretation of slavery in the South began to change. The specific and perpetual enslavement of African people had seemed to Jeffersonian Americans as an evil that was ebbing away. “In the late 18th century, most Americans believed that slavery, as institutionalized dependence, was neither good nor practical, and so would fade before the action of natural forces under the new, free political system,” writes John Patrick Daly in When Slavery Was Called Freedom: Evangelicalism, Proslavery, and the Causes of the Civil War.
But as abolitionists began to succeed in the northern states, chattel slavery of black human beings began to be theologically promoted as something to be proud of, possibly even holy, in the South. “Good slaveholders, they maintained, gave the institution its character—that is, goodness,” Daly writes. “This formulation allowed proslavery spokesmen to denounce the historically evil institution of slavery while defending Southern practices: Slaveholders in the evil form of slavery were bad men; the Southerners were good, and the source of their wealth untainted. Good—and especially evangelical—slaveholders supposedly redeemed the institution of slavery.”
Second, the old colonial state slaveowners were making a business out of selling the people they enslaved south and west. This became a lynchpin of the region’s wealth as agriculture declined there. Black people were chained together in Virginia and the Carolinas and marched to Georgia, to Florida, to Mississippi, to Texas. Whatever networks of family and community they’d been able to build within the oppressive violence of slavery were destroyed (again).
Ed Baptist tells this story in The Half Has Never Been Told: Slavery and the Making of American Capitalism. “The massive and cruel engineering required to rip a million people from their homes, brutally drive them to new, disease-ridden places, and make them live in terror and hunger as they continually built and rebuilt a commodity-generating empire,” he writes, “this vanished in the story of a slavery that was supposedly focused primarily not on producing profit but on maintaining its status as a quasi-feudal elite, or producing modern ideas about race in order to maintain white unity and elite power.”
Third, the gin-powered cotton economy relied on huge financial investments to open up new cotton land ever farther south and west. A series of financial bubbles ran in those directions, with literal securities issued to slaveowners secured by the bodies of enslaved people.
“African American bodies and childbearing potential collateralized massive amounts of credit, the use of which made slaveowners the wealthiest people in the country,” write Ned and Constance Sublette in The American Slave Coast: A History of the Slave-Breeding Industry. “When the Southern states seceded to form the Confederacy they partitioned off, and declared independence for, their economic system in which people were money.”
To make their loan payments, these speculator-slavers created the brutal “whipping machine,” which drove massive productivity gains at the expense of the health and well-being of the already oppressed people working in the fields.
”The returns from cotton monopoly powered the modernization of the rest of the American economy, and by the time of the Civil War, the United States had become the second nation to undergo large-scale industrialization,” Baptist writes. “In fact, slavery’s expansion shaped every crucial aspect of the economy and politics of the new nation—not only increasing its power and size, but also, eventually, dividing U.S. politics, differentiating regional identities and interests, and helping to make civil war possible. The idea that the commodification and suffering and forced labor of African Americans is what made the United States powerful and rich is not an idea that people necessarily are happy to hear. Yet it is the truth.”
It was this marriage of new ideological underpinning, the incredible profits the gin-powered cotton industry could produce, and the new modes of capitalization and management that American slaveowners developed that make American slavery different and worse from those that preceded it.
The drive to keep opening up cotton land to feed the slaver-speculator economy also led to genocidal atrocities against Native Americans, as well as the imperial project of snatching the western part of the continent from Mexico, which had abolished slavery in the 1820s.
In April 1861, with the slaveholder’s rebellion beginning, The Atlantic published an essay by Charles Francis Adams Jr., the grandson of John Quincy Adams, called “The Reign of King Cotton.”
“Throughout the South, whether justly or not, it is considered as well settled that cotton can be profitably raised only by a forced system of labor,” Adams wrote. “With this theory, the Southern States are under a direct inducement, in the nature of a bribe, to the amount of the annual profit on their cotton-crop, to see as many perfections and as few imperfections as possible in the system of African slavery.”
But the bribe didn’t stop getting paid at the Mason-Dixon line. Even New England, hotbed of abolitionism and birthplace of this magazine, got rich on textiles spun in the factories along the Merrimack. Where do you think they got the cotton for the City of Spindles? Baptist tells the story of the Collins Axe Works, which sold hundreds of thousands of axes into the western parts of the South, where they were given to enslaved black people to clear the forests. Hundreds of millions of trees fell through black labor performed with these axes. And back on the Farmington River, a white factory owner and his associates got rich.
“All told, more than $600 million, or almost half of the economic activity in the United States in 1836, derived directly or indirectly from cotton produced by the million-odd slaves—6 percent of the total U.S. population—who in that year toiled in labor camps on slavery’s frontier,” Baptist calculates.
There is no escaping the basic facts of our history. Plato, Muhammad, and the Aztec empire did not have the cotton gin or the luxuries that came from the securitization of enslaved people. Native American slaveholders didn’t shape and take advantage of emergent American capitalism to subdue a continent.
Given all this, no wonder the neo-Confederates keep fighting to keep their heroic monuments. Understanding the breadth and depth of the American slavery’s evil would undermine not just their dedication to busts of Robert E. Lee, but the whole moral project of seeing whiteness as a sign of virtue.
This is what Confederate flag wavers mean when they say they are “fighting for their heritage.” They are fighting for the right to declare their ancestors good, despite the evidence of the horrors they perpetrated, which rival anything that happened in the 20th century.
And what they’re counting on is that Americans, no matter when their families arrived across seas or rivers, will excuse the Confederate flag-wavers because they want to believe only the best stories about our country, too.
There is no excuse. That other people at other times owned slaves—Greek, African, or Native American—does not excuse the system of oppression that we erected on this continent to build this country.
“Many of those people were there to protest the taking down of the statue of Robert E. Lee. So this week, it’s Robert E. Lee, I noticed that Stonewall Jackson’s coming down,” President Trump said yesterday at a press conference. “I wonder, is it George Washington next week? And is it Thomas Jefferson the week after? You know, you really do have to ask yourself, where does it stop?”
What if the answer is that it doesn’t? The evil of slavery and the white supremacy it embedded in the fabric of the country go all the way back to the beginning. And our history needs to honestly tell the story of James Madison dying without freeing a single one of the 100 enslaved people who worked for him right alongside his call, quoted in The Atlantic in 1861, to leave the words slavery out of the Constitution so that it would “be the great charter of Human Liberty to the unborn millions who shall enjoy its protection, and who should never see that such an institution as slavery was ever known in our midst.”
We can excise the words, but we can never scrub the blood from the soil.


Both as a service and a company, Google has always been a convenient stand-in for the greater internet. “Googling” became shorthand for searching for something on the web. The “don’t be evil” line from its IPO prospectus became a catch-all politics for Silicon Valley.
So it’s not surprising that the outbreak of a new strain of reactionary politics has found its way to Google’s doorstep. First, Google fired the engineer James Damore for writing a memo explicitly opposing the company’s diversity and inclusion policies through a specious reading of the biology literature on gender and IQ differences in humans. Damore became an instant celebrity on the right and among free-speech absolutists.
Then, so-called “New Right” figure and noted conspiracy theorist Jack Posobiec—who President Trump retweeted this week—called for a march on Google offices in cities across the country for this Saturday, August 19. “Google is a monopoly, and its [sic] abusing its power to silence dissent and manipulate election results,” the announcement reads. “Their company YouTube is censoring and silencing dissenting voices by creating ‘ghettos’ for videos questioning the dominant narrative. We will thus be Marching on Google!”
This week, too, the Daily Stormer, a long-running neo-Nazi site, got cut off from its original hosting and went to Google, who also cut off the site.
Looking at this news in isolation, you might say: Google has become a right-wing target because it is a liberal institution. Certainly, it has not gone unnoticed on the right that Googlers overwhelmingly supported Hillary Clinton, both financially and operationally. And more generally, the Bay Area technology industry has become a key base of support for Democratic candidates.
But the ideology underpinning Silicon Valley does not fall on a strict left/right spectrum. For many years, scholars who study the internet’s dynamics have been saying that the public sphere—the place where civic society is supposed to play out, the place “free speech” advocates desire to see preserved—has been privatized by Facebook, Google, and the other big internet companies. Most on the left saw this privatization as part of a larger conservative (or—GASP—neoliberal) movement that was sapping the strength of the government-secured commons.
Zeynep Tufekci, a sociologist at Princeton University, made this connection in 2010. “This is about the fact that increasing portions of our sociality are now conducted in privately owned spaces. The implications of this are still playing out,” she wrote.
She cited a litany of examples running long before Google and Facebook: the outsourcing of key government functions to private contractors, the “dominance of corporate-owned media over the civic public sphere,” and the replacement of public parks with malls and “privately owned town squares.”
By moving our speech online, we entered a mall, where Facebook or Twitter or Google control the rules, not the U.S. government. Companies have different imperatives, the first of which is—for these companies, above all—to make money on advertising. By and large, this has led to a maximally permissive informational environment. On the occasions they have intervened to censor nipples or ban particular kinds of sites, their response has been: Hey, it’s a free market and you can always post photos to your LiveJournal, search with Bing, and chat on Gab.
These attitudes are why sometimes people describe Silicon Valley as “libertarian.” But most Silicon Valley people are wealthy Democrats who support progressive social causes, but do not want entrepreneurship and business restrained by the government (too much). They are the classic “socially liberal, fiscally conservative” people who the left (dirtbag and otherwise) loves to pillory.
Can the U.S. Government Seize an Anti-Trump Website's Visitor Logs?
Take the Los Angeles–based web host and domain-name registrar DreamHost, for example. On the one hand, they are fighting the Department of Justice, which has requested details on 1.3 million visitors to an anti-Trump website. “That information could be used to identify any individuals who used this site to exercise and express political speech protected under the Constitution’s First Amendment,” the company wrote on its blog. On the other, they are part of the cloud infrastructure of neo-Nazi groups like the American Nazi Party.
All of which puts both left-wing critics and right-wing marchers into different kinds of binds.
For the right, Google and Facebook are private actors who obey the market. Last I checked, strong-arming companies into doing the will of the people by extra-market means is not high on the list of conservative principles. Why can’t the market just decide? If people don’t like Google’s or Facebook’s decisions, they can head for the digital exits.
And on the left, Google and Facebook and the rest are, in fact, manifestations of neoliberalism. But because they’ve privatized the public sphere of discourse, they now possess the power to shut down white-supremacist sites, stop Daily Stormer links from circulating, and do it all without anyone having a legal or constitutional basis for stopping the “censorship.” The biggest de-platforming of all would come from getting the internet platforms themselves to use their power to stop the speech of white supremacists, neo-Nazis, birthers, “ironic racist” 4channers, and anti-Semites. And, indeed, a message projected onto Twitter headquarters this week called for just such a move:
In #Charlottesville, Nazis are marching, many with Twitter accounts. At #twitter HQ, I'm asking a simple question. pic.twitter.com/v3gkoaKU8B
As more and more of daily life online is consumed by the political storm in America, the “socially liberal, fiscally conservative” position that the tech companies have staked out is getting harder and harder to hold.


Suppose you were to click on this link: This one, right here.
It will take you to the website of Disrupt J20, which organized some of the “direct action” protests on the day of President Donald Trump’s inauguration in Washington, D.C. The site contains general information about civil disobedience and political protests, and it advertises several Washington-specific events.
Some of the protests on Inauguration Day turned violent, and the U.S. government has since charged more than 200 people with felony rioting or destruction of property in connection to events on January 20. It alleges that some of the suspects were connected to the Disrupt J20 effort.
Yet if you clicked that link above—even if you were nowhere near Washington on Inauguration Day—the government is now allegedly interested in you.
The U.S. Department of Justice is attempting to seize the visitor logs and IP addresses of anyone who has visited DisruptJ20.org, as well as any email addresses, user logs, and photos collected by the website, according to DreamHost, a Los Angeles–based web host and domain-name registrar.
This data encompasses more than 1.3 million IP addresses, as well as the email addresses and photos of thousands of people, the company said. DreamHost is not politically connected to DisruptJ20, but it provided paid web-hosting services for the group.
DreamHost has so far refused to comply with the government’s search warrant, arguing that it constitutes “investigatory overreach and a clear abuse of government authority.”
“That information could be used to identify any individuals who used this site to exercise and express political speech protected under the Constitution’s First Amendment. That should be enough to set alarm bells off in anyone’s mind,” said a blog post published to the company’s website on Monday.
A spokesperson for the U.S. Attorney’s Office for the District of Columbia did not respond before publication. A spokesman for the U.S. Department of Justice declined to comment.
Digital-privacy and civil-rights advocates were quick to criticize the scope of the government’s warrant. But experts in computer crime law said it wasn’t immediately obvious that the warrant was illegal.
“The Department of Justice isn’t just seeking communications by the defendants in its case. It’s seeking the records of every single contact with the site—the IP address and other details of every American opposed enough to Trump to visit the site and explore political activism,” wrote Ken White, a criminal-defense lawyer and former assistant U.S. attorney.
He continued:
The government has made no effort whatsoever to limit the warrant to actual evidence of any particular crime. If you visited the site, if you left a message, they want to know who and where you are—whether or not you did anything but watch TV on inauguration day. This is chilling, particularly when it comes from an administration that has expressed so much overt hostility to protesters, so relentlessly conflated all protesters with those who break the law, and so deliberately framed America as being at war with the administration’s domestic enemies.
“No plausible explanation exists for a search warrant of this breadth, other than to cast a digital dragnet as broadly as possible,” said Mark Rumold, a senior staff attorney at the Electronic Frontier Foundation, in a blog post. The EFF is assisting DreamHost in its opposition to the warrant.
“The Fourth Amendment was designed to prohibit fishing expeditions like this. Those concerns are especially relevant here, where [the Department of Justice] is investigating a website that served as a hub for the planning and exercise of First Amendment–protected activities,” he said.
In an email, Rumold added that the government had successfully seized visitor logs for other websites in the past. “But I’ve never seen anything on this scale, where we’re talking about millions of users and there’s no attempt whatsoever to narrow the scope (either by date, time, or user),” he told me.
“I don’t think there are precedents one way or another on this,” Orin Kerr, a law professor at George Washington University, told me.
“It’s not obvious to me whether the warrant is problematic,” he elaborated in an article at The Washington Post. The government’s search warrant instructs DreamHost to turn over all its records about DisruptJ20.org. As Kerr understands it, DreamHost wants the government to only legally be able to ask for certain records about the website. He continues:
There’s an interesting and unresolved issue presented here: What’s the correct level of particularity for a website? Courts have allowed the government to get a suspect’s entire email account, which the government can then search through for evidence. But is the collective set of records concerning a website itself so extensive that it goes beyond what the Fourth Amendment allows? In the physical world, the government can search only one apartment in an apartment building with a single warrant; it can’t search the entire apartment building. Are the collective records of a website more like an apartment building or a single apartment? I don’t know of any caselaw on this.
A hearing in D.C. Superior Court is scheduled for Friday.
President Trump has addressed the January 20 protests directly at least twice. Two days after they occurred, he belittled them and the Women’s March, on January 21, in a tweet: “Watched protests yesterday but was under the impression that we just had an election!” he said. “Why didn’t these people vote? Celebs hurt cause badly.”
Two hours later, he tweeted an addendum: “Peaceful protests are a hallmark of our democracy. Even if I don’t always agree, I recognize the rights of people to express their views.”


On August 21, the moon's orbit will bring it directly between the Earth and the sun, creating a total solar eclipse in the United States for the first time since 1979. One of the first towns perfectly positioned for the most dramatic view is Keizer, Oregon. Resident Matt Rasmussen is one of many people living along the eclipse’s “path of totality” looking to make the most of this once-in-a-lifetime opportunity—and not just to take in the sights.
Rasmussen said a friend living in nearby Portland, which will only see a relatively mundane partial eclipse, casually suggested he try to rent out his house for the weekend of the eclipse. “She said we should post on Airbnb because she bet we could get a mortgage payment out of it,” he says. “I laughed, and randomly set up our house at what we thought was a large amount, never expecting to have a taker. We were booked within two days.”
Rasmussen charged $2,000 for a single night, as much as 10 times the typical price, which he guesses would be around $200 or $300. Other Oregonians contacted through Airbnb’s messaging system for this article tell similar stories.
The eclipse has been a hot topic along its path for months, with preparations taking many forms. California is prepping its solar-heavy power grid to deal with the temporary drop in sunlight. School districts situated on the eclipse path in Illinois and Missouri are canceling class on August 21. Kentucky officials are stocking up on a drug to treat heroin overdoses, and NASA is readying a raft of science experiments. In Oregon, as with other states on the eclipse’s cross-country path, government officials and locals are preparing for the swarm of visitors to these otherwise quiet, out-of-the-way towns. Residents have been told to prepare for power outages, cell-tower failures, internet-service outages, and other headaches. Gridlock figures to be a problem on roads across Oregon, straining emergency services.
“We’re planning for a very busy few days,” Peter Murphy, a spokesperson for the Oregon Department of Transportation, said in an email. He said 23 two-person crews will be stationed every few miles along U.S. Route 97, ready to remove any vehicles causing delays. “A lot will depend upon weather that day. Clouds on the coast or Willamette Valley may send travelers our way, and that will be a challenge to manage. Our message is ‘Arrive early, stay put, and leave late.’”
Rasmussen's father-in-law works on the coast, where officials are also anticipating traffic will slow to a halt. “From what I hear about the mid-Oregon coast, 50 miles west of us, they are going to station ambulances with supplies every few blocks along U.S. 101,” Rasmussen says, because authorities anticipate the roads, campgrounds, and beaches will be so jammed that ambulances may have a hard time getting through.
With a forecast like that, it’s no wonder some people are heading out of town—and hoping to profit in the process.
Zachary Burns of Redmond, a town located about 100 miles farther inland than Keizer, recalls his telescope-hobbyist father first told him about the eclipse about a year ago. He has heard varying estimates for how many visitors are coming to his part of central Oregon, but even the most conservative guesses are in the hundreds of thousands.
“I originally had the thought to leave town and camp to avoid the crowds,” he says. “Then I realized every camping spot was full. Then I heard every hotel was full. This was months ago, almost six months prior to the eclipse. That’s when I realized it might be a good way to bring in a little extra income.”
Burns rented his home for the night before the eclipse for $1,200. He figures it would likely go for $300 a night on any other weekend. Four times the typical rate isn’t bad, but he says he may have actually underbid himself. “As the date draws closer and prices rise, I’ve realized I could have listed the house for possibly $2,000 a night,” he says.
Others are opening up their property as a campground for multiple visitors. Bethany Stelzer from the town of Kimberly, another 100 miles inland from Redmond, said her family is offering campsites on the grounds of her orchard for $1,500 a night—each.
“I’ve heard from several locals that they are renting their homes or fields as campsites,” she says. “It was my understanding that the Oregon state parks sold out all of their campsites in the path of the eclipse fairly quickly, so the community stepped up to offer more options.”
For his part, Burns says he ultimately decided against camping during the eclipse, instead opting to stay with his parents. He made the decision in part because anywhere he could hope to find will be teeming with people. Then there are safety considerations; he worries any natural disasters, like late-summer wildfires, could make traffic jams even more dangerous.
Extra money from a night’s rental or a guided tour isn’t life-changing, but any extra bit helps given central Oregon’s relatively high cost of living, residents say. The money is also a nice tradeoff for the disruption the eclipse—or, more accurately, all the people headed to see it—will bring.
Some residents of central Oregon are more optimistic about the small-town welcome visitors will receive. Shawn Stanfill lives in Madras, about 25 miles north of Redmond. The father of Madras police chief Tanner Stanfill, he says law enforcement have taken the necessary steps to be ready to help in any situation, and he insists residents of small towns are always willing to lend a hand. The elder Stanfill is listing a couple properties for $1,500 a night, though he cautions those looking to view the eclipse from a relatively remote location like Madras need to be prepared to spend an extra two days getting in and another two getting out, so extreme will be the glut of visitors.
Zachary Burns says his employer, a restaurant supply company, has been ordering supplies weeks in advance to ensure eateries have what they need—though no one really knows what to expect. “I’ve heard varying estimates of the number of people coming to the area, but even lower estimates seem to think in the hundreds of thousands,” he says. “Should be an adventure.”
While the eclipse has become a serious moneymaking opportunity for many Oregonians, as well as those in states eastward along the eclipse’s path, those contacted for this article say they still intend to make the most of the eclipse itself. One resident said he plans to hike up a mountain trail known only to locals, where he’ll be unlikely to run into any out-of-town sky-gazers. Considering all the commotion and chaos that the eclipse could bring, they’ve probably earned at least that small measure of solitude.


How should Americans fight against a resurgent white-nationalist movement in the United States? This weekend, they returned to an artifact from an earlier era of anti-Nazism. Tens of thousands of people rediscovered—and promptly shared and retweeted—a clip from Don’t Be a Sucker, a short propaganda film made by the U.S. War Department in 1943.
When it first debuted, Don’t Be a Sucker would have played in movie theaters. Now it has made its 21st-century premiere thanks to a network of smaller screens and the Internet Archive, where it is available in full. Almost 75 years after it was first shown, Don’t Be a Sucker lives again as a public object in a new and strange context.
1947 anti-fascist video made by US military to teach citizens how to avoid falling for people like Trump is relevant again. pic.twitter.com/vkTDD1Tplh
Its opening clip is a direct and plain-language parable in anti-fascism. It begins as a flushed man brandishes a pamphlet and addresses a crowd: “I see negroes holding jobs that belong to me and you. Now I ask you, if we allow this thing to go on, what’s going to happen to us real Americans?” He proceeds to blame blacks, Catholics, Freemasons, and immigrants for the nation’s ills.
“I’ve heard this kind of talk before, but I never expected to hear it in America,” says an older man with an Eastern European accent.
He introduces himself to a younger man next to him: “I was born in Hungary but now I am an American citizen. And I have seen what this kind of talk can do—I saw it in Berlin. I was a professor at the university. I heard the same words we have heard today.”
“But I was a fool then,” he continues. “I thought Nazis were crazy people, stupid fanatics. Unfortunately it was not so. They knew they were not strong enough to conquer a unified country, so they split Germany into small groups. They used prejudice as a practical weapon to cripple the nation.”
There ends the viral clip. But the original, 17-minute film Don’t Be a Sucker—which can be viewed in full below—continues, slipping into a short history of the rise of the Nazi Party in Germany. We see the movement evolve from an angry group of men in the streets to a party organization armed with an official state paramilitary. There’s a montage of Nazi crimes: A Jewish shop owner is carried away by police officers, a group of union members are attacked, and a college professor is arrested after telling his students that there is no scientific basis for the existence of a “master race.” (The version below is from the film’s 1947 rerelease.)*
Michael Oman-Reagan, an anthropologist and researcher in British Columbia, was the first to post the clip on Saturday evening, in a tweet comparing the orator’s rhetoric to President Donald Trump’s. His post has since been retweeted more than 85,000 times.
But he was not alone in linking the events in Charlottesville to the Second World War. Orrin Hatch, a Republican of Utah and the president pro tempore of the Senate, said in a tweet on Saturday: “We should call evil by its name. My brother didn’t give his life fighting Hitler for Nazi ideas to go unchallenged here at home.”
What makes the film so remarkable? It’s not as if Don’t Be a Sucker encapsulates some lost golden age of American anti-racism. Indeed the contradictions of the 1940s are inseparable from the film. In its opening montage, it shows a multiethnic group of kids—white, black, and East Asian—playing baseball. Yet in 1943, the same year it was released, the U.S. federal government kept more than 100,000 Americans imprisoned solely for the crime of being Japanese. And it was on its way to implementing one of the great anti-black wealth transfers of American history.
Still, Don’t Be a Sucker seems wise. It seems to know how democratic solidarity falters, how prejudice and factionalism can fracture a nation, and how all these forces might manifest in the United States of America. This wisdom may have emerged from simple practicality: Though the U.S. Army and Navy remained segregated for another five years, they were already vast and diverse enterprises by 1943. Simply put, different people had to work together to win the Second World War. The same was true of the whole country.
And in that, Don’t Be a Sucker may point to a deeper driver of the American experiment in multi-ethnic democracy. Building a diverse commonwealth has never been just an idealistic aspiration or moral avocation. It has been a requirement of the republic’s survival—the sole remedy to the cancer of white supremacy.
* This article has been updated to clarify that the clip is from the 1947 rerelease of the film.


Opening their paper on Friday morning, readers of The Wall Street Journal encountered a financial item of unusually wide interest. “Here’s a question that’s probably not on the CFA exam,” write Mike Bird and Riva Gold. “What happens to financial markets if two nuclear-armed nations go to war?”
What, indeed? We soon learn the consequences could be dire. Short-term interest rates would rise and long-term rates would fall. In a small skirmish between North Korea and the United States, the S&P 500 Index might post 20-percent losses “before it became clear that the United States would prevail.” But were another nuclear-armed power like Russia or China to get involved, the European Central Bank would have to take extreme action and issue “highly dovish forward guidance.”
Yet even amid this market turmoil, the savvy broker might still protect their investment. Sure, it’s true that the Japanese yen—a traditional safe haven—makes for a tricky bet when Tokyo is 800 miles downwind of Pyongyang. But there’s at least one good option left, according to analysts at the Nordea Group:
German bunds, the perennial refuge of panicked investors, would be good to own during a nuclear conflict too, with aggressive buying pushing the spread between German two- and 10-year bunds to 0.5 percentage point, from above one percentage point now.
At last, a good spread between German bonds. What a relief.
Nowhere does the story mention several other consequences of nuclear war: the urban firestorms; the plumes of sun-blotting black smoke; the crop die-offs across Asia, Africa, and North America; and the breakdown in the global communication network, whose destruction would render the German bund meaningless (no matter how favorable its yield curve). Nor did the story pause to note the millions of dead.
In the second week of August 2017, the American public began to do something that felt distinctly 20th-century: consider the consequences of a nuclear war. Two things became clear. First, nuclear anxiety had arrived again as a mass cultural force in American life—or, at least, in the accelerated internet-era version of it. Second, the public (and the American president) was obviously out of practice in thinking about it.
The episode began in earnest on Wednesday, when The Washington Post reported that at least one intelligence agency believed that North Korea could now miniaturize its nuclear weapons to fit into an intercontinental ballistic missile. If true, it represents an alarming technological breakthrough for the nation.
Then the president spoke. At an unrelated event at his private golf course in New Jersey, President Donald Trump warned of “fire and fury like the world has never seen” if North Korea continued to make threats against the United States. The next day—after aides tried to signal that his comments were improvised—he repeated them, saying maybe “fire and fury” was not “tough enough.”
Finally, on Friday morning, Trump tweeted that the U.S. military was “locked and loaded should North Korea act unwisely.”
Nuclear war—suddenly, everyone was talking about it, because the president was talking about it, in ways he isn’t supposed to.
Every late-night host riffed on the apocalypse. “Even Trump is scared by what he’s saying—look at him, he’s literally hugging himself,” quipped Seth Meyers, host of Late Night. (Trump gripped his torso as he uttered “fire and fury.”) A set of Democratic-connected advocacy groups, most of them not particularly radical, held an “emergency rally against nuclear war” at the White House.
And every professor or researcher of nuclear-weapons policy—normally confined to the dusty corners of university libraries and international security conferences—found themselves on a treadmill of radio and TV interviews. “[Nuclear weapons] are this kind of layer over the world, this abstract, intangible thing. We don’t talk or think about them,” says Lovely Umayam, who researches nuclear weapons at the Stimson Center, a think tank in Washington, D.C.
She said she felt glad there was renewed interest in the one technology that hangs over all U.S. international relations. But she also worried at how reactive the attention seemed. For the past week, she told me, she’s heard one constant question during TV and radio interviews: “Should we be concerned?”
“As an expert, I say, no, not quite,” she said. “We could really walk back on these words and develop de-escalation mechanisms. It’s horrible [Trump and Kim Jong Un] are talking this way, but it’s not the end of the world yet.”
“But then, as an anthropologist, I want to say: Yes, you should be concerned! You should always be concerned. And that you have to ask an expert that question—what does it say about your literacy of [nuclear] issues?” she said.
Kristyn Karl, a professor of political science at Stevens Institute of Technology, agreed that the public’s interest in nuclear weapons was way up—even if their understanding wasn’t. “The public is currently more aware of nuclear threats than they have been since the end of the Cold War,” she told me in an email.
That doesn’t mean they know much about them.
Americans flunk questions about basic nuclear security, Karl said, “such as identifying nuclear states, the scale of nuclear arsenals, etc.” Younger Americans also have little experience with nuclear weapons, especially compared with Baby Boomers.
Alex Wellerstein, a historian of nuclear weapons, also at the Stevens Institute, agreed that people seem more interested now. But he worries that they won’t stay that way once this crisis passes.
“It’s clear there is a sharp uptick of interest on nuclear questions,” he said in an email. “The question is, what kind of interest is it? Is it the kind of interest that will lead to a more sustained public interest on these topics? Or is it an ephemeral fear of the sort that comes and goes in a crisis?”
“American nuclear anxiety seems almost totally focused on foreign policy issues from small states—specifically Iran and North Korea. In that sense it is somewhat different than the period of the Cold War when the threat was much larger,” he said:
What I fear is that Americans will erroneously think that a war with either Iran or North Korea would be “no big deal” whereas we are (and were) much more aware that a war with Russia was totally unthinkable. War with Iran should be considered unthinkable (one need only look at what our war with Iraq has cost us, what monsters it created), and war with North Korea would come at a dearer cost than I think most people appreciate.
But when it comes to the prospect of nuclear annihilation, what is unthinkable and what isn’t? Americans are finding themselves back in the uneasy practice of imagining not the end of the world, but all the intermediate steps between now and then—the first warnings on the news, the orange streaks in the sky, the agony of waiting for ignition.
Writing three decades ago, the essayist and physician Lewis Thomas imagined a war with Russia and fell into despair. “My mind swarms with images of a world in which the thermonuclear bombs have begun to explode, in New York and San Francisco, in Moscow and Leningrad, in Paris, in Paris, in Paris. In Oxford and Cambridge, in Edinburgh,” he wrote:
This is a bad enough thing for the people in my generation. We can put up with it, I suppose, since we must.
What I cannot imagine, what I cannot put up with ... is what it would be like to be young. How do the young stand it? How can they keep their sanity? If I were very young, 16 or 17 years old, I think I would begin, perhaps very slowly and imperceptibly, to go crazy.
For today’s young people, looking to an uncertain future, at least there are German bonds to buy.


When CEO Sundar Pichai addressed a controversial memo about diversity that circulated inside Google, culminating in the termination of its author, James Damore, he began by telling the company’s 72,053 employees that “we strongly support the right of Googlers to express themselves, and much of what was in that memo is fair to debate, regardless of whether a vast majority of Googlers disagree with it.”
“However,” he added, “portions of the memo violate our Code of Conduct and cross the line by advancing harmful gender stereotypes in our workplace. Our job is to build great products for users that make a difference in their lives. To suggest a group of our colleagues have traits that make them less biologically suited to that work is offensive and not okay. It is contrary to our basic values and our Code of Conduct.”
I have a question for the CEO.
Given that the full text of the memo is public, that it is the subject of a national debate on an important subject, that many educated people disagree with one another about what claims it made, and that clarity can only help Google employees adhere to the company’s rules going forward, would you be willing to highlight the memo using green to indicate the “much” that you identified as “fair to debate” and red to flag the “portions” that you deemed Code-of-Conduct violations?
Absent that, it seems to me that Google employees will remain as uncertain as ever about what they can and cannot say at the company. As an illustration, consider Alan Jacobs, an English professor at Baylor University who declares himself confused about your meaning:
Google’s position could be:
I think those are the chief options.
Actually, I can think of still more options—especially if only tiny “portions” of the memo crossed Google’s line—which only underscores the dearth of clarity available to your employees. As a general matter, for example, I wonder if you believe the truth of a proposition is relevant to whether it violates the Code of Conduct. Can something be both the scientific consensus on a subject and unmentionable?
Jacobs adds, “I seriously doubt that Google will get much more specific. Their goal will be to create a climate of maximal fear-of-offending, and that is best done by never allowing employees to know where the uncrossable lines are. That is, after all, corporate SOP.” I’d guess legal incentives are a more powerful motivator of strategic vagueness. Are we being too cynical? Over the course of its history Google has often struck me as a unique company. And surely elevating clarity here would fulfill the mission of making all pertinent information universally accessible and useful.


DNA is fundamentally a way of storing information. Usually, it encodes instructions for making living things—but it can be conscripted for other purposes. Scientists have used DNA to store books, recordings, GIFs, and even an Amazon gift card. And now, for the first time, researchers from the University of Washington have managed to take over a computer by encoding a malicious program in DNA.
Strands of DNA are made from four building blocks, represented by the letters A, C, G, and T. These letters can be used to represent the 1s and 0s of computer programs. That’s what the Washington team did—they converted a piece of malware into physical DNA strands. When those strands were sequenced, the malware launched and compromised the computer that was analyzing the sequences, allowing the team to take control of it.
“The present-day threat is very small, and people don’t need to lose sleep immediately,” says Tadayoshi Kohno, a computer security expert who led the team. “But we wanted to know what was possible and what the issues are down the line.” The consequences of such attacks will become more severe as sequencing becomes more commonplace. In the early 2000s, it cost around $100 million to sequence a single human genome. Now, you can do it for less than $1,000. The technology is not just cheaper, but also simpler and more portable. There are even pocket-sized sequencers that allow people to analyze DNA in space stations, classrooms, and jungle camps.
But with great ubiquity comes great vulnerability. DNA is commonly used in forensics, so if troublemakers could hack sequencing machines or software, they could change the course of an investigation by altering genetic data. Or, if machines are processing confidential data about genetically modified organisms, hackers could steal intellectual property.
There’s also the matter of personal genetic data. The United States is currently trying to sequence the DNA of at least 1 million Americans to pave the way for precision medicine, where treatments are tailored to an individual’s genes. “That data is very sensitive,” says Peter Ney, a student in Kohno’s lab. “If you can compromise [the sequencing pipeline], you could steal that data, or manipulate it to make it seem like people have genetic diseases they don’t have.”
“We want to understand and anticipate what the hot new technologies will be over the next 10 to 15 years, to stay one step ahead of the bad guys,” says Kohno. In 2008, his team showed that they could wirelessly hack their way into a heart implant, and reprogram it to either shut down or deliver debilitating jolts. In 2010, they showed that they could hack into the control system of a Chevrolet Impala, taking control of the car. Then, they turned their attention to DNA sequencing. “It’s an emerging field that other security researchers haven’t looked at, so the intrigue was there,” says Kohno. “Could we compromise a computer system with DNA biomolecules?”
They could, but reassuringly, it wasn’t easy. To make their malware work, the team introduced a vulnerability into a program that’s commonly used to analyze DNA data files. They then exploited that weakness. That’s a bit of a cheat, but the team also showed that such vulnerabilities are common in software for analyzing DNA. The people who created these programs didn’t really have hacking in mind, and so their products tend to be insecure, and rarely follow best practices for digital security. With the right molecular malware, it could be possible for adversaries to compromise these programs and the computers that run them.
“I liked the creativity a lot, but their exploit is unrealistic,” says Yaniv Erlich, a geneticist at Columbia University and the New York Genome Center. (Earlier this year, Erlich encoded a computer virus in DNA, but he didn’t code it so that it would launch on its own when the DNA was sequenced.) In practice, the team’s malware would create a glitch that most sequencing centers would spot and fix. An adversary could only assume control of a compromised computer if they had impeccable timing, and struck immediately after the strands were sequenced.
Still, Erlich agrees that programs for analyzing DNA have “relatively relaxed security standards.” There are rumors, he says, that one big research institution was hit by ransomware, because they used the default admin passwords on their sequencing machines.
“My hope is that over the next 5 to 10 years, people take a strong interest in DNA security, and proactively harden their systems against adversarial threats,” says Kohno. “We don’t know of such threats arising yet and we hope that they’ll never manifest.”


Until the 20th century, only the wealthy or dying might have witnessed someone trying to cool the air indoors—even though building a fire to keep warm in the winter would have been perfectly reasonable. Extreme heat was seen as a force that humans shouldn’t tamper with, and the idea that a machine could control the weather was deemed sinful. Even into the early 1900s, the U.S. Congress avoided the use of manufactured air in the Capitol, afraid voters would mock them for not being able to sweat like everyone else.
While adoption of air-conditioning demanded industrial ingenuity, it also required renouncing the vice of cooling the inside air. But in the process of shedding its hypothetical moral slight against the heavens, the air conditioner has perpetrated worse, actual sins against the Earth.
* * *
Despite the shadow of immorality, breakthroughs in air-conditioning developed out of desperation. Doctors scrambling to heal the sick took particular interest. In 1851, a Florida doctor named John Gorrie received a patent for the first ice machine. According to Salvatore Basile, the author of Cool: How Air-Conditioning Changed Everything, Gorrie hadn’t initially sought to invent such an apparatus. He’d been trying to alleviate high fevers in malaria patients with cooled air. To this end, he designed an engine that could pull in air, compress it, then run it through pipes, allowing the air to cool as it expanded.
Outside of his office though, people saw no practical need for this achievement. It wasn’t until the pipes on Gorrie’s machine unexpectedly froze and began to develop ice that he found a new opportunity. Still, this accomplishment was lampooned as sacrilege in The New York Globe: “There is Dr. Gorrie, a crank ... that thinks he can make ice by his machine as good as God Almighty.”
The use of ice and snow to chill drinks or to help cool a room was nothing new. In the 17th century, the inventor Cornelius Drebbel used snow that had been stored underground during the summer to perform an act he called “turning summer into winter.” In his book Absolute Zero and the Conquest of Cold, Tom Shachtman speculates that Drebbel achieved his effect by mixing snow with water, salt, and potassium nitrate, which formed ice crystals and significantly cooled the space. King James, who invited Drebbel to demonstrate his innovation, reportedly ran from the demonstration in Westminster Abbey, shivering.
Ice would be used two centuries later to cool another man in power, U.S. President James A. Garfield. On July 2, 1881, Charles Guiteau fired two shots from his revolver into Garfield’s back. The aftermath inspired naval engineers to develop a method to keep a president cool, as he slowly died that summer.
The astronomer Simon Newcomb oversaw development of the apparatus that struggled to chill Garfield’s sickroom. Newcomb rigged together an engine connected to pipes that powered a fan to blow over a giant bucket of ice. In written reports, Newcomb explained that his apparatus held “some six tons [of ice] in all, through which the air might pass in one direction and return in the other.” The device lowered the room’s temperature from 95 to 75 degrees—and ate up hundreds of pounds of ice an hour.
As news of Newcomb’s machine slowly grabbed the public interest, distrust of cooling the air began to wane. Inventors developed fanciful schemes to beat the heat. One believed he could take a balloon connected to a fire hydrant and a hose and create personal rainstorms. Another came up with the idea of towers with carbon dioxide bombs at the top that would explode above a neighborhood and cool the air upon detonation. Some of these curiosities managed to win patents, but few proved useful in practice.
* * *
Two decades after Garfield’s death, Willis Carrier coined the term “air-conditioning.” Although it wasn’t an overnight sensation, Carrier’s breakthrough came in July 1902, when he designed his Apparatus for Treating Air, first installed in the Sackett Williams Publishing building in Brooklyn, New York. The device blew air over tubes containing a coolant. Its purpose was to reduce humidity more than to reduce air temperature; excess water in the air warped the publishing house’s paper.
In 1899, Alfred R. Wolff had preceded Carrier with an air-cooling device, installed in the dissecting room of Cornell Medical College in New York City. Later, the same year Carrier installed his first apparatus in Brooklyn, Wolff placed his machine at the New York Stock Exchange. Instead of keeping cadavers fresh for study, it brought comfort to the horde of men at work.
The technology began to spread. Frigidaire sold the first “room cooler” for the home in 1929. H.H. Schultz and J.Q. Sherman marketed an air conditioner that leaned against the windowsill, but the first window-mounted unit, as we know it today, was the 1932 Thorne Room Air Conditioner. It looked like the grill of an old car shoved through a window. In her book Cool Comfort: America’s Romance with Air-Conditioning, Marsha Ackermann recounts a radio interview in which Carrier announced his vision. He imagined a world in which “the average businessman will rise, pleasantly refreshed, having slept in an air-conditioned room. He will travel in an air-conditioned train, and toil in an air-conditioned office.”
Air-conditioning’s major public debut was at the 1939 World’s Fair. Carrier hosted the Carrier Igloo of Tomorrow expo, where 65,000 visitors would experience air-conditioning for the first time, boosting consumer interest. Over the next decade, as the air conditioner shrank in size, advertisements for the machine shifted their appeals from men in the workplace to women at home. In some early ads the air conditioner sits in the window among a proud family admiring their machine like a spacecraft that had landed in the living room.
Basile points out another, less obvious move that increased the device’s popularity: In 1959, the U.S. Weather Bureau created its “discomfort index”—we know it today as the heat index, a measure of temperature and humidity combined. The discomfort index gave an unexpected boost to air-conditioning by, as Basile says in his book, putting “people in mind of cooled air.” Now the public could gauge if it was too hot to go outside. If they could afford it, there were plenty of air-conditioner manufacturers offering solace from the weather.
By the 1960s, millions of air conditioners were being sold every year in the United States. Windows across cities and suburbs were being plugged with the machines. As of 2011, the Energy Information Administration’s Residential Energy Consumption Survey says that 87 percent of households in the United States have an air conditioner or central air. That’s compared to 11 percent in Brazil and only 2 percent in India.
* * *
While the public’s reluctance to air-conditioning might have hampered the initial development of air-conditioning technologies, its eventual popularity has proved detrimental to the Earth’s atmosphere.
By 1989, the Montreal Protocol was enacted in an effort to cut the release of chlorofluorocarbons, or CFCs, into the atmosphere. Freon, a CFC used in the early A/Cs, was among the features of older air-conditioning units that contributed to ozone depletion.
Even though refrigerants have been modified to use fluorine instead of chlorine, and thereby to avoid impacting ozone, air-conditioning still exerts enormous environmental impact. According to Daniel Morrison, the acting deputy director of communications at the U.S. Department of Energy, residential and commercial buildings used more than 500 billion kilowatt-hours of electricity for air-conditioning in 2015 alone. That’s almost 20 percent of the total electricity used in buildings, amounting to $60 billion in electricity costs annually. Air-conditioning is also one of the main contributors to peak electric power demand, one symptom of which is rolling summer blackouts.
Of all the devices people use today, the air conditioner has not experienced major design makeovers like the television or the telephone. But there are companies trying to revolutionize the future of air-conditioning—both in aesthetics and efficiency. Some of these efforts rehearse earlier qualms about the unseemliness of interior cooling, making air-conditioning more personal. CoolWare, for example, makes an A/C collar, which wraps around the neck and delivers water-cooled air via small fans. Wristify offers a similar product as a bracelet. Kuchofuku makes an air-conditioned work shirt of a similar design.
A Cyprus-based company called Evapolar has introduced what it calls “the world’s first personal air cooler.” It’s a small cube with a water reservoir and a fan that creates a breeze and purifies the air. Evapolar promotes the idea of a “microclimate” designed to cool a single person’s work or sleep space, and thereby to avoid wasting energy by cooling entire rooms or buildings. “Just as our phones became personalized, we believe that the climate device should also become personalized,” Evapolar spokesperson Ksenia Shults tells me.
Dyson and Xiaomi are also introducing small, personalized air purifiers into the market. All these devices remain niche (and fairly uncool, as it were), but stranger things have become mainstream.
Even today, air-conditioning remains controversial. Due to their environmental impact, some advocates call for disuse of these machines. Others accuse the air conditioner of chauvinism, forcing women in the workplace to dress one way inside and another outside. It has become both a symbol of human ingenuity and of weakness, acclimatizing human bodies so that they are less resilient against natural heat without the aid of machines.
More than just an appliance, the air conditioner is a memento mori. It was a device people invented to avoid a few individual deaths, and yet one whose adoption might have a role to play in the passing of a temperate climate for everyone. As summer proceeds, listen to the chorus of machines humming in the windows, outside the houses, atop the office buildings. They offer a reminder that humanity’s ingenuity can come at a cost. Maybe our forebears weren’t entirely wrong to see peril in the act of cooling the air.
This article appears courtesy of Object Lessons.


Popular imagery of the atom bomb is oddly sterile.
For all we know of the horrors of nuclear weapons, the visual that’s most often evoked is ethereal, if ominous: a silent, billowing cloud, aloft in black and white.
The reasons for this are understandable. Nuclear weapons have been tested far more often than they’ve been used against people. And the only two times they were used in warfare—in Hiroshima, then Nagasaki, 72 years ago—photographers captured many scenes of devastation, yet video recording was scant.
The People Who Built the Atomic Bomb
Survivors of the bombings have shared what they saw and heard before the terror. John Hersey’s famous report, published in 1946 by The New Yorker, describes a “noiseless flash.” Blinding light and intense pressure, yes, but sound? “Almost no one in Hiroshima recalls hearing any noise of the bomb,” Hersey wrote at the time. There was one person,  a fisherman in his sampan on the Inland Sea at the time of the bombing, who “saw the flash and heard a tremendous explosion,” Hersey said. The fisherman was some 20 miles outside of Hiroshima, but “the thunder was greater than when the B-29s hit Iwakuni, only five miles away.”
There is at least some testing footage from the era that features sound. It is jarring to hear. The boom is more like a shotgun than a thunderclap, and it’s followed by a sustained roar. Here’s one example, from a March 1953 test at Yucca Flat, the nuclear test site in the Nevada desert.

The National Archives description of the footage is matter-of-fact—which is the purpose of archival descriptions, but which seems strangely detached, considering: There’s the mountain ridge in early morning. An atom bomb is exploded. Burning. Pan of the mushroom against darkened sky. The cloud dissipates as the sky lightens. A yucca plant and Joshua trees in foreground. Hiller-Copters buzz in. And, finally, General John R. Hodge standing at a microphone, blinking into the morning sun.
“This test, I think, went very well,” he said. “I was quite interested in how the troops reacted. I didn’t find any soldier there who was afraid.”
“They took it in stride,” he added “as American soldiers take all things.”


A federal court has unsealed new documents in the case against an Israeli teenager, Michael Kadar, who has been accused of making at least 245 threatening calls to Jewish Community Centers and schools around the United States. According to the documents, Kadar advertised a “School Email Bomb Threat Service” on AlphaBay, an online marketplace for illicit goods and services that was shut down by the federal government in July. Authorities have identified an individual in California who allegedly ordered and paid for at least some of Kadar’s threats.
A newly unsealed search warrant alleges that Kadar charged $30 for an email bomb threat to a school, plus a $15 surcharge if the buyer wanted to frame someone for it. “There is no guarantee that the police will question or arrest the framed person,” Kadar allegedly wrote in his ad.
I just add the persons name to the email. In addition my experience of doing bomb threats putting someones name in the emailed threat will reduce the chance of the threat being successful. But it’s up to you if you would like me to frame someone.
Kadar charged double for a threatening email to a school district or multiple schools, but districts with more than 12 schools required a “custom listing.” He noted that he was available “almost 24/7 to make emails,” and he promised to refund non-successful threats.
An Israeli American Teen Has Been Arrested in the JCC Bomb Threats Case
Kadar got good reviews. One AlphaBay user wrote that the threats were “Amazing on time and on target. We got evacuated and got the day cut short.” Based on the date when the comment was posted, it appeared to refer to a threat made to Rancho Cotate High School in Rohnert Park, California, north of San Francisco.
The Justice Department seized AlphaBay in late July—Attorney General Jeff Sessions called it “the largest dark net marketplace in history.” The documents in the Kadar case suggest that authorities had been tracking AlphaBay for a while: The search-warrant application alludes to screenshots of Kadar’s activity on the marketplace taken in mid-March.
It’s possible that the information discovered in the Kadar case contributed to the AlphaBay investigation. The Kadar documents were unsealed on July 19, the day before the Justice Department announced that AlphaBay had been shut down. Previously, the search warrant had been sealed because it was “relevant to an ongoing investigation into the criminal organizations as not all of the targets of this investigation will be searched at this time.” The search warrant and related legal documents were unsealed because the FBI and local authorities in California may need them to pursue criminal charges against the suspected buyer or buyers, or they may eventually be producible in the discovery phase of a criminal proceeding. The filings were first publicly flagged by Seamus Hughes, the deputy director of the Program on Extremism at George Washington University.
When Kadar was arrested in late March, members of the Jewish Community were shocked that an Israeli teenager appeared to responsible for many of the bomb threats that had forced Jewish Community Centers and schools to repeatedly evacuate their buildings last winter. Authorities arrested another suspect, Juan Thompson, in connection with some of the threats, but he appeared to make only a handful of the calls and was allegedly attempting to get revenge on an ex-girlfriend. The new documents suggest that even more people may have been involved as buyers—but how many, who, and why they did it are all not yet clear, and the document does not specifically state that any of the threats to Jewish institutions were issued at the behest of clients. So far, the investigation has led to a surprising pair of suspects. It’s not clear what kind of person will emerge as a suspect next.


As Uber battles taxis and other ride-hailing apps in cities across the world, the company is beginning to move quickly into a much larger transportation market: trucking.
This spring, Uber unveiled Uber Freight, a brokerage service connecting shippers and truckers through a new app. Conceptually, “Uber for trucking” seems like a logical extension of the passenger transport business.
But the logistics industry has totally different dynamics. For one, it’s business to business. Most truckers are owner-operators or they’re part of very small companies with a handful of vehicles. The industry has well-established ways of doing things. Truckers basically work in the places where Uber’s ride-hailing service doesn’t. And unlike Uber’s ride-hailing service, the company can’t bring a huge new supply of drivers onto the market to change the dynamics of transportation. As it is, there are somewhere north of 3 million truck drivers in America, between long-haul and delivery.
A Burdensome Regulation Screening Truck Drivers for a Sleep Disorder
Uber Freight was born out of the marriage of an internal team with members of Otto, after Uber acquired the latter company early last year. Since then, the teams have split up into self-driving research and development, managed by Alden Woodrow, formerly of Google X, and the Uber Freight team. Freight has a floor of one of Uber’s offices in downtown San Francisco and a large operations team in Chicago.
Uber has had a brutal last year. The company's culture has been critiqued from the inside and outside as sexist and fratty. The problems led to the ouster of a series of top executives, including founder Travis Kalanick. Even in trucking, Uber's acquisition of Otto has led to a lawsuit filed by Alphabet's self-driving car division, Waymo, related to the alleged theft of sensor technology. One Uber employee I know recently joked, "Uber's become a four-letter word."
I visited the company’s San Francisco office with Uber Freight’s product lead, Eric Berdinis. He’d come to Uber via Otto after a stint at Motorola working on the Moto 360 smartwatch, among other things. He graduated from the University of Pennsylvania in 2013, which makes him roughly 27 years old.
We walked the floor that is Berdinis’s domain. The engineering team is on the west side of the building, ops on the east. In the ops room, heat maps of America glowed on mounted televisions, showing where Uber is doing the most business. Texas was hot. This is certainly one of the places where software is nibbling away at the world.
Then we tucked into a conference room for an extensive interview. We talked through how to actually build “Uber for trucking,” what really hurts truckers, whether Otto oversold the speed at which self-driving trucks would arrive, and what drivers think of Travis Kalanick.
Alexis Madrigal: Let’s talk about Uber Freight and self-driving trucks. When Uber started, self-driving cars were pretty far away. When Uber Freight starts, perhaps self-driving trucks are not that far away. How much do you think self-driving trucks would change the economics for you guys?
Eric Berdinis: In my time at Otto, we did spend a decent amount of time thinking about the economics of trucking once it happens, even if it is a decade out. Now that I have been spending more time on the freight side, I haven’t been as close to that. But the teams are in communication about how these things might work together at some point.
Madrigal: And what is the relationship between the self-driving and Freight teams now?
Berdinis: They were born from a similar origin story. At least, I came from that team. The day-to-day workings are pretty separate. They are going down the path of finding their first customers and we’re scaling up the business and building the network. We’re in sync on what’s happening, but no active workstreams together like that.
Madrigal: Are you hearing from drivers that they are worried about it?
Berdinis: You see that come up every once in a while.
Madrigal: I know this isn’t what you’re doing on a day-to-day basis anymore, but how could you see the automation playing out?
Berdinis: I’ll first start by saying that one of the last things I worked on on the Otto side was the Otto-Budweister partnership and the video and the whole thing around that. Once I joined Uber Freight full time, I was thinking to myself, “We really made it seem like this thing was coming sooner than it is. We probably scared a lot of people. We kind of hyped this thing up.”
And it is showing what the future will be like. But it won’t be coming as fast as the video made it seem. The reality is that the transition to any kind of self-driving truck future is quite a ways away.
But in terms of how we think about that future, we actually do see a future where jobs don’t get impacted in the way that people expect them to. We wouldn’t be doing Uber Freight, which is a human-driven product, if we didn’t think that there was a responsible way for the future to look with humans and self-driving trucks.
Madrigal: Can you describe the future you see where there are autonomous trucks but jobs are not negatively impacted?
Berdinis: The answers aren’t perfectly clear yet, but the way that we’re building out this product is heading toward a direction that is the most driver-friendly possible. Once we have a more defined plan for how self-driving trucks and Uber Freight could work together, the specific will be more clear. There are lots of path that that could happen. Nothing to go into detail on now.
Madrigal: Has the recent trouble at Uber affected you all more or less than the standard employee at the company?
Berdinis: Uber Freight, because it has been incubated from the beginning with the Otto acquisition, we’ve always had really strong leadership internally. So, there has not been a huge impact from any of the searches for COO or CEO. The board is extremely excited about freight. They love having Uber with a diverse set of business opportunities. It hasn’t affected shippers. It hasn’t affected drivers. If you asked a driver, “Did you hear about Travis Kalanick?” They’d be like, “What are you talking about?”
Madrigal: But you did have a big departure from Otto in [founder Anthony] Levandowski. And there’s the Waymo lawsuit. Does that affect you guys on the Uber Freight ops side?
Berdinis: It really doesn’t. Because there are no self-driving components to Uber Freight. We definitely get questions like you’re asking me now. But it’s not like our technologies have anything to do with self-driving.
Madrigal: How did Uber Freight get started?
Berdinis: Curtis Chambers, who I think was the #7 employee at Uber, was tasked with exploring new opportunities in transportation. He was there for the start of uberX. He started UberEATS. Then, around the time that Otto was started, which was January/February of last year, Curtis was off with a few salespeople and engineers talking to trucking companies and starting to figure out if Uber should get into trucking. With the Otto acquisition, that solidified. The team we had created and the team Curtis had created—3 or 4 people on each side—we said, okay, let’s build out Uber Freight.
Madrigal: And the model you settled on is that Uber Freight essentially works as a broker between people with stuff to ship and truckers?
Berdinis: There is a defined model for how you build a company in the brokerage industry, which is the middle man between shippers and carriers. There have been a lot of brokers that have come along since the 1980s, when brokers became a formal thing.
Madrigal: Because of deregulation.
Berdinis: Right. So, there was a playbook for that. But it was completely unknown for how we do this in a tech-forward way that doesn’t totally follow the normal step-by-step that a brokerage would go after.
Madrigal: Which would just be lining up both sides of the marketplace, getting loads and getting trucks.
Berdinis: You make a promise to a shipper and hustle to find a driver and then, boom, that’s your first load. You just keep doing that at scale. It’s very easy to do it manually because you’re just calling and negotiating. You can muscle through that. But how do you get drivers to use an app or embrace a new way of doing things, especially when: 1) These drivers don’t really use technology in their day-to-day lives, and 2) when we’re really small, they log in and there are like five loads. That’s not a very useful product. So how do we get past the chicken-and-egg problem to the point where we are today when drivers come back every single day. And some of them are 100 percent on Uber Freight, like they completely transitioned their business.
Madrigal: How did you do that?
Berdinis: We didn’t actually put the apps out into the stores, the point where you can log in and book a load. That wasn’t until February for Android and March for iOS. So between September when we moved the first load until February, it was a lot of manual work, old-school hustle, get the loads, get the drivers.
Madrigal: Did you hire people from the other brokers?
Berdinis: Yeah, for sure. Uber has a very specific kind of ops executor. The Uber-style ops executor is very analytical, lot of them from finance backgrounds. They can work very hard and think through problems in a very analytical, data-driven way. And then there is the brokerage-style ops person, who is much more on the execution side. They know the industry really well. They can hear in the driver’s tone of voice if they are lying about a flat tire or just delayed from their previous shipment. All that kind of stuff. So, marrying the two kinds of operators together helped us build that ops team.
Madrigal: You guys decided to regionally build out. So the first market is ... Dallas?
Berdinis: We call it the Golden Triangle: Dallas, Houston, San Antonio. There are pretty even flows of freight in and out of each of those cities. So if we can capture that triangle, as soon as you drop off a load in Dallas, you can pick up a load to either Houston or San Antonio. There are other kinds of natural triangles around the country, but just within that triangle area, that makes up about 10 percent of the country’s freight.
Madrigal: Relative to other brokerages, you’re better capitalized and possibly better organized, and you don’t have to make money right away. There are a lot of advantages you guys have in going into a market like that. But what were the hard things about it?
Berdinis: When we we were starting up, before we publicly launched, most of the drivers we had talked to had never heard of Uber. They operate between cities and between cities, Uber doesn’t exist. So, it’s not top of mind. Once we did launch publicly, we started to see the camaraderie with their taxi friends. But you also hear other drivers coming and pushing back against them, saying, “With taxis, they created new supply. And that’s why there is competition. With trucking, Uber Freight is not creating new truck drivers.” We’re actually just giving loads in a more efficient way. We’re paying quickly. Over the last nine months, we’ve gotten pretty deep in the crazy pain points that drivers have and are going one by one to knock them off.
Madrigal: What are those?
Berdinis: It all comes back to earnings at the end of the day.
Madrigal: Because they are small business owners.
Berdinis: And drivers get paid from shippers, net 30– and net 60–day terms. [Meaning, the people shipping stuff have 30 or 60 days from the work being completed to pay the truckers.] If their truck breaks down, they struggle with the 60-day terms because they are working week to week. As a result, there is this huge industry called “factoring,” it’s kind of like payday loans. The trucker says, “Give me 95 percent of this receivable but today, versus making me wait 60 days.” That’s just 2–5 percent skimmed off of every single load. And when these drivers are only making a few percent profit margin, that could be all the profit they are making. The whole payment process does not work and it is causing a lot of trucking companies to go out of business.
Madrigal: What else has surprised you in making this foray into logistics?
Berdinis: I’ll start with the app itself. There was lots of apprehension at the beginning when we started calling drivers. A, they’d never heard of Uber. So the sell was hard. And B, a lot of them had never downloaded an app. They might have an iPhone, but we’d say, “Go to the App Store.” And they’d say, “What is that?” It was 45 minutes per driver walking them through the download process and password. We started to think that if we had to do this for tens of thousands of drivers, we could never scale.
But as time went on, drivers started showing up to us, instead of us going to them, it started self-selecting for drivers who know how to use apps and get it. The usage of the app was far exceeding what our expectations were.
We are seeing that not only are the ones who booked loads with us booking more loads every single week that they come back. But drivers who never booked a load with us, continued opening up the app almost every single day to check for new opportunities. We saw this crazy engagement. There’s not a lot of ways for drivers to see what loads are available out there. And just having the list and the price—that visibility in and of itself—is a huge mental shift.
Madrigal: It’s almost like the early stories around cell phones and farmers in whatever country being able to check the prices at market.
Berdinis: This is like that for a lot of these truck drivers. We were super skeptical that the divers would know how to use the app. But whether it is self-selection or whatever, we found this incredible affinity to come back and check more. We were pleasantly surprised by that.
Madrigal: Where does all this go from here? What are the next steps?
Berdinis: Texas was our original focus and yesterday, we announced six new states or regions we’ll do our same kind of density play in. That’s gonna help us understand if we can replicate the success we’ve seen in Texas in these other markets.
Madrigal: Do you have a GM for those markets, the way Uber’s passenger business would?
Berdinis: We don’t have a GM for those markets. It is all centralized from the ops team. When Uber launches new cities, they have a GM. They have a pretty standard process. For Freight, it’s not exactly like that. Because freight moves between cities. And the lines are not as clean as “Here’s Los Angeles that’s launched.”
Madrigal: Okay, that’s one shift. What’s the other big thing?
Berdinis: Up until now, the way drivers interacted with the app, they have to go into the app and search for what they want. We’ve learned that drivers have pretty specific preferences. Uber drivers don’t really have preferences. Maybe they can use this tool that helps them drive home at the end of the day. But during the day, the whole city is where they are working. With truck drivers, you can’t tell a local driver, someone driving within 100 miles of their home, to go take a load from Houston to New York. There is no point to surfacing that. So, now the app is a lot more proactive about personalizing that search-and-discovery experience.
So, we’re sending out push notifications. Hey, this load is one you’ve taken before. It just showed up on our system. Do you wanna book it? And then when they get into an app, there is a whole For You section. It’ll say: Recommended Because The Load Will Take You Home. Recommended Because You’ve Done This Load Before.
Madrigal: Netflix-style.
Berdinis: Right.
Madrigal: Are there other companies that want to digitize the brokerage business?
Berdinis: There’s a few “Uber for trucking companies.” They don’t call themselves that anymore, but they used to. There’s Transfix out of New York, Convoy out of Seattle. And if you search “Uber for trucking” you’ll see dozens that came and went.


An anonymous Google software engineer’s 10-page fulmination against workplace diversity was leaked from internal company communications systems, including an internal version of Google+, the company’s social network, and another service that Gizmodo, which published the full memo, called an “internal meme network.”
“I’m simply stating that the distribution of preferences and abilities of men and women differ in part due to biological causes,” the Googler writes, “and that these differences may explain why we don’t see equal representation of women in tech and leadership.”
Why Is Silicon Valley So Awful to Women?
The memo has drawn rage and dismay since its appearance Saturday, when it was first reported by Motherboard. It seemed to dash hopes that much progress has been made in unraveling the systemic conditions that produce and perpetuate inequity in the technology industry. That includes increasing the distribution of women and minorities in technical jobs, equalizing pay, breaking the glass ceiling, and improving the quality of life in workplaces that sometimes resemble frat houses more than businesses.
These reactions to the screed are sound, but they risk missing a larger problem: The kind of computing systems that get made and used by people outside the industry, and with serious consequences, are a direct byproduct of the gross machismo of computing writ large. More women and minorities are needed in computing because the world would be better for their contributions—and because it might be much worse without them.
* * *
Workplace equity has become a more visible issue in general, but it has reached fever pitch in the technology sector, especially with respect to women. When the former Uber engineer Susan Fowler published an explosive accusation of sexism at that company earlier this year, people took notice. When combined with a series of other scandals, not to mention with Uber’s longstanding, dubious behavior toward drivers and municipalities, the company was forced to act. CEO Travis Kalanick was ousted (although he remains on the board, where he retains substantial control).
Given the context, it’s reasonable to sneer at the anonymous Googler’s simple grievances against workplace diversity. Supposedly natural differences between men and women make them suited for different kinds of work, he argues. Failure to accept this condition casts the result as inequality, he contends, and then as oppression. Seeking to correct for it amounts to discrimination. Rejecting these premises constitutes bias, or stymies open discourse. The Googler does not reject the idea of increasing diversity in some way. However, he laments what he considers discriminatory practices instituted to accomplish those goals, among them hiring methods designed to increase the diversity of candidate pools and training or mentoring efforts meant to better support underrepresented groups.
Efforts like these are necessary in the first place because diversity is so bad in the technology industry to begin with. Google publishes a diversity report, which reveals that the company’s workforce is currently composed of 31 percent women, with 20 percent working in technical fields. Those numbers are roughly on par with the tech sector as a whole, where about a quarter of workers are women.
Racial and ethnic diversity are even worse—and so invisible that they barely register as a problem for the anonymous Googler. I was chatting about the memo with my Georgia Tech colleague Charles Isbell, who is the executive associate dean of the College of Computing and the only black tenure-track faculty member among more than 80 in this top 10–ranking program.
“Nothing about why black and Hispanic men aren’t software engineers?” he asked me after reading the letter, paraphrasing another black computer scientist, Duke’s Jeffrey R.N. Forbes. “Did I glaze over that bit?” Isbell knows that Google’s meager distribution of women far outshines its terrible racial diversity. Only 2 percent of all U.S. Googlers are black, and only 4 percent are Hispanic. In tech-oriented positions, the numbers fall to 1 percent and 3 percent, respectively. (Unlike the gender data, which is global, the ethnic diversity data is for the United States only.)
These figures track computing talent more broadly, even at the highest levels. According to data from the Integrated Postsecondary Education Data System, for example, less than 3 percent of the doctoral graduates from the top-10 ranked computer science programs came from African American, Hispanic, Native American, and Pacific Islander communities during the decade ending in 2015.
Given these abysmal figures, the idea that diversity at Google (or most other tech firms) is even modestly encroaching on computing’s incumbents is laughable. To object to Google’s diversity efforts is to ignore that they are already feeble to begin with.
* * *
The Googler’s complaints assume that all is well in the world of computing technology, such that any efforts to introduce different voices into the field only risk undermining its incontrovertible success and effectiveness. But is the world that companies like Google have brought about really one worthy of blind praise, such that anyone should be tempted to believe that the status quo is worth maintaining, let alone celebrating?
Many things are easier and even better thanks to Google search (or maps, or docs)—or Facebook, or smartphones, or any of the other wares technology companies put on offer. But overall, the contemporary, technologized world is also in many ways a hellscape whose repetitive delights have blinded the public to its ills.
Products have been transformed into services given away “free” as an excuse to extract data from users. That data is woven into an invisible lattice of coercion and control—not to mention as a source of enormous profit when sold to advertisers or other interested parties. Apps and websites are designed for maximum compulsion, because more attention means more content, and more content means more data and thereby more value. All that data is kept forever on servers corporations control, and which are engineered—if that’s the right word for it—in a way that makes them susceptible to attack and theft.
Thanks to the global accessibility of the internet, these services strive for universal deployment. Google and Facebook have billions of “customers” who are also the source of their actual products: the data they resell or broker. The leverage of scale also demands that everyone use the same service, which dumps millions together in unholy community. Online abuse is one consequence, as are the campaigns of misdirection and “fake news” that have become the front for a new cold war.
Because of that universal leverage, work of all kinds has also been upset by or consolidated in computing services. Retail, travel, entertainment, and transportation, of course, but even professions like real estate, law, and education appear to be at risk of dismantlement via automation and global dissemination. This sea change is excused by platitudes about “innovation” and “disruption.”
All told, the business of computing is infiltrated with a fantasy of global power and wealth that naturally coheres to the entrenched power of men over generations. To mistake such good fortune for inborn ability is to ignore the existence of history.
Men—mostly white, but sometimes Asian—have so dominated technology that it’s difficult even to ponder turning the tables. If you rolled back the clock and computing were as black as hip-hop, if it had been built from the ground up by African American culture, what would it feel like to live in that alternate future—in today’s alternate present? Now run the same thought experiment for a computing forged by a group that represents the general population, brown of average color, even of sex, and multitudinous of gender identity.
Something tells me the outcome wouldn’t be Google and Twitter and Uber and Facebook. It’s depressing that it takes a determined exercise in speculative fiction even to ponder how things might be different were its works made by different hands.
Not just the services or apps, either. Given that the fundamentals of computing arose from a long legacy of ideas mostly forged by white men, it’s hard to imagine how the fundamental operation of computers at the lowest level might have been different had ideas from alternative sources underwritten it.
The business of computing is also bound to incumbents. Failing to acknowledge this truth hamstrings earnest efforts to overcome that power through diversification. For example, advocating for more women entrepreneurs (about 17 percent of start-ups have a woman founder) or venture-capital partners (about 7 percent are women) seems like a terrific path toward diversity and equity. But the venture-backed start-up itself is still a slave to the marketplace design that its mostly male precursors had already created and entrenched. Change in established companies faces the same challenges. A search for a new Uber chief executive is underway, although it remains unclear whether Uber’s culture can be changed, even with a new leader.
Even the fateful Googler’s memo enjoys the spoils of a world already designed for male supremacy. What is this letter, after all, but a displaced Reddit post? Certain but non-evidential. Feigning structure, but meandering. Long and tedious, with inept prose and dead manner. This false confidence underwrites all the claims the memo contains, from its facile defense of jingoism as political conservatism to its easy dismissal of anyone not predetermined to be of use.
And Google built an “internal meme network” expressly for the purpose of sharing material like the memo in question! How to interpret such a thing except as Google’s own private Reddit, where the bravado of the white man’s internet comes home to roost at the office? Even worse, in her statement responding to the anti-diversity memo, Google’s vice president of diversity, integrity, and governance, Danielle Brown, appears to celebrate this offering as one among “so many platforms for employees to express themselves,” such that “this conversation doesn’t end with my email today.” The problem, it seems, is also its own solution.
As my colleague Mark Guzdial puts it, women used to avoid computer science because they didn’t know what it is. Now they avoid it because they know exactly what it is.
* * *
Soon, the fall term will commence at Georgia Tech. I will take to the lectern in the introductory course to our bachelor of science degree in computational media. The program also hopes to make headway against the diversity struggle. Conceived after the dot-com crash and inaugurated in 2004, the degree draws half its courses, faculty, and management from computing and half from the liberal arts. The goal was to address the increased connection between computing, expression, and communication.
The results have been promising. Computational media has achieved consistently high gender equity, for example. As of spring 2017, computer science was composed of only 24 percent women, whereas women made up 52 percent of the computational media students. That might give it the greatest proportion of women among accredited computing undergraduate majors in the country. Ethnic diversity is also better: 11 percent of computational media students are black and 9 percent are Hispanic, compared with 6 and 5 percent, respectively, in CS.
But that apparent victory might be a Pyrrhic one. All the anxieties that plague the anonymous Googler also afflict programs like ours, which provide part of the funnel to tech companies like Google. As computing rose from the dot-com ashes in the mid-2000s, enrollments skyrocketed. But computational media remains small—a tenth the size of computer science, and shrinking in total number and percentage of overall computing students during the same years CS has been on the rise. As a part of that decline, it appears to be losing men to computer science in particular, and perhaps falsely inflating the program’s claims to gender equity in the process.
When it was designed, computational media hoped to attract students with an interest in areas that blend computing and creativity, among them film, the web, television, games, and so on. That move failed to anticipate the foundational grievance that courses through the Google memo: that of “dumbing down” computing with interlopers. Students, more anxious and more professionally oriented than ever, seek the surety of the computer science degree. Academic faculty and industrial managers, meanwhile, fear yielding to “CS Lite,” a derogatory name for compromising technical expertise.
We should have known that for some computational media inevitably would threaten to feminize computing, relegating technical creativity to service work at best, emotional labor at worst. And so, while Georgia Tech can lay claim to an impressively gender-equal accredited computing degree, it’s not clear that such an accomplishment does anything more than pay lip service to diversity, distracting attention from our ever-growing contribution to the perverted reality of a world run by the computer programmers we graduate into companies like Google.
Darkened under the shadow of this Google jeremiad, I’m not sure what to say to my students when I stand before them later this month. Computation ended up having a much more widespread and much more sinister impact on media writ large—not just traditional media forms like music and news, but also on media as a name for every interaction between people and the world, from socializing to health, education to transit. It’s not possible to rewind the clock on the past, nor to burn it all down and start anew. But training up more women and minorities to service technological power’s existing ends—founding start-ups, working at Google—only transfers the lip service from educational programs to tech companies. They process diversity into glossy reports that placate shareholders and the public, all the while putting on the same show with a slightly different cast of characters.
Reader, I want so desperately to leave you with an alternative. A better option, a new strategy. One that would anticipate and defang the inevitable maws crying, “Well, what’s your solution, then?” But facile answers spun off-the-cuff by white men in power—aren’t these the things that brought trouble in the first place?
Maybe there is an answer, then, after all: Just to shut up for a minute. To stop, and to listen, and even to step out of the way. Not entirely, and not forever. But long enough, at least, to imagine how some of the lost futures of pasts left unpursued might have made for different, actual presents—and that might yet fashion new futures. Only a coward would conclude that none of them are better than the one that’s become naturalized as inevitable.


In their “hot mic” moment last week, Senators Susan Collins and Jack Reed gave cold bipartisan voice to a deep fear: The president of the United States is stunningly unprepared for his job and just may be—to use a technical political science term I learned in graduate school—two cans short of a six pack. Between the Senate’s late-night “damn the torpedoes” voting frenzy to repeal something, anything, from Obamacare, and the president’s early morning tweets proclaiming his “complete power” to pardon himself and his relatives, what used to be business as usual in Washington never looked so good.
It is comforting to think that Trump is the only thing standing between us and the good old dysfunctional ways of Washington. But I have my doubts. The president’s disruption engine is powered by three paradoxes. Each was made possible by technological innovations. All will endure long after this ringmaster moves his circus to another town.
Paradox #1: More information, less credibility
Trump’s cries about fake news get receptive audiences in part because we live in the most complex information age in human history. The volume of data is exploding, and yet credible information is harder to find. The scale of this information universe is staggering. In 2010, Eric Schmidt, the chairman of Google’s parent company Alphabet, noted that every two days, we create as much information as we did from the dawn of civilization up to 2003. Today Google processes close to 62,000 search queries a second. That’s more than 5.3 billion queries a day.
Information is everywhere, but good information is not. Why? Because the barriers to entry are so low. In the Middle Ages, when paper was a sign of wealth and books were locked up in monasteries, knowledge was considered valuable and creating it was costly. To be sure, there was some flat-earthy nonsense locked up in those tomes and religious and political rulers used their claims to knowledge as political weapons. Today the challenge is different. We now live at the opposite extreme, where anyone—from foreign adversaries to any crackpot with a conspiracy theory—can post original “research” online. And they do. Telling the difference between fact and fiction isn’t so easy. A few months ago, one of my graduate student researchers included information in a paper that I found oddly inaccurate, so I checked the footnotes. The source was “RT”—as in the outlet formerly known as Russia Today, a propaganda arm of the Kremlin. Stanford students aren’t the only ones struggling with real fake news. In December, the Pakistani defense minister rattled his nuclear saber in response to an Israeli tweet. Except the Israeli tweet wasn’t real.
Meanwhile, attitudes toward traditional information sources like the mainstream media and universities are souring, particularly among Republicans. Confidence in newspapers has declined by more than 20 points since 1977. Last month, a Pew survey found that for the first time, a majority of Republicans had a negative view of American universities.
The antidote to bad information used to be more information. Not anymore. What good is more information if people don't trust it—or if the traditional methods of sorting the good information from the bad (including the weighty brands of certain news organizations) don't work anymore? The marketplace of ideas is experiencing market failure. When information proliferates and credibility shrinks, reasoned argument suffers and democratic society decays.
Paradox #2: More connectivity, less civility
Today nearly half the world is online. By 2020 more people are expected to have cell phones than running water. But civility has not accelerated in tandem. In earlier times, it took some effort to deliver hurtful messages. In the U.K.’s Parliament building, seating in the House of Commons is designed to space the opposition at least two sword lengths apart from the ruling party—just in case.  Distance has its benefits. Years ago, I got a letter from a federal inmate claiming I was part of a 9/11 conspiracy and the murder for which he had been convicted. He went to a lot of trouble to write me with all that multi-colored ink. He even had to pay for the stamp. Now, I can get anonymous vitriol on Twitter, or in my email, or in the comments section of something I write—instantly, for free.
Sure, connectivity has created tremendous positive changes, including new markets in developing nations and new bonds among kindred spirits across vast distances. But connectivity has also made nasty discourse more convenient and socially acceptable. The step between harmful speech and chilled speech is a small one. Civility is not a convenience. It is a cornerstone of free speech in a liberal society. Trump may personify America’s descent into coarse discourse and amplify its spread. But it didn’t start and will not stop in Trump Tower or the White House. The root causes lie deeper.
Paradox #3: The wisdom of crowds, the duplicity of crowds
Technology has unleashed the wisdom of crowds. Now you can find an app harnessing the experiences and ratings of likeminded users for just about anything. The best taco truck in Los Angeles? Yelp. The highest rated puppy crate? Amazon. Youth hostels in Barcelona? TripAdvisor. Researchers are even using the wisdom of crowds to better predict which internet users may have pancreatic cancer and not even know it yet—based on the search histories of other cancer patients.
But the 2016 presidential election revealed that not all crowds are wise, or even real. The wisdom of crowds can be transformed into the duplicity of crowds. Deception is going viral.
On social media, one person can masquerade as hundreds, even thousands, with fake personas. Thanks to advances in artificial intelligence, it’s also possible to create armies of automated social media bots to develop, manipulate, and spread deceptive information at speeds and scales unimaginable before now. Facebook is so concerned about the duplicity of crowds, in April the company issued a “call to arms” report about what it’s doing to stop bad actors from manipulating public discourse and deceiving people.
Disruption used to be a good word, signifying creativity and innovation—shaking up things in a good way. The Founding Fathers were disruptive, imagining a nation ruled by laws and not kings. Their great American experiment inspired generations and helped transform half the world into democracies. NASA was disruptive, pushing the frontiers of science to land a man on the moon. Silicon Valley tech companies have disrupted all sorts of industries to become the engines of the global economy.
But disruption often has unintended consequences. More information, connectivity, and crowdsourcing are also shrinking credibility, eroding civility, and empowering the duplicity of crowds. These technological chickens are coming home to roost, and they’re likely to stay here even when Trump is gone.



Our increasingly smart machines aren’t just changing the workforce; they’re changing us. Already, algorithms are directing human activity in all sorts of ways, from choosing what news people see to highlighting new gigs for workers in the gig economy. What will human life look like as machine learning overtakes more aspects of our society?
Alexis Madrigal, who covers technology for The Atlantic, shares what he’s learned from his reporting on the past, present, and future of automation with our Radio Atlantic co-hosts, Jeffrey Goldberg (editor in chief), Alex Wagner (contributing editor and CBS anchor), and Matt Thompson (executive editor).
Links:
Further Reading:


I sit down at the table, move my napkin to my lap, and put my phone on the table face-down. I am at a restaurant, I am relaxed, and I am about to start lying to myself. I’m not going to check my phone, I tell myself. (My companion’s phone has appeared face-down on the table, too.) I’m just going to have this right here in case something comes up.
Of course, something will not come up. But over the course of the next 90 minutes I will check my phone for texts, likes, and New York Times push alerts at every pang of boredom, anxiety, relaxation, satiety, frustration, or weariness. I will check it in the bathroom and when I return from the bathroom. I don’t really enjoy this, but it is very interesting, even if some indignant and submerged part of my psyche moans that I am making myself dumber every time I look at it. As, in fact, I am.
A smartphone can tax its user’s cognition simply by sitting next to them on a table, or being anywhere in the same room with them, suggests a study published recently in the Journal of the Association for Consumer Research. It finds that a smartphone can demand its user’s attention even when the person isn’t using it or consciously thinking about it. Even if a phone’s out of sight in a bag, even if it’s set to silent, even if it’s powered off, its mere presence will reduce someone’s working memory and problem-solving skills.
These effects are strongest for people who depend on their smartphones, such as those who affirm a statement like, “I would have trouble getting through a normal day without my cell phone.”
Have Smartphones Destroyed a Generation?
But few people also know they’re paying this cognitive smartphone tax as it plays out. Few participants in the study reported feeling distracted by their phone during the exam, even if the data suggested their attention was not at full capacity.
“We have limited attentional resources, and we use some of them to point the rest of those resources in the right direction. Usually different things are important in different contexts, but some things—like your name—have a really privileged status,” says Adrian Ward, an author of the study and a psychologist who researches consumer decision-making at the University of Texas at Austin.
“This idea with smartphones is that it’s similarly relevant all of the time, and it gets this privileged attentional space. That’s not the default for other things,” Ward told me. “In a situation where you’re doing something other than, say, using your name, there’s a pretty good chance that whatever your phone represents is more likely to be relevant to you than whatever else is going on.”
In other words: If you grow dependent on your smartphone, it becomes a magical device that silently shouts your name at your brain at all times. (Now remember that this magical shouting device is the most popular consumer product ever made. In the developed world, almost everyone owns one of these magical shouting devices and carries it around with them everywhere.)
In the study, Ward and his colleagues examined the performance of more than 500 undergraduates on two different common psychological tests of memory and attention. In the first experiment, some participants were told to set their phones to silent without vibration and either leave them in their bag or put them on their desk. Other participants were asked to leave all their possessions, including their cell phone, outside the testing room.
In the second experiment, students were asked to leave their phones on their desk, in their bag, or out in the hall, just as in the first experiment. But some students were also asked to power their phone off, regardless of location.
In both experiments, students who left their phones outside the room seemed to do best on the test. They also found the trials easier—though, in follow-up interviews, they did not attribute this to their smartphone’s absence or presence. Throughout the study, in fact, respondents rarely attributed their success or failure on a certain test to their smartphone, and they almost never reported thinking they were underperforming on the tests.
Daniel Oppenheimer, a professor of psychology at the University of California, Los Angeles, noted that this effect is well-documented for enticing objects that aren’t smartphones. He was not connected to this research, though his research has focused on other vagaries of digital life. Several years ago, he and his colleagues suggested that students remember far more of a lecture when they take notes by hand rather than with a laptop.
“Attractive objects draw attention, and it takes mental energy to keep your attention focused when a desirable distractor is nearby,” Oppenheimer told me in an email. “Put a chocolate cake on the table next to a dieter, a pack of cigarettes on the table next to a smoker, or a supermodel in a room with pretty much anybody, and we would expect them to have a bit more trouble on whatever they’re supposed to be doing.”
He continued: “We know that cell phones are highly desirable, and that lots of people are addicted to their phones, so in that sense it’s not so surprising that having one visible nearby would be a drain on mental resources. But this study is the first to actually demonstrate the effect, and given the prevalence of phones in modern society, that has important implications,” he said.
Ward will continue researching the psychological costs and benefits of the new technologies that have permeated everyday life. His dissertation at Harvard looked at the implications of delegating cognitive tasks to the cloud. “Big things are happening so quickly. It’s the 10th anniversary of the iPhone, and the internet’s only been around for 25 years, yet already we can’t imagine our lives without these technologies,” he said. “The joyful aspects, or positive aspects—or the addictive aspects—are so powerful, and we don’t really know the negative aspects yet.”
“We can yell our opinions at each other, and people are going to agree or disagree with them, and set up luddites-versus-technolovers debates. But I wanted to get data,” he told me.
It’s worth noting that the type of psychological research Ward conducts—trials on willing, Western undergrads, often participating in studies to fulfill course credit—has suffered a crisis of confidence in recent years. Psychologists have had difficulty replicating some of the most famous experiments in their field, leading some to argue that all psychology experiments should be replicated before they are published. This study has not yet been replicated.
One possible consequence of Ward’s work extends beyond smartphones. Most office workers now know that “multi-tasking” is a fallacy. The brain isn’t doing two tasks at once as much as it’s making constant, costly switches between tasks. But Ward says that assiduously not multi-tasking isn’t very helpful, either.
“When you’re succeeding at not multitasking—that is, when you’re doing a ‘good job’—that’s not exactly positive as well,” he said. That’s because it takes mental work, and uses up attentional resources, to avoid distraction.
Instead, he recommends that the most dependent users just put their smartphone in another room.



I used to scandalize my friends with this confession: “I don’t own a hair dryer.”
It was as if I’d told them I ride a horse to work. But their surprise was justified: 90 percent of U.S. homes own a hair dryer. They come standard in most hotel rooms. The hair dryer is tangled up with the history of fashion, the evolution of women’s roles, and the development of gendered social spaces.
In the beginning, the hair dryer wasn’t a home appliance. In 1888, Alexandre-Ferdinand Godefroy debuted his “hair dressing device” in a French salon. It wasn’t pretty: His dryer was a clumsy, seated machine, resembling a vacuum cleaner—essentially a giant hose connected to a heat source. At the time, women wore their hair long and looped, or curled into elaborate updos. For formal occasions, they might have ribbons, feathers, or flowers woven into their locks. Godefroy’s invention aimed to speed up the labor involved with these creations. But his machine failed to circulate air effectively, so the time saved wasn’t significant. The prototype was far too unwieldy to become widespread anyway.
Hair dryers didn’t take off until the first handheld units became available, in the early 1920s. These metal, gun-shaped models arrived right when women’s hairstyles were shifting from the mountainous piles of Gibson Girl curls that required dozens of bobby pins to the tidy, easier-to-shape bobs of flappers. It was a radical break from past styles. As Rachel Maines, a technology historian at Cornell University, explained to The New York Times, “Having clean, shiny, fluffy hair—that’s a 20th-century thing.” This new trend was also happy news for the hair dryer. Dirty hair could hide in a pompadour, but a shorter ’do that hung free would reveal limp or stringy hair.
Early handheld hair dryers were still difficult to use. Their metallic (often aluminum) casings made them hard to wield. Also, drying times were far longer than today’s norm, as the devices drew only 100 watts of electricity compared to the 2,000 watts of modern versions. That made them exhausting to use over the long periods of time required for drying. Some early versions had pedestals to give tired arms a rest. Nevertheless, these dryers were considered a marvel of convenience, marketed as having “loads of hot or cold air instantly. Just by pressing the handle button.”
The handheld versions for the home were joined by hooded models for the salon. Made of metal and later of plastic, and applying an even, all-over heat, hooded dryers entered widespread use in 1930s. In the decades that followed, they became a defining trait of the salon scene.
This was an unsettled time for American women. First they joined the workforce during the war effort, in the 1940s. Later, they were driven back into the home. During these postwar years, the salon became a cherished second space for women outside the home. The task of “setting” hair into the molded hairdos popular in the day, such as Veronica Lake’s cascading S-shaped waves or Grace Kelly’s sculpted bob, required regular appointments at the salon, establishing it as a popular weekly meeting spot. The image of a row of women idly flipping through magazines under a hair dryer hood became a symbol of postwar prosperity and of women’s new leisure time.
In an effort to bring that salon cachet into the home, the bonnet hair dryer debuted in 1951. This model had a soft, shower cap-style headpiece that the user would attach to a motor via a hose. In a Sunbeam commercial from the 1960s, the bonnet dryer was advertised to be “so fast that it actually dries hair in an average of 22 minutes.” These models were also meant to mimic the salon experience: “Just select any one of four temperatures. Then, relax,” the commercial suggested. They came in little handled carrying cases that could be toted around, but typically the user would stay seated in a single spot while hot air circulated. Advertisements frequently showed models chatting on the phone, suggesting that salon-level socializing and the community it inspired wouldn’t be lost if women did their own grooming at home.
Another invention that sprang from hooded hair dryers was the “wave machine.” The hairstylist Marjorie Joyner, known for her talent in creating marcel waves, connected pot-roast rods to a dryer, and mechanized marcelling was born. Hair salons were racially segregated in these years, but the wave device became popular in both black and white salons alike. With this machine, Joyner appears to have become the first African-American woman to secure a patent.
* * *
In the 1960s and ’70s, the sexual revolution left its mark on fashion—and hair. The rigid gender divisions of the previous decades began to soften. Icons like the Beatles and the Monkees were wearing their hair longer in mod “mop tops,” and influencing other men to do the same. That helped spur the counterculture trend of long, hippie locks. Companies moved quickly to capitalize on this potential new hair-dryer market. As one Clairol ad said to its male reader: “Congratulations. You have more hair today than a year ago.” But then it explained to men that the “secret” to mastering this new look “isn’t just more hair. It’s cleaner hair, blown dry—to give it bulk and body it out.”
Hairstylists gained celebrity status in these decades, thanks to stylist-to-the-stars Vidal Sassoon and films like Shampoo, which starred Warren Beatty as a hunky hairdresser irresistible to his customers. Suddenly, a functional grooming tool had sex appeal. The stylist-as-Casanova persona can still be found today, in celebrity stylists like Harry Josh. Miranda Kerr promoted his signature dryer by blowing it across her décolletage during a photo shoot, treating it more like a seductive bottle of perfume than an appliance.
During the ’60s, plastics began to dominate consumer goods, and hair dryers were no exception. Once made from metal or occasionally Bakelite, now hair dryers joined a flood of “fantastic plastic” products facilitated by companies like DuPont and Dow Chemical. But apart from an alteration to its materials and the addition of various attachments and heat conductors, like ceramic and tourmaline, the hair dryer has changed very little since its birth. Writing for Fast Company in 2011, James Gaddy lamented the device’s boring uniformity, complaining that they “all look the same.” Gaddy denounced all models as little more than “a holding pen for the small motor-driven fan and heater inside.”
It wasn’t until the 1970s that regulations were drafted to improve dryer safety. And only as recently as 1991 were these devices legally required to contain ground fault circuit interrupters, which greatly decrease the danger of high-voltage injury or death. Older models still resurface in the news for plunking into bathtubs and electrocuting their owners, as in the case of the young Palomera sisters (ages 7 and 9), who were cooling off in the tub when their old dryer dropped in.
* * *
Hair dryers weave in and out of public and private spaces, making them different from other grooming tools. Hair depilators and eyelash curlers remain hidden behind closed doors. But hair dryers began in public and continue to occupy public space. Some salons will even place a chair in their picture window, putting the hair-drying experience on full display and marketing it to passersby.
In the last decade, hair dryers have taken up public real estate anew thanks to an explosion of hair-drying bars in urban areas that deal exclusively with the washing and drying of hair (no cuts or dyeing treatments). The company Drybar, one of the most popular, has more than 70 locations across the United States and Canada. Styles are modeled on the extremely coiffed looks paraded on the red carpets of award shows and on reality TV. These ultra-manicured hairdos are a status symbol akin to a handbag or diamond ring. Maintaining them requires a commitment of $40 to $50 per week on the impermanence of hairdos that one humid day can dismantle.
But the hair dryer may now be at a crossroads. In 2016, Dyson, the maker of vacuums, fans, and hand dryers, set out to remodeling the hair dryer. As it had done with its Airblade hand dryers, Dyson hopes to revolutionize the market, encouraging more women to take their hair back into their own hands. The company shifted the motor to the base of the dryer, making it smaller and supposedly improving drying time. Though many of Dyson’s changes are more aesthetic than functional, this is a market where looks matter.
At the same time, the fashion pendulum has begun to swing away from high-polish, TV-ready looks toward a more relaxed, no-effort appearance. Celebrities like Alicia Keys have embraced the no-makeup look, and the #iwokeuplikethis movement has reinvigorated a fresher, less preened appearance. Hair might become less conforming and more free and breezy again—which could push hot air out of the public eye and back behind the bathroom door.
I finally succumbed and bought a hair dryer. I had spent years flying out the door with a damp head of hair, but I decided my soggy morning appearance was doing me a disservice. It communicated a certain young, relaxed attitude that went against the professional adult I wanted to become. Years later, I still feel awed that after 10 minutes of fanning a dryer around, my hair can be tamed. Now I see why ads for hair dryers were once laced with a million exclamation points, and showed women who were smitten over their new grooming gadgetry. As one reads, you can store your hair dryer away “or you can keep it out in the open and make a pet out of it.”
This article appears courtesy of Object Lessons.


Over the past year, the old idea of enforcing market competition has gained renewed life in American politics. The basic idea is that the structure of the modern market economy has failed: There are too few companies, most of them are too big, and they’re stifling competition. Its supporters argue that the government should do something about it, reviving what in the United States we call antitrust laws and what in Europe is called competition policy.
Stronger antitrust enforcement—it’s enough of a thing, now, that Vox is explaining it.
The loudest supporters of this idea, so far, have been from the left. But this week, a newer and more secretive voice endorsed a stronger antitrust policy.
Steve Bannon, the chief strategist to President Donald Trump, believes Facebook and Google should be regulated as public utilities, according to an anonymously sourced report in The Intercept. This means they would get treated less like a book publisher and more like a telephone company. The government would shorten their leash, treating them as privately owned firms that provide an important public service.
What’s going on here, and why is Bannon speaking up?
First, the idea itself: If implemented, it’s unclear exactly how this regime would change how Facebook and Google run their business. Both would likely have to be more generous and permissive with user data. If Facebook is really a social utility, as Mark Zuckerberg has said it is, then maybe it should allow users to export their friend networks and import them on another service.
Both companies would also likely have to change how they sell advertising online. Right now, Facebook and Google capture half of all global ad spending combined. They capture even more global ad growth, earning more than three quarters of every new dollar spent in the market. Except for a couple Chinese firms, which have a lock on their domestic market but little reach abroad, no other company controls more than 3 percent of worldwide ad spending.
So if the idea were implemented, it would be interesting, to say the least—but it’s not going to become law. The plan is a prototypical alleged Bannonism: iconoclastic, anti-establishment, and unlikely to result in meaningful policy change. It follows another odd alleged Bannon policy proposal, leaked last week: He reportedly wants all income above $5 million to be taxed at a 44-percent rate.
Which bring me to the second point: Bannon’s proposal is disconnected from the White House policy that he is, at least on paper, officially helping to strategize. The current chairman of the Federal Communications Commission, Ajit Pai, is working to undo the rule that broadband internet is a public utility (which itself guarantees the idea of “net neutrality”). Trump named Pai chairman of the FCC in January.
Bannon’s endorsement of stronger antitrust enforcement (not to mention a higher top marginal tax rate) could very well be the advisor trying to signal that he is still different from Trump. Bannon came in as the avatar of Trump’s pro-worker, anti-immigration populism; he represented the Trump that tweeted things like:
I was the first & only potential GOP candidate to state there will be no cuts to Social Security, Medicare & Medicaid. Huckabee copied me.
As the president endorses Medicaid cuts and drifts closer to a Paul Ryan-inflected fiscal conservatism, Bannon may be looking for a way to preserve his authenticity.
Third, it’s the first time I’ve seen support for stronger antitrust enforcement from the right. So far, the idea’s strongest supporters have been Congressional Democrats. Chuck Schumer has elevated the idea to the center of the “Better Deal” policy agenda in 2018. Before that, its biggest supporters included Bernie Sanders, who railed against “Too Big to Fail” banks in his presidential campaign; and Elizabeth Warren, who endorsed a stronger competition policy across the economy last year.
Finally, while antitrust enforcement has been a niche issue, its supporters have managed to put many different policies under the same tent. Eventually they may have to make choices: Does Congress want a competition ombudsman, as exists in the European Union? Should antitrust law be used to spread the wealth around regional economies, as it was during the middle 20th century? Should antitrust enforcement target all concentrated corporate power or just the most dysfunctional sectors, like the pharmaceutical industry?
And should antitrust law seek to treat the biggest technology firms—like Google, Facebook, and perhaps also Amazon—like powerful but interchangeable firms, or like the old telegraph and telephone companies?
There will never be one single answer to these questions. But as support grows for competition policy across the political spectrum, they’ll have to be answered. Americans will have to examine the most fraught tensions in our mixed system, as we weigh the balance of local power and national power, the deliberate benefits of central planning with the mindless wisdom of the free market, and the many conflicting meanings of freedom.


Lavanderia, one of San Francisco’s largest laundromats, is an urban relic. Its peeling aquamarine walls house some 110 machines. Telenovelas play on a TV and arcade games from the 1990s are tucked into unexpected nooks. After opening in 1991, Lavanderia—like so many other laundromats in big cities—became a social hub in a neighborhood where renters lacked the space or funds for their own machines.
But, again like so many other laundromats in big cities, Lavanderia’s future is uncertain. While families have been hauling their dirty towels, sheets, and underwear there for decades, the business’s future earnings now pale in comparison to the value of the land it sits on—rents have skyrocketed in recent
The Poverty Just Over the Hills From Silicon Valley
years in the Mission District, the historically Latino neighborhood where Lavanderia, whose name means laundromat in Spanish, is located.
In its heyday, the 5,200-square-foot laundromat brought in over $1,000 a day in quarters. But in the past decade, its owner, a wealthy tech entrepreneur named Robert Tillman, has seen revenues dry up. Business was so bad at his nine other Bay Area laundromats that he sold them off over the years. Lavanderia is the only one Tillman has left, and he’d like to turn it into a 75-unit apartment building, with some units generating as much as $55,000 each year.
The erosion of Tillman’s laundromat business is a side effect of a national trend: Developers are remaking urban neighborhoods across the country, constructing apartment buildings for waves of young, wealthy workers and installing washers and dryers in each unit, leaving local laundromats without clientele. “Offering a washer and dryer in-unit is a trend we’re certainly seeing,” says Paula Munger, the director of industry research for the National Apartment Association. A recent survey by the industry group found the addition of washers and dryers to be one of the most common upgrades to apartments in recent years.
That has posed a problem for laundromats. According to data from the Census Bureau, the number of laundry facilities in the U.S. has declined by almost 20 percent since 2005, with especially precipitous drops in metropolitan areas such as Los Angeles (17 percent) and Chicago (23 percent). (While that data includes both laundromats and dry cleaners, laundromats account for the bulk of the drop.) In the disappearance of laundromats, a longtime staple of urban living, one can detect yet another way that cities have changed in response to an influx of higher-earning residents.
Collectively earning $5 billion each year, as estimated by the Coin Laundry Association, the U.S.’s coin-operated laundromats are overwhelmingly mom-and-pop operations and share a tightly knit history with the American city. While the first self-serve laundromat opened in 1934 in Fort Worth, Texas, the industry didn’t really take off until the ’50s, after many cities became more densely populated. “Like our mantra goes, ‘The more people, the more dirty clothes,’” Brian Wallace, the president of the Coin Laundry Association told me. Technological leaps in washer and dryer efficiency during the ’80s allowed the industry to expand even more. Laundromats, communal spaces that brought people together to perform a mundane chore, became a fixture of the urban experience, with Hollywood using them to stage serendipitous meetings, as it did in the 1985 film The Laundromat.
By the ’90s, the industry was strong enough to attract Tillman’s attention. Tillman, who’s now 61, earned his first fortune from tech ventures—among other things, he was behind DigitalGlobe, a satellite-imaging company that has supplied orbital views of earth to Google and the U.S. government. Back on the ground, Tillman, a graduate of Stanford’s business school, recognized that people still needed clean clothes, the age of satellites notwithstanding. Soon, he came to own 18 laundromats. “The ’90s were a great time for the laundry business,” Tillman recalls.
After the dot-com bubble burst, San Francisco’s rapid inflow of wealth has hurt his businesses. Lavanderia’s revenue has slid 33 percent since 2004, according to the business’s accounting records. Terry Smith, who repairs machines and collects the quarters at Lavanderia and other Bay Area laundromats, anecdotally reports that lately he’s been dumping fewer coins into his jingling collections sack as he makes his rounds. Even Tillman’s eight laundromats in Albuquerque were impacted by the urban transformations seen across the country. “I saw where the business was headed,” Tillman says. By 2007, he sold off all his laundromats except for Lavanderia—part of a 15-percent drop in the number of the Bay Area’s laundry facilities since the early 2000s, according to Census Bureau data.
Laundromats’ margins are further thinning as the price of water and sewage services have risen across the country. Utilities make up by far the heftiest of Lavanderia’s expenses, costing over $100,000 each year. Add to that the roughly $30,000 Tillman spends fixing his aging washers and dryers, and the laundromat is left with about $140,000 of profit each year, a number that continues to dwindle.
At the same time, laundromats were never a great bargain for low-income customers. Families can do multiple loads of laundry a week, and at a laundromat, that can cost $100 or more per month. There is rarely an alternative: Landlords are typically reluctant to install the right plumbing and hookups in already cramped apartments in what are often older buildings.
With this calculus in mind, Tillman would like to turn Lavanderia into a six-story apartment building that few of his current customers could likely afford to live in. He is hardly the first laundromat owner to conceive of such a plan. As Adam Lesser, the owner of Fiesta Laundromat, just a few blocks from Lavanderia, puts it, “I’m over here cleaning out lint. What the hell am I doing?”
Tillman filed a proposal with San Francisco’s planning department outlining his intentions three and a half years ago, but the project has stalled in the face of anti-development activism. Erick Arguello, a longtime Mission resident, heads one of the groups opposing Tillman’s project. He has seen one laundromat after another close in the neighborhood in the past several years: Super Lavar, where his family of seven used to go, turned into an upscale restaurant. Cleaner Wash, a small laundromat also in the Mission, was bought for over $1.5 million and turned into a high-end gym. “We have large families and you have to walk three or four blocks to go do your laundry,” Arguello says. “You also lose that sense of community. The laundromat was a family affair growing up.”
Meanwhile, as the project is on hold, Tillman has been putting off much-needed repairs. These days he’s trying to drum up local support at community meetings in the Mission so he can finally raze Lavanderia. Not all laundromat owners are pursuing Tillman’s route, though. In June, at the Coin Laundry Association’s conference, held every two years in Las Vegas, owners explored ways to make laundromats more appealing to a hipper clientele, such as by offering wi-fi. “The representations have been more positive and younger—a place where young people meet,” Wallace says, adding that the industry faces further challenges from on-demand laundry apps. Some laundromats have already morphed into cafes where customers can drink craft beer or sip a latte while waiting for their loads to finish. That may be a positive turn for the coin-laundry industry, but it does sound a whole lot like what’s happening to Lavanderia’s neighborhood anyway.


The New York Times likes to think of itself as a family newspaper. It is also the self-described paper of record. It may not be either, but it’s definitely not both all the time.
Take, for example, the moment when the Times had to choose whether to quote the new White House communications director in a particularly colorful tirade against his colleagues. Anthony Scaramucci, who joined the Trump administration last week, eviscerated the White House chief of staff, Reince Priebus, and the administration’s chief strategist, Stephen Bannon, in an interview with a New Yorker reporter on Wednesday.
“Reince is a fucking paranoid schizophrenic, a paranoiac,” Scaramucci said.
And also: “I’m not Steve Bannon. I’m not trying to suck my own cock.”
Then, for good measure: “I’m not trying to build my own brand off the fucking strength of the president. I’m here to serve the country.”
In this case, the Times really went for it, publishing all three quotes verbatim. Maybe not every journalist would make the same call, but most would understand why the Times went this route. Many publications try to avoid gratuitous foul language, even in quotes, unless the meaning of the thing being conveyed depends on it. Otherwise, matters of taste notwithstanding, bad language is often just distracting. Plenty of people curse in casual conversation; rarely is it actually meaningful.
But when the White House director of communications uses language like, well, you know, to describe the president’s inner circle, it’s in the public interest to know exactly what was said. (The Atlantic quoted Scaramucci, too, by the way.) The Times didn’t immediately grant my request to speak with an editor Thursday night, but a spokesperson did direct me to comments by the paper’s deputy managing editor, which he’d published to Twitter.
The Times published Scaramucci’s profanity only after top editors, including the executive editor Dean Baquet, “discussed whether it was proper,” Clifford Levy wrote. “We concluded that it was newsworthy that a top Trump aide used such language. And we didn’t want our readers to have to search elsewhere to find out what Scaramucci said.” Given what the newspaper has had to navigate before, it’s likely the vulgar reference to Bannon was the most difficult call among the three. Indeed, wrote one of the top editors at the Times, Sam Dolnick, the debate was “one for the ages.”
“A couple of years ago I got in trouble for ‘hand job.’ In a quote,” tweeted Emily Bazelon, a staff writer for The New York Times Magazine. In fact, Bazelon’s reference to hand jobs, at least the reference that appears in her 2014 magazine story about college romance, was not a direct quote but a line she paraphrased.
Either way, it’s not like the Times never prints vulgar language.
There was the Access Hollywood tape last fall, which featured Trump bragging about being able to grab women without their consent. The Times repeatedly printed the vulgar terms he used. It also published an offensive term—uh, rhymes with “blunt”—that a Trump adviser had used to describe Hillary Clinton, only to remove the word from an op-ed after the fact with a brief editor’s note flagging the change.
The Evolution of Slang
There have been other instances in which obscenities found their way into the Times. F-bombs are sprinkled throughout book excerpts, for example, and in web-only extras—quoting the poet Allen Ginsberg, in the case of a 2007 blog post. The word “fuck” also appeared in the full text of the Starr Report, which detailed President Bill Clinton’s sexual relationship with a 22-year-old White House intern, Monica Lewinsky, and which the Times printed in 1998. The report included, for instance, a quote from Lewinsky saying she wished the president would “acknowledge ... that he helped fuck up my life.” In a separate story that day, the paper described how the graphic language in the report was making things challenging for newscasters, in particular. “On CBS, Bob Schieffer looked profoundly embarrassed as he read cold from the report,” the Times wrote. Another Clinton-era curse word that made it into the paper? “Dumb-ass,” which Rolling Stone had mistakenly quoted Clinton as having said, a dispute that the Times covered.
It isn’t so easy to track the vulgarities the Times has printed, however, in part because it has used text-reading software to digitize much of its archival material. On one hand, this is why the newspaper’s archival presentation is so impressive. But it’s also why a search for any given curse word is liable to turn up a ton of false positives. To a computer, for instance, the 1975 headline “Court Shift on Sanity Debated” scans as “Court Shit on Sanity Debated.” Which is funny, sure, but not actually what the paper printed at the time. The Times online archive is full of this sort of thing.
The newspaper’s reporters have not infrequently written themselves into contortions to avoid foul language. “Barnyard expletive” is a favorite cop-out. (Personally, I prefer “baloney” if you want to get cute about it, but maybe that’s just me.) Mostly, they end up describing unsavory words in vague terms like “a vulgarity that refers to part of the male anatomy” or “a vulgarism for a part of the female anatomy.” Countless Times articles about George Carlin, the comedian who was famous for his bit about the “seven words you can never say on television,” dutifully avoided printing them. (A blessing, perhaps, in the YouTube age, as they’re best delivered by Carlin himself.) “A Master of Words, Including Some You Can’t Use in a Headline,” one article’s headline cheekily acknowledged.
These days, the paper tends to find creative workarounds for foul language. In previous eras, however, they’ve avoided covering a story altogether on account of vulgarity. A 1901 story described a trial with testimony that was “of such a character” that the Times could not print it. (They may not have had a choice; the Times noted that the papers in London, where the trial was underway, had refused to publish the testimony first.)
In a 2007 story about a hardcore punk band, the writer Kelefa Sanneh laid out clearly what it would take for the Times to print the group’s colorful name. “Well, the name won’t be printed in these pages,” Sanneh wrote, “not unless an American president, or someone similar, says it by mistake.”
“I made a mistake in trusting in a reporter,” Scaramucci tweeted on Thursday night. “It won’t happen again.”



Elon Musk and Mark Zuckerberg are having a spat about whether or not artificial intelligence is going to kill us all.
Musk, the chief of Tesla and SpaceX who has longstanding worries about the potentially apocalyptic future of AI, recently returned to that soapbox, making an appeal for proactive regulations on AI. “I keep sounding the alarm bell,” he told attendees at a National Governors Association meeting this month. “But until people see robots going down the street killing people, they don’t know how to react.”
In a Facebook Live broadcast, Zuckerberg, Facebook’s CEO, offered riposte. He called Musk a “naysayer” and accused his doomsday fears of unnecessary negativity. “In some ways I actually think it is pretty irresponsible,” Zuck scolded. Musk then retorted on Twitter: “I’ve talked to Mark about this. His understanding of the subject is limited.”
Seeing the CEOs of publicly traded tech companies go at it like Tay and Kanye is unfamiliar territory. Open sneers between public figures is normally reserved for tabloid socialites or feuding celebrities. But this is 2017—the president attempts to enact policy via Twitter, after all—so expectations must be adjusted. Rappers and reality-television stars feud because their prosperity is directly yoked to their public image. That’s true for tech business leaders now, too. Musk and Zuckerberg aren’t engaged in a debate about ideas. They are peacocking their personal identities in order to serve their future interests.
* * *
I’ve argued before that “artificial intelligence” has become so overused that the term is almost meaningless. Like “algorithm” before it, technologists, businesspeople, and journalists wield the idea like a magic wand that turns ordinary computer software and devices into world-saving (or world-ending) marvels. And given AI’s long history of wonder and dread in science fiction, people are primed to expect it to usher in utopia or dystopia.
When a term has a wealth of possible meanings, it is easy to ascribe one’s favorite meaning to it. “Disruption” is like this, as is “fake news.” The term “climate change” is now used by the right and left alike for opposite purposes. The Republican talking-points pollster Frank Luntz advocated for it over “global warming” to the G.W. Bush administration, because it sounded less severe. Change can be good, the reasoning goes.
Artificial intelligence has left the orbit of computer science, and even science fiction, and become an abstract talking point. When people make use of it, especially powerful actors like Musk and Zuckerberg, it serves a perlocutionary function: as personal branding.
When it comes to personal brands, Musk’s is easier to characterize. He’s long been compared to Tony Stark, the fictional industrialist and alter ego of Iron Man in Marvel comics. After Musk sold his first company, an online publishing service called Zip2, to AltaVista for $307 million in 1999, he co-founded X.com, which was eventually renamed PayPal and sold to eBay for $1.5 billion in 2002. Musk’s PayPal partner Peter Thiel turned to venture investing with the spoils, but Musk decided to make space rockets instead and SpaceX was born. His subsequent ventures, including electric/autonomous car maker Tesla, solar-cell manufacturer SolarCity, the Hyperloop tube-transit concept, and the new, associated tunneling-equipment firm The Boring Company—all of these ventures represent infrastructural invention of the Tony Stark variety.
As the statistician Mark Palko recently noted, Musk has a material interest in maintaining the Tony Stark alter-ego persona. When Musk waxes futuristic on self-driving cars, underground transit, brain-embedded computers, or Mars colonies, he reinforces the current and future value of his various ventures.
Portraying AI as an existential threat to humanity is consistent with this interest. If intelligent machines might strip humanity of its unmatched leverage over the natural and artificial environment, then industrial solutions must be pursued in order to stop them. Even if the threat of a robot apocalypse is unlikely, Musk has reason to advocate for aggressive contingency plans.
It’s difficult to match Zuckerberg’s business persona to a specific comic-book hero (Peter Parker? Reed Richards?). But unlike Musk, Zuck’s business and personal interests reside at the level of ideas rather than materials. Facebook is his singular venture, an enormously successful company that deals entirely in digitized text, images, video, and sound. These are representations—ideas and concepts—rather than concrete goods.
When Zuckerberg has looked beyond these immaterial representations, he has always done so in order to corner the market on more opportunities for symbol-creation and dissemination. Facebook’s purchase of Instagram and WhatsApp offer examples. And Zuckerberg’s big hardware acquisition, the VR-headset maker Oculus, represents a new terrain for virtual experience, not a new means of taming cities, continents, or the cosmos.
From this vantage point, software is always friendly and tame—or at least domesticable. Zuckerberg has billions of users and millions of advertisers who want to reach them, and terror about the future of computers only alienates those ordinary people from the friendly future he hopes to deliver to them. Zuckerberg learned this lesson the hard way, when his demonstration of a home-grown AI for his house, which he named Jarvis, was met with sneers and mockery. His recent tour of ordinary people and places in the United States shows just how completely he learned this lesson. The man is newly serious about reinforcing computing as a friendly, or at least innocuous, force on everyday life.
Especially given Facebook’s undeniable impact on the 2016 election—a feat that is hardly benign, but which Zuckerberg seems to have defused expertly anyway. This is why Zuckerberg is an “optimist,” as he puts it, when it comes to artificial intelligence. To say otherwise would suggest that computers are intrinsically risky. That fear, even if hypothetical, has potentially dire consequences for Zuckerberg’s business and personal future.
* * *
When figures like Musk and Zuckerberg talk about artificial intelligence, they aren’t really talking about AI—not as in the software and hardware and robots that might produce delight or horror when implemented. Instead they are talking about words, and ideas. They are framing their individual and corporate hopes, dreams, and strategies. And given Musk and Zuck’s personal connection to the companies they run, and thereby those companies’ fates, they use that reasoning to help lay the groundwork for future support among investors, policymakers, and the general public.
On this front, it’s hard not to root for Musk’s materialism. In an age when almost everything has become intangible, delivered as electrons and consumed via flat screens, launching rockets and digging tunnels and colonizing planets and harnessing the energy of the sun feel like welcome relief. But the fact that AI itself is an idea more than it is a set of apparatuses suggests that Zuckerberg might have the upper hand. Even if it might eventually become necessary to bend the physical world to make human life continuously viable, the belief in that value starts as a concept, not a machine.


Toddlers crave power. Too bad for them, they have none. Hence the tantrums and absurd demands. (No, I want this banana, not that one, which looks identical in every way but which you just started peeling and is therefore worthless to me now.)
They just want to be in charge! This desire for autonomy clarifies so much about the behavior of a very small human. It also begins to explain the popularity of YouTube among toddlers and preschoolers, several developmental psychologists told me.
If you don’t have a 3-year-old in your life, you may not be aware of YouTube Kids, an app that’s essentially a stripped-down version of the original video blogging site, with videos filtered by the target audience’s age. And because the mobile app is designed for use on a phone or tablet, kids can tap their way across a digital ecosystem populated by countless videos—all conceived with them in mind.
The videos that surface on the app are generated by YouTube’s recommendation algorithm, which takes into account a user’s search history, viewing history, and other data.* The algorithm is basically a funnel through which every YouTube video is poured—with only a few making it onto a person’s screen.
This recommendation engine poses a difficult task, simply because of the scale of the platform. “YouTube recommendations are responsible for helping more than a billion users discover personalized content from an ever-growing corpus of videos,” researchers at Google, which owns YouTube, wrote in a 2016 paper about the algorithm. That includes many hours of video uploaded to the site every second of every day. Making a recommendation system that’s worthwhile is “extremely challenging,” they wrote, because the algorithm has to continuously sift through a mind-boggling trove of content and instantly identify the freshest and most relevant videos—all while knowing how to ignore the noise.
And here’s where the ouroboros factor comes in: Kids watch the same kinds of videos over and over. Videomakers take notice of what’s most popular, then mimic it, hoping that kids will click on their stuff. When they do, YouTube’s algorithm takes notice, and recommends those videos to kids. Kids keep clicking on them, and keep being offered more of the same. Which means video makers keep making those kinds of videos—hoping kids will click.
This is, in essence, how all algorithms work. It’s how filter bubbles are made. A little bit of computer code tracks what you find engaging—what sorts of videos do you watch most often, and for the longest periods of time?—then sends you more of that kind of stuff. Viewed a certain way, YouTube Kids is offering programming that’s very specifically tailored to what children want to see. Kids are actually selecting it themselves, right down to the second they lose interest and choose to tap on something else. The YouTube app, in other words, is a giant reflection of what kids want. In this way, it opens a special kind of window into a child’s psyche.
But what does it reveal?
“Up until very recently, surprisingly few people were looking at this,” says Heather Kirkorian, an assistant professor of human development in the School of Human Ecology at the University of Wisconsin-Madison. “In the last year or so, we’re actually seeing some research into apps and touchscreens. It’s just starting to come out.”
Kids’ videos are among the most watched content in YouTube history. This video, for example, has been viewed more than 2.3 billion times, according to YouTube’s count:

You can find some high-quality animation on YouTube Kids, plus clips from television shows like Peppa Pig, and sing-along nursery rhymes. “Daddy Finger” is basically the YouTube Kids anthem, and ChuChu TV’s dynamic interpretations of popular kid songs are inescapable.

Many of the most popular videos have an amateur feel. Toy demonstrations like surprise-egg videos are huge. These videos are just what they sound like: Adults narrate as they play with various toys, often by pulling them out of plastic eggs or peeling away layers of slime or Play-Doh to reveal a hidden figurine.
Kids go nuts for these things.
Here’s a video from the YouTube Kids vloggers Toys Unlimited that’s logged more than 25 million views, for example:

The vague weirdness of these videos aside, it’s actually easy to see why kids like them. “Who doesn’t want to get a surprise? That’s sort of how all of us operate,” says Sandra Calvert, the director of the Children’s Digital Media Center at Georgetown University. In addition to surprises being fun, many of the videos are basically toy commercials. (This video of a person pressing sparkly Play-Doh onto chintzy Disney princess figurines has been viewed 550 million times.) And they let kids tap into a whole internet’s worth of plastic eggs and perceived power. They get to choose what they watch. And kids love being in charge, even in superficial ways.
“It’s sort of like rapid-fire channel surfing,” says Michael Rich, a professor of pediatrics at Harvard Medical School and the director of the Center on Media and Child Health. “In many ways YouTube Kids is better suited to the attention span of a young child—just by virtue of its length—than something like a half-hour or hour broadcast program can be.”
Rich and others compare the app to predecessors like Sesame Street, which introduced short segments within a longer program, in part to keep the attention of the young children watching. For decades, researchers have looked at how kids respond to television. Now they’re examining the way children use mobile apps—how many hours they’re spending, which apps they’re using, and so on.
It makes sense that researchers have begun to take notice. In the mobile internet age, the same millennials who have ditched cable television en masse are now having babies, which makes apps like YouTube Kids the screentime option du jour. Instead of being treated to a 28-minute episode of Mr. Rogers’s Neighborhood, a toddler or preschooler might be offered 28 minutes of phone time to play with the Daniel Tiger’s Neighborhood app. Daniel Tiger’s Neighborhood is a television program, too—a spin-off of Mr. Rogers’s—aimed at viewers aged 2 years old to 4 years old.
But toddlers and preschoolers are actually pretty separate groups, as far researchers are concerned. A 2-year-old and a 4-year-old might both like watching Daniel Tiger, or the same YouTube Kids video, but their takeaway is apt to be much different, Kirkorian told me. Children under the age of 3 tend to have difficulty taking information relayed to them through a screen and applying it to real-life situations. Many studies have reached similar conclusions, with a few notable exceptions. Researchers recently discovered that when a screentime experience becomes interactive—Facetiming with Grandmère, let’s say—kids under 3 years old actually can make strong connections between what’s happening onscreen and offscreen.
Kirkorian’s lab designed a series of experiments to see how much of a role interactivity plays in helping a young child transfer information this way. She and her colleagues found striking learning differences among what young children learned—even kids under 2 years old—when they could interact with an app versus when they were just watching a screen. Other researchers, too, have found that incorporating some sort of interactivity helps children retain information better. Researchers at different institutions have different definitions of “interactivity,” but in one experiment it was an act as simple as pressing a spacebar.
“So there does seem to be something about the act of choosing, having some kind of agency, that makes a difference for little kids,” Kirkorian says. “The speculative part is why that makes a difference.”
One idea is that kids, especially, like to watch the same things over and over and over again until they really understand it. I watched the Dumbo VHS so many times as a little kid that I would recite the movie on long car rides. Apparently, this is not unusual—at least not since the age of VCRs and, subsequently, on-demand programming and apps. “If they have the opportunity to choose what they’re watching, then they’re likely to interact in a way that meets their learning goals,” Kirkorian says. “We know the act of learning new information is rewarding, so they’re likely to pick the information or videos that are in that sweet spot.”
“Children like to watch the same thing over and over,” says Calvert, of Georgetown. “Some of that is a comprehension issue, so they’ll repeatedly look at it so they can understand the story. Kids often don’t understand people’s motives, and that’s a major driver for a story. They don’t often understand the link between actions and consequences.”
Young kids are also just predisposed to becoming obsessive about relatively narrow interests. (Elephants! Trains! The moon! Ice cream!) Around the 18-month mark, many toddlers develop “extremely intense interests,” says Georgene Troseth, an associate professor of psychology at Vanderbilt University. Which is part of why kids using apps like YouTube Kids often select videos that portray familiar concepts—ones that feature a cartoon character or topic they’re already drawn to. This presents a research challenge, however. If kids are just tapping a thumbnail of a video because they recognize it, it’s hard to say how much they’re learning—or how different the app environment really is from other forms of play.
Even the surprise-egg craze isn’t really novel, says Rachel Barr, a developmental psychologist at Georgetown. “They are relatively fast-paced and they include something that young children really like: things being enclosed and unwrapped,” she told me. “I have not tested it, but it seems unlikely that children are learning from these videos since they are not clearly constructed.”
“Interactivity is not always a good thing,” she added.
Researchers differ on the degree to which YouTube Kids is a valuable educational tool. Obviously, it depends on the video and the involvement of a caregiver to help contextualize what’s on screen. But questions about how the algorithm works also play a role. It’s not clear, for instance, how heavily YouTube weighs previous watching behaviors in its recommendation engine. If a kid binge-watches a bunch of videos that are lower quality in terms of learning potential, are they then stuck in a filter bubble where they’ll only see similarly low-quality programming?
There isn’t a human handpicking the best videos for kids to watch. The only human input on YouTube’s side is to monitor the app for inappropriate content, a spokesperson for YouTube told me. Quality control has still been an issue, however. YouTube Kids last year featured a video that showed Mickey Mouse-esque characters shooting one another in the head with guns, Today reported.
“The available content is not curated but rather filtered into the app via the algorithm,” said Nina Knight, a YouTube spokesperson. “So unlike traditional TV, where the content is being selected for you at a specified time, the YouTube Kids app gives each child and family more of the type of content they love and anytime they want it, which is incredibly unique.”
At the same time, the creators of YouTube Kids videos spend countless hours trying to game the algorithm so that their videos are viewed as many times as possible—more views translate into more advertising dollars for them. Here’s a video by Toys AndMe that’s logged more than 125 million views since it was posted in September 2016:

“You have to do what the algorithm wants for you,” says Nathalie Clark, the co-creator of a similarly popular channel, Toys Unlimited, and a former ICU nurse who quit her job to make videos full-time. “You can’t really jump back and forth between themes.”
What she means is, once YouTube’s algorithm has determined that a certain channel is a source of videos about slime, or colors, or shapes, or whatever else—and especially once a channel has had a hit video on a given topic—videomakers stray from that classification at their peril. “Honestly, YouTube picks for you,” she says. “Trending right now is Paw Patrol, so we do a lot of Paw Patrol.”
There are other key strategies for making a YouTube Kids video go viral. Make enough of these things and you start to get a sense of what children want to see, she says. “I wish I could tell you more,” she added, “But I don’t want to introduce competition. And, honestly, nobody really understands it. ”
The other thing people don’t yet understand is how growing up in the mobile internet age will change the way children think about storytelling. “There’s a rich set of literature showing kids who are reading more books are more imaginative,” says Calvert, of the Children’s Digital Media Center. “But in the age of interactivity, it’s no longer just consuming what somebody else makes. It’s also making your own thing.”
In other words, the youngest generation of app users is developing new expectations about narrative structure and informational environments. Beyond the thrill a preschooler gets from tapping a screen, or watching The Bing Bong Song video for the umpteenth time, the long-term implications for cellphone-toting toddlers are tangled up with all the other complexities of living in a highly networked on-demand world.

* Unlike YouTube’s main website, YouTube Kids does not use an individual child’s geographic location, gender, or age to make recommendations, a spokesperson told me. YouTube Kids does, however, ask for a user’s age range. The YouTube spokeswoman cited the Children's Online Privacy Protection Rule, a Federal Trade Commission requirement for operators of websites aimed at kids under 13 years old, but declined to answer repeated questions about why the YouTube Kids algorithm used different inputs than the original site’s algorithm.


On Monday, the editorial staff of Snopes.com wrote a short plea for help. The post said that the site needed money to fund its operations because another company that Snopes had contracted with “continues to essentially hold the Snopes.com web site hostage.”
“Our legal team is fighting hard for us, but, having been cut off from all revenue, we are facing the prospect of having no financial means to continue operating the site and paying our staff (not to mention covering our legal fees) in the meanwhile,” the note continued.
It was a shocking message from a website that’s been around for more than 20 years—and that’s become a vital part of internet infrastructure in the #fakenews era. The site’s readers have responded. Already, more than $92,000 has been donated to a GoFundMe with a goal of $500,000.
So, what’s going on? Well, it probably won’t surprise you that there’s a startup tech company and a lawsuit involved. There are claims and counterclaims. But if you want to know the gory details that are available in the court filings, here we go.
Snopes began in the early 1990s as a small website built by the husband-and-wife team of David and Barbara Mikkelson. Snopes was what you sent to your cousins who circulated crazy conspiracy theories from their Hotmail accounts. In 2003, the Mikkelsons  founded a parent company, Bardav, for the site.
All the way up into the 2010s, it had that look and feel, too, of a previous era of the internet. And perhaps because of that, its pronouncements on the veracity of subjects had a kind of authority that other media fact-checkers lacked. People, at least as many as possible in today’s crazed informational environment, trusted Snopes.
The founders divorced in 2015, some titillating details of which became public. Both founders received 50 percent of the company.
In the summer of that year, Bardav had entered into an agreement with a newish San Diego company called Proper Media to “provide content and website development services as well as advertising sales and trafficking” to Snopes. Proper Media’s principals were Chris Richmond, who co-owns a wiki called TV Tropes, and Drew Schoentrup, both now described in court filings as residents of Puerto Rico (more on that shortly).* Each of these men had a 40 percent share in the company. They were joined by three other people who had smaller equity stakes: Tyler Dunn, Ryan Miller, and Vincent Green.
In July 2016, Barbara Mikkelson sold her half of Bardav to these five men, leaving her ex-husband with five new partners in the company. Because Bardav was an S corporation, its shareholders had to be people, not other companies. So, the stock purchase agreement between Mikkelson and the men assigned them each equity on the same split that they had in Proper Media.
Diamond Creek Capital financed a big chunk of the deal with help from Barbara Mikkelson herself. Each of the five men in on the deal from the Proper Media side signed personal-liability notes with Diamond Creek Capital.
For a time, it seemed as if the arrangement was working out. The San Diego Union Tribune visited the Proper Media offices, out of which Snopes employees were working. The story featured Vincent Green, a former Marine who’d been an intern only months before, and Brooke Binkowski, the site’s managing editor and a long-time journalist.
“Before we came on board, there was not even a content-management system for the site,” Green told the paper. “It was an excruciating process for developing content. What you see now is our quick and dirty change-over from 20 years of bad code to something more responsive and functional.”
But behind the scenes, there was trouble. Proper Media’s CEO and president had moved to Puerto Rico, according to a cross-complaint filed by Green, and corroborated by their own filings. They set up a separate company there, which Green claims was a tax-avoidance scheme that he told them he was uncomfortable with.
Meanwhile, in a story as old as media, the site’s editors worried that the co-owners didn’t understand what Snopes was, and that they only wanted to juice its revenues, so they could sell it.
On February 18—in a much disputed series of events—Green and Proper Media’s largest shareholders, Richmond and Schoentrup, had a contentious meeting. In the weeks that followed, Green either left or was forced out, and he went to work at Bardav, which is to say Snopes, where he remains.
On March 10, in an action that Proper Media disputes, David Mikkelson canceled the contract that had been in place governing interactions between Bardav and Proper Media. Mikkelson claims that he had a right to do so as CEO and sole director. Proper Media says that he could not because it was understood that Drew Schoentrup was a director of the company as well, even though he had not been elected through a formal process.
Also in dispute are Green’s shares, which when combined with Mikkelson’s, would give the two of them putative control of the company. Proper Media contends that, more or less, the shares belonged to Green and he was just holding them on behalf of the company.
There are many other claims and counterclaims flying around the filings related to the lawsuit. It’s not interesting to go through all of them in detail, but what can be said: This is a mess.
Proper Media’s lawyer, Karl Kronenberger, told me that they’ve alleged that “David Mikkelson has engaged in gross financial, technical, and corporate mismanagement.” Mikkelson told me that Proper Media “continue to hold themselves out as authorized advertising representatives. They have continued to collect the revenue and they have not paid us any advertising revenue.”
What does the future hold for Snopes? That could become slightly more clear next Friday, when there is a hearing in San Diego to address competing motions. Mikkelson is seeking an injunction to force Proper Media to hand over control of the site. Meanwhile, Proper Media is seeking to remove Mikkelson as a director of Bardav.
In the meantime, it looks like the GoFundMe will at least cover the site running for a while longer, but based on conversations with those who know the site’s financial picture, Snopes’s operating expenses are close to $100,000 a month. If a resolution to the dispute isn’t reached soon, it could mean the end of both Proper Media and Snopes.
Which would be a terrible end for the kind of website that bracingly defied the logic of corporate digital media. It hadn’t pivoted to video. It was a site people trusted. It was technologically unsophisticated. It was profitable.
Stay tuned.
* This article originally stated that Chris Richmond founded TV Tropes. We regret the error.


You have to be a certain level of Twitter-obsessed to know what I mean when I say we’ve entered another Taye Diggs situation, so allow me to explain.
A few years ago, the actor Taye Diggs started following a lot of journalists on Twitter. And journalists being journalists—not exactly a humble bunch—started to casually brag to one another about it, only to find out that Diggs was following seemingly everyone on Twitter. (Same thing with the actress Melissa Joan Hart. Sorry to be the one to break it to you.)
It finally came out that Diggs was following so many random people (699,000 at last count) because he’d hired a “social network dude” to do it for him, as The Washington Post reported in 2014. In other words, it was a branding thing.
It’s fair to guess that something similar is going on with Anthony Scaramucci, the new White House communications director. Amid the news that Scaramucci would join the White House (and that press secretary Sean Spicer had resigned in protest), people began noticing that Scaramucci was following them on Twitter. More than a dozen of my colleagues at The Atlantic, at least, are among the followed.
Plus this person:
I just noticed that that scaramucci fellow is following me on these here twitters. 😯
And this one:
I'm confused. Not sure when Anthony Scaramucci began following me ?? pic.twitter.com/GCJL7Uh7lc
And this one:
just doing some research on anthony scaramucci and he's already following me? pic.twitter.com/j07JmPdc9x
And this one:
Jesus, @Scaramucci is following me...
But not this person:
At this point I'm kinda offended now that Scaramucci *isn't* following me tbh
What’s the advantage of following so many randos, one might ask? In some cases, it’s a strategy to increase a person’s follower count—operating on the principle that if you follow someone on a social platform, that person will follow you back. Get enough followers and you may even grab the attention of an algorithm that will suggest you as someone to follow. (Scaramucci is following 168,000 accounts; he has 487,000 followers.)
In other words, it’s a way of being opportunistic and garnering people’s attention—without actually ever engaging with them. So, the perfect strategy for someone in politics, a cynic might say.
Incidentally, Scaramucci is following me, too. So I sent him a message to ask about it. He didn’t write back.


Last year I fell in love with a toaster.
It looks like most others. A brushed, stainless-steel housing. Four slots, to accommodate the whole family’s bread-provisioning needs. It is alluring but modest, perched atop the counter on proud haunches.
But at a time when industry promises disruptive innovation, Breville, the Australian manufacturer of my toaster, offers something truly new and useful through humility rather than pride.
The mechanism that raises and lowers the bread from the chassis is motorized. After I press a button atop the frame, the basket silently lowers the bread into the device to become toast. On its own, this feature seems doomed to mechanical failure. But the risk is worthwhile to facilitate the toaster’s star ability: the “A Bit More” button. That modest attribute offers a lesson for design of all stripes—one that could make every designed object and experience better.
* * *
Toast is an imperfect art. Different breads brown at different rates. Even with the very same bread, similar toaster settings can produce varied results. When my bread doesn’t come up dark enough, I dial in a guess for another browning run. Usually I go overboard and burn the toast in the process. It’s toaster telephone game.
The “A Bit More” button enters here, at the friction point between good and great toast. When the toast reveals itself to me above the Breville’s chassis, I visually gauge its browness. If insufficient, I press the button, which actuates the basket motor. Down it goes for a brief, return visit to the coil. Then back up again, having been toasted, well, just a bit more.
The button also makes toasting bread, normally a quantitative act, more qualitative. The lever dials in numerical levels of browning, and the “A Bit More” button cuts it with you-know-what-I-mean ambiguity. That dance between numbers and feelings apologizes even for a slightly over-browned slice of toast by endearing the eater to the result the button helped produce.
Sure, I’m talking about toast. But Breville’s “A Bit More” Button is nothing short of brilliant. It highlights an obvious but still unseen problem with electric toasters, devices that have been around for more than a century. And then it solves that problem in an elegant way that is also delightful to use. It’s just the kind of solution that designers desperately hope to replicate, and users hope to discover in ordinary products. But agreeing on a method for accomplishing such achievements is harder.
The “A Bit More” Button was conceived by the industrial designer Keith Hensel, who worked for Sunbeam and then as Breville’s principal designer until his unexpected death in 2013, at the age of 47. His specialty was household products, like toasters, kettles, and blenders.
Breville’s head designer, Richard Hoare, tells me that Hensel, with whom he worked closely, fell upon the idea by “focusing on user empathy.” Hensel had been pondering the problem people have with toasters. “Your bread comes up too light, so you put it back down, then get distracted and forget, and it goes through a full cycle and burns,” Hoare relates. “Keith thought, why can’t the consumer have more control? Why can’t they have ‘A Bit More?’”
According to Hoare, the design team called the button by that name from the start. Some people within Breville thought it was too colloquial, and other options were considered. “Extra Darkness” was one, and “10% Extra” another. “These were confusing and clunky,” says Hoare. “In the end ‘A Bit More’ was the clearest.” Breville, which holds several patents in motorized toaster basket tech, started selling toasters with the feature in 2008.
When it came to persuading Breville to adopt and manufacture the idea, Hoare admits that it took some time for people to see the significance. I imagined that manufacturing cost and complexity might have been a factor, but Hoare waves that off. Instead, he tells me that describing features in a way ordinary users might, rather than by means of brand-speak, had sparked debate between design and marketing. The design team insisted that the colloquial version would resonate with users. “We have had so many say, ‘I love that it’s actually called A Bit More,’” says Hoare. “‘It was so cool when I saw it printed there next to the button.’”
* * *
The explanation elicits nods of respect. This, it seems, is how good design happens. But how would a designer, or a business, or even an ordinary person replicate the triumph of “A Bit More” in other contexts?
Hoare’s recollection corresponds with a trend in contemporary design practice—and one that claims to be particularly adept at producing outcomes like “A Bit More.” It’s called user-experience, or UX, design, a discipline that strives to craft pleasurable and useful encounters between people and things. Originally derived from human-computer interaction, or HCI, where user-interface design was its ancestor, UX purports to offer a general approach to design of all kinds, from software design to product design to architecture and urban planning.
But UX practice talks out of both sides of its mouth. On the one hand, UX fancies itself an empirical discipline. Its processes include ethnographic user research, specification drafting, iterative design, user testing, and so forth. UX inherits mid-century form-follows-function design ideals. It also embraces more recent trends, like participatory design, which deeply integrates stakeholders into the design process. Data are often incorporated into UX for affirming, denying, or directing elsewhere a design team’s attention.
On the other hand, UX design also privileges out-of-the-box genius to solve design problems. Apple, often considered to typify UX, is famous for conducting design in secret via a small cadre of geniuses. Steve Jobs is the ultimate example, a figure who held that “people don’t know what they want until you show it to them.” In the design-genius mentality, how a toaster (or smartphone, or building) ought to work becomes a type of soothsaying, whereby the designer earns the status of mastermind. Research becomes retrospective justification, the designer’s ingenuity validated by user adoption of the product—irrespective of how well it really serves their goals or interests.
Neither polarity of UX-style design really helps explain how one might best arrive at Breville’s “A Bit More” button. On one side is intuition. Keith Hensel, the genius who died too soon, possessed a sixth sense for taming the Maillard reaction and a congenial manner for proselytizing his solution. On the other side is evidence, via the research and participant observation conducted to cash out the “user empathy” Hoare cites as a compass bearing.
UX proponents tell tall tales about how good design really takes place. Bottom-up, evidentiary design implies that the designer is ultimately unnecessary, a mere facilitator who draws out a solution from the collective. The designer becomes a bureaucrat. And top-down, genius design becomes indistinguishable from salesmanship. As a result, design dissolves into other, more established disciplines like business intelligence, product marketing, and corporate evangelism. It’s an error that makes good design look far easier and more replicable than it really is. And worse, it allows people to conclude that their own expertise—from data analytics to advertising to illustration—is a sufficient stand-in for design.
* * *
The error comes from two assumptions. The first holds that human beings are the only arbiters of designed objects and experiences. A design is good if it works for humans, even if individual humans might disagree about what that means. In this conception, design is a purely instrumental process, carrying out human will with ever-increasing refinement.
But design-by-genius proves that this approach is often flawed. Jobs was right: People don’t seem to know what they want, or need. Who could imagine verbalizing even the relatively simple challenge of toasting such that the “A Bit More” button would emerge as a remedy? Some UX proponents have realized this trap and reframed their design practice as a dialectic between the gifted and the rabble. But soon enough, the resulting compromises again feel like a smorgasbord of every possible influence, from management to technology to sociology. Design becomes a nickname for any possible approach to design, which reduces it to a shibboleth for designers.
The second erroneous assumption is that contemporary designers believe they are reformers. Agents of change. It could be social or political change. It could be aesthetic or cultural change. It could be the selfish change of professional aspiration and its related station. It could be the change associated with progress. Designers are ambitious sorts of folk—arrogant, even—and none would want to be associated with stasis, or even with mere cyclicality. What a waste, just to mow lawns or brown bread every day! Let us instead reinvent lawn care! Let us reinvent breakfast!
But neither Hensel nor Hoare adopt such hubris over toasters. Breville’s toaster is not remarkable because it reinvents toasting, nor because it resolves an obvious user grievance, nor because it changes the world. What makes the device compelling is that it makes toasting what it always was, but even more so. Everything remains the same, but ever so slightly adjusted. Trim-tab-tweaked to more adeptly sail the current tack, the same one all bread-browners have navigated since the advent of the electric toaster.
Counterintuitively, design is a process conducted for the benefit of human use, but which takes place outside the domain of human experience. Unlike politics, entertainment, health, and other domains that also impact human life directly, design exerts force on the material world before the human one. (Design shares this quirk with engineering, but engineers are fortunate enough to work at technical depths, where ordinary people can’t see.)
To design a toaster (or a building, or a smartphone) effectively, the designer must depart the realm of the human, at least temporarily, and enter the universe of the object. Despite what the UX folk might say, designers are not diplomats negotiating treaties between humanity and toasterdom. Instead, they are explorers who breach the alien frontier of toasterness. Certainly, that cosmos does not exclude the human agents who would brown bread for sustenance. But it does not rely upon them exclusively, either. Toasting is a practice bigger than people and toasters both, otherwise it would be possible without one party or the other.
* * *
Allow me to indulge an analogy from philosophy. In both the genius and consensus registers, UX design predicates its success on knowledge: either the second sight of the designer, or the negotiated consensus of the user. Philosophers call the study of knowledge epistemology, and this approach to design is entirely epistemological. Just find the proper knowledge and the right design will emerge.
But when conducted best—including in Breville’s case, and despite Hoare’s insistence otherwise—design is more related to the philosophy of what things are, called ontology. It is a discipline of essence, that great bugbear of contemporary life, not of knowledge. Pursuing greater compatibility with a thing’s essence requires that the designer focus on the abstraction formed by the designed object and its human users together—whether it be toasting, dwelling, publishing, socializing, or anything else.
The designer’s job is not to please or comfort the user, but to make an object even more what it already is. Design is the stewardship of essence—not the pursuit of utility, or delight, or form. This is the orientation that produces solutions like the Breville “A Bit More” button. The design opportunities that would otherwise go unnoticed emerge not from what people know about or desire for toasting, but from deeply pursuing the nature of toasting itself.
The distinction might seem academic. Isn’t the UX-oriented toaster designer simply seeking deeper knowledge of the essence of toasting? Doesn’t the designer-genius simply have special, immediate access to that deep nature? Indeed, that’s possible. But mostly through accident. The designer who starts from the problem of knowledge will only ever find essence by happenstance. And that is why so few things feel like the “A Bit More” button. They are struggling so hard to meet requirements, please people, or exalt the arbitrary choices of their creators that they fail to pay respect to the thing in question—and thereby to see what it is, even, let alone to change it for the better.


Photography owes much of its early flourishing to death. Not in images depicting the aftermath of violent crimes or industrial accidents. Instead, through quiet pictures used to comfort grieving friends and relatives. These postmortem photographs, as they are known, were popular from the mid-19th through the early-20th centuries—common enough to grace mantelpieces. Many can be viewed anew at online resources like the Thanatos Archive.
Historians estimate that during the 1840s, the medium’s first decade, as cholera swept through Britain and America, photographers recorded deaths and marriages by a ratio of three to one. Budding practitioners had barely learned to handle the bulky machinery and explosive chemicals before they were asked to take likenesses of the dead: to bend lifeless limbs into natural poses and mask tell-tale signs of sickness, racing against rigor mortis.
Many people find photos of the dead creepy or morbid. No question, postmortem photographs are sorrowful images. They capture the ravages of illness. They depict grieving parents. They show wives caressing the faces of lost husbands, just for a chance to be tender toward them one last time. And they portray unbearably beautiful children, poised as if asleep, surrounded by the toys they played with while alive. But today, the sorrow of these images lies elsewhere: in treating pictures of the dead like obscenities rather than as memento mori.
* * *
Photography extended the centuries-old traditions of death masks and mortuary paintings, which commemorate the dead by fixing them in an illusion of life. But compared to these earlier media, photographs possessed an almost magical verisimilitude. “It is not merely the likeness which is precious,” wrote Elizabeth Barrett Browning of a postmortem portrait, “but the association and the sense of nearness involved in the thing ... the very shadow of the person lying there fixed forever!” For many, procuring a postmortem photo must have felt like a funerary ritual—a way of allowing the dead to become fully dead. But this new invention also had something of resurrection about it. It animated a body, astonishing viewers each time they gazed upon it.
During the 1840s and early 1850s, a postmortem photo would likely have been the first and only portrait of someone. At $2 each (roughly $60 today), photographs were costly, and in America’s open expanses, studios were miles away from most households. But death changes things. People who had never given a thought to the medium now turned to it in desperation. Decades later, in trade journals like The Philadelphia Photographer, veteran practitioners wrote of how parents would arrive at their doorsteps with stillborn infants, to whom they hadn’t even given a name. “Can you photograph this?” implored one young mother, opening a wooden basket to reveal “a tiny face like waxwork.”
Almost all the postmortem photographs from this period are daguerreotypes. The dominant mode of photography for its first 15 years, the daguerreotype was rendered on a copper sheet burnished to look like a mirror. When held at the right angle, a grieving widow would have seen her image meld with that of her husband, a striking reunion after death. Daguerreotypes were produced as three-dimensional objects, meant for the hand as much as the eye. They came in small cases of leather or ebony, opened by a delicate handle. Inside, the image lay cuddled in velvet. Like tiny reliquaries, daguerreotypes kept safe the image of one’s beloved. They kept other things, too, like a baby’s silken curl or a piece of a girl’s ribbon.
Many postmortem pictures show parents cradling their children, or wives alongside their deceased husbands. The corpse figures prominently, but so do the shattered expressions of those left behind. A surprising number of fathers appear—at this time, men could openly admit their grief. There are parents so young they look like children themselves. Many subjects make trembling attempts at self-composure.
Rituals help the living overcome the desire to die with the dead. As a ritual, postmortem photography helped check grief. By pressing subjects to execute specific poses and gestures, death photos helped the living externalize personal loss. The faces of many mourners evidence the struggle. How else to interpret a daguerreotype of a mother lying next to her child?







Many photographs from the 1840s and ’50s depict a corpse posed in a semblance of sleep. The convention makes death look easy and gentle—a rest from labor. “It has a heavenly calm in it,” the English author Mary Russell Mitford remarked of her father’s cast in 1842. But this conceit has an ulterior motive: to trick the viewer into believing that death is sleep, no metaphor about it. Consider the image above, of a boy who bears no trace of decay in his luscious round face. And yet for every photo like this one, a dozen more exist in which photography’s irrepressible realism exposes the charade, in the form of fever sores or sunken eyes. Such images mix comfort with a kind of cruelty.
Postmortem daguerreotypes are piercingly intimate. They bring the viewer close enough to the face of the dead to see a boy’s long lashes, or a girl’s spray of freckles. Many were taken at home. No props here: These are the chairs the dead once sat in, the toys their living bodies held. It is in these daguerreotypes especially that we discover what the French critic Roland Barthes called the “punctum” of a photograph: the accidental element that “wounds” a viewer with its poignancy. In a daguerreotype labeled “Our Darling,” for example, the humble detail of the girl’s dirty fingernails reveals the truth of every postmortem photograph: the life that the dead left behind.
* * *
Beginning in 1851, daguerreotypy gave way to the wet collodion process, which made photography cheaper, faster, and reproducible. The medium soared in popularity, and the market for postmortem photography expanded. As it did, the aspirations for postmortem photos also rose. By the 1860s, death photos began explicit attempts to animate the corpse. Dead bodies sit in chairs, posed in the act of playing or reading. In one striking tintype dated 1859, a young boy perches on a seat, eyes open, holding a rattle. A close look reveals a wrinkle on the left side of the backdrop: a clue that someone, most likely the photographer’s assistant, is propping the child up. In a cabinet card from the 1890s, a young girl holds a plaything in one hand and a doll in the other. Parents and photographers engage in a nostalgic game of make-believe. But the dead children refuse to play along, looking more inanimate, somehow, than their toys.
This slide into sentimentality, even if grotesque, coincides with a profound shift in Western attitudes toward death. The 1870s witnessed the advent of a religious upheaval in America and Western Europe. Traditional arguments about immortality lacked the weight they carried only a few decades earlier, especially among the middle and upper classes. Accounts of death during this period no longer expressed the piety and spiritual fervor of earlier times.
No wonder, then, that the effort to tame and beautify death in daguerreotypes collapsed in the late 19th century. In its place, a confusion of approaches appeared. Some postmortem photos still portrayed peaceful, domestic images of the dead. But the faces in those images are mostly Latin American, Eastern European, and working class. It was a sign, perhaps, that these groups possessed a deeper faith in God—or in photography.
Meanwhile, members of the white middle-class began to procure photos of themselves in mourning, no corpse in sight. Many of these subjects are women, attired in black crepe. They weep into handkerchiefs, or turn their backs to the camera. The photograph’s earlier stoicism gives way to the performance of grief, as if melodrama were supplanting faith. Other mourning photographs foreground the act of remembrance. Bereaved ones stand or sit next to portraits of the dead, recalling the anthropologist Nigel Clark’s comment that in an age of disbelief, death has nowhere to go but memory.











Beginning in the 1890s, postmortem photography turned toward burial. No pretense at life here: just death, flat and absolute, marked by coffins and cemeteries and a community that carries on. Reproduced on postcards, these images traveled to distant friends and relatives. They became vulnerable to the postman’s stamp and other desecrations. The postmortem photograph had devolved from a near-sacred object to a formality, a social obligation. By the mid-1920s, it disappeared from public view, defeated by Kodak and its happy promotion of snapshot photography. Underneath photography’s new lively glee, however, the fear of death quietly smoldered. Photographic reminders of it began to be judged as obscene.
Every so often, postmortem photography experiences a brief resurgence. The organization Now I Lay Me Down to Sleep, for example, recruits volunteer photographers to take images of stillborn or dead infants for grieving parents. And a few years ago, it was a trend among teenagers and 20-somethings to take selfies at funerals. “Caskies,” they coined them. These trends hardly became mainstream, earning more reproach than approval.
* * *
The dead help the living face what lies ahead. In exchange, the living must translate the lives of the dead into history. They can find myriad ways to do so, from visiting gravesites to writing someone’s biography. But photography has become so commonplace that images of death have lost most of their original meaning.
Many postmortem photographs are hard to look at. They’re too graphic or too desperate in their attempts to simulate life. But others provide an almost visceral connection to the past. Visiting the Thanatos Archive, I linger over the faces of the bereaved, remembering what it feels like to lose someone you love. I learn the names of the dead before me: Odie, Sulisse, Viola. I discover the strange ways people die (brain fever, an accidental swallowing of coyote poison) and the all-too-familiar ways they do (cancer, an accidental gunshot). And I surrender to my own fears of dying. I see, as if in palimpsest, my demise in these portraits of strangers, and I recognize that mortality connects us all.
This article appears courtesy of Object Lessons.


Marion Tinsley—math professor, minister, and the best checkers player in the world—sat across a game board from a computer, dying.
Tinsley had been the world’s best for 40 years, a time during which he'd lost a handful of games to humans, but never a match. It's possible no single person had ever dominated a competitive pursuit the way Tinsley dominated checkers. But this was a different sort of competition, the Man-Machine World Championship.
His opponent was Chinook, a checkers-playing program programmed by Jonathan Schaeffer, a round, frizzy-haired professor from the University of Alberta, who operated the machine. Through obsessive work, Chinook had become very good. It hadn't lost a game in its last 125—and since they’d come close to defeating Tinsley in 1992, Schaeffer’s team had spent thousands of hours perfecting his machine.
The night before the match, Tinsley dreamt that God spoke to him and said, “I like Jonathan, too,” which had led him to believe that he might have lost exclusive divine backing.
So, they sat in the now-defunct Computer Museum in Boston. The room was large, but the crowd numbered in the teens. The two men were slated to play 30 matches over the next two weeks. The year was 1994, before Garry Kasparov and Deep Blue or Lee Sedol and AlphaGo.
Contemporary accounts played the story as a Man vs. Machine battle, the quick wits of a human versus the brute computing power of a supercomputer. But Tinsley and Schaeffer both agreed: This was a battle between two men, each having prepared and tuned a unique instrument to defeat the other. Having been so dominant against humans for so long, Tinsley seemed to thrill at finally having some entity that could give him a real game. He had volunteered to play friendly matches against the computer in the run-up to their two world championship matches. And Schaeffer, though he was a bull-headed young man, had become the most effective promoter of Tinsley’s prowess and legacy.
But there, in that hall, a quirk of human development was troubling Tinsley. His stomach hurt. The pain was keeping him up all night. After six games—all draws—he needed to see a doctor. Schaeffer took him to the hospital. He left with Maalox. But the next day, an X-ray revealed there was a lump on his pancreas. Tinsley understood his fate.
He withdrew. Chinook became the first computer program in history to win a human world championship. But Schaeffer was crushed. He’d devoted years of his life to creating a program that could beat the best checkers player ever, and just as he was about to realize this dream, Tinsley quit. Seven months later, Tinsley died, never having truly lost a match to Chinook.
And that would lead Schaeffer to undertake a 13-year computational odyssey to exorcise the man’s ghost. With Tinsley gone, the only way to prove that Chinook could have beaten the man was to beat the game itself. The results would be published July 19, 2007, in Science with the headline: Checkers Is Solved.
“From the end of the Tinsley saga in ’94–’95 until 2007, I worked obsessively on building a perfect checkers program,” Schaeffer told me. “The reason was simple: I wanted to get rid of the ghost of Marion Tinsley. People said to me, ‘You could never have beaten Tinsley because he was perfect.’ Well, yes, we would have beaten Tinsley because he was only almost perfect. But my computer program is perfect.”
* * *
Jonathan Schaeffer did not begin his career intending to solve checkers. He was a chess player, first. Good, not great. But he also loved computers—and had done a Ph.D. in computer science, so he decided to build a chess-playing program. He called it Phoenix and it was one of the better chess programs of many that were created in the 1980s. In 1989, he “crashed and burned” at the World Computer Chess Championships, however. At the same time, the team that would form DeepBlue, the chess software that would eventually defeat Kasparov, was coming together. Schaefer realized he would never build the world computer champion.
A colleague suggested that perhaps he should try checkers, and thrillingly, with just a few months of work, his software was good enough to bring to the Computer Olympiad in London to compete against other checkers-playing bots. And it was there that he began to hear about Marion Tinsley, the great.
At the highest levels, checkers is a game of mental attrition. Most games are draws. In serious matches, players don’t begin with the standard initial starting position. Instead, a three-move opening is drawn from a stack of approved beginnings, which give some tiny advantage to one or the other player. They play that out, then switch colors. The primary way to lose is to make a mistake that your opponent can jump on.
This would seem to make checkers a game amenable to computer play. That was certainly the idea back in the mid-1950s, when an IBM research scientist named Arthur Samuel began to experiment with getting a checkers-playing program to run on an IBM 704. He worked on the problem for the next 15 years or so, publishing several important papers on what he called—and what we all would now call—“machine learning.”
Machine learning is the underlying concept for the current wave of artificial intelligence. The descendants of that early work now promise to revolutionize whole industries and labor markets. But Samuel’s programs never had much success against actual humans. In May 1958, several members of the Endicott Johnson Corporation Chess and Checkers Club trounced the computer, much to the delight of the Binghamton Press and Sun-Bulletin.
“The human brain, sometimes lost sight of in an age of satellites, frozen foods, and electronic data processing machines, returned to former glories early today,” the paper said. “The 704, Dr. Samuel explained, does not think. What it does, he said, is to search its ‘memory,’ stored on tape, of checkerboard situations it has encountered previously. Then it rejects choices which have turned out badly in the past and makes moves which turned out well.”
This still would work pretty well as a description of what’s known as “reinforcement learning,” one of the basket of machine-learning techniques that has revitalized the field of artificial intelligence in recent years. One of the men who wrote the book Reinforcement Learning, Rich Sutton, called Samuel’s research the “earliest” work that’s “now viewed as directly relevant” to the current AI enterprise. And Sutton also happens to be one of Schaeffer’s colleagues at the University of Alberta, where Google’s DeepMind AI shop recently announced it will set up its first international research office.
While his techniques were groundbreaking, 10 years later Samuel had made, in his own words, “limited progress” on the problem, though he’d kept working at it at IBM, while visiting at the Massachusetts Institute of Technology, and then with a Department of Defense grant at Stanford University. As with so many of the currently hot AI techniques, he just did not have the computational horsepower or datasets that he needed to make the beautiful ideas work.
So when Schaeffer began building his own software, he went straight back to Samuel, and found that he could not exactly follow Samuel’s path. He’d have to build a new system. At first they called it The Beast, but eventually they named it Chinook after the warm winds that sometimes blow through Alberta.*
The work consumed Schaeffer. As he described it in his 1997 book, “Sometimes, when I had difficulty getting to sleep, I would fantasize about the exhilaration that I would experience when Chinook finally defeated the Terrible Tinsley.” His wife would interrupt his reveries asking, “You’re thinking about him again, aren’t you?”
As the software developed through the years, it retained two core components. The first is easy to understand: It’s a “book” of complete computations of every possible checkers position with a certain small number of pieces on the board. So, if there were six pieces left—and as time went on, seven, then eight—Schaeffer’s software knew every possible combination. Back in the early ’90s, it took vast amounts of computer time to be able to run all those calculations.
But as Schaeffer and his team did more and more computation, Chinook got better and better. It started to be able to beat people. But they knew that they couldn’t calculate every possible position.
The rules of checkers are simple, but the number of potential moves is massive—there are 5 x 1020 possibilities. Schaeffer has an analogy to help people understand how ridiculous that is: Imagine draining the Pacific Ocean and then having to fill it back up with a tiny cup, one at a time. The sea is the number of checkers possibilities. The cup is each calculation.
The second component of the system is a little trickier to grasp. Chinook needed to search through possible moves beginning with the start of a match. Like many similar systems, Chinook would look ahead many possible moves and then try to score each permutation’s desirability. At first, Chinook could only look ahead 14 to 15 moves out, but as computers and the software improved, it could look further and further. “As with chess, deeper was always better,” Schaeffer told me.
In late 1990, the American Checker Federation allowed Chinook to play in the U.S. Championships. The software went undefeated and played Tinsley to a draw six times. That earned the software the right to challenge Tinsley for the world championship.
And after Chinook’s 1990 performance, Tinsley called up Jonathan Schaeffer and asked if he might like to play a few friendly matches.
* * *
From 1950 to 1990, Tinsley had been the world champion of checkers whenever he wanted to be. He’d occasionally retire to work on mathematics or devote himself to religious study, but he’d eventually return, beat everyone and become champion again. In that 40-year span, he lost five total games and never once dropped a match.
Derek Oldbury, probably the second best player of all time, wrote an encyclopedia of checkers. It was effusive in its praise for the master: “Marion Tinsley is to checkers what Leonardo da Vinci was to science, what Michelangelo was to art and what Beethoven was to music.”
It’s hard to know what to make of Marion Tinsley from the perspective of the 21st century. He seems otherworldly or, at the very least, othertimely. His life was composed of checkers, mathematics, and his abiding faith. He was kind, by literally every account, and yet his style of play was relentless and aggressive.
It is not uncommon to say that someone lived a “life of the mind,” but in Tinsley’s case both the life and the mind were unusual. For years, as an undergraduate and then graduate student at the Ohio State University, he spent eight hours a day on checkers. He never married. “I haven’t seen a checker marriage that worked out,” he told a reporter. “It is a very rare woman who can be married to a real student of checkers.” His mother lived with him well into the 1980s. A 1993 profile in The Philadelphia Inquirer found Tinsley in a big, blue sweater over a shirt and tie. His lunch was “a jar of milk, half an apple, and a peanut butter sandwich.”
His relationship with the racial dynamics of the South is also fascinating. “I had thought of going to Africa as a self-supporting missionary," he told Sports Illustrated in 1992, "until a sharp-tongued sister pointed out to me that most people who wanted to help blacks in Africa wouldn't even talk to blacks in America.”
Instead, he became a lay minister at a predominantly black church and left Florida State University’s math department to teach at the historically black Florida Agricultural and Mechanical University. He spent 26 years there. A yearbook from towards the end of his time there showed a deep and lively campus life in which Tinsley might have been the only white person over the age of 40. No contemporary accounts ask any of the black students what they made of their checkers-champion professor, but a colleague was able to describe Tinsley’s role in a local obituary: “At his retirement dinner, literally everybody; young, old, black, white, students, faculty members ... gave testimonies about the impact he had had on their lives.” For a man of his time and upbringing in Kentucky, his path seems almost miraculous.
One thing is for sure: Tinsley was a genius. His genius had been refined and whittled into a strange and wonderful shape. He was the very best at this one thing and pretty ordinary in all else. One part of him had become almost like an artificial intelligence—narrow but extraordinarily capable—while the rest of him lived this simple human life.
When a reporter visited him in Tallahassee, Florida, in 1993, lavender azaleas lined the driveway leading up to his two-story brick house. His home had bare white walls. A room upstairs contained his checkerboard and heavily used checkers books. Tinsley liked to sit in a velour La-Z-Boy. He could never quite explain what checkers meant to him—why he’d pored over sequences of moves for almost his entire life, why he’d kept a magnetic checkerboard by his bedside to work out new combinations. It was something close to godly, though.
“Checkers is a deep, simple, elegant game,” he once said. Playing another human great was “like two artists collaborating on a work of art,” Tinsley said another time.
And then there is his most quotable line: “Chess is like looking out over a vast open ocean; checkers is like looking into a bottomless well.”
It was as if the checker moves were Scriptures that he could endlessly meditate on and understand in new ways. “Out of the clear blue sky an improvement of a published play will just come to mind, as if the subconscious has been working to come to light,” he told the Chicago Tribune in 1985. “A lot of my discoveries come that way, out of the clear blue sky. Some of my insights into the Scriptures come the same way.”
* * *
When Tinsley came to Edmonton in 1991 to play the friendly matches against Chinook, Schaeffer was also blown away that the world champion would agree to play this computer for fun.
The two men sat in his office and began the matches, Schaeffer moving for Chinook and entering changes in the game into the system. The first nine games were all draws. In the tenth game, Chinook was cruising along, searching 16 to 17 moves deep into the future. And it made a move where it thought it had a small advantage. “Tinsley immediately said, ‘You’re gonna regret that.’” Schaeffer said. “And at the time, I was thinking, what the heck does he know, what could possibly go wrong?” But, in fact, from that point forward, Tinsley began to pull ahead.
“In his notes to the game, he later wrote that he had seen all the way to the end of the game and he knew he was going to win,” Schaeffer said.
The computer scientist became fixated on that moment. After the match, he ran simulations to examine what had gone wrong. And he discovered that, in fact, from that move to the end of the game, if both sides played perfectly, he would lose every time. But what he discovered next blew his mind. To see that, a computer or a human would have to look 64 moves ahead.
“I was absolutely stunned,” Schaeffer told me. “How do you compete with somebody whose understanding of the game is so deep that he immediately knows through experience or knowledge or doing some amazing search that he was gonna win that position?”
Schaeffer still struggles to make sense of Tinsley’s incredible skill. When he wrote his book about the saga, One Jump Ahead, Schaeffer received a letter from Tinsley’s academic supervisor. It read, Schaeffer told me, “that he was an exceptionally talented person and he was capable of doing one thing brilliantly. If it wasn’t checkers, he probably would have been a brilliant mathematician.”
When someone’s motivations are not fame or money, we seem to require a higher level of explanation, some emotional engine the rest of us do not possess. The closest Tinsley ever came to describing his motivations came in the 1993 Philadelphia Inquirer feature. He was an introvert who “felt unloved” by his parents, who he thought doted on his sister. To gain their approval, he competed at math and spelling bees. “And as a twig is bent, it grows: As I grew up, I still kept feeling that way.”
* * *
The hunger to excel propelled Tinsley to college at 15, where he really discovered the passion that would dominate his life. He won his first world title in 1955.
And in 1992, he agreed to put his title on the line in the first Man-Machine World Championship against Chinook. The match was sponsored by the computer manufacturer Silicon Graphics and held in London. “I can win,” Tinsley told The Independent. “I have a better programmer than Chinook. His was Jonathan, mine was the Lord.”
For the two weeks leading up to the event, another world-class player, Don Lafferty, trained with him down in Tallahassee, going over their matches and reviewing positions deep into the night.
The 1992 games were held at the Park Lane Hotel, which had hosted world chess championships as well as the Computer Olympiad that had been Chinook’s public debut two years earlier. The room was large and two-storied, with a balcony that overlooked the players and the refrigerator-sized computer that was running Chinook.
Schaeffer and Tinsley sat across from each other, and a large screen rendered the movement of the pieces. Tinsley drew first blood, besting Chinook in game five. But then in game eight, Chinook delivered a stunning win; it was Tinsley’s sixth loss in 40 years.
Despite the years of toil and dreams of success, Schaeffer felt sadness in that moment. “We’re still members of the human race,” he wrote in his book, “and Chinook defeating Tinsley in a single game means that it will only be a matter of time before computers will be supreme in checkers, and eventually in other games like chess.” Schaeffer might have won, but the humans have lost.
After a series of draws, Chinook won again in game 16. No living player had ever defeated Tinsley more than once. Incredibly, almost unbelievably, the software was on top. They were on the verge of making computing history.
Then, in an episode that Schaeffer still finds too painful to describe, Chinook had some sort of error, which forced them to resign the game, tying the match up. “Tinsley viewed it as God helping him out,” Schaeffer said. “It was a religious experience for Tinsley and one of the most devastating experiences of my life.”
Schaeffer and Chinook were never able to get back on top.Tinsley came from behind to win the match, retaining his title.
“I think, if I can keep my health, I don’t believe there will ever be a computer that can beat me,” Tinsley told CNN after the match.
* * *
All of which set the table for the 1994 matchup in Boston.
In the run-up, Chinook had played Derek Oldbury and trounced him. Shortly thereafter, Oldbury died. “Chinook plays Oldbury. Chinook beats Oldbury. Oldbury dies,” Tinsley joked with Schaeffer. “He must have died of Chinookitis!”
Schaeffer was not amused. He was a young man programming computers to play an old man’s game. The best players were dying off. And it was, in the eyes of some in the checkers world, a bit untoward for this guy with his fancy machines and code to come around beating up on the frail masters.
As he’s gotten older, Schaeffer has come to wish that he’d appreciated Tinsley more during their entanglement. “I also look back at my time with Marion Tinsley, not realizing how fleeting it would be and how much I appreciated it. I didn’t take advantage of that and reap the benefits of a warm and deep friendship that I had with him,” he told me. “I look back at it and with hindsight; I can say that I am disappointed in myself. I wish I would have done things differently, but at the time, when you’re obsessed, you can only see in one direction.”
Even in his narration of taking Tinsley to the hospital, Schaeffer cannot get over the idea that the older man’s illness might prevent him from having the chance to beat him. Tinsley says, “I’m ready to go,” and Schaeffer acts befuddled. The old man knew he was dying, and the younger didn’t get it.
After Tinsley resigned, Don Lafferty took his spot and dueled with the computer, but Schaeffer’s sights became set on the larger prize of solving checkers. Human checkers players came to despise Schaeffer, by his own telling. They sent him letters trash talking him and his software. He released One Jump Ahead in 1997, and still checkers players kept coming for him.
From 1997 to 2001, he suspended his project to solve the game, which meant creating a computer program that always knew the right move. It would be unbeatable. It would be perfect.
When he returned, his team expanded the endgame database—Chinook’s perfect knowledge—to any situation where there were 10 pieces or less on the board. That’s 39 trillion positions.
They continued improving Chinook’s “best-first” search strategy, which allowed them to do a fraction of the calculations necessary to compute every single possible play in the game. It took Schaeffer harnessing computers all over the world, drawing on and expanding his expertise in parallel computing. He conscripted machines everywhere from Switzerland to Lawrence Livermore National Laboratory, a major Department of Energy facility that often deals with nuclear weapons.
“There was somebody else there [running a program] called BOMB and I was running checkers programs,” Schaeffer told me. “It was a very strange situation. Security should have been concerned.” And they were. They paid him a visit after discovering gigabytes of data flowing out of a national lab to Edmonton, Alberta.
Through the years, the computers cranked away. As Samuel’s machines had before him. As future DeepMind machines would. This was a narrow kind of intelligence to develop, but it was part of the long arc of the development of artificial intelligence, which is expected to revolutionize the world within this generation.
Schaeffer saw what he was doing as the completion of Samuel’s initial dream. After his team’s success at the 1990 U.S. championships, he reached out to the old IBM man to share the news. The unhappy word came back: Samuel had just died, one of the oldest computer programmers still working. Born in Emporia, Kansas, in 1901, his last login to Stanford’s computer network was recorded in February of 1990. He’d seen the diffusion of electricity, the release of the Model T, two world wars, humans on the moon, and the very first glimmers of machine intelligence.
Finally, in 2007, Schaeffer was able to announce in Science that after working on checkers for 19 years, he had solved the game. The forward search had met the endgame database somewhere in the middle, like an AI transcontinental railroad, with the high-profile publication as Schaeffer’s golden spike. His team had figured out the sequences for 19 of the 300 tournament openings, but as it turned out, those 19 were all that were needed to prove that the game, played perfectly, amounts to a draw.
* * *
The game of checkers grinds on. The sport has been officially conquered by computers, but the wave of artificial intelligence has long moved on to more difficult challenges, leaving the humans to beat up on each other in places like the Honeysuckle Inn and Conference Center in Branson, Missouri. There are still tournaments and champions and even some prize money. There’s a group of young Italians who are challenging for global dominance.
Schaeffer thinks he’s laid checkers to rest himself. He’s working on a book about the history of computer chess that’s coming out later this year. He’s a dean at University of Alberta. “I’ve tried to move on and exorcise some of the ghosts of the story, but it keeps coming back,” he tells me. “I guess it will be part of me until the day I die.”
Before he does, though, he has one more checkers item on his to-do list. “I’d like to make a pilgrimage to Ohio and visit his grave,” Schaeffer said.
The grave is marked by a simple headstone: Marion F. Tinsley. In the upper right corner, there’s a checkerboard. In the upper left, a Scriptural reference, Hebrews 13:1: “Let brotherly love continue.”
Followed by: “Be not forgetful to entertain strangers: for thereby some have entertained angels unawares.” Imagine Schaeffer, once one of those strangers, completing his pilgrimage and looking down at this grave, as if down a well. For Tinsley, the spiritualist, the metaphor of checkers as a well without end was both poetic and true. But Schaeffer, the engineer, knew that no well is bottomless. And humans will always sound the depth.
* This article originally stated that the Chinook winds blow through Edmonton. We regret the error.


In Ralph Waldo Emerson's famous essay on self-reliance, the 19th-century writer and naturalist sang the praises of spiritual isolation and the evils of distraction, bemoaning the forces that conspired to direct his attention to "emphatic trifles." He would not be cowed, he said, but would stand resolute in the face of such bad influences: "The power men possess to annoy me, I give them by a weak curiosity ... If we cannot at once rise to the sanctities of obedience and faith, let us at least resist our temptations."
Don't tell Ralph about Twitter.
I joined Twitter in 2009 at the urging of my husband, who works in technology. "What am I going to do, tell the internet what I ate for breakfast?" I asked him. Eight years later, I'm the one checking Twitter over my morning toast while he gets ready for work. Twitter has become the place where I get my news, where I check in on my friends, where I go to make jokes and read good essays. As a lifelong sufferer of anxiety, it is where I go to talk about what I’m feeling when I’m anxious, and maybe find some camaraderie. And as a lifelong sufferer of anxiety, using Twitter is also making my anxiety worse. The like-minded community I’ve built on Twitter has made confessing anxiety easier than ever, but the comparison Twitter enables has made the experience of anxiety worse. And when it comes to Twitter, you have to take the good with the bad.
Psychologists typically distinguish between two types of anxiety: trait anxiety, a persistent and lasting tendency to experience fear and worry; and state anxiety, a temporary response of fear to a threatening situation. Many forms of social media can agitate both trait and state anxiety, and perhaps none more so than Twitter, which reminds the perpetually anxious that we always have something to be anxious about and instills a sense of anxiety in even the most laid-back user. Twitter’s constant flow of new information and the fact that users tend to follow people who are more accomplished and successful than they are creates an especially potent cocktail of comparison for anxious people. "Twitter really inflames my professional anxiety," says Caitlin Cruz, a freelance journalist based in New York. "But it's also given me a lot of professional success." Cruz deleted the Twitter app from her phone a few weeks ago, which she says has made her life more bearable.
Twitter users have to contend with competing voices that yell at you as soon as you log on. You haven't written a best-selling novel yet? Here's a “30 Under 30” list of best-selling novelists! You're over 30? Here's an article about how you're a bad parent! You haven't had children yet? This bestselling author has three, and she's under 30! Twitter is a megaphone for achievements and a magnifying glass for insecurities, and when you start comparing your insecurities with another person's achievements, it's a recipe for anxiety.
"Generally speaking, the comparisons that we make on social media are more likely to be 'upward' comparisons," says Azadeh Aalai, a professor of psychology at Montgomery College in Maryland. "We're comparing ourselves to the individuals who appear to be higher status and are achieving more" than we are, which can lead to feelings of envy, discontent, and anxiety. It's also not the whole story. When I was young, my mom used to warn me against "comparing my insides to other people's outsides." Using Twitter, I am constantly comparing my insides—my anxieties, fears, and insecurities—with other people's outward selves: their accomplishments, polished selfies, and edited articles. There will always be someone who’s doing better than I am in any aspect of my life. And because I, like many people, tend to follow people I admire or who are already famous, I am constantly aware of just how much better other people are. Twitter also gives me a quick and handy way to quantify my worth: this many likes, this many retweets. I'd like to think I'm more than the sum of my followers, but there are plenty of days when I don't feel that way.
Anxiety functions by constantly reminding you to pay attention to it. And so does Twitter. Twitter draws users back for more and more and more. Smartphones are designed to provide instant gratification, and many of Twitter's features depend on our biological fear of scarcity, says Pamela Rutledge, the director of the Media Psychology Research Center. The push notifications, the little number next to our mentions, the bar that tells us how many tweets have been sent since we last refreshed the page—all of these details are designed to keep users coming back, afraid that we might have missed something vital. "Social media doesn't really promote moderation," Aalai says (in what could perhaps be the understatement of the year).
The desire to know what is going on at every moment is quenched when met with the firehose of information that is Twitter. But my anxiety skyrockets when I’m met with the seemingly endless amount of bad news about tragic events going on around the world—ISIS bombings, systemic racism, refugees in crisis, the threat of war, political upheaval. Many Twitter users I surveyed cited feeling powerless in the face of overwhelming fear as one of the biggest causes of their anxiety. Even if it does offer the occasional practical solution—donating to the International Rescue Committee, calling a congressperson, sharing a GoFundMe—Twitter remains dominantly focused on the world's ills in a way that can decimate a person's sense of efficacy and replace it with profound despair.
If Twitter is full of bad news and anxiety-inducing fodder for comparison, why are we there in the first place? Some people, like the writer Lindy West, have left Twitter altogether due to harassment and trolling, while the New York Times columnist Bret Stephens just announced he would leave Twitter because it had become "pornified politics," although he isn't really leaving—"I'll keep my Twitter handle, and hopefully my followers," he wrote, and an editorial assistant will update the profile for him. But the rest of users are there, presumably, because they find some value amid the constant updates and jokes and hot takes. Twitter provides a sense of camaraderie.
Twitter provides a platform for neurotic people to share their fears. And for those of us who work from home or on the road, Twitter becomes an office space and the people we interact with become our coworkers. A recent Harvard University study found that "the act of disclosing information about oneself activates the same part of the brain that is associated with the sensation of pleasure"—the same pleasure center that is activated by food, money, and sex. Confessing my anxiety on social media, then, is an attempt not to feel so alone. Anxiety isolates the people who suffer from it, convincing them that they are the only ones who think in this distorted way. Bringing this kind of myopic thinking into the light and examining it can help combat it, and Twitter can actually be a useful place for doing just that. "You're anxious? Me, too!" is the kind of rallying cry that unites anxious people. But even as we find our tribe of fellow worriers, the question remains: What are we using Twitter for?
Rutledge encourages Twitter users to think about why they're online. "If you're checking Twitter a hundred times a day, what are you avoiding doing?" she asks. "That's where you need cognitive override," or the ability to step out of the moment at hand and evaluate how realistic your feelings are given your use of this technology. "When we're anxious, we feel compelled to be continually scanning the environment," Rutledge says. "That's how we make ourselves feel safe." It's what our ancestors did to anticipate attacks from enemies or saber-toothed tigers, but the advantage now isn’t quite as clear. Assuming we live in a world that is connected enough that we won't completely miss important news, there isn’t a real need to be constantly scanning the feed, looking for threats.
The cycle of anxiety on Twitter use can be especially bad for women, non-binary and queer people, and people of color. "Vulnerable populations in face-to-face interactions are similarly going to be vulnerable in virtual interactions," says Aalai. These are often people who benefit greatly from Twitter because they can speak directly to the friendly audience who follows them, cutting out the potential for harassment they might receive in other places. But trolls follow, too: A 2014 Pew study shows that 25 percent of women ages 18–24 have been sexually harassed online (as opposed to 13 percent of young men), and 23 percent have been physically threatened. Fifty-one percent of African-American and 54 percent of Hispanic internet users had experienced some form of harassment online, as opposed to 34 percent of white internet users.
"You have to make a conscious decision about whether Twitter is still adding value," Rutledge says. The difficult part is that "value" is entirely subjective, and it's hard to make (good) decisions when our brain isn't working at full capacity. A recent study from the University of Chicago found that "the mere presence of one’s own smartphone reduces available cognitive capacity." And a recent New Republic article asked journalists whether they could live without Twitter. The answer was uniformly "no," although many people acknowledged that life without Twitter would be "better." It reminds me of the apostle Paul's words about sin in the letter to the Romans: "I do not understand my own actions. For I do not do what I want, but I do the very thing I hate."
In 1855, the poet Walt Whitman sent Ralph Waldo Emerson a copy of his newly published collection of poetry, Leaves of Grass. "I find it the most extraordinary piece of wit and wisdom that America has yet contributed," Emerson wrote to him. "I greet you at the beginning of a great career, which yet must have had a long foreground somewhere, for such a start." Emerson saw in Whitman’s moving poetry the long and careful career of devoted practice that had gone before.
Some years later, Twitter CEO Jack Dorsey said Leaves of Grass was one of the most influential books in his career, comparing its “efficiency” to great programming. It didn’t seem to strike Dorsey as ironic that Whitman took years to craft the efficiency of language that Dorsey praised. Dorsey called Whitman a “total entrepreneur,” looking, as many of us do, for the presence of his own values in the person he admired. And that is one of the reasons people are drawn to Twitter—it gives them access to the inner lives of people they would otherwise never interact with. But in so doing, they may also start to fear that they will never become the person they want to be—never be as smart or prolific or original or beautiful as the composite of people they follow. That gap is where anxiety thrives.
In the meantime, I'm itching to know what's going on in the world. Who knows what's happened since I started writing? Is there some new political scandal? Has someone tweeted something outrageous? How am I adding up to the people I follow? I know I could wait. I could go for a walk, or read a book, or take a bath. But I think I'll check. Just one more time.


Every year, billions of dollars change hands in needlessly clumsy ways. Parents realize they’re short on cash and go out of their way to stop at an ATM so they can pay their babysitter; grandparents mail checks as birthday gifts, which take days to arrive and days to clear. Even as more and more of life is lived through a screen, paper is still how the vast majority of Americans give each other money.
How Apple Pay Gets People to Part With More of Their Money
In the past few years, a handful of tech companies have recognized these inefficiencies, introducing apps—such as Circle Pay, Square Cash, and Venmo—that let users transfer money to one another’s bank accounts using their phones, relatively frictionlessly. Among other things, they let users enter their bank-account information and then transfer money to others who have done the same. With Venmo, one of the more popular of these services, there is an additional wrinkle: Once money is transferred, the exchange shows up in the app’s social feed, a running record of who went out for drinks with whom, or whose roommate pays the electricity bill each month. (Users can elect to make a transfer private, but most don’t.) The app has among many—mostly young, city-dwelling people—attained a level of linguistic uptake reserved for the likes of Google and Uber: “Just Venmo me,” they say, after picking up a dinner bill.
The feature that sets Venmo apart is the social feed, which brings transparency to a class of transactions that used to be entirely private. The feed—an emoji-laden stream of often-indecipherable payment descriptions and inside jokes—seems frivolous; it is not a social-media destination in the way that Facebook or Twitter is. But as a public record, it is quite revealing of social dynamics—who’s hanging out with whom, and perhaps where. A friend of mine told me that Venmo proved invaluable in trying to determine if her ex and his new girlfriend were still dating.
In other words, pointless and goofy as it seems, people do pay attention to what they see in Venmo’s feed. And there’s actually a way this parade of public transactions might give the app a significant advantage over its many competitors.
The reason, says Richard Crone, who runs a payments-focused firm called Crone Consulting, has to do with how Venmo makes money—or, more precisely, how it will make money. Currently, Venmo doesn’t directly generate all that much revenue for the company that owns it, PayPal. (Contrary to what some of its users may have guessed, the app doesn’t make money “on the float”—that is, by investing whatever funds users keep as a positive balance in their Venmo accounts.)
Things could look different not too long from now. Venmo’s plan, which it has already initiated and will expand in the coming year, is to facilitate more transactions between businesses and their customers. Last summer, Venmo introduced partnerships with about a dozen apps (including the food-delivery service Munchery and the fast-food chain White Castle) that now let users pay straight from their Venmo accounts. The idea, Crone explains, is that Venmo would take a cut—its standard rate is 2.9 percent plus a small flat fee, which is at the higher end of what merchants pay for a typical credit-card transaction—of not just in-app purchases like these, but of in-person transactions at physical checkout counters, where customers spend trillions of dollars a year.
This is where the social feed comes in. “You walk into any retailer, any restaurant, any service provider—what do they want you to do? Like them on Facebook, follow them on Twitter,” Crone says. Working with retailers would give Venmo a business model similar to credit-card issuers and processors—“but with much more upside,” he says, “because the retailers spend far more trying to get you to like them on Facebook and follow them on Twitter and all these other things that they could just get as a byproduct of the payment.” That is, if someone paid for a taco using Venmo, their friends might see where they ate lunch.
One limit on this strategy’s effectiveness is that consumers might not be eager to publicize every one of their purchases. But Venmo is aware of this: In the year or so since it started trying its service out with a few businesses, the default setting has been for payments not to be shared in the social feed. Still, Bill Ready, PayPal’s chief operating officer, recently told Barron’s that when the initiative is expanded, “social aspects will be not only present, but also be what’s most attractive to our users.” And Fast Company has reported that a job-application prompt for prospective Venmo employees last year mentioned research finding that “Venmo users are more open to purchasing at new businesses … that they learn about from friends on Venmo.”
But the other, even more lucrative aspect of becoming merchants’ preferred means of payment is access to information about where customers are spending their money. “The real value is in the data, and the ability to render customized ads and offers, and generate a revenue stream from that,” Crone says. “We estimate that the value of mobile payments per enrolled active account is worth more than $400 per year in revenue, to whoever does it—Venmo, Apple Pay, Android Pay, Samsung Pay, a bank, Visa, or Mastercard."
If Venmo or another service were to gain access to this payment data, the typical recipients of it would start missing out. Even though digital-payments apps are built on top of banks’ infrastructure, banks wouldn’t see the details of consumers’ spending, but instead just requests from an app to add or withdraw money from an account. Referring to the value of owning the platform that consumers directly interact with, Crone says, “The one who enrolls is the one who controls”—a phrase he went on to repeat five times in one conversation I had with him.
Crone thinks that banks are worried about Venmo’s ambitions. Banks have rightly recognized that convenience, affordability, and ease of use are not characteristics that appeal uniquely to 20-somethings, and so they have in recent years collaborated on a payment platform that does more or less what Venmo does. The product of that collaboration, called Zelle, began showing up last month on the screens of tens of millions of Americans who use mobile-banking apps on their phones.
Zelle differs from Venmo in three important ways. The first is that Zelle appears within users’ banking apps, as opposed to being an app all its own. The second is a consequence of the first: Because Zelle was developed by banks and appears in their apps, transfers will register in users’ bank accounts in minutes, whereas with Venmo, that currently takes days. (Zelle’s association with banks is also a selling point when it comes to security, which Venmo has in the past gotten some bad publicity for, but has since seemed to have gotten under control.) Third, Zelle lacks Venmo’s performative social component.
And, technically speaking, Zelle is not new—the name and the bright purple buttons reading “Send,” “Request,” and “Split” are just the consistent branding given to a payments platform called clearXchange that banks have been using, under other names, for years. In fact, last year, clearXchange processed about $175 million in payments per day, while Venmo’s daily rate was only $54 million. Still, the pre-Zelle clearXchange, which wasn’t given a consistent name or look across banks, left room for more-intuitive apps like Venmo to claim a chunk of the market. They had plenty of time to do that, given how long it took for more than 30 banks, normally in competition with each other, to cooperate and release a cohesive product.
Looking to the future, Crone notes that Venmo’s social feed is something that banks would by their nature have trouble reproducing. “You expect your bank to keep things private,” he says. “With Venmo, you expect them to publicly post.” (When I talked to Melissa Lowry, the vice president of marketing and branding at Early Warning, the industry group that helped develop Zelle, she said that Zelle could add a social component down the line, if banks decided that was something they wanted.)
Of course, because of its reputation for security and privacy, Zelle is arguably much better positioned than Venmo to handle business-to-consumer disbursements, such as when, instead of sending a check, an insurance company transfers a customer money for making car repairs, or when a market-research firm compensates people for participating in a focus group. ClearXchange has already been handling such payments, and it’s not hard to imagine people using Venmo at a fancy restaurant to broadcast their good taste and Zelle for payments that say less about their social status than they do about mundane financial chores.
And at the moment, Venmo’s ability to capitalize on its social dimension is far from certain. When I asked Talie Baker, a senior researcher at Aite Group, about what Crone observed, she wrote in an email, “I think it’s a good point he makes that companies won’t mind the transaction fees with the built-in advertising BUT, consumers LOVE their credit cards.” (That said, PayPal appears ready to work with credit-card companies in order to gain a foothold at physical checkouts.)
An additional wrinkle, of course, is Venmo depends on banks’ payment systems in order to function, so it might be hesitant about encroaching too much on their revenue streams. But if Venmo started getting a larger market share than the banks are comfortable with, would they try to do something about it? “That'd be a pretty dangerous and bold move,” Crone says. He adds that in some ways the banks actually don’t mind that services like Venmo have gotten bigger, because they have the banks processing payments that they wouldn’t have processed otherwise. “If they are not able to position themselves in the social sphere of payments,” he says, “then they'll still participate in the back-end plumbing."
After the announcement last month of Zelle’s impending rollout, a number of media outlets presented it as a direct competitor to Venmo. Reuters called it banks’ “answer” to Venmo. Engadget said that Zelle is “tak[ing] on” Venmo. Bloomberg went so far as to wonder if the app was a “Venmo killer.”
In truth, the dynamic between the two payment platforms will not be quite so oppositional; paper money and checks are still so prevalent that Zelle and Venmo could both gain immense popularity and still not run up against each other for many years. Crone thinks the most likely outcome is that the various companies involved with payments will end up cooperating as much as they compete. “There won’t be one mobile payment to rule them all,” he says. As much competition as there currently seems to be, the future that both Crone and Baker foresee for digital payments leaves plenty of room for emoji—and more ☮️  than ⚔, at that.


Imagine a group of volunteers, their chests rigged with biophysical sensors, preparing for a mission in a military office building outfitted with cameras and microphones to capture everything they do. “We want to set up a living laboratory where we can actually pervasively sense people, continuously, for a long period of time. The goal is to do our best to quantify the person, the environment, and how the person is behaving in the environment,” Justin Brooks, a scientist at the Army Research Laboratory, or ARL, told me last year.
ARL was launching the Human Variability Project, essentially a military version of the reality-TV show Big Brother without the drama. The Project seeks to turn a wide variety of human biophysical signals into machine-readable data by outfitting humans and their environment with interactive sensors.
The Army is not alone. The Air Force, Marine Corps, Navy, and their special operations forces are also funding research to collect biophysical data from soldiers, sailors, Marines, and pilots. The goal is to improve troops’ performance by understanding what’s happening inside their bodies, down to how their experiences affect them on a genetic level. It’s not exactly genetically engineering soldiers into superhero Captain Americas; the U.S. military insists they have no intention of using biometric data science for anything like the genetic engineering of superior traits. But it’s close. The military is after the next best thing.
If today’s Pentagon leaders get their way, the next generation of fighter jets, body armor, computer systems, and weapons will understand more about the pilots, soldiers, and analysts using them than those operators understand about the machines they are using. The very experience of flying the plane, analyzing satellite images, even firing a gun could change depending on what the weapon, vehicle, or software detects about the person to whom the weapon is bound. To make this dream real, Pentagon-backed researchers are designing an entirely new generation of wearable health monitors that make Silicon Valley’s best consumer fitness gear look quaint. They’re discovering how to detect incredibly slight changes in focus, alertness, health, and stress—and to convey those signals to machines. Design the boots well enough and the super soldier will arrive to fill them.
Army Research Laboratory researchers already monitor individual subjects from six months to two years. Brooks wants to expand that to other military training environments, such as the U.S. Military Academy at West Point, New York, and then to more than a dozen universities. He hopes the data will reveal how people of varied size, weight, height, health, level of alertness, etc., differ in terms of the signals they send out—hence the name “human variability.” That, in turn, will help researchers gather much more precise information on how different people interact with their environment. The ultimate goal is sensors that can tell the Pentagon how each human soldier performs, or could perform, to their best ability, from battlefield to home front.
“It’s not just while they’re at work, but also when they go on leave,” says Brooks. “This is continuous, with the highest practical resolution that we can obtain for a long period of time. Hopefully, we would see information going into many programs” to build future gear. “A greater understanding of natural human variability would then feed pretty much any system that adapts to the person.”
It’s an ambitious undertaking, considering the current limitations of body-worn sensors. Over the past two years, the military bought more than $2 million worth of Fitbits and other biomedical tracking devices. But it turns out that off-the-shelf consumer devices aren’t good enough for the military’s biotracking ambitions. So researchers are creating a new class of wearables, based on new research into embedding electronic components into fabric. If the electrodes are too small, the signal is worthless; too big, and they feel like an artificial electric shell separating the wearer from the real world. The connection between the environment and the human must remain seamless.
One application for such sensors is helmets that record brain activity while their wearers do their jobs. An ARL team is preparing for continuous electroencephalography, or EEG, by using 3-D printing to create helmets that fit perfectly to each individual soldier’s head. But the military is not eager to embed wires and metal into gear that’s meant to protect a soldier during a massive blast. So the lab is constantly looking at new materials, solutions, and tradeoffs, inching toward sensors that collect information without getting in the way of soldiering. Lab technicians showed me one experimental electrode that they were making that was so small and soft to the touch it seemed to have no metal in it at all (they are in fact constructed of nanofibers that conduct electricity, encased in silicon).
* * *
The Air Force, as well, needs a next generation of wearables to help tomorrow’s combat aircraft understand their pilots. Modern fighter jets expose human bodies to physical forces that are still not entirely understood. In 2010, multiple F-22 pilots reported in-flight episodes of confusion, shortness of breath, and skin-color changes—all symptoms of hypoxia, or decreased oxygen in the blood. The reason was speed.
“I pull a G in the airplane, blood has a tendency to collect in some of those dependent areas of the body, like the arms and legs and that,” said Lloyd Tripp, a program manager for aerospace physiology and toxicology at the Air Force Research Laboratory’s 711th Human Performance Wing. Two years later, the Air Force began to affix sensors inside the helmets of F-22 pilots to read the blood-oxygen level of their temporal artery.
Around the same time, the Russian military was also seeing confusion and skin-color changes among their pilots who pulled high G-forces, Tripp said. Lacking the same sensor technology, Russian commanders began to give pilots blood transfusions before their flights. It didn’t work. Russian pilots flying at supersonic speeds suffered hypoxia at greater rates. “They didn’t actually admit that for quite a few years,” he said. Correct diagnoses enabled the U.S. Air Force to read the problem and improve performance.
Beyond helmets, Air Force researchers are working on what they call a comprehensive cognitive monitoring system. This means exploring what sensor technologies work well for what purposes, and what signals can be detected without interfering with or disturbing the pilot—who is, after all, supposed to be flying a combat mission. Depending on what you seek to measure, they found, you may no longer need a physical sensor on the body. You can now collect incredibly intimate and important internal health data with cameras.
Take cerebral oxygenation, the amount of oxygen in the tissue of specific portions of a pilot’s brain. You can measure this key biophysical signal by shining infrared light on the forehead because the blood in front of the skull is about as oxygenated as the brain tissue behind the skull wall. “If I’m shining that infrared light through the skin, I can see the amount of oxygen within the blood in that tissue. As I increase G-force, I’m decreasing the amount of oxygen that I have here and that decrease in oxygen is directly correlated back to decreases in cognitive function,” said James Christensen, a portfolio manager with the 711th Human Performance Wing.
Another research project configured simple laptop-camera lenses to detect whether a person’s hemoglobin is oxygenated, which makes blood shows up slightly redder, or de-oxygenated, which is slightly bluer. Essentially, this lets you read a person’s heart rate from a distance.
Even your breath says something about your physical state. “The ratio between oxygen and carbon dioxide will change as I become more and more fatigued. That’s important because as I’m fatigued, it takes about 24 hours for me to actually recover 100 percent,” Christensen said. “That fatigue is important because my muscles can’t strain to push the blood back to my head and so the probability of me losing consciousness increases significantly.”
Good sensors can even detect changes in metabolism that indicate weariness and stress before the person notices. When you’re stressed, you exhale fat—or rather, water-soluble molecules called ketones that your liver produces from fat. Stress is detectable by the molecular content of your breath.
“We’re working with some folks over at our materials lab and they have a couple of companies that are looking at sensors that are going to be placed in the [pilot’s oxygen] mask that’ll look at those types of fatigue-related volatile organic compounds,” says Christensen.
Your eyes, too, give you away. “Imagine eye-tracking cameras,” Christensen said. “If those can collect not just the motion data and the eye-motion data, but those are also getting heart rate and respiration, then we can have no hardware on you at all and still get all the same physiological metrics ... A certain amount of cognitive workload tends to correlate pretty highly with stress generically. You can combine heart rate with several other measures to get at workload stress; vigilance, even.”
“We are comparing it, just for reference, with wet medical electrodes on the chest. Under most conditions, you can do about as well as wet electrodes,” he said. The lab is “testing the limits of how far away can you get and still get a reliable signal. It turns out, it’s mostly an optics problem.” That means cameras and lenses alone can detect those subtle changes in stress and attention. It’s just a matter of figuring out which ones.
There are privacy ramifications to collecting so much information. A simple camera can gather enough biometric data on an individual to understand how small changes in heart rate can be a sign of stress. For a fighter pilot, an analyst, or a soldier, this might help warn of decreased cognitive ability. But among the general population, stress can also be a signal of deception, depending on the context in which that stress expresses itself, such as an interview at a checkpoint. Today’s military-funded biophysical research shows that it’s possible to detect that stress response from 100 meters away, and perhaps even at longer distances. In theory, if you could create a lens that could capture infrared data at sufficient resolution (currently, only a theoretical possibility), you could measure brain tissue oxygenation from low-earth orbit. You could see stress from space.
When performed without a subject’s awareness or permission, biophysical monitoring can be a violation of privacy. But conducted as part of an experiment with knowing volunteers, like elite soldiers eager to understand their bodies and improve their own performance, it becomes a powerful tool. One former special operations training psychologist, who currently works for a major league baseball team, said the elite soldiers he had served with were eager to improve their performance through data. In the Air Force, pilots want to improve how they fly, complete their missions, interact with their equipment, etc.
Bit by bit, this science is making its way into actual gear and weapons. In the year 2020, Navy SEAL teams and Army Rangers could take down high-value targets while wearing an exoskeleton that’s earned the nickname ‘Iron Man.’ Biophysical sensors will play a big role in the way the suit functions.
In February and March, the Air Force successfully tested a new helmet with “physiological monitoring capabilities,” as Tripp put it. Its heads-up display shows different information based on how the pilot is feeling and other factors. The goal is to give every pilot a slightly different experience based on their unique physical and mental strengths and weaknesses, as well as their physical condition at the moment. Lab researchers and contractors anticipate it will guide the design of the next U.S. fighter jet, to be launched between 2025 and 2030.
“I may do a really, really good job on a spatial cognitive task where I’m looking at a radar warning display, and maybe James doesn’t,” Tripp said. “The thought, down the road, is to quantify my performance in these decreased physiological conditions from a cognitive perspective, and then use the changes in physiology to make the airplane smart about what kind of help I need.”
Kaleb McDowell, lead of ARL’s Center for Adaptive Soldier Technology, said there will be a fundamental give-and-take when designing the weapons of the future. People perform better when their tools are crafted specifically for them. But it’s hard to design for individuals quickly and at the scale of hundreds of thousands of troops. That’s why the design of weapons software today flows toward averages—and mediocrity. “You’re designing it to be simple for everyone,” McDowell said. “A guy that’s great spatially doesn’t use the spatial capabilities on any system that you see today. A woman that has a great math capability isn’t using that in today’s systems because no one’s conceiving of a system that actually relies on that capability. You just design it for everyone to use.”
So McDowell wants to build weapons that adapt to their users. “I want my system to be able to rely on, say a great memory, poor math capability, and a great spatial capability. I want the system to be able to say, ‘This person’s really creative. How do I tap into that imagination when doing this dull task?’”
But that also affords the military far greater insight into what job or mission they are giving to what soldier. Researchers say that that is a key benefit of the new data-collection programs. “The basic goal here is: We want to get greater precision and accuracy in predicting which people will succeed in particular job areas or missions,” Air Force research psychologist Glenn Gunzelmann said at a National Defense Industrial Association event in March.
* * *
What if the Air Force could use an airman’s personal history to predict how he would perform in his surroundings—even in battle? The military already keeps massive records on troops’ lives that, if structured properly, might furnish a treasure trove of mineable health data.
Kirk Phillips, associate chief for bioenvironmental engineering at the U.S. Air Force, and his colleague Richard Hartman are pioneering a program called Total Exposure Health. The goal is simple: collect and analyze as much data as possible about what happens to soldiers beyond the battlefield, right down to the kinds of molecules to which they are exposed. And in the military, a lot is recorded.
“In the Air Force, for instance, if you want your house treated for an infestation, that gets recorded,” Hartman said. “We have more opportunity to interact with [people in the military] in that total environment. Where they live and where they work. It’s something that is better known to us. They receive health care from us. We can measure their exposure at work so we can offer to measure their exposure at home. We can know what exposures are in the environment because nobody is saying, ‘Why are you measuring the amount of chemicals in the soil?’”
If you could take that information and convert it into structured data, algorithms could produce all sorts of new insights about how individuals are interacting with their environment, in real time and in incredible detail. Phillips believes that exposure science has enormous applications in the emerging field of epigenetics research.
Here’s where Phillips’s vision becomes both revolutionary and controversial.
Epigenetics is what your genes do with the change that you experience. It’s based not on your immutable DNA, but rather on your micro-RNA, the tiny molecules that turn on or off in response to stimuli. Think of a stress hormone that your body creates in response to an event. When your stress level goes down, new micro-RNA are formed and that controls gene expression in everything from your metabolism to how well you recover from disease. But it’s incredibly difficult to understand these interactions, precisely because everyone’s genetic makeup is different. Phillips hopes Total Exposure Health will yield a fuller picture of how specific sets of experiences affect specific sets of micro-RNA inside a specific soldier.
“Let’s say that external stress happens to be a chemical exposure you may never encounter, or there may not even be a micro-RNA that turns that part of your gene off that it activated. You may have the gene that activates under that exposure, and I may not. You may be very susceptible to a chemical that I have very little susceptibility to,” Phillips said.
Phillips thinks that if he can detect these kinds of things for the military, Total Exposure Health could revolutionize civilian healthcare as well. It offers high specificity on individual health on a scale of billions of people.
“You’ve probably read in the newspaper that they did a big study and they looked at red wine. They tried to see whether there was a health benefit to drinking red wine. Another study says: Maybe. Another study says: Not really. That’s because it’s population-health-based,” he said. “They’re just trying to pick a random population to see a population level change. If you have a gene that’s not very prevalent in a population, then you won’t get a population result of that exposure. Precision health and medicine says, ‘I should understand your gene in a way that I can understand whether your gene is activated by red wine and whether that activation is a health benefit, or health detractor.’”
“Right now, you might go, ‘I’ve read studies about red wine and they seem to be all over the place. I don’t know whether I should do it.’ Health care of the future would look like this: Your physician would say, ‘You know what? We looked at your genome. We know that red wine activates in the genome in a way that provides the health benefit. You don’t have the gene, so only drink red wine to the level that you find it pleasurable in social situations.’”
If Phillips is right, Total Exposure Health may ultimately give millions of people an incredibly detailed understanding of how their health choices affect their future. Not just, for example, how much alcohol is unhealthy for an average person of their age, weight, etc. to consume, but how much red meat, caffeine, sleep, etc. is good for them specifically.
It is becoming possible to know the health outcome of any action with an accuracy that would have seemed supernatural just a few years ago. The ability to comprehend the probabilities that form the future is the ability to influence it. The interplay of our genes and our experiences, of nature and nurture, moves from the mysterious to the knowable, or at least toward the more knowable.
* * *
For the military, this opens up new choices that are pulled directly from dystopian science fiction: anticipating what soldier is best suited for what assignment or mission.
In 150 BC, the Greek writer Polybius observed that Roman military units were doing something that no known army had done before: keeping careful and consistent records. The Romans could ration grain and wine across soldier classes and types because they had a uniform system of record-keeping for just that purpose. The reduction of unpredictability was proving a great battlefield advantage.
Imagine a military doing the same thing today but on a level both grander and more granular, where the substance to be rationed is a particular type of soldier personality, or even a specific kind of neurotransmitter.
Again: U.S. military officials are adamant that they are not genetically engineering military personnel and have no plans to do so. But they do not expect potential adversaries to share the same constraint, especially if it offers advantage over the military might of the United States. (Remember the movie Rocky IV? Just consider the Russian government’s recent systemic and secretive use of performance-enhancing drugs to win at the last Winter Olympics. Now imagine a battlefield of soldiers.) It’s a future to either embrace or learn to defend against.
If you were to use biometrics to genetically design a superior military, how would you do it? The outlines are visible today.
Individuals disposed toward risk-taking are probably better suited for particularly dangerous deployments and missions. But those same individuals are poorly suited for other aspects of military life, or less exciting military vocations, according to a landmark 2000 study by U.S. Army Maj. Michael Russell. He proposed that there were two primary military personalities: soldiers who exhibited a need for action and unpredictability (high stimulus-seeking) and people who were attracted to the military because its life offers a high degree of structure and discipline. A military needs both types to perform at peak but therein lies a fundamental contradiction. Military life is incredibly structured. War is unstructured. The stronger your attraction to one set of stimuli on the spectrum, the greater your aversion to the other.
“It has long been recognized that a peacetime army differs in many ways from that of an army at war. This is intuitively obvious: Destruction of personnel and equipment, even enemy equipment and personnel, is somewhat antisocial,” Russell wrote, achieving a new plateau in euphemism by calling blowing up the enemy “somewhat antisocial.”
“To plan the ultimate defeat of an entire army or nation on the battlefield requires at least a dose of narcissism. Therefore, those personality attributes that make for a war hero are primarily from cluster B. These people do not function as well in garrison. Such individuals thrive on challenge and require constant stimulation,” he wrote.
Merle Parmak, a military psychologist and a former Estonian Army captain, discovered that individuals who perform better in a highly structured, less exciting environment can also have great military careers, but perhaps not on the front lines. To a certain extent, you can train risk-taking soldiers to better accept the rigid boredom of military life away from the action, just as training can help structure-minded military personnel to better cope with the unpredictability of combat. But sticking the wrong person in the wrong job has costs.
Now consider the role that dopamine plays in risk-taking, according to an established and rapidly growing body of research. Dopamine levels are at least partially controlled by the monoamine oxidase A gene, or MAOA. A specific variant of MAOA called VNTR 2 was correlated with violent antisocial behavior, but only in the context of a stressful life event in adolescence.
If the connection between genetic factors, life experience, and risk-taking can be better observed, can they also be controlled? This is the question that will loom over military leaders in the decades ahead.
The Pentagon’s projections for future conflict are these: highly confusing and stressful urban warfare engagements. Population demographics pushing people into megacities means more door-to-door fighting, and more rules to protect civilians against adversaries who don’t have the same commitments to internal law or norms. War in the future ... sucks.
Depending on the intensity level of different conflicts in which the United States is engaged, the level of violence, the effectiveness or the simple ruthlessness of the enemy, the military may feel pressure to keep up with an adversary short on reservation. Should the United States find itself in such a conflict, Pentagon leadership may feel very differently about genetic engineering to secure better soldier performance, especially since doing so might degrade U.S. military advantage at less cost.
Should some future leader—of any country—make the decision to abandon the ethical frameworks we live by today, the tools will be there for him or her to make a swift transition.
But even genetically engineered humans might lose the battle in the end. The pace of war exceeds the speed at which humans can observe what’s happening, conceptualize a strategy, and deliver commands to pull off complicated counter-maneuvers. This is sometimes called the observe, orient, decide, and act, or OODA, loop, and it’s moving from a thing that humans do on the battlefield to a thing machines do. If you listen to the Pentagon’s top strategists when they talk about the future, this concern rises repeatedly. “When you think about the day-trading world of stock markets, where it’s really machines that are doing it, what happens when that goes to warfare?” William Roper, the head of the Pentagon’s Strategic Capabilities Office, asked at last year’s Defense One Tech Summit. “It’s a whole level of conflict that hasn’t existed. It’s ... scary to think about what other countries might do that don’t have the same level of scruples as the U.S.”
Given a choice between losing a major conflict and taking advantage of next-generation science to create a new advantage, it’s not hard to predict what any military will choose.
This article appears courtesy of Defense One.


The president of the United States wants a see-through wall at the U.S.-Mexico border, in part because he’s worried about people getting hit in the head by any bags of drugs that might be hurled over the divide.
“As horrible as it sounds, when they throw the large sacks of drugs over, and if you have people on the other side of the wall, you don’t see them—they hit you on the head with 60 pounds of stuff? It’s over,” Trump told reporters on Thursday. “As crazy as that sounds, you need transparency through that wall.”
To which at least one border security expert responds: “Over a 2,000 mile border, I think you’d have a higher chance of getting hit by a meteorite than a bag of drugs.” That’s Sanho Tree, a fellow at the Institute for Policy Studies who focuses on drug policies and the border. When I talked to Tree this morning, he was amused and astonished by the president’s remarks.
“The most important thing is, Donald Trump just admitted that he’s worried about people throwing drugs over a wall that’s designed to stop drugs!” Tree said. “Wile E. Coyote needs to take this wall back to Acme, because the road runner just owned him.”
Well, Trump did acknowledge he sounded crazy, didn’t he?
The thing is, the suggestion of a transparent wall isn’t Trump’s alone. Top officials from the Department of Homeland Security and U.S. Customs and Border Protection have advocated for a design that allows agents to see through portions of the divide.
“Not just for what the president describes as bags being thrown over, but just as important is being able to get a view of the threat that is actually staging on the other side of the fence and is preparing to take on some kind of breaching effort,” says David Aguilar, the former deputy commissioner of Customs and Border Protection who now works as a consultant for Global Security and Innovative Strategies. “The reality of the situation is that a see-through capability is critically important—but what is even more vital is situational awareness.”
And situational awareness doesn’t just come from being able to peek through slats in a wall, he says, though he emphasizes that “the president is right, the transparency is important.” Modern border security also requires surveillance towers, tethered drones, cameras, sensors, and other technology. “It is a combination of all of those things,” Aguilar told me. “Technology cannot be everywhere all the time.”
Trump may have gotten a bit confused about the threat of soaring bags of drugs. (“It’s not easy to describe,” Aguilar says.) What the president may have been thinking of, Aguilar suggests, is the problem with people throwing rocks or concrete blocks over the wall—sometimes a tactic used by spotters to divert attention, he told me. “Chucking things that can hurt our agents,” Aguilar says. “There have been very, very, very serious injuries with cement blocks being thrown.” Such injuries are rare now that there are more protections in place at the border, he says, but assaults against agents have climbed in recent months. CBP could not immediately provide data about how many agents have been struck or injured by objects thrown over the border.
From a drug-policy perspective, Tree says, the president’s ideas are dangerously misguided. “The idea that this wall will stop a few migrants carrying a couple of kilos compared with the tons that come through the tunnels?” he said. “Stopping a couple of kilos is really ridiculous. The vast majority of illegal drugs come through legal checkpoints. The wall will do nothing to stop that.”
Tree also worries that the president’s approach will inspire drug traffickers to cut their heroin supplies with even more fentanyl, a synthetic version of the drug that’s much stronger than heroin. Anticipating a potential dip in the heroin supply, he says, they’re more likely to reach for fentanyl as a way to stretch profits.
“People will drop like flies in this country,” Tree says. “It illustrates how little thought the president has actually given to any of these problems. He won’t take any advice. He won’t talk to anyone who won’t agree with him. That’s very dangerous.”
“Well, look,” says Aguilar, the former CBP leader, “There’s no doubt that the president’s approach to the border has brought attention to it. Now there’s attention, and then there’s attention. From an enforcement perspective, the attention that has been brought is an absolutely a positive thing. Now the question needs to be asked: What do you need?”


There is an old debate (at least, counting in internet years) that tends to crop up after major cybersecurity breaches such as the widespread WannaCry ransomware attack in May. In the aftermath of such incidents, some decry the sorry state of cybersecurity and insist that if only tech firms, with their wealth of resources and technical expertise, were allowed to go after the perpetrators of these attacks, they would do a much better job of stopping the damage and deterring other perpetrators than the slow, plodding, over-worked, under-resourced, jurisdiction-bound law-enforcement agencies.
Which raises a question: Beyond the standard set of protective tools—encryption, firewalls, anti-virus software, intrusion-detection systems, two-factor authentication—should companies be allowed to go outside the boundaries of their own networks and crash the servers that are attacking them, or delete data that has been stolen from them off their adversaries’ machines? The answer of most companies and cybersecurity experts is no. But that doesn’t stop a vocal minority—usually researchers at libertarian think tanks and lawyers concerned by how restrictive anti-hacking regulations have become—from suggesting otherwise.
The New Economics of Cybercrime
The notion that companies should be given more leeway to engage in “active defense”—the cybersecurity euphemism for offense—has been quelled for years by the Computer Fraud and Abuse Act (CFAA) in the United States and its counterparts in other countries, which effectively make it illegal for people to access computer systems that don’t belong to them without permission from the owners. But some lawmakers feel the need to carve out an exception to that blanket ban for companies that infiltrate external networks in the name of self-defense. In March, Georgia Representative Tom Graves proposed the Active Cyber Defense Certainty Act (ACDC), which would change the CFAA so that it would not apply to victims of cyberattacks who accessed attackers’ networks to “gather information in order to establish attribution of criminal activity to share with law enforcement” or to “disrupt continued unauthorized activity against the victim’s own network.”
Stewart Baker, a former homeland security assistant secretary under George W. Bush and a current partner at the D.C. law firm Steptoe & Johnson, would also like to see companies be allowed to hack back. A sharp critic of the current state of law enforcement’s cybersecurity efforts, Baker has for years been trying to make it easier for private firms to be able to pursue their adversaries in cyberspace. “For a company to go to the FBI and say ‘I’ve been hacked, can you find the hacker,’ it’s like going to a university town’s police force and saying, ‘Somebody stole my bike’—you’re lucky if they don’t laugh at you,” Baker says. “The government is completely consumed just trying to take care of its own data and tracking its own attackers. It doesn’t have the resources to help firms and probably never will.”
While it may seem irrelevant to liken denial-of-service attacks and data breaches to bicycle theft, most discussions of cybersecurity laws and policies, for better or for worse, happen by analogy to the physical world. And to Baker, there are two other important ways of thinking about the importance of legislation like the ACDC. One is the widely accepted principle of an organization’s right to defend its own interests. The other is the idea that there are many tiers of people with different responsibilities in between ordinary civilians and actual law enforcement. “In the physical world,” Baker explains, “there are all kinds of people in the middle, between innocent civilians and full-on military and law-enforcement protective personnel, who have intermediary authorities—bounty hunters, private investigators, mall cops—all people who have some additional authority and who ought to be able to use that additional authority because it was deemed necessary not to rely exclusively on the police. If that’s where we end up in the physical world, why would we not welcome the idea of having intermediate authorities between ordinary civilians hunkering down behind their firewalls and the police?”
Baker’s advocacy is driven not by industry interests so much as his own deeply held belief that government officials and law enforcement agencies are incapable of addressing online threats themselves. “It’s like the NRA saying, ‘When seconds count, the police are only minutes away,’ except the police are days away when you’re talking about cybercrime,” Baker says.
Baker and Representative Graves, though, are in the minority. At least among most people willing to speak on the record, legalizing proactive responses to cybercrime is a wildly unpopular idea. Its critics range from law enforcement officials who worry it will lead to confusion in investigating cyberattacks, to lawyers who caution that such activity might well violate foreign laws even if permitted by the U.S., to security advocates who fear it will merely serve as a vehicle for more attacks and greater chaos, particularly if victims incorrectly identify who is attacking them, or even invent or stage fake attacks from adversaries as an excuse for hacking back.
And if big tech firms are clamoring for the opportunity to go after their attackers more aggressively, they are certainly not doing so publicly. “I haven’t heard from particular companies that they want to have that activity authorized,” says Greg Nojeim, the director of the Freedom, Security and Technology Project at the Center for Democracy and Technology, a think tank. At least a couple companies have actively gone after adversaries in the past—Google reportedly breached a computer in Taiwan in 2010 while investigating attacks on its customers, and in 2014 the FBI examined whether some banks had hired hackers to crash servers being used by Iran—but known examples are few and, on the whole, relatively tame.
“I think a lot of companies would be hesitant to take the position,” Nojeim continued, “that it’s okay to engage in active-defense measures on somebody else’s network out of fear that their own networks would then become targets.” He, like many critics of broad hacking-back legalization, makes certain distinctions for defensive activities he views as less problematic. For instance, he is comfortable with “beaconing,” the practice of attaching code to sensitive files that will report back to their owners the IP address of machines they are copied onto when stolen.
Others argue that the crucial limits relate to who is permitted to hack back rather than what they are permitted to do. For instance, Jeremy Rabkin, a law professor at George Mason University, has advocated for putting together a list of cybersecurity firms vetted by the U.S. government, so that companies could hire an approved hack-back vendor to go after its online adversaries. “A lot of things can go wrong when people start mucking around in your files and your systems,” Rabkin told me. “You have to trust these people; you have to be sure that they’re not going to steal stuff or tip off other people.” In his estimation, there are only a handful of firms—highly regarded security companies and contractors that have longstanding relationships with the U.S. government and ex-military personnel, mostly—that can be trusted to pull this off.
Michael Chertoff, the former secretary of homeland security under George W. Bush who now runs his own consulting firm, argues that any private firm’s activities should be not only government-approved but also closely coordinated with U.S. officials. “If it’s not done at the direction of the government, then you get into something which is not terribly different from what the Russians do,” he says, referring to the Russian government’s reliance on intelligence gathered by criminals, allowing it to benefit from crimes without accepting responsibility for them. Ultimately, Chertoff doesn’t think that countering cyberattacks is something the government needs help with or for which it is interested in outsourcing its responsibilities.
Patrick Lin, a professor of philosophy at California Polytechnic State University, has finer-grained logistical concerns about any legislation that opens up the possibility of hacking back, regardless of what one makes of whether it is justified or not. “It is much too premature to allow for hacking back, even if the practice isn’t immoral,” Lin says. “At minimum, there needs to be a clear process to authorize or post-hoc review cyber counterattacks to ensure they’re justified, including penalties for irresponsible attacks. That oversight infrastructure hasn’t even been sketched out.” (There’s little discussion of such oversight in the current discussion draft of the ACDC, though under the most recent draft, released in May, companies would be required to report their activities to the FBI.)
At a moment when most people are concerned with trying to reduce online attacks, proposals to legalize hacking back and encourage more cyber conflict are a bit of an oddity. They rely on the implicit assumption that offense is the best defense, even though offense and defense have, in general, looked entirely different from each other online: The tools for defending computers like encryption and network monitoring bear almost no resemblance to the tools used to attack computers, such as botnets and phishing. Legalizing hacking back would conflate those two domains and, in doing so, likely make it that much harder to distinguish between the good guys and the bad guys online.


Half the world’s population already lives in cities, and that number is expected to jump to 70 percent by the end of the century. To accommodate the new urban dwellers, cities will have to build higher—and that will mean doubling down on ways to transport residents from the ground up into the sky.
The medieval town of Rottweil, in rural South Germany, may seem like an odd place to contemplate the high-tech future. (The locale’s claim to fame is breeding the Rottweiler dog.) But ThyssenKrupp, an industrial company based out of Essen, managed to do so last month, at a flashy event promising to change how we design, build, and occupy tall buildings.
“For 150 years, elevators have been dominated by ropes,” says Andreas Schierenbeck, CEO of ThyssenKrupp Elevator. It’s technology that by now, most of the world knows well: cables hoist a car up and down the elevator shaft, making stops along the way.

But with this promise of increasing urbanization, ThyssenKrupp sought to fill an opportunity to make tall buildings more efficient. Their new technology, known as MULTI, throws out the traditional elevator configuration in favor of a ropeless system that can move both horizontally and vertically. The conventional steel rope most elevators run on adds considerable weight to a building, and becomes more strained the taller you build, ultimately restricting a tower's overall height. By eliminating the cables—and the height restrictions that come with them—ThyssenKrupp executives brag it’s a technology that could send “an elevator up to the moon.” Indeed, it’s the stuff of Star Trek and Willy Wonka—but it could eventually make its way to a city near you.
The company unveiled a functioning MULTI system at ThyssenKrupp’s 807-foot-tall concrete test tower, which has been a proving ground for the system over the past two-and-a-half years. The result is an elevator utilizing the same magnetic technology that moves Japan’s bullet trains. In this model, elevator cars—not unlike train cars—move along magnetic tracks, uninhibited by traditional cables. Linear motors and a multiple-level brake system replace cables. Cabs are able to change direction from vertical to horizontal thanks to a rotating “exchanger.”
“We’ve been waiting for these developments for a while,” says Roger Soto, a design principal with the global architecture firm HOK. Soto led the design of the Capital Market Authority Tower in Riyadh, Saudi Arabia, which includes another ThyssenKrupp elevator innovation. When the 1,260-foot tower opens in 2018, it’ll utilize the company’s TWIN elevator system, in which two elevator cabs travel independently—one above the other—in the same shaft.
The elevator is “pretty critical” to skyscraper design, Soto explains, as it makes up the building’s core. But the current cable system takes up more space the higher you go. For the CMA Tower, the TWIN allowed HOK to build taller on a smaller floor plate: “The TWIN system allowed us to actually pack the elevators into the core in a way that made the tower more efficient and economical,” Soto explains.
The horizontal movement “is something I’m still trying to get my head around,” he says. “But I think the elevator can free us from certain constraints we have right now, and allow us to innovate in the way we conceive of towers.”

At the MULTI unveiling event, Antony Wood, executive director of the Council on Tall Buildings and Urban Habitat, spoke to the shifting trends already happening in skyscraper design. For one, most of the innovation has moved out of North American cities like Chicago and New York, and is happening across Asia and the Middle East. (The world’s tallest tower, Burj Khalifa, opened in 2010 and extends 2,717 feet in Dubai.)
We’re also using these towers differently. Instead of office towers built to symbolize a single company—the Chrysler and Sears towers, for example—they are often operating as mixed-use “mini cities” with a combination of residential, office, hotel, and public space. As Soto put it, “We’re thinking more about creating social connections in a vertical setting.”
Wood called the MULTI “the holy grail of elevators” to address such shifts. For one, the system allows multiple elevator cabins to operate on a loop, moving more people in a continuous flow. Eliminating the space traditionally reserved for elevator shafts, it also frees up square footage for more apartments or office space. Schierenbeck estimates the system can achieve up to a 50 percent higher transport capacity, while increase a building's usable area by as much as 25 percent.
But at what point does an elevator become a totally different mode of transit? “This is technology that’s not really distinguishable as an elevator anymore,” says Daniel Safarik, the China Office Director at Council on Tall Buildings. “They’re more like transportation vehicles of some kind. If you can get an elevator to go sideways or diagonally, then what’s the difference between the car, the subway, and the elevator? They start to have a lot of the same properties.”
Safarik thinks the real innovation of the MULTI is the possibility to send elevators underground, where they could move horizontally to connect buildings and transit hubs. It’s been proposed before. Last year, London architecture firm Weston Williamson & Partners proposed a MULTI system that traveled underneath buildings vertically before descending down to Tube platforms, creating connections between stations. Innovative as it may sound, Safarik sees the “jurisdiction between what’s a building and what’s infrastructure” posing the biggest challenge to MULTI transforming cities in this way.
The stalwarts of the elevator industry have their own concerns. “Tradition still has a strong role here,” says Rick Sayah, vice president of the New Jersey elevator consulting firm Van Deusen & Associates. “The reproducibility of work is how we’re able to maintain safety, training and maintenance.” His questions, too, revolve around what the technology exactly is, and who should regulate it: “Is this an elevator? Is it a vertical extension of the transit system? Will it require a new paradigm of building code?” He believes it’ll be a challenge to build new safety codes around a technology that’s so unlike the traditional elevator.
Schierenbeck says, “In the last five years, ThyssenKrupp developed comprehensive functional safety concepts using a multi-step braking system capable to handle all possible scenarios of operation.” The company, which has yet to obtain a safety certificate for the technology, expects cars can begin testing with people inside in the next few months.
The East Side Tower in Berlin, designed by Netherlands-based OVG Real Estate, will be the first with a MULTI system. The company has released few details on the project, besides an anticipated opening in 2019.
Safarik, of the Council on Tall Buildings, believes the technology is likely to come to the United States as part of a hospital, campus, or government complex, where horizontal elevators can shuttle people from one building to the next. “It’s not particularly freaky to imagine,” he says. “It’s a logical thing, even if it has always been in the realm of science fiction.”
This post appears courtesy of CityLab.


I am the Danube River.
My spirit is sparkling and swift. I yearn for new experiences and deep connections with people. I’m adaptable, but to a fault; I rarely see danger ahead. I’m capable of infidelity without much remorse. I’m also great at ceramics.
So says Meet Yourself As You Really Are, the oldest, longest, and WTF-est personality quiz I’ve taken. Published in 1936, Meet Yourself is a 336-page home-psychoanalysis test that promises to “‘X-ray’ the reader’s fundamental character.” It does so with an interminable line of questions both probing and random. Are your parents dead? Have you ever had the sensation of standing outside your own body? Do Mickey Mouse cartoons freak you out? What do you think of unskimmed milk?
As you tally “yes” and “no” answers, the book directs you to new sections based on your responses. Somewhere in the middle, you’re categorized as one of 15 rivers—the Nile, Seine, Thames, Missouri, and so on—and eventually you’re offered long-winded personality breakdowns. “As you travel across the network of questions and data by your private track, your story unfolds and your character is explained,” the introduction teases. The book has been described as a Freudian Choose Your Own Adventure, which is accurate enough: It’s like Give Yourself Goosebumps, but instead of escaping the Carnival of Horrors at the end, you learn that you have commitment problems.
Clearly, personality quizzes have some sort of perennial appeal. Facebook newsfeeds are filled with BuzzFeed quizzes and other oddball questionnaires that tell you which city you should actually live in, which ousted Arab Spring ruler you are, and which Hogwarts house you belong in. But these new online quizzes have a dark edge that their analog predecessors didn’t. In the wake of the U.S. election, a secretive data firm hired by Donald Trump’s campaign boasted that it has been using quizzes for years to gather personal information about millions of voters. Its goal: the creation of digital profiles that can predict—and possibly exploit—Americans’ values, anxieties, and political leanings.
Whether this firm, Cambridge Analytica, has actually used predictive profiles to influence people isn’t certain; reports suggest it hasn’t, at least not directly. But the company’s methods nonetheless expose the growing scale of personality analysis online—and the dangers that come with it. On the internet, anything you do is like taking a personality quiz: Everywhere you click reveals something about you. And you’re not the only one who sees the results.
On Meet Yourself’s spine, a silver-painted mirror depicts the personality quiz’s allure: See yourself as you really are. What happens when the mirror is two-way?
* * *
No one seems to know when the personality quiz first gained a foothold in popular culture. The journalist Sarah Laskow has traced its origin in America at least as far back as the late 19th century, “when ladies’ magazines started gaining traction and the yellow press would try anything to sell papers.” But the quiz has persisted with remarkable consistency since, with spikes in popularity during a quick magazine boom immediately post-WWII, the Cosmopolitan quizzes of the 1960s and ’70s, and today’s ubiquitous BuzzFeed quizzes.
This stretch neatly overlaps with the history of the popular quiz’s buttoned-up, high-achieving sibling: the personality test. In-depth psychological assessments like the Myers-Briggs Type Indicator began popping up in the first half of the 20th century for the purpose of scanning and sorting employees in industrial workplaces. While many of these tests, including the Myers-Briggs, have since been dismissed by the scientific community as unreliable—if not dangerously discriminatory—they, too, have persisted, perhaps in part because they at least provide a framework for otherwise-difficult office conversations. Somewhere around 10,000 companies, 2,500 colleges and universities, and 200 government agencies still use the Myers-Briggs in the U.S. today, including the majority of Fortune 500 companies.
Affiliation with these more legitimate-seeming forms of personality analysis has always given the personality quiz a vague air of authority. Indeed, if there’s any one way to characterize quizzes’ mystique, it’s probably that, through all their many iterations, they have somehow managed to tightrope-walk the line between entertainment and science, or at least something approaching science. “BuzzFeed quizzes are crafted to create the illusion of truth, or potential truth,” writes the journalist Caroline O’Donovan, in explaining the fad. She quotes Summer Anne Burton, one of BuzzFeed’s editors: “You sort of write them like horoscopes, with tidbits people can relate to.”
Meet Yourself discovered this strategy long before today’s click-miners. The book’s co-authors, the British novelist William Gerhardie and the Spanish aristocrat Prince Leopold Loewenstein—whose son, incidentally, went on to be the financial manager of the Rolling Stones—flip freely between promises of profound insights and guarantees of fun.
If the limited information out there about the book’s critical reception is any indication, people have never taken Meet Yourself too seriously. A short review in a March 1937 issue of Ohio’s Piqua Daily Call deems it “a fairly amusing way of filling in an odd hour or so,” and includes this sick 1930s burn: “If you sit down prayerfully with it and answer all of its impertinent questions, you will never again be phased by any little thing like an income tax or civil service examination blank.” Decades later, a columnist for The Independent ran into the book on a trip with her boyfriend to her parents’ house, and cracked up when it announced to the family that the boyfriend was “a conqueror of women.”
As BuzzFeed’s quizzes really started gaining steam a few years ago, a deluge of think-pieces attempted to make sense of why people just can’t get enough of them, even when they clearly have little to do with reality. Reasons included narcissism, existential searching, and boredom. Laskow made the case that people simply like talking about themselves. These probably are all true, to some extent. But they overlook something deeper about the nature of personality itself.
* * *
Simine Vazire believes that a good personality test rarely tells you anything you don’t already know. As director of the Personality and Self-Knowledge Lab at the University of California, Davis, she studies how people come to understand who they are. “We know a lot just by being in our bodies, by being ourselves,” she says. Tests promising to unveil hidden truths about their takers—tests known as projective in psychology—are mostly bogus.
What tests offer instead, Vazire suggests, is reflection. “When you have someone summarize to you what you just told them, it gives you a sense of, ‘Oh, yeah, that’s what I was trying to say.’ I think this is just a version of that,” she says. “Because you’re talking about yourself, and you’re answering a bunch of questions about yourself, a test can summarize this information for you. It can give you a precise or better language for summarizing yourself, even if it’s based on what you told it.”
This reflection isn’t the quasi-mystical type of self-knowledge Meet Yourself claims to be after. It doesn’t show you as you really are, but rather helps you articulate who you know yourself to be.
That distinction might sound trivial, but it actually makes a critical point about how personality functions. In its textbook definition, personality is all about patterns: “individual differences in characteristic patterns of thinking, feeling, and behaving,” as the American Psychological Association puts it. Personality, in other words, is not some set thing. It’s the result of a messy web of tendencies and habits, all informed by some incalculable mix of biology, disposition, and learned behavior.
That makes personality slippery. While psychologists debate the granular details of its definition, what it amounts to is that personality is something of a paradox. People tend to have a sense of their own character, but this sense is never complete. We know ourselves, but we don’t.
This is why people love personality quizzes. Beyond vanity and narcissism and harmless fun, taking a personality quiz helps me get out of my own head, to see whether my experience of myself matches up with how others experience me. This is the same reason I sometimes catch myself staring into the mirror even after I’ve double-checked that my fly is zipped and fretted over my oh-so-slightly thinning hair. There’s an element of affirmation, even awe and wonder, to the reflection. That is me in the world.
In this light, personality quizzes actually appear pretty beneficial, or at least innocuous. And maybe they would be, if they left me to my own musings and no one else ever saw the results. But a psychological need for self-reflection gets complicated when the mirror also snatches up information for other people to use.
* * *
In 2008, Michal Kosinski joined a research project that helped revolutionize how data about people is collected. As a graduate student in Cambridge University’s Psychometrics Centre, a department that studies online psychological assessment, he and his classmate David Stillwell distributed a short personality quiz on Facebook that told people how they rated among psychology’s “big five” personality traits: openness, conscientiousness, extroversion, agreeableness, and neuroticism (OCEAN, for short). Upon receiving their results, quiz-takers had the option of sharing their Facebook profiles with Kosinski and Stillwell.
It didn’t take long for the two researchers to amass the largest-ever collection of psychometric scores paired with Facebook profiles. “It was surprisingly easy to get people engaged,” says Kosinski, who’s now a professor of organizational behavior at Stanford’s graduate school of business. “[Stillwell] just sent it to his friends on Facebook. Those friends took the questionnaire, then shared it on their profiles, and suddenly you had thousands of people taking it every day.” Over a few years, Kosinski and Stillwell gathered info from millions of Facebook users.
The two researchers used this data to build a new system for predicting people’s personalities. With access to their subjects’ OCEAN traits and Facebook information side by side, Kosinski was able to correlate what people were like with the personal details available about them online—Facebook likes, gender, age, and so on. Soon, he had an algorithm that, based on analyzing Facebook likes alone, could guess how people think, feel, and act—that messy web of tendencies and habits—with startling accuracy. With 70 likes, the model could predict someone’s personality, as measured by a 100-question personality test, better than that person’s friends could. With 300 likes, it could outperform a husband or wife.
This predictive approach pioneered by Kosinski is what stirred up so much controversy in the U.S. election. Kosinski himself had nothing to do with the data firm hired by Trump’s campaign. Motherboard and Das Magazin have reported that his research appears to have been brought over to Cambridge Analytica by a young colleague of his with ties to the firm’s parent company, the London-based Strategic Communication Laboratories Group, or SCL. (Cambridge Analytica denies that the company or its methodologies have any connection to Kosinski.) After the election, the firm first declared that its brand of “psychographic” profiling played a major role in Trump’s victory, then conceded that it never actually used the approach to influence voters.
Whatever the impact so far, predictive methods like Kosinski’s and Cambridge Analytica’s are pushing open the door to a new—and unsettling—stage of personality analysis. It’s no revelation that personal data can be collected online, and there are already plenty of ways this data can be used to influence people: In politics, for instance, Barack Obama targeted individual voters with psychometrics well before Cambridge Analytica was even formed. But Kosinski saw just how much information can be gained. With personality quizzes, he found what might be the most direct route into people’s hearts and minds yet.
Kosinski never requires Facebook users to give up their profiles to take his quizzes. But the worrying implication of his kind of approach is that online personality analysis can easily blur the line between opting in and opting out. When algorithms can be trained to accurately infer your personality based on anything you do, the internet is a personality quiz—or, at least, it can be, so long as each page visit, web search, and “like” can be gathered and correlated.
Online, before you even click on a quiz, you’re already filling something out.
* * *
There are a lot of good reasons to worry about this technology. Imagine an advertising company that knows you’re self-conscious about your weight, so tries to sell you diet pills. Or—in a hypothetical that often comes up in this kind of discussion—imagine a political campaign that knows you’re prone to anxiety, so targets you with ads about the dangers of the Islamic State. “Big data companies already know your age, income, favorite cereal and when you last voted,” notes a New York Times report on Cambridge Analytica. “But the company that can perfect psychological targeting could offer far more potent tools: the ability to manipulate behavior by understanding how someone thinks and what he or she fears.”
While Kosinski thinks people’s rapidly diminishing privacy online is indeed dangerous, he’s quick to point out that there are potential benefits to personality profiling, too. Targeted ad campaigns could get kids to quit smoking, he suggests. Personalized political messages could inform voters, not pull their strings.
Companies like Cambridge Analytica have a commercial stake in exaggerating their techniques’ reach, as well. “What they’re selling is not exactly snake oil, though it can work as a placebo for panicky candidates who are down in the polls with weeks to go before Election Day,” the journalist Leonid Bershidsky has argued. “But just like artificial intelligence or, say, the blockchain, [data science has yet to produce] killer apps that can ensure a political victory or business success.”
I’m not convinced the nerdy podcasts and obscure track-and-field clubs I like on Facebook will hand the reins of my life to some shadowy corporation anytime soon. But I do think the threat is real—real enough, at least, that I wouldn’t give away my profile information for a personality assessment.
Something Kosinski told me gave me an uneasy feeling I haven’t been able to shake, too. There’s research that has been done on people’s trust in algorithms. A subject talks to an expert on a topic, and the expert offers some sort of insight on that topic, backed by one of two possible justifications: either a) the expert has thought about this for a long time, or b) the expert’s computer calculated the solution. The results show that people are more likely to trust the computer. “We’re being trained by algorithms that they’re always right,” Kosinski says.
Surely, such trust isn’t always misplaced. Vazire, the UC Davis psychologist, admits that she’d probably trust an algorithm over an expert—if she knew the algorithm to be accurate. But what if it’s not? What if, say, it’s built upon data collected by researchers who are prone to error and bias? Or what if it’s intentionally incorrect—sneakily incorrect? Conceivably, an algorithm could know so much about you that it could say exactly what would make you think, act, or feel a certain way.
That’s where the impulse to take a personality quiz keeps me up at night. I’m wired to seek out ways to reflect on who I am, but who I am is slippery—and that makes me open to suggestions. If people’s faith in algorithms continues to grow, it might not be long before I trust a computer to tell me about my personality more than I trust friends or family—or more than I trust myself.
That’s a strange future to imagine. But, hey, I am the Danube River. I’m adaptable. I’m sure I’ll adjust.


Okay, what the hell happened to the New York subway?
It wasn’t so long ago that New Yorkers could smugly look to cities like Boston (Aww, what a darling toy train!) and Washington (Six whole lines? Adorable!) and appreciate the scope and reliability of their underground transit system.
But between fare hikes, overcrowding, frequent breakdowns, mechanical failures, signal gaps, janky cars, and rickety tracks, New York’s Metropolitan Transportation Authority is facing millions of angry riders and a multibillion-dollar repair job that is likely to span decades.
One of the weirdest aspects of all this is that the problems seemed to come all at once, out of nowhere. The subway was mostly fine! But, then, it was most definitely not.
Subway delays have more than doubled over a five-year period. Track fires increased. “The current state of the subway system is unacceptable,” the MTA’s newly reinstated chairman, Joseph Lhota, said in a statement late last month as he outlined an emergency improvement plan. “We must rebuild confidence in the authority with a complete overhaul of the system.”
It’s not yet clear exactly what that overhaul will look like—or, crucially, how much it will cost. Initially, the MTA said it would need as much as $20 billion for systemwide maintenance and repairs. (Lhota didn’t reply to my request for an interview.) In the meantime, New Yorkers are left wondering how things got so awful.
Where to begin? For one thing, there’s the old—so very, very old—infrastructure. “In fact it’s so old that the MTA can no longer buy replacement parts from the manufacturer,” James Somers wrote in a 2015 essay for The Atlantic. “It has to refurbish them itself.”
The 1960s-era Brightliners, those stainless-steel C-train cars, break down constantly—every 33,000 miles on average, The New York Times recently reported. That’s compared with the average subway car, which breaks down every  400,000 miles, and the newest cars, which break down every 750,000 miles, according to the newspaper. Then there’s the signaling system that Somers wrote about. On top of being ancient and unreliable, signals are inspected far less frequently than they were a decade ago. They’re languishing despite sorely needed upgrades that could otherwise improve efficiency to accommodate the growing throngs of riders.
Why New York Subway Lines Are Missing Countdown Clocks
Oh yeah, the people. Here’s where we get into tipping-point territory that explains how things seemed to get so bad so quickly. There are more passengers now than there have been since the 1940s. The all-time ridership record was set in 1946, the year 2 billion passengers rode the subway. Ridership exceeded 1.7 billion last year, and broke records set in 1948. These days, overcrowding is the reason for about one-third of the system’s delays any given month, the MTA says. (Then there’s Penn Station, which is facing its own chronic failures after “decades of neglect,” as New York Governor Andrew Cuomo put it in May.)
“The other factor is there’s no political capital in doing preventative maintenance,” says Andrew Natsios, the former chairman of the Massachusetts Turnpike Authority who helped save Boston’s notorious Big Dig highway project. (He’s now the director of the Scowcroft Institute of International Affairs at Texas A&M University.) “The consequence is that systems deteriorate much faster. No one gets elected for voting for preventative maintenance. It’s very sad.”
Even when state officials have set aside the necessary money for upgrades, funds haven’t been managed well, and repairs are rarely made on schedule. Cost overruns are a big issue on any major infrastructure project, but especially when you’re dealing with an old urban area. At least New York officials now seem to be taking the subway’s many problems seriously. Lhota’s return to the authority and a recently announced modernization competition are reasons for cautious optimism. Then again, “there are always problems you don’t expect,” Natsios told me.
He gives an example from his time in Boston, when Big Dig construction disturbed the city’s many rats—and the rats disturbed the city’s many humans. “The rat population under Boston goes back to the Colonial period,” he said. “You suddenly had tens of thousands of rats on Boston streets. They had to spend tens of millions of dollars on rat extermination.”
At least in New York’s subways, the rats are already expected. But the rodents are a growing problem of their own. On Wednesday, the New York mayor, Bill de Blasio, declared war (again) on the city’s rat population, pledging $32 million to kill as many as possible. (“We want more rat corpses,” the mayor said.) That’s a paltry sum compared with the $20 billion estimate for a complete overhaul of the subway system. For now, the rats of New York have this in common with the city’s subway cars: They aren’t going anywhere.

* This article originally featured a caption that mischaracterized this illustration as an early sketch of the existing subway system. We regret the error.


Nothing like a presidential scandal to turn something utterly mundane into a national obsession. But here we are, once again, talking about emails.
The emails. Oh, the emails!
During the 2016 presidential campaign, email became a symbol of Hillary Clinton’s alleged corruption. Clinton’s aides wiped about 33,000 emails from a private server she used during her time as secretary of state, and Donald Trump made the deletions a cornerstone of his campaign against her.
“How can anyone vote for Hillary when she careless with emails that jeopardize our security,” he tweeted in September 2015. “She is not to be trusted.”
Then, in June 2016: “How can Hillary run the economy when she can't even send emails without putting entire nation at risk?” And also: “One of the reasons Hillary hid her emails was so the public wouldn't see how she got rich- selling out America.”
A year later, in July 2016: “If Russia or any other country or person has Hillary Clinton's 33,000 illegally deleted emails, perhaps they should share them with the FBI!”
That last one still raises an eyebrow. Especially today. What the American people didn’t know at the time, and what we know now, is that Trump’s son, Donald Trump Jr., had been doing some scandalous emailing of his own related to Russia.
On June 3, 2016, Trump Jr. received an email from Rob Goldstone, a Trump associate, former tabloid reporter, and entertainment publicist. Goldstone promised information that “would incriminate Hillary and her dealings with Russia and would be very useful to your father.” The New York Times on Tuesday first reported the contents of the email exchange, which Trump Jr. also released on Twitter. “This is obviously very high level and sensitive information but is part of Russia and its government’s support for Mr. Trump,” Goldstone had written. Trump Jr. replied: “If it’s what you say I love it especially later in the summer.” (You can read a transcript of the entire exchange here.)
The Triumph of Email
In an earlier era, this sort of exchange might have taken place over the phone, or via telegram, or, you know, face to face. So it makes sense that the scandal of the moment is tied to a ubiquitous form of communication. Especially when that form of communication is so easily forwarded, or copied, or hacked. Given that it was the utter carelessness that Donald Trump seemed to find most offensive about Clinton’s use of a private server, one can only imagine the conversation he’s having with his son today. This scandal isn’t really about email, after all, it’s about how Trump Jr. used it.
Clinton’s supporters have long cried, “but her emails!” as a way to minimize the Clinton email scandal and maximize Trump’s hypocrisies. Yes, Clinton should have known better than to use an outside server to conduct government business. But Trump Jr. is now learning a similarly rudimentary lesson. Email is not a secure way to communicate, but lots of people are sloppy with it anyway. The common-sense advice is this: Don’t write anything in an email that you wouldn’t want to see on the front page of the newspaper tomorrow. (This is a thought exercise that no longer requires imagination for Trump Jr.)
“Part of the reason Donald Trump Jr. is finding himself in hot water right now is he forwarded an entire email chain,” said Amy Webb, the writer and founder of the Future Today Institute. “I can’t state that enough. Email is penetrable. Unless you are using a lockdown system and encryption, anybody can access it.”
The fact that an email-centered scandal has boomeranged back to the Trump White House isn’t the only irony, Webb says, noting reports that the White House has gutted its Office of Science and Technology Policy. (The White House disputes this characterization.)
“These are some fundamental misunderstandings by the people who are now deciding technology policy,” she said, “in the absence of real scientists and technologists who know what they’re doing.”
For many people, it’s not at all practical to avoid email entirely. And it’s arguably lucky for the American people that those in positions of power are sometimes careless. A paper trail, even a digital one, can reveal important information that politicians would otherwise keep secret. But, as Trump Sr. once suggested, carelessness with something so simple as email is a red flag. A person who’d jeopardize security that way, he said, is “not to be trusted.”


The president was seething.
His problem was with the press, yes, but also with the technology they used. Electronic media had changed everything. People were glued to their screens. “I have never heard or seen such outrageous, vicious, distorted reporting,” he said in a news conference.
The age of television news, Richard Nixon told reporters gathered that day in October 1973, was shaking the confidence of the American people. He didn’t yet know his presidency would reach a calamitous end. When Nixon announced he would resign, in August 1974, he spoke directly into a television camera. The recording remains stunning half-a-century later—mostly because of the historic nature of the moment, but also because of the power of broadcast.
Even in an informational era transformed by the web, video is a gripping format. In the chaos of real-time news, especially, there’s an advantage to being able to see something with your own eyes.
Or, there used to be.

At a time when distrust in journalistic institutions is swelling, technology that further muddies the ability to discern what’s real is rapidly advancing. Convincing Photoshop-esque techniques for video have arrived, and the result is simultaneously terrifying and remarkable.
Computer scientists can now make realistic lip-synched videos—ostensibly putting anyone’s words into another person’s mouth.
The animated gif that you see above? That’s not actually Barack Obama speaking. It’s a synthesized video of Obama, made to appear as though he’s speaking words that were actually inputted from an audio file.
The clip comes from researchers at the University of Washington, who developed an algorithm to take audio of someone talking and turn that into a realistic video of someone speaking those words. In the video below, you can see a side-by-side comparison of the original audio—which came from actual Obama remarks—and the generated video.

Obama was a natural subject for this kind of experiment because there are so many readily available, high-quality video clips of him speaking. In order to make a photo-realistic mouth texture, researchers had to input many, many examples of Obama speaking—layering that data atop a more basic mouth shape. The researchers used what’s called a recurrent neural network to synthesize the mouth shape from the audio. (This kind of system, modeled on the human brain, can take in huge piles of data and find patterns. Recurrent neural networks are also used for facial recognition and speech recognition.) They trained their system using millions of existing video frames. Finally, they smoothed out the footage using compositing techniques applied to real footage of Obama’s head and torso.
The researchers wrote a paper about their technique, and they plan to present their findings at a computer graphics and interactive techniques conference next month.
“The idea is to use the technology for better communication between people,” says Ira Kemelmacher-Shlizerman, a co-author of the paper and an assistant professor in the department of computer science and engineering at the University of Washington. She thinks this technology could be useful for video conferencing—one could generate a realistic video from audio, even when a system’s bandwidth is too low to support video transmission, for example. Eventually, the technique could be used as a form of teleportation in virtual reality and augmented reality, making a convincing avatar of a person appear to be in the same room as a real person, across any distance in space and time.
What Makes Tom Hanks Look Like Tom Hanks
“We’re not learning just how to give a talking face to Siri, or to use Obama as your GPS navigation, but we’re learning how to capture human personas,” says Supasorn Suwajanakorn, a co-author of the paper. Not surprisingly, several major technology companies have taken notice: Samsung, Google, Facebook, and Intel all chipped in funding for this research. Their interest likely spans the realms of artificial intelligence, augmented reality, and robotics. “I hope we can study and transfer these human qualities to robots and make them more like a person,” Suwajanakorn told me.
Quite clearly, though, the technique could be used to deceive. People are already fooled by doctored photos, impostor accounts on social media, and other sorts of digital mimicry all the time.
Imagine the confusion that might surround a convincing video of the president being made to say something he never actually said. “I do worry,” Kemelmacher-Shlizerman acknowledged. But the good outweighs the bad, she insists. “I believe it’s a breakthrough.”
There are ways for experts to determine whether a video has been faked using this technique. Since researchers still rely on legitimate footage to produce portions of a lip-synched video, like the speaker’s head, it’s possible to identify the original video that was used to create the made-up one.
“So, by creating a database of internet videos, we can detect fake videos by searching through the database and see whether there exists a video with the same head and background,” Suwajanakorn told me. “Another artifact that can be an indication is the blurry mouth [and] teeth region. This may be not noticeable by human eyes, but a program that compares the blurriness of the mouth region to the rest of the video can easily be developed and will work quite reliably.”
It also helps if you have two or more recordings of a person from different views, Suwajanakorn said. That’s much harder to fake. These are useful safeguards, but the technology will still pose challenges as people realize its potential. Not everyone will know how to seek out the databases and programs that allow for careful vetting—or even think to question a realistic-looking video in the first place. And those who share misinformation unintentionally will likely exacerbate the increasing distrust in experts who can help make sense of things
“My thought is that people will not believe videos, just like how we do not believe photos once we’re aware that tools like Photoshop exist,” Suwajanakorn told me. “This could be both good and bad, and we have to move on to a more reliable source of evidence.”
But what does reliability mean when you cannot believe your own eyes? With enough convincing distortions to reality, it becomes very difficult to know what’s real.


A consortium of newspaper publishers are preparing to take the unusual step of begging the nation’s legislature for the right to collectively negotiate with Facebook and Google.
(Generally, antitrust laws forbid this kind of collective bargaining because it reduces economic competition, except in specifically legislated cases such as labor unions.)
It’s easy to see why publishers want to team up and bargain as one: Facebook and Google continue to take what they want from the news media—content to churn through the News Feed and ever-more-recent search results—while soaking up all the advertising profits available. This is a game where the distributor keeps almost the whole pie, and only so many news organizations can survive on the scraps the Internet platforms allow to fall to the floor.
So, how’d we get here?
We can radically boil it down to three movements.
1. Starting at the beginning: Ask most newspaper people and they’d tell you the “original sin” was that news gatherers decided to give away their product for free when they went online. This is a ridiculous frame (for reasons I’ll get into) but this would be most people in the news business’ common understanding.
It was the ethos of the time that stories should flow around the Internet, unburdened by payment systems. The key underpinning of the whole web was the link and anybody had to be able to view a link for it to be of maximum value. That was just an axiom of the early web.
So, (most) newspapers went along. Foreshadowing. But everything had changed: They were in an entirely different business and didn’t realize it.
2. Google took advantage of all the high-quality pages, and the links between them, being freely placed on the World Wide Web. They took all those links and turned them into the data that powered their search engine.
Google became the best place to find out about the world. One could know what was going on without a newspaper or a TV or a radio.
As Google gained users, newspapers gained greater reach and readership than they’d had before. Sure, most of them were visitors from outside the metro areas that their advertisers wanted to reach, but the business would fill itself in, everyone hoped.
Google, meanwhile, was figuring out its own business model, which turned out to be sponsored links running next to search results. They created an easy way to buy advertising and a backend that could tell you precisely how well ads were performing. (Other people were doing similar things, but only one of them has become one of the most valuable companies in the world.)
Turns out that many small, local advertisers preferred the Google way of doing ads to the newspaper way of doing ads. Combine that with the hit from Craigslist taking mainstay classified-ad business, and newspaper print advertising was in trouble.
That would have happened with or without the so-called “original sin.” The disruption came to the papers: their local/regional business practices got stomped by the efficiency of the technology business.
In the new advertising world, a reader wasn’t worth very much. Maybe a few cents a visit. But there was always a hope: more scale. More visitors meant more pageviews, which meant more ads, which meant maybe the news organizations could pay their journalists with enough pennies from web ads.
Thus, people are always searching out scale, traffic, visitors, pageviews, ad inventory.
3. And who has the scale? Facebook.
For a long time, Facebook was happy to send readers to news articles, but it was incidental to their larger concerns. And then came, roughly, August 2013, when Facebook turned on the taps for news organizations. Charlie Warzel noted it in Buzzfeed at the time: “traffic from Facebook referrals to partner network sites are up 69% from August to October of this year.” But he was just spelling out what many in digital media knew from their own traffic reports. Traffic was through the roof and it was all coming from Facebook.
“Facebook has sent unprecedented levels of traffic to publishers across the internet in recent months, a dramatic and unexpected increase affecting a large range of sites serving a wide variety of content,” Warzel wrote.
Facebook enacted this monster change in the information flow of the world in two ways. One, it seemed to start privileging news content from the best, say, quartile of publishers. Stories from these publishers were more likely to show up in your newsfeed relative to stories from other publishers (or people).
More significantly, for the long-run, Facebook also began—essentially—running house ads for different media brands. For several months, they suggested that users should follow media brand pages. This was basically an in-kind gift from Facebook to a variety of publishers. And it gave them much larger pages from which to promote their stories.
The tactics worked. Facebook began to generate huge scale for publishers. Now many sites have “audiences” of 15-20-25-30 million unique visitors, many times the size of even the largest print publications. (USA Today’s average circulation is down substantially to something like 3 million.)
But that also put publishers at the mercy of Facebook’s News Feed algorithm and corporate practices.
In the midst of the Facebook traffic surge, David Carr wrote a column about Facebook’s new dominance. “The traffic they send is astounding and it’s been great that they have made an effort to reach out and boost quality content,” a publishing exec told him.  “But all any of us are talking about is when the other shoe might drop.”
The other shoe, we presumed at the time, was that Facebook would close the taps for one reason or another. And they partially did when they began emphasizing video, which is one reason so many media companies are “pivoting to video.” They’re merely running where Facebook’s corporate finger is pointing.
But worse, the scale that Facebook rapidly induced also caused inflation of a sort. The currency of digital advertising was traffic and suddenly there was a lot more of it. So what do you think happened? All that scale put downward pressure on how much advertisers were willing to pay publishers. Which meant that you needed Facebook’s scale just to maintain your business.
Facebook’s control of the attention of 2 billion people and the media has done a lot more strange things to “the news.” Just a couple examples: Everything’s spikier now—meaning fewer and fewer stories drive more and more traffic. More publications are clustering around fewer themes and news events because most of the available Facebook traffic will come around a single news item, whether it’s a John Oliver rant or a Trump associate’s meeting with someone or a doctor dragged off a plane or covfefe.
These are some of the complex effects that what I’ve long called Facebookworld has brought to journalism. And so I can see why newspapers would like to sit down with Facebook to tell them about their problems. Tech has wound its way around journalism like a vine around an old tree.
We’ve just seen the consequences of the strange information ecosystem Facebook and Google have created. In the crucial months leading up to the election, hoaxes, lies, and fakery proliferated right alongside (or on top of) serious coverage from serious reporters. Since the election, a revitalized press has taken to covering the many problems and scandals of the Trump administration with intensity.
In other words, in the last year, we’ve been shown the value of a trained press corps supported by journalistic institutions that know what they’re doing.
And at the same time, papers and other journalistic outfits continue to struggle with creating digital businesses that can support enough journalists to make a difference.
The press has never been more needed nor more endangered.
The newspapers probably won’t get the help they need from Congress. And even if they do, they are in a weak position vis-a-vis the two most powerful information gatekeepers the world has ever known.
The good news is that Facebook and Google, both the individuals inside of them and the corporate structures, may finally want to help the business of journalism. The bad news is that, at this point, these two companies may not be able to fix what they’ve broken.


Updated: Sunday, July 9 at 9:09 p.m. ET
Nothing connected to the internet is safe from hackers. And I mean nothing.
Modern cybersecurity is a constant cycle of breaches and patches. Systems are compromised, security experts play catch up, and eventually hackers find a new way in. Each side tries to outwit the other. But at any given moment, one of them is always a step ahead.
President Donald Trump doesn’t seem to understand that. “Putin & I discussed forming an impenetrable Cyber Security unit so that election hacking, & many other negative things, will be guarded,” he tweeted on Sunday.
Yes, Russia. Yes, really.
Setting aside the question of what “many other negative things” Trump and Putin plan to guard, and how; and setting aside the absurdity of the idea that the United States would partner with Russia, of all countries, on a cybersecurity initiative, there is a basic question to answer: Is “impenetrable cybersecurity” even possible?
No, it is not. (Trump later acknowledged as much, but more on that in a minute.)
“Anything connected to the internet is by definition vulnerable,” says Robert Cattanach,   a partner with the law firm Dorsey and Whitney who specializes in cybersecurity. “The clients I work for who are serious about protecting their ‘crown jewels’ keep that information in an isolated, unconnected server, locked in a limited-access room with no connectivity to the outsider world.”
“Protection only can go so far,” he added. “After that you’re relying on detection processes and response protocols.”
Trump’s Incoherent Ideas About ‘the Cyber’
Give hackers enough time and money to break into a system, and they’ll often find a way to do it.  And the thing is, it’s not always easy—or even possible—to detect a breach. That’s especially the case when you get to the highest levels of hacking, with state actors fighting against each other across complex networked systems.
“If you’re at war with a foreign power and they drop a bomb on you and that’s why your power doesn’t work, you know who did it,” John Kelly, the founder of network analytics firm Graphika told me in a conversation before the Putin-Trump meeting. “One of the problems with cyber attacks is you may not know who did it. And even if your intelligence services know, you can’t prove it to the world.”
No wonder, then, that Putin is telling the United States to prove it; and asking for evidence of Russia’s alleged interference in the 2016 presidential election, according to the Secretary of State, Rex Tillerson. When you couple actual hacking with what Kelly calls “hacking of the mind,” like attempts to sway public opinion, things get thornier still. Several American intelligence agencies confirmed in a Director of National Intelligence report earlier this year that they have “high confidence” that Putin ordered an influence campaign in 2016 aimed at undermining “public faith in the US democratic process.” Social publishers like Facebook have suggested they’ve detected similar activity across their networks.
Yet without hard evidence, and at a time when Americans’ faith in democratic institutions is nosediving, Russia can deny, deny, deny. Apparently that’s enough for Trump, who says now that Putin has “vehemently denied” meddling in the election, he wants to “move forward in working constructively with Russia!”
“If I had to choose, I’d let the power grid fail, and let our democracy be strong,” Kelly says. “If you completely erode the underpinnings of democratic society, everything else can be working, but you’re still broken. That’s a lot harder to fix.”
[Update:] On Sunday night, about 30 minutes after this article was originally published, Trump seemed to walk back his earlier comments on cybersecurity. “The fact that President Putin and I discussed a Cyber Security unit doesn’t mean I think it can happen,” he tweeted. “It can’t.”


Two dozen mountains drape themselves diagonally across the middle of Acadia National Park on Mount Desert Island in Maine. The peaks start near Bar Harbor in the northeast, tumble down across Jordan Pond, Somes Sound, and Echo Lake, and end with Mansell and Bernard Mountains in the southwest. Every one of these mountains, like the state of Maine, all of New England, and much of the continent of North America, has been scoured by glaciers during multiple ice ages of the past. The most recent glacier began its retreat about 17,000 years ago.
In the mid-19th century, the Swiss-born geologist Louis Agassiz visited Maine on a vacation from Harvard University. The glacier-scoured bedrock formations on Mount Desert Island gave him plenty of evidence to support his theory of the Ice Age: that enormous glaciers, rather than a flood, had once covered much of Europe and North America. A few years after his visit, in 1867, Agassiz published two essays on “Glacial Phenomena in Maine” in The Atlantic (part I, part II). He used a French term to name what he saw, calling the glacially shaped formations roches moutonnées. The meaning of this term has caused confusion in geology ever since.
* * *
To this day, many geologists follow Agassiz in considering all the rounded peaks and outcrops on Mount Desert Island to be roches moutonnées. Other geologists, including Duane and Ruth Braun, the co-authors of Guide to the Geology of Mount Desert Island and Acadia National Park, reserve this curious French term for the most distinctly asymmetrical of the mountains, ones with a long gradual slope along the northern “up-ice” side (the direction from which the glacier advanced) and cliffs on the lee or “down-ice” side. A mountain known as the Beehive, whose steep, plucked, south-facing cliffs terrify hikers with vertigo, illustrates the accepted American definition of roche moutonnée.
Indeed, the “Glossary of Glacial Terms” published in 2013 by the U.S. Geological Survey (USGS), defines roche moutonnée in just this way, as “an elongated, rounded, asymmetrical, bedrock knob produced by glacier erosion. It has a gentle slope on its up-glacier side and a steep to vertical face on the down-glacier side.” Interestingly, the USGS definition of the landform makes no attempt to translate the French roche moutonnée—and perhaps for good reason. It’s tricky, it turns out, to turn the expression into sensible English.
Roche means “rock.” Moutonnée is an antiquated word. Derived from the French word for sheep, mouton, it has no exact English equivalent. Some dictionaries have no listing for it; others say that the adjective means “sheep-like” or “fluffy.” In the 17th and 18th centuries, the word was used to describe hair that was curled and styled to resemble lamb’s wool.
As a geological term, roche moutonnée has been used for more than two centuries in a widespread, if not consistent way. The translation often given for the expression on websites and in textbooks is “sheepback,” a reasonable compromise. In a beautiful mid-19th century painting by Rosa Bonheur, for example, it’s possible to see asymmetrical “mountains” in the contours of her sheep’s hindquarters.
Stephen Marshak, an author of popular college geology textbooks, notes the resemblance to a sheep’s hindquarters in his definition of roche moutonnée. When one looks at the “granite and metamorphic rock hills of Acadia National Park,” he explains in Earth: Portrait of a Planet, “the hill’s profile resembles that of a sheep lying in a meadow.” Similarly, Julia A. Jackson’s comprehensive Glossary of Geology gives as synonyms for roche moutonnée both “sheepbackrock” and “sheeprock,” suggesting that if in doubt about a glacial landform, one should simply imagine a sheep’s contour.
The similarity between glacially scoured rocks and sheep is even more intriguing if one interprets the expression roches moutonnées more broadly, as did the 19th-century French geologist Albert de Lapparent. In his book Les Anciens Glaciers (“Ancient Glaciers”), Lapparent explained the origin of the term this way:
All the unevenness of the rock appears rounded on the upstream side, and [. . .] when one turns one’s eyes downstream, one has the impression of a cluster of sheep backs, whence comes the name “roches moutonnées,” used to designate these landforms so very characteristic of glacial action.
For Lapparent in 1893, the expression makes sense in the plural: A cluster of glacially sculpted, asymmetrical rocks look like a “suite” of sheep, whose backs are similarly curved and plunging. Lapparent assumes that the term came into being because of this resemblance, and his authority on the matter influenced scientists throughout the following century.
Some geology websites, however, understand a different sort of similarity between sheep and rocks. They translate the French expression not as “sheepback” but as “fleecy rock,” shifting attention from contour to texture. The rounded “backs” of glacially polished rock hardly look fleecy or “frizzy” (another translation). “Moutonnée” does mean something closer to “fleecy” than it does to “sheepback”—but one has to strain to see these sculpted landforms as woolly. They look smooth and slick instead.
* * *
It takes a bit more detective work to figure out what is going on. Many geology textbooks and glossaries, including the current entry in Wikipedia, note that the term roche moutonnée originated with Horace-Bénédict de Saussure, an 18th-century Swiss geologist who traveled extensively through the Alps. He made his observations about 50 years before the advent of modern glacial theory, but he noticed a pattern that would become important for future glaciologists. According to Saussure, in some spots in the Swiss Alps, one could find smoothed rocks with a peculiar appearance: They looked to him like wigs. The “rounded, contiguous, repeated rocks (rochers) resemble,” wrote Saussure in 1786, “a thick mane of hair, or the wigs we also call moutonnées.”
It’s not entirely clear which wigs Saussure has in mind, as it’s hard to find a wig actually called a “moutonnée.” One mid-18th century hairstyle worn by French women was called the tête de mouton; it involved a smooth back and rows of curls in the front, in what one might imagine to be the same sort of asymmetrical arrangement as the roche moutonnée.
Most geologists, though, have assumed that Saussure had in mind a more dramatic mane of hair. They assert that he was referring to wigs shaped and attached with mutton grease pomade. In that case, the slick, hairy waviness explains the name’s origin—not a resemblance to the contour of a sheep’s back. This second theory is the one that the British geologists Douglas Benn and David Evans promote in their 1998 study Glaciers and Glaciations. Saussure named these landforms, the authors explain, “based on a fancied resemblance to the wavy wigs of [the late-18th century], which were called moutonnées after the mutton fat used to hold them in place.” Authoritative indeed, though Benn and Evans give no source.
Since its first coinage in 1786, Saussure’s name for glacially sculpted rocks and mountains has taken on a life of its own. In the 19th century, when the modern theory of glaciers was introduced, glaciologists plucked two terms—rochers and moutonnées—from Saussure’s famous studies of the Alps and brought them together in the expression roches moutonnées, losing sight of the essence (and essential wit) of the analogy he had used. Saussure’s happy observation that certain landforms looked like big hair or mutton wigs was put aside; wigs had gone out of fashion by the 19th century, and it must have seemed obvious to geologists that Saussure meant for us to see the contour of ovine bodies in roches moutonnées, not their fleece. At one point in the late 19th century, a scientist named Grenville Cole reread the original passage in Saussure and, alarmed, publicized the mistaken translation. But his attempt to reinstate the wig analogy was only partly successful, and through the decades the two not quite compatible explanations have coexisted in strange and wonderful ways.
Even now, geology websites make heroic attempts to reconcile the wig and the sheep-hindquarters theories. A Candian earth sciences website gives my favorite example. Describing Old Fort Point hill in Alberta, the website tells us that Saussure, who was “the first to clearly define the term geology,” also coined roches moutonnées to name landforms that “reminded him of the mutton wigs worn by the French aristocracy.” But then it continues:  
Smooth and rounded on the back, combed upward in the front, such a wig looked rather like a sheep lying down. Which is pretty much the shape of any bedrock hill carved by moving glacial ice. Old Fort Point is a classic roche moutonnée.
As a crowning touch, the website displays an image of a fluffy lamb’s wool wig, whose resemblance to a resting sheep and to the asymmetrical Old Fort Point hill pictured next to it can be seen—with some effort, anyway. Recognizing the humor in the fanciful, multi-layered analogy, the site goes on to note that “a sheep-back hill” is also the home of bighorn sheep. Thus, the two readings of Saussure are wedded together, making a reader unlikely to forget this piece of geological lore.
* * *
The Old Fort Point website has it mostly right: The glacially sculpted boulders and hills known today as roches moutonnées look like 18th-century wigs, which apparently got their name from mutton fat, and which also happen to look like the backs of sheep whose fat was used to slick them down. But there’s one last twist. The rounded up-ice surface of a roche moutonnée has on its back parallel scratched lines running roughly north-south, in the direction of the advancing ice sheet. Rocks lodged under the heavy ice scored the bedrock underneath, and from a distance, these parallel scratches (called striations) could be said to resemble combed hair.
The explanation of the roche moutonnée is surely more complicated than Saussure meant it to be. It has adapted itself nicely, though, to the needs of geologists who try to make sense of the formations they study. Saussure himself, I like to think, would have admired the bald peaks of Mount Desert Island, whose name, too, comes from the French. In 1607 the explorer Samuel de Champlain noted the place on his map of the Maine coast, calling it l’île des monts déserts, the island of bare mountains. Had Saussure been with him on that voyage, the mountains that looked so stark might have sprung to life—as a suite of grazing sheep or a well-oiled wig.
This article appears courtesy of Object Lessons.


Yet another announcement came yesterday: Volvo, the Swedish slash Chinese car company, announced that it will only offer electric or hybrid vehicles by 2019. It was widely hailed as a bold move.
Previously, the company had committed to selling 1 million hybrid and fully electric vehicles by 2025.
Right now, hybrids have 2 percent of the passenger car market in the U.S., and completely electric vehicles are still a rarity. Even in EV-friendly California, battery-powered cars only hold 2.7 percent of the market.
But in a car industry roiled by self-driving vehicles and self-promoting Tesla, which is now valued as highly as General Motors and far more highly than Ford, automakers have to sell more than cars to be seen as exciting by Wall Street. They’ve got to be technology companies, not manufacturers. And that means developing autonomous systems, rethinking the motive power under the hood, and figuring out the art of bold pronouncements.
So, I compiled all the grand promises that the world’s traditional carmakers have made in the past two years or so, and one thing is clear: Either the automotive world is going to undergo a radical transformation around 2020, or these companies have seriously erred in their planning.
Volkswagen
Volkswagen corporate is engaged in a major initiative they’ve dubbed “Together-Strategy 2025,” which ties together the electrification and smartening of cars. As part of that, they’ve promised to “bring highly automated driving functions to market as a core competency from 2021.” Recently, they introduced an on-demand self-driving car-like thing, which sort of looks like a character in Thomas the Tank Engine: Future Edition. They call it Sedric.
Audi, which is a part of the Volkswagen Group, has been more aggressive. They announced they’re rolling out “Level 3” automation—which means a car that can drive itself some of the time—in the A8 model this year with promises to bring fully autonomous vehicles to market in 2020. Audi has also promised that 30 percent of sales will be partially or fully electrified vehicles by 2025.
On the electric side, the company has promised a sporty little electric vehicle called the I.D. by 2020. VW had bet heavily on what it termed “clean diesel” technology as its powertrain of the future. But as last year's major scandal revealed, their engines weren't that clean under normal road conditions. Instead, the company's engineers had built them to run artificially well under testing conditions (and only under testing conditions). That's left the company with a reputation to repair and a newfound interest in the benefits of electric vehicles.
General Motors
GM’s big self-driving play so far has been its Super Cruise system for semi-autonomous highway driving. After a delay, Super Cruise will debut this fall on the Cadillac CT6. Drivers will only be allowed to switch on the autopilot when they’re on the highway, and the company has built sensors that track drivers' head movements and prod them to keep focused on the road, even when the computer is driving.
Of course, the company has plans to deliver fully self-driving cars in the early 2020s. They acquired self-driving startup Cruise Automation (for $1 billion) and have plans to hire 1,100 more technology-focused workers in California, centered around San Francisco.
GM has already delivered on a major promise in electrification with the introduction of the Chevy Bolt EV earlier this year. It’s rated at 238 miles per charge — and costs about $36,000 before tax incentives, which in California could bring the price down to $25,000.
The company has also promised that it will have 10 electric models in the Chinese market by 2020, and expects sales of 500,000 EVs there by 2025.
Hyundai-Kia
Hyundai showed off an electric, autonomous concept car at CES in 2017 called the Ioniq, but it has been circumspect about its plans.
But the Hyundai-Kia automotive group’s parts supplier Hyundai-Mobis has already laid out its roadmap for the next decade. The company will offer some components for autonomous systems by 2020, with mass production coming in 2022. They described themselves as in “fast follower” rather than “leader” mode. And they’re trying to make self-driving work without the laser systems that add to the price and complexity of systems like Waymo’s.
Ford
In August 2016, Ford promised a fully self-driving car—sans steering wheel—by 2021. The company reaffirmed that commitment this spring. Ford’s plan is to have self-driving cars available in some sort of ride-sharing fleet.  The company is also aggressively investing in and snatching up startup companies working on the foundational technologies for self-driving cars like sensors and maps.
As for electrification, Ford has promised it will add an EV SUV with a range of 300 miles by 2020.
Nissan-Renault
Last month, Nissan and Renault announced a partnership to create a self-driving car ride-sharing service. The companies said the service wouldn’t launch by 2020, but “certainly within 10 years,” or by 2027.
Nissan CEO Carlos Ghosn had made several big promises about self-driving cars. Back in 2013, he said that the company would have autonomous vehicles for sale at reasonable prices by 2020. He reaffirmed that in 2015 (with some caveats). And in 2016, the company said it would have 10 car models with some autonomy baked in by 2020.
The 2018 Nissan Leaf will have enough self-driving features to allow for autonomous single-lane highway driving.
Electrification-wise, Nissan has made the Leaf the second-best-selling electric vehicle (after Tesla, of course).
Daimler
Like many other car companies, Daimler—the maker of Mercedes—sees an opportunity in moving their business toward ride-hailing with self-driving cars at the core of the operation. Daimler cut a deal with Uber in January to partner on self-driving cars. And in April, they announced another one with Bosch to deliver autonomous taxis to the road in 2021.
Also in April, the company announced that it was bringing up its electric vehicle rollout date from 2025 to 2022. That’s when Daimler expects to have 10 EVs in its lineup.
BMW
BMW has been something of a frontrunner in autonomous vehicle development. In 2016, the company committed to introducing a fully autonomous vehicle by 2021. In May of this year, BMW struck a deal with Intel and major automotive supplier Delphi to create an alliance to meet that goal, possibly with other car companies who might want to join in.
The company expects to sell 100,000 electric vehicles in 2017.
Toyota
Toyota, the world’s largest automaker, has played it cool for the most part. In 2013, the company emphasized that humans must remain in control of vehicles. But in October 2015, Toyota promised that vehicles “with an autopilot switch” would be on the roads of Tokyo by 2020. (They’re working with chipmaker Nvidia.)
The company has had mainstream success with hybrid vehicles, but has not gone all-in on electrification like some of its competitors. In part, that’s because Toyota continues to push fuel-cell vehicles.
Honda
Honda has promised to have fully autonomous cars on the road by 2025. Along the way, the company plans to have lower level self-driving features such as supercharged cruise control in cars by 2020. That’s several years behind its competitors. But hey: It’s no sure thing that the first-generation of autonomous vehicles are going to be a success.
Mazda
Mazda might be the only true contrarian among the traditional automakers: They still want to make cars that you buy and drive. They are only interested in developing driver-assistance technology, their CEO said in April.
Subaru
Japan’s smallest carmaker does not currently have a promise around self-driving cars — and is planning its first plug-in hybrid electric car next year.
Tata
Tata Motors, India’s largest carmaker, has asked Indian authorities to test an autonomous vehicle, but many people are skeptical that self-driving cars will work well on India’s chaotic roads.
SAIC
We don’t know much about China’s largest car maker’s self-driving car plans, aside from a partnership with U.S. sensor maker Savari. The company maintains a research center in Silicon Valley and recently received a permit to test its vehicles on California roads.


Volvo, the Chinese-owned Swedish automaker, announced Tuesday that starting in 2019 it will only make fully electric or hybrid cars.
“This announcement marks the end of the solely combustion engine-powered car,” Håkan Samuelsson, Volvo’s president and chief executive, said in a statement.
The move is a significant bet by the carmaker that the age of the internal-combustion engine is quickly coming to an end—though the Gothenburg, Sweden-based automaker is lone among the world’s major automakers to move so aggressively into electric or hybrid cars. Volvo sold around half a million cars last year, significantly less than the world’s largest car companies such as Toyota, Volkswagen, and GM, but far greater than the 76,000 sold by Tesla, the all-electric carmaker.
Volvo said Wednesday that starting in two years, its cars will be electric, plug-in hybrids or mild hybrids. Between 2019 and 2021, Volvo will launch three fully electric cars and two high-performance electric cars from Polestar, which will be spun off into a separate premium car company. Volvo said these five cars “will be supplemented by a range of petrol and diesel plug in hybrid and mild hybrid 48 volt options on all models.”
“This means that there will in future be no Volvo cars without an electric motor, as pure ICE cars are gradually phased out and replaced by ICE cars that are enhanced with electrified options,” the company’s statement said.
Ever since it was bought in 2010 by Geely, the Chinese automaker, from Ford, Volvo has invested heavily in a range of new vehicles, and seen a steady growth in its share of the market. The New York Times adds:
But by focusing on electrification, Volvo can concentrate its limited research and development resources on new technologies rather than continuing to invest in fuel-powered motors that may become obsolete. ...
Volvo … will also be able to draw on technology developed by its parent company. Geely sells electric cars in China, one of the fastest growing markets for battery-powered vehicles.
    
Samuelsson, the Volvo executive, said that Wednesday’s announcement is in line with the company’s plan to have sold 1 million electrified cars by 2025.
“When we said it we meant it,” he said. “This is how we are going to do it.”


The rules are simple, okay? No threats of violence. No targeted abuse or harassment. No inciting anybody else to engage in targeted abuse or harassment. No hateful conduct.
Now think about Donald Trump’s tweeting habits. Is he breaking those rules, which come from Twitter’s terms of service?
Trump has long been criticized for his impulsiveness, but less than six months into his presidency, alarm over his Twitter conduct has hit fever pitch.
On Sunday morning, Trump tweeted a short video clip showing him pummeling another person outside of a wrestling ring—with the other person’s face blocked out by the CNN logo. If that’s not a direct threat of violence against the American citizens who work for CNN, it’s certainly a celebration of violence.
#FraudNewsCNN #FNN pic.twitter.com/WYUnHjjUjg
The president is not only aware of the firestorm he’s ignited, he appears to be relishing it. “My use of social media is not Presidential,” Trump tweeted on Saturday. “it’s MODERN DAY PRESIDENTIAL.”
These latest messages came came on the heels of a bizarre barrage of tweets—odd even by the president’s standards—that set off a new round of scrutiny of his use of social media. Beginning on June 29, Trump began tweeting repeated insults at Joe Scarborough and Mika Brzezinski, the hosts of the MSNBC talk show, Morning Joe. Trump’s treatment of Brzezinski was particularly strange. In addition to calling her “dumb,” “crazy,” and “low I.Q.” in three separate tweets, he claimed that she and Scarborough traveled to Mar-a-Lago for New Year’s Eve and insisted on seeing Trump while Brzezinski was “bleeding badly from a face-lift.” (Brzezinski and Scarborough published a rebuke in The Washington Post, calling the president’s claim “a lie.”)
In true Trump fashion, the president doubled down, calling Scarborough “crazy” and Brzezinski “dumb as a rock.”
Does that constitute targeted harassment? And given Trump’s huge following—more than 33.1 million Twitter followers on his primary account—does a string of attacks against the same two individuals constitute inciting harassment? “We don’t comment on individual accounts, for privacy and security reasons,” a Twitter spokesperson told me on Saturday. Twitter also declined to tell me whether, when considering the question of a user inciting harassment, it takes into consideration that person’s number of followers or public status—a movie star or politician, for example.
Twitter’s website does offer some clarification on how it assesses abusive behavior. The company says it assesses whether the primary purpose of an account is to harass or send abusive messages; and it looks at whether the reported behavior is “one-sided.”Setting aside Twitter’s notoriously bad track record for actually enforcing its own standards on harassment, the question of one-sidedness poses an interesting problem here.
When one of the people involved in a Twitter fight isn’t just a public official but also the president of the United States, is it fair to consider anyone he’s attacking an equal player in a fight?
We know what Trump would say. This is a man whose 2007 book Never Give Up has multiple chapters dedicated to the subject of fighting with people. There’s Chapter 5 (“I Love a Good Fight”) and Chapter 29 (“You Will Be Attacked For Trying to Change Anything”) and Chapter 38 (“When You’re Attacked, Bite Back”). If Trump doesn’t like what a person says about him, he attacks them. Period.
But Trump’s Twitter conduct also raises a question about what Twitter is, and what it should be. Often, the service is treated as a new kind of public square, a place for the unfiltered exchange of ideas (and, clearly, hurling of insults). Silicon Valley has rarely stepped in to correct the persistent cultural conflation between the actual right to free speech—that is, the constitutionally protected right that says the government cannot make a law that inhibits people’s freedom of expression—and the idea that people should get to say whatever they want wherever they want to without consequence. (Complicating things further, Twitter must answer to its shareholders, and having the president use its service so routinely—and so bombastically—certainly keeps the service relevant.)
In reality, though, Twitter is a media company. Just like CNN and The New York Times are media companies. Except, unlike in a traditional model where publishers and readers are distinct groups, everyone can be both on Twitter. So what’s a company like Twitter to do when one of its users—who is also the president of the United States, by the way—incessantly publishes attacks against individuals? Nothing, apparently. At least nothing yet. The thornier question is: What should it do? Only rarely would any news organization turn down the opportunity to exclusively print or broadcast a message from the president. (U.S. senators and presidential candidates, however, are another story.) Though it’s not like the president doesn’t have plenty of opportunities for his voice to be amplified. He has said he likes Twitter because it’s a direct channel to the American people, but he has his own website where he could be live-streaming or blogging, for instance. He is also a constant subject of media attention; his press conferences—when the White House permits it—are broadcast over cable and network television.
Presidents have historically made good use of new media platforms. Franklin Roosevelt’s fireside chats may seem quaint to us now, but they were a revolutionary experiment with a nascent media platform when they began in the 1930s. But, as with all things Trump-related, looking to norms and historic conventions can only get you so far. Imagine if Roosevelt had used his radio access to relentlessly criticize individual Americans by name. Trump knows that his critics are disgusted by the way he represents the country on Twitter, and he trusts that his supporters delight in their disgust.
“It never stops, and I wouldn’t have it any other way,” he wrote in The Art of the Deal. “I try to learn from the past, but I plan for the future by focusing exclusively on the present. That’s where the fun is. And if it can’t be fun, what’s the point?”
If Twitter were to suspend or even outright ban Trump, his most fervent left-wing critics would surely rejoice. His supporters would likely boycott Twitter. Their outrage could help him keep their support. And in Trump’s worldview, this may well look like a win-win.


The Golden Record was never meant for this planet. Yet it has remained an object of curiosity on Earth, even after decades of hurtling through the void of outer space.
In fact, the Golden Record has had something of a revival lately. For years, there’s been talk of making a modern, internet-crowdsourced follow-up to the original 1977 version. The original record plays a prominent role in the new young-adult novel, See You in the Cosmos, by Jack Cheng. And a recent Kickstarter campaign to reissue the record on vinyl raised nearly $1.4 million, seven times more than its fundraising goal. Last fall, around the time that Kickstarter campaign launched, I found myself revisiting the record’s tracks.
In doing so, I stumbled upon a mystery.
* * *
In the late summer of 1977, NASA launched twin spacecrafts—Voyager 1 and Voyager 2—as part of a mission to better understand Jupiter, Saturn, and the outer solar system. As a bonus: Each probe carried a gold-plated copper phonograph that contained sounds and images from Earth. The idea was to send something into the universe that demonstrated humanity’s wish to join a “community of galactic civilizations,” as President Jimmy Carter put it at the time, and to express good will to intelligent life elsewhere. It was also meant as a cosmic postcard, of sorts, a way of sharing the experience of living on Earth with intelligent life elsewhere.
The record, curated by a team led by the astrophysicist Carl Sagan, featured the music of Beethoven, Chuck Berry, Kesarbai Kerkar, and Blind Willie Johnson, and various folk music from around the world. Images, placed electronically on the phonograph, included photographs of a mother nursing her baby; a woman with a microscope; an astronaut in space; highway traffic in Ithaca, New York; the pages of an open book; a violin with sheet music; men laying bricks to build a house in Africa; a woman eating grapes at a supermarket; and a number of diagrams and illustrations of concepts like continental drift and vertebrate evolution. There were also audio clips depicting scenes of life on Earth—the sounds of rushing wind and the roar of ocean tides, whale songs, elephants trumpeting, human footsteps and human laughter.
It occurred to me last fall that I’d never actually heard the laughter track—and that I wanted to. What sort of laugh did the record’s producers select as a depiction of our species? And whose laugh was it? It could have been a chuckle, a snort, a guffaw, a snicker. It could have been anything from the irresistible staccato of a baby giggling to a the deep-throated mwahaha of a Hollywood villain. But my idle curiosity led me to more and more questions, and those questions turned into a months-long investigation into the origins of the Golden Record.
The contents of the record have been debated for decades—among its creators, certainly, but also among Golden Record enthusiasts. The project posed an impossible cultural and technological challenge to begin with: How do you carve an entire planet into the grooves of a single record—a record that’s also lightweight enough to piggyback  onto a 1970s space probe—all while capturing the richness of the human experience? Oh, and by the way, please make the whole thing potentially comprehensible to the alien civilization that might discover it 100 million years from now.
The team that conceived of the project first envisioned it as a record to be played at the conventional 33 and one-third revolutions per minute, with music on one side and non-musical information like photographs on the other. Eventually, they settled on 16 and two-thirds revolutions per minute—which meant a slightly lower quality sound, but not terribly so. The slower speed also meant they’d have 90 minutes available for music rather than the original 27 minutes.
The 12-minute audio essay that included the sounds of waves, elephants, and laughter was meant to capture the auditory experience of life on our planet. Those sounds were organized as a montage echoing the evolution of life on Earth—beginning with a “giddy whirl of tones reflecting the motions of the Sun’s planets in their orbits,” as the record’s creative director, Ann Druyan, put it in her 1978 essay about the project. Those ethereal notes gave way to sounds of an earthquake, thunder, mud pots, wind and rain, crickets and frogs, hyenas, birds, chimpanzees, and eventually humans. The first appearance of a human in this montage is the sound of footsteps, then laughter. That laugh is the first human utterance in this representation of the evolution of our species. This is fitting.
“Laughter is ancient,” says Robert Provine, the author of Laughter: A Scientific Investigation. “Laughing, like crying, is a human instinct. It’s not under conscious control. Whereas crying is a solicitation of caregiving, laughter is the signal of play. It is the sound of play, literally.”
Laughter is one of humanity’s most joyful peculiarities. Infants typically laugh long before they can speak. Laughter transcends differences in language entirely, yet remains a deeply important element of cultural and social interaction. Humans aren’t the only creatures that laugh—a chimpanzee’s laugh sounds like a dog panting; a rat laughs in ultrasonic chirps—but the rhythm and cadence of human laughter is unique to us. Humans also know, without really realizing, exactly what laughter sounds like. It’s a signal of play that we know immediately when we hear it, characterized by short bursts of sounds that last about one-fifteenth of a second and repeat in intervals each fifth of a second, Provine told me.
Selecting the right laughter for the Golden Record would have been less fraught, presumably, than selecting which language to feature on the record. Indeed, the language question created all sorts of difficulties. Sagan had suggested a day or two of recording at the United Nations headquarters in New York City, where delegates from each member nation might record a “hello” in their native language. “I had hoped that something like half of the voices could be male and half female, in order to reflect the distribution of sexes on the planet Earth,” he wrote in Murmurs of Earth, a book about the project published in 1978. “I was told that this was quite difficult on entirely other grounds. Virtually all the chiefs of delegations were male, and it was unlikely that they would delegate the privilege of saying ‘hello’ to the stars to anyone else.”
And so laughter, for all its complexities on Earth, was simpler to represent than language on the Golden Record. Except, when you listen to the “Footsteps, Heartbeat, Laughter” track of the “Sounds of Earth” audio that’s on the Voyager website—or to the version uploaded by NASA to Soundcloud—you won't actually hear any laughter. What happened to it? And whose voice is supposed to be there? I started to ask around.
“You are right that on the Voyager website there is a 30-second clip of a heartbeat and footsteps but not laughter,” a staffer at NASA’s Jet Propulsion Laboratory told me in an email when I asked what had happened. “Indeed, the NASA Soundcloud also has the same clip.” But there was a version of the recording where you could clearly hear the laughter, the staffer told me, on a website hosted by the Massachusetts Institute of Technology. NASA couldn’t verify the authenticity of that recording, but it was a start.
I listened, and there it was. The laughter is secondary to the heartbeat and footsteps. It starts low then tapers off into a light cackle. It sounds genuine, like the person is truly delighted by something. It only lasts a few seconds.
The laughter was there—but was this version of the Golden Record legitimate? My attempts to contact the website’s maker yielded little. The audio on the site originally came from the Library of Congress, I was told, but Library of Congress staffers couldn’t tell me for sure whether it was the same as the copy they had. (Nor did the Library of Congress have any additional information on the identity of the laugher.) Eventually, an archivist at the JPL was able to send me a sound file containing all 21 tracks from “The Sounds of Earth.” This was the official recording, but much better quality than what NASA had put online. It sounded just like the version on the MIT site—the crunch of footsteps, a steady heartbeat, and then the welling up of laughter. (Start around the 6:10 mark to skip right to the laughter in the audio-player embedded below.)

But that still didn’t solve my original question of whose laughter was on the record, or the question of why it had disappeared from the other official recordings. I couldn’t find anybody who could answer either question.
Not NASA, not the Library of Congress, not the Carl Sagan Institute at Cornell, where Sagan was a professor, and not NASA’s Jet Propulsion Laboratory. “JPL-Caltech was not involved in the creation of the audio recordings on the Voyager Golden Record,” a spokesperson there told me, “and has no knowledge of the identities of the persons recorded or those who did the recording.”
The official book about the Golden Record, Murmurs of Earth, contains a trove of fascinating background about the making of the record—including the painstaking process of gathering audio for the Sounds of Earth section—but there’s nothing in it about whose laugh is featured. I tried to arrange a time to speak with Ann Druyan, the producer and filmmaker who spearheaded audio collection for that portion of the record, but I couldn’t reach her. (Druyan and Sagan worked closely on the project, and later married.)
So I started contacting people listed in the “acknowledgements” section of Murmurs to see if anyone could remember. Nothing.
“I wish I could help you... but I don’t know,” wrote Lise Menn, a linguist and professor at the University of  Colorado who is listed in the book as having provided audio for the mother-and-child track. “I’m not even sure whose voice is on the mother-and-baby segment, although it may have been from [the linguist Margaret] Bullowa’s files—I was her assistant. If Dr. Druyan doesn’t know, I think we’re at a dead end.”
Fall turned to winter, winter to spring, spring to summer. It seemed that my mystery would remain unsolved. Meanwhile, the Golden Record was gliding ever farther into space, receding from our planet at 35,000 miles per hour. The two Voyager probes are now nearly 14 billion miles and 11 billion miles away from Earth, respectively. “Out there, our concepts of velocity become provincial,” Timothy Ferris, one of the record’s producers, wrote 10 years ago, anticipating the moment at which the space probe Voyager would tip across the threshold to interstellar space. That finally happened in 2014. At the time, Ferris described Voyager as a toy boat on a dark sea of stars swirling in “gigantic orbits around the center of the Milky Way galaxy.” He isn’t the first to evoke maritime comparisons. In 1986, the astronomer William Gutsch described Voyager as “a bottle cast by its creators, adrift on a cosmic ocean.” The Golden Record was a message in that bottle.
The people who made the record believed that, in the vacuum of space, it could remain in working condition for a billion years. The capsules protecting the records were configured in such a way, Sagan once wrote, that “all of the pictorial information, human and cetacean greetings, and ‘The Sounds of Earth’... will survive essentially forever.”
“If I had to guess, I’d say it’s as fresh and new as the day it was placed aboard the spacecraft,” David Doody, an engineer on the Voyager mission at NASA’s Jet Propulsion Laboratory told me a few years ago. “It’s been stored in a vacuum more perfect than any attainable on Earth, and protected from dust and cosmic rays by an aluminum metal case.”
Out there in space, the record swims onward through the stars. Back here on Earth, despite people’s enduring fondness for the project, little bits of context are vanishing. The answers to my questions about the laughter on the Golden Record seemed destined to be lost to time.
Then, two days ago, I received an unexpected message from someone at the Jet Propulsion Laboratory. “I met Ann Druyan today,” the lab’s Elizabeth Landau told me, “and she told me that the laughter on the Golden Record is Carl Sagan’s!”
I made half a dozen phone calls and sent out a flurry of emails—to the Golden Record’s producers, to NASA, to Druyan, to Sagan and Druyan’s children—all in an attempt to corroborate the account I’d received.
Was the laughter indeed Sagan’s? It certainly seemed plausible, but I had to be sure. Finally, I reached Sasha Sagan, the daughter of Carl Sagan and Ann Druyan.
“I just double checked with my mom to be absolutely sure, and yes, it is indeed my dad’s laugh!” Sagan wrote. “She said that laugh was the very first impression she ever had of my dad when she heard it upon entering Nora Ephron’s apartment [where they first met] in 1974, and included it in the Voyager sound essay because she wanted it to live on forever.”
The Golden Record has always been a love letter to humanity, and a love letter to the cosmos, but it’s also quite clearly a love letter between Druyan and Sagan, too. Druyan has said as much in past interviews when she’s described the afternoon she spent meditating at Bellevue Hospital. She went there to have electrodes attached to her body so a machine could register the pattern of electrical impulses in her brain and nervous system. The data from that session was etched onto the Golden Record, the idea being that some future, alien civilization might be able to convert EEG data back into comprehensible thoughts.
“This was two days after Carl and I declared our love for each other,” Druyan said in a radio interview years ago, “and so part of what I was thinking in this meditation was about the wonder of love, and of being in love. And to know it’s on those two spacecrafts... Now, whenever I’m down, I’m thinking, still they move, 35,000 miles an hour, leaving our solar system for the great wide open sea of interstellar space.”
It’s a cosmically romantic story. And if it sounds too good to be true, the Golden Record producer Timothy Ferris told me, that’s because he suspects it might be. “This is news to me,” he told me. He remembers Sagan’s laugh clearly, he said. “It doesn’t sound like what’s on that track. He had a great laugh. He had a good sense of humor.”
I wrote back to Druyan’s daughter to see what she thought. Was there even a chance Druyan was mistaken? Human memory is fallible, Druyan acknowledged in response via Sasha Sagan. It’s been 40 years since they made the Golden Record, and more than 20 years since Sagan died in 1996. But Druyan is quite certain about Sagan’s laughter and its place in the universe. “It’s highly unusual, virtually unique,” she said in an email via her daughter. “I chose it because of its exceptional lack of inhibition and because it was Carl’s.”
This morning, I went back and listened to the NASA file on SoundCloud and realized something else: It turns out the laughter is there, only it’s barely perceptible. Without hearing the better-quality version of the recording first, it’s next to impossible to register. “I've heard how bad the online versions are,” Ferris told me. It’s not clear why the version on NASA’s SoundCloud is so low-quality compared with the pristine audio in its archives.
The laughter is so faint. It’s mostly lost in the static. Billions of miles away, though, the original Golden Record is out there, still in mint condition. Which means Sagan’s laughter—if it is indeed his—may yet be heard in some faraway galaxy, by some species we cannot imagine. But that’s a mystery for another time.


Robot panic seems to move in cycles, as new innovations in technology drive fear about machines that will take over our jobs, our lives, and our society—only to collapse as it becomes clear just how far away such omnipotent robots are. Today’s robots can barely walk effectively, much less conquer civilization.
But that doesn’t mean there aren’t good reasons to be nervous. The more pressing problem today is not what robots can do our bodies and livelihoods, but what they will do to our brains.
“The problem is not that if we teach robots to kick they’ll kick our ass,” Kate Darling, an MIT robot ethicist, said Thursday at the Aspen Ideas Festival, which is co-hosted by the Aspen Institute and The Atlantic. “We have to figure out what happens to us if we kick the robots.”
That’s not just a metaphor. Two years ago, Boston Dynamics released a video showing employees kicking a dog-like robot named Spot. The idea was to show that the machine could regain its balance when knocked askew. But that wasn’t the message many viewers took. Instead, they were horrified by what resembled animal cruelty. PETA even weighed in, saying that “PETA deals with actual animal abuse every day, so we won’t lose sleep over this incident,” but adding that “most reasonable people find even the idea of such violence inappropriate.”
The Spot incident, along with the outpouring of grief for the “Hitchbot”—a friendly android that asked people to carry it around the world, but met an untimely demise in Philadelphia—show the strange ways humans seem to associate with robots. Darling reeled off a series of other ways: People name their Roombas, and feel pity for it when it gets stuck under furniture. They are reluctant to touch the “private areas” of robots, even only vaguely humanoid ones. Robots have been shown to be more effective in helping weight loss than traditional methods, because of the social interaction involved.
People are more forgiving of robots’ flaws when they are given human names, and a Japanese manufacturer has its robots “stretch” with human workers to encourage the employees to think of the machines as colleagues rather than tools. Even when robots don’t have human features, people develop affection toward them. This phenomenon has manifested in soldiers bonding with bomb-dismantling robots that are neither anthropomorphic nor autonomous: The soldiers take care of them and repair them as though they were pets.
“We treat them as though they’re alive even though we know perfectly well they’re machines,” Darling said.
That can be good news—whether it’s as weight-loss coaches or therapy aides for autistic children—but it also opens up unexplored ethical territory. Human empathy is a volatile, unpredictable force, and if it can be manipulated for good, it can be manipulated for bad as well. Might people share sensitive personal information or data more readily with a robot they perceive as partly human than they would ever be willing to share with a “mere” computer?
Social scientists (and anxious parents) have wondered for years about the effect of violent video games on children and adults alike. Even as those questions remain unresolved, an increasing number of interactions with robots will create their own version of that debate. Could kicking a robot like Spot desensitize people, and make them more likely to kick their (real) dogs at home? Or, could the opportunity to visit violence on robots provide an outlet to divert dangerous behaviors? (Call it the Westworld hypothesis.)
An even more pungent version of that dilemma could revolve around child-size sex robots. Would such a thing provide a useful outlet for sex offenders, or would it simply make pedophilia seem more acceptable? Making the dilemma more challenging, it’s extremely difficult to research that question.
The sway that even rudimentary robots can hold over humans was clear near the end of Darling’s talk. A short robot whirred out on stage to alert her that she had five minutes left to speak. The audience, which had just listened to a thoughtful, in-depth litany of the ethical challenges of human-robot interactions, cooed involuntarily at the cute little machine. And Darling, who had just delivered the litany, knelt down to pat its head.


Earlier this week, I asked parents to share their approach to protecting the privacy of their children as they begin to use devices with Internet access and social networks. The inquiry was inspired by an Aspen Ideas Festival talk where Julia Angwin and Manoush Zomorodi revealed how their reporting on privacy changed their parenting.
The parents who’ve replied so far are in agreement that the task is difficult.
Our first correspondent is a married woman in her mid-40's with a 12-year-old child. She lives in Irvine, California. She recently created a technology contract with her child.
She writes:
My 12-year-old doesn't yet have a Facebook account, and doesn't remember how to use her Instagram account. I've showed her Snapchat, but her friends don't use it, and she hasn't pushed for it. (She was understanding when I told her I'd deleted it because the filters were so racist.) I expect her upcoming 7th-grade year to involve a lot of change in what has up to now been very limited use of social media. She just this year got a smartphone, several years after many in her upper-middle-class-neighborhood public school did. She has a cheap tablet that she uses to watch Cartoon Network and Youtube. She watches hours of Youtube with little supervision, mostly young adults who are passionate about animation or crafting, and she texts with friends. So my level of awareness about what she does in digital spaces is fairly low - I have no idea who most of the you-tubers are who she watches. We do talk about them, and she knows we can access her browser
history, for now.
We did have a conversation this year after one of my husband's periodic browser-history checks turned up some moderately adult content (YouTube animations illustrating funny-in-retrospect sexual experiences, like getting caught by a parent), but I'm okay with her using the internet to look around a bit. We have talked for a while now about how the internet can get intimacy wrong, and these conversations are an important part of the general ongoing conversations about intimacy, sex, contraception, and consent.
My job as a librarian does involve a slightly higher than average level of involvement with online privacy issues, but that hasn't translated to at-home chats about higher-level information security. We're still more focused on issues around communication, and how the online setting can make it even harder to make mature, empathetic decisions that it is normally, especially for teens whose brains are still developing. So her concept of personal information is evolving, and I think it's going to be challenging to navigate that at the very same time she's navigating the complicated personal-growth time of the early teen years.
I would characterize my attitude towards dealing with the online world in parenting as resentful but resigned. I'm not scared of the internet, like I don't worry about rando sex offenders. Instead, I feel like the companies and governments that are collecting dossiers of information about individuals can't be trusted to get things right, and I feel like I need to teach my kid to deal with that, and that's a drag. We're trading so much for cute cat videos and easy access to coffee filters and hand cream.
She really likes those cute cat videos, though.
* * *
James has worked for more than 20 years in the IT industry and has nine children ranging in age from eighteen to three years old. “I'm probably in the minority in that I'm definitely not ‘out-teched’ by our kids, despite years of trying to get them interested in what goes on under the hood,” he writes. “My wife and I are one of the last cohorts to remember, fully, the time prior to the Internet and so it's much easier for us to end-run the whole thing by simply not participating. Suggesting the same to our kids or their peers is akin to asking them to give up one eye and both feet.”
Their advice distills down to a few first principles:
1. If you are not paying for the app, website, or service, you are not the customer. You are the product, and the way that they're making all their money.
2. There is no privacy online. Doesn't matter how clever you think you are with a nickname or how careful you are with your pictures and comments. Say the wrong thing at the wrong time and you will be unmasked, publicly, for the world to gawk at.
3. Don't say anything online you wouldn't own in person, or want read back to you by us over dinner.
As to our awareness - one rule we have for the few that have taken some steps into social media (Facebook and Instagram for the most part) - is that they stay connected with us so we can see what's going on. We're aware of at least one finsta; the child (who is 16) is also aware that we're aware.
Using tools borne from the professional experience I mentioned above, internet traffic is monitored regularly (and filtered). I pull up the dashboard, explain how it all works, show what I'm able to see - and not see. So far, these lessons seem to have take hold, especially as the kids get older and see the consequences in the news, whether it's doxing, texting scandals, or cautionary tales of students (or professionals) getting called out for their online activities and losing scholarships (or jobs) as a result of them.
* * *
Our next correspondent is a mother of two boys, eight and ten, and the Data Privacy Consultant for the California Department of Education. In her work, she has seen “both the power of data to tell compelling stories and power of data to wreak havoc.”
She explains:
When used for good, data can ensure personalized instruction / interventions for kids who are struggling, food for kids who are hungry, and more. When used for ill, data can be inconvenient (e.g., useless to answer important questions), annoying (e.g., telemarketers), and terrifying (e.g., identity theft). In constantly-connected, perpetually-hacked digital spaces, collection of any data comes with risks. As such, one must constantly be asking: (1) Is collecting/sharing these data legal and necessary to answer important questions? (2) Do the expected benefits outweigh the inherent risks? and (3) Is every conceivable, plausible measure being taken to minimize data collection, protect data assets, manage/utilize data to maximize benefit?
But awareness of those perils does not cause her to keep her kids offline:
My approach to screen time mirrors my approach to life in general. I don’t believe in living in a bubble, avoiding uncomfortable truths, pretending that I can control things that I cannot. But I do believe in being reasonable and thoughtful, staying informed and sharing information with others, striving each day to learn and do better. I believe in being cautious without being alarmist…in managing and minimizing risks without suffocating benefits. When it comes to my kids and any topic—including but certainly not limited to screen time—I hope I can imbue them with both the confidence to explore and the knowledge/skills to ask questions/seek help when they find they’ve wandered too far. In both the physical and the digital world, parenting is a perpetual lesson in letting go, a daily affirmation that control is an illusion.
I recently heard an analogy that I think makes a lot of sense. The analogy is this: We don’t teach our kids to be water safe by showing them a video, pretending water doesn’t exist, or refusing to let them get in the pool. We teach them to be water safe by suiting up, jumping in the pool with them, and helping them learn the skills that will minimize their risk of drowning. Carrying this analogy into the online space, my personal stance is that—whether we like it—our kids are in the pool. Technology is everywhere. As such, it’s up to us to get in there with them and guide them through mistakes and dangers so that someday…even when we’re not around to pull them out of the water…they can save themselves with smart decisions and well-honed skills. I’m a big fan of Web sites like Common Sense Media, FERPA Sherpa, On Guard Online, US Department of Education’s Protecting Student Privacy, and Soul Behind That Screen.
Here are the rules in her house:
(1) No screen time is permitted before my hubs and I wake up in the morning. If the boys rise before us, they need to find other activities (e.g., reading, Legos) to fill their time.
(2) My hubs and I are the holders of the passwords. If the boys want online access, we’re the gatekeepers. The boys understand that at any moment, Mom and Dad can and will check their history to see what they’ve been up to.
(3) Any abuse of screen time privileges will result in immediate revocation of said privileges. We have a very small house and Mama’s got very good ears. If a Minecraft or basketball YouTube video veers into inappropriate language or content, the boys are responsible to shut it down…IMMEDIATELY. If Mom or Dad have to come in to shut it down, the screen is going OFF for a good, long time.
(4)  When playing games with potential to connect with others, they are required to:
a.       Never connect with anyone they don’t know
b. Never share personal information
c.       Only use online IDs that are nonsensical and won’t reveal anything about who they are, how old they are, where they live, etc.
d. Only connect with friends whose identities have been verified by me or their dad (e.g., through a text to other parents verifying that the user name my kiddos want to connect with is affiliated with the kiddo we think it is)
e.       Tell my hubs or I immediately if anyone is pressuring them to share information
(5) Screen time ends at least 30 minutes before bed.
(6) We watch together. Especially when watching shows that aren’t necessarily targeted to their demographic, either my hubs and I watch ahead of time to make sure everything is copacetic before giving approval and/or we watch together so that we can shut things down or answer questions as appropriate. One example is Gilmore Girls. This is one of my favorite shows and I’ve been binge watching it with the boys. It’s sparked a lot of important conversations about coming of age. It has been a great tool for us to bond and talk about important topics. If they ask to watch or play something that I don’t think they’re ready for, I’ll give them the respect of watching at least a portion/researching so that I can cite specific reasons (language, sex, violence, etc.) that I think they should wait or avoid consuming the content altogether. Especially as they grow older, I won’t be around all the time to hover. I want them to be analytic, critical thinkers who make thoughtful decisions. As such, I try to let them experience what it’s like to ask, debate, research, consider the input of others, and draw conclusions. I try to avoid too many “No! Because I said so’s.”
(7) Family time, exercise, and chores are greater than screen time. Those who live in the house connect in the house, help in the house, behave in healthy ways in the house. The second the screen gets in the way of responsibilities to one another and ourselves, it’s time to go cold turkey for a bit and remember that we are NOT addicts fiending for a drug but humans who have the capacity to enjoy things in moderation.
* * *
Veronica is the parent of a 3-year-old, and while she has thought deeply about the ways she will try to protect her digital privacy she feels that regulatory changes are what’s ultimately needed:
The production and collection of my child's digital traces is really beyond my control. For instance, although I asked her pre-school not to share photos of her on Facebook, other parents do, and this leads to the fact that the Facebook's DeepFace technology has already stored her facial recognition data. Sometimes I ask parents to remove the pictures, but social media content is deeply interconnected with highly emotional and personal relationships, with the need to share experiences and give meaning to them, and sometimes - as a parent - it is simply not possible to 'opt out'.
Yet social media are just one dimension of the datafication of children, and the impossibility to opt out. Most of my daugthers' data is collected and stored by a plurality of agents, from her preschool digital records to her health records (both stored on outsourced platforms), from social media to cloud systems. I have little control of how her data is collected, stored, shared and exploited.
As a parent I will of course talk to her about digital privacy, and what should and should not be posted on social media. I will probably use the technique used by one parent in London: everytime the daugther wanted to download an app on her phone she would need to study the terms and conditions… However, I believe that the issue is far more complex than simply developing ways to teach our children how to protect their privacy. As parents,  we should not only be talking about digital privacy and how we can instruct our children to protect it, but rather about 'data justice'. What we need is to campaign for more regulation and transparency.
* * *
Alex writes:
This was a topic my wife and I discussed at length during her pregnancy.  Our daughter is now 8months old. From the second she was born she was old enough for us to be concerned about her presence on the internet.  We haven't decides on rules for when she is older, but from the start we agreed and had a strict policy required for us, family, and friends: her face would NOT be used in any social media posts. Our concerns ranged from privacy policies on instagram and facebook, to who controls the rights to those photos once posted. We impose pretty harsh penalties for not abiding by our rules, mostly a length of time where you do not get access to our child or their photos as distributed by us.
Being 31 and growing up in the ancient times (Before Social Media), I know that had my parents plastered my image on the internet for all to see, I would have held it against them well into my adulthood. This is a simple matter of respect and trust for your child. I want her to make the decisions about her online footprint and understand the consequences of what happens if you are not thoughtful about your internet presence in our current world.
I won’t make that choice for her.
If you’ve taken a different approach than these correspondents write conor@theatlantic.com to share your approach.


The former head of DARPA, Arati Prabhakar, has a dream. It’s a civilian utopian neuroscience dream that’s kind of the inverse of the scenarios that the far-out research wing of the military normally develops.
In a presentation at the Aspen Ideas Festival, which is co-hosted by the Aspen Institute and The Atlantic, she’d just shown a video from University of Utah research in which a soldier who’d lost his arms “felt” a virtual door through neural stimulation.
She cautioned that the research was very new and “not yet a robust capability, but even at this stage, we can start to see that there will be some mind-bending questions about how we use these technologies in the future.” Prabhakar noted that from a technological point of view, “there’s not much distance from restoration to enhancement.”
She asked the audience: “Do you think the future we’re going to live in a society where neuro-enhancements will be a privilege? Will they be a right? Might they be a mandate? Or maybe the whole idea is gonna creep us out so much that we won’t want anything to do with it.”
Mandated neuroenhancements certainly seem creepy to me!
“But imagine if you could learn a new language as fast as a 6-year-old,” she continued. “Or imagine if you could experience a whole new palette of colors or a fourth physical dimension in space.”
I’m not even sure what that means, but I would most definitely want to experience a fourth physical dimension in space.
But then she delivered the true utopian dream:
“Imagine if we could connect among ourselves in new and deeper ways and imagine if those connections happened in a way that gave us so much empathy and understanding of each other that we could put our minds together, literally, to take on some of the world’s hardest problems.”
She did not expound on the image, but one imagines she’s thinking about a kind of direct brain-to-brain interface.
DARPA has, after all, invested a lot in direct electronic brain interfaces. In one research program, they’re working on “intuitive” neural interfaces for controlling prosthetic limbs. In another, they’re creating “an implantable neural interface able to provide advanced signal resolution and data-transfer bandwidth between the brain and electronics.” The goal there is to create a translator between “the electrochemical language used by neurons in the brain and the ones and zeroes that constitute the language of information technology.”
And once you’ve got intuitive neural controls and a translator that lets you send brain signals into computers and back again, it does not seem an incredible leap to hook two (or … a million?) humans up together.
“If we could get to that future, we would look back at today’s reality and it would look like black-and-white,” Prabhakar said. “It would look like flatland.”
It’s a pretty fascinating speculative fiction. Compelling, perhaps, but also repulsive. And, if we can use recent history as any guide, wiring humans together in ever tighter communication loops doesn’t always turn out so well.


It is hard to imagine more fitting names for code-gone-bad than WannaCry and Eternal Blue. Those are just some of the computer coding vulnerabilities pilfered from the National Security Agency’s super-secret stockpile that have been used in two separate global cyber attacks in recent weeks. An attack on Tuesday featuring Eternal Blue was the second of these to use stolen NSA cyber tools—disrupting everything from radiation monitoring at Chernobyl to shipping operations in India. Fort Meade’s trove of coding weaknesses is designed to give the NSA an edge. Instead, it’s giving the NSA heartburn. And it’s not going away any time soon.
As with most intelligence headlines, the story is complicated, filled with good intentions and unintended consequences. Home to the nation’s codebreakers and cyber spies, the NSA is paid to intercept communications of foreign adversaries. One way is by hunting for hidden vulnerabilities in the computer code powering Microsoft Windows and and all sorts of other products and services that connect us to the digital world. It’s a rich hunting ground. The rule of thumb is that one vulnerability can be found in about every 2,500 lines of code. Given that an Android phone uses 12 million lines of code, we’re talking a lot of vulnerabilities. Some are easy to find. Others are really hard. Companies are so worried about vulnerabilities that many—including Facebook and Microsoft—pay “bug bounties” to anyone who finds one and tells the company about it before alerting the world. Bug bounties can stretch into the hundreds of thousands of dollars.
Writing the Rules of Cyberwar
The NSA, which employs more mathematicians than any organization on Earth, has been collecting these vulnerabilities. The agency often shares the weaknesses they find with American manufacturers so they can be patched. But not always. As NSA Director Mike Rogers told a Stanford audience in 2014,“the default setting is if we become aware of a vulnerability, we share it,” but then added, “There are some instances where we are not going to do that.” Critics contend that’s tantamount to saying, “In most cases we administer our special snake bite anti-venom that saves the patient. But not always.”
In this case, a shadowy group called the Shadow Brokers (really, you can’t make these names up) posted part of the NSA’s collection online, and now it’s O.K. Corral time in cyberspace. Tuesday’s attacks are just the beginning. Once bad code is “in the wild,” it never really goes away. Generally speaking, the best approach is patching. But most of us are terrible about clicking on those updates, which means there are always victims—lots of them—for cyber bad guys to shoot at.
WannaCry and Eternal Blue must be how folks inside the NSA are feeling these days. America’s secret-keepers are struggling to keep their secrets. For the National Security Agency, this new reality must hit especially hard. For years, the agency was so cloaked in secrecy, officials refused to acknowledge its existence. People inside the Beltway joked that NSA stood for “No Such Agency.” When I visited NSA headquarters shortly after the Snowden revelations, one public-affairs officer said the job used to entail watching the phones ring and not commenting to reporters.
Now, the NSA finds itself confronting two wicked problems—one technical, the other human. The technical problem boils down to this: Is it ever possible to design technologies to be secure against everyone who wants to breach them except the good guys? Many government officials say yes, or at least “no, but…” In this view, weakening security just a smidge to give law-enforcement and intelligence officials an edge is worth it. That’s the basic idea behind the NSA’s vulnerability collection: “If we found a vulnerability, and we alone can use it, we get the advantage.” Sounds good, except for the part about “we alone can use it,” which turns out to be, well, dead wrong.
That’s essentially what the FBI argued when it tried to force Apple to design a new way to breach its own products so that special agents could access the iPhone of Syed Rizwan Farook, the terrorist who, along with his wife, killed 14 people in San Bernardino. Law-enforcement and intelligence agencies always want an edge, and there is a public interest in letting them have it.
As former FBI Director James Comey put it, “There will come a day—and it comes every day in this business—where it will matter a great deal to innocent people that we in law enforcement can’t access certain types of data or information, even with legal authorization.”
Many leading cryptographers (the geniuses who design secure communications systems) and some senior intelligence officials say that a technical backdoor for one is a backdoor for all. If there’s a weakness in the security of a device or system, anyone can eventually exploit it. It may be hard, it may take time, it may take a team of crack hackers, but the math doesn’t lie. It’s nice to imagine that the FBI and NSA are the only ones who can exploit coding vulnerabilities for the good of the nation. It’s also nice to imagine that I’m the only person my teenage kids listen to. Nice isn’t the same thing as true. Former NSA Director Mike Hayden publicly broke with many of his former colleagues last year. “I disagree with Jim Comey,” Hayden said. “I know encryption represents a particular challenge for the FBI. ... But on balance, I actually think it creates greater security for the American nation than the alternative: a backdoor.”
Hayden and others argue that digital security is good for everyone. If people don’t trust their devices and systems, they just won’t use them. And for all the talk that security improvements will lock out U.S. intelligence agencies, that hasn’t happened in the 40 years of this raging debate. That’s right. 40 years. Back in 1976, during the first “crypto war,” one of my Stanford colleagues, Martin Hellman, nearly went to jail over this dispute. His crime: publishing his academic research that became the foundational technology used to protect electronic communications. Back then, some NSA officials feared that securing communications would make it harder for them to penetrate adversaries’ systems. They were right, of course—it did get harder. But instead of “going dark,” U.S. intelligence officials have been “going smart,” finding new ways to gather information about the capabilities and intentions of bad guys through electronic means.
The NSA’s second wicked problem is humans. All the best security clearance procedures in the world cannot eliminate the risk of an “insider threat.” The digital era has supersized the damage that one person can inflict. Pre-internet, traitors had to sneak into files, snap pictures with hidden mini-cameras, and smuggle documents out of secure buildings in their pant legs or a tissue box. Edward Snowden could download millions of pages onto a thumb drive with some clicks and clever social engineering, all from the comfort of his own desktop.  
There are no easy solutions to either the technical or human challenge the NSA now faces. Tuesday’s global cyber attack is a sneak preview of the movie known as our lives forever after.
Talk about WannaCry.


There’s a paradox in technology. For something new to become widespread, familiar, and mass-market, it must create enough novelty and curiosity to draw people’s attention. But novelty alone is not enough to reach saturation. To permeate life, a technology must elicit more than novelty and curiosity in its users. It must become ordinary. It must recede into the background, where it continues to run but ceases to be noticed by the humans  who made it pervasive.
This is the story of all successful technologies. The locomotive, airplane, and automobile. The electric light, the telephone, the washing machine, the personal computer. So humdrum are these once-revolutionary machines that no one gives them a second thought, unless they break down.
Ten years after its introduction, the iPhone—and the smartphone category it created—is starting to recede into the background. Apple has sold a billion of the things alone. Android devices account for a billion and a half smartphone users. Glass-and-metal rectangles fill hands, lounge on tables, tuck in pockets, illuminate faces. Like toasters and gas stations, like bus ads and Starbucks, anywhere you look you’ll see an iPhone.
Now that the iPhone is everywhere, it can finally disappear.
* * *
As it quickly replaced its predecessors, the iPhone challenged people to make sense of it, and to integrate it into their lives. At first, it was a gadget. I remember my first encounter with it, in 2007: A friend had bought one early, and he eagerly brushed and pinched it to show me how the multi-touch screen worked. Those were new once, and merely operating one was exciting on its own. It helped that the iPhone didn’t quite work right yet. It was slow, it stuttered, it froze. Almost like it was getting used to its owners as much as they were getting used to it.
Within a year or two, it became a compulsion. First for work on the run, as emails and texts and reminders, previously available only to executives and bureaucrats on BlackBerries and Palm Treos, became a staple of the everyperson’s information diet. Then for play, when games, apps, and social media lured people back to the device just to see if anything new had happened while they were away from it.
Several years in, that compulsion became a ritual. All the bad, deleterious urges to warm one’s face in front of the iPhone transformed into a mode of living. A way of being. This is just what life was like: Scrolling vigorously while standing in line for coffee. Retreating into the device during lulls in dinner conversation. Operating it unwisely, and knowingly so, while driving. Smartphones began replacing computers and laptops for some, and televisions and movie theaters for many. People began acquiring devices for their children, and even for their toddlers, and even for their babies.
For me, each of these phases suggested its own analogy to make sense of the novelty. For the gadget period, the iPhone was like a toy dog—a thing to hold and carry and stroke and dote on. A device that could be cared for, and conspicuously so.
For the compulsive era, the iPhone was like a cigarette. A nervous tic, facilitated by a handheld apparatus that releases relief when operated. Along with it comes dependence, and an awareness of that dependence. A shame in it, even, and an attendant vow to stop, if only stopping was possible. And yet, as with the cigarette, the iPhone also conferred an air of cool. It gave people something to do with their hands. Though compulsive, it offered a compulsion everyone shared.
For the ritual phase, the iPhone was like a rosary. In the secular age, industry in general and the computer in particular has taken the place of the church. Instead of God, technology has become the ultimate means of understanding and changing the world. Algorithms became mystical, delivering truths. What can be thought and what should be done is equal to what the computer allows and makes possible. Its toy-dog quirks having been tamed, its compulsive nature having been accepted, the iPhone became the magic wand by which all worldly actions could be performed, all possible information acquired. There’s a reason an ancestor of the iPhone was code-named “Pocket Crystal” inside Apple.
What the iPhone isn’t, anymore, is a phone, in the traditional sense. A smartphone, yes, although that’s a product category for industrialists and business writers. And yet, people still call it a “phone” colloquially: I can’t find my phone! or Let me just check my phone or Sorry, I was on my phone. The last one condenses much of the reality of iPhone use. Where once one might have been “on the phone”—speaking into the singular telephony device in a home—now everyone is engaged with, and transmitting information by means of, their own private device. All the time.
The metaphor for this phase of the iPhone’s life has long eluded me. But recently I fell upon it, thanks to the writer Claire Donato. “From the rectangle, she downloads instructions for preparing a slick coat for the green beans and potatoes,” I heard Donato read from an in-progress novel late this spring.
The rectangle. Abstract, as a shape. Flat, as a surface. But suggestive of so much. A table for community. A door for entry, or for exit. A window for looking out of, or a picture for looking into. A movie screen for distraction, or a cradle for comfort, or a bed for seduction. A hole of infinite depth, yet also a veil whose blackness covers up that chasm.
The rectangle cuts to the truth of the iPhone as a widely adopted, mass-market device. When Steve Jobs first introduced the iPhone, he described it as “a widescreen iPod with touch controls, a revolutionary mobile phone, and a breakthrough internet communications device.”
“Three things,” he called it, before lingering on the reveal. “These are not separate devices. This is one device.”
This reality seems so obvious today that it’s quaint to recall when things could have been otherwise. Convergence didn’t collapse all media into a single format, as some had predicted. Instead it channeled both old and new ways of working, living, and playing through a common opening. It flattened it. “A rectangle is a flat shape,” Donato tells me when I ask how she arrived at the metaphor. “I can’t stop thinking about how the internet is such a flat surface. We’re compressed here.”
There’s obvious sorrow in Donato’s words. It doesn’t always feel good, this rectangle flatland. The MIT professor and psychologist Sherry Turkle has lamented it from the vantage point of a social critic. To her, the era of the rectangle is one of lost conversation, of the masses “alone together.” But it has also become the basis for ordinary life. Perhaps some can withdraw from it into Charles Riverside salons or art-gala afterparties, but for most, life is now conducted by rectangle. It is technological populism.
And in that respect, the abstract, flat image of the rectangle disarms the curiosity and novelty of the iPhone. As its first decade ends, the rectangle can finally become a technology of ordinary life. Drop the “ordinary,” even. Just life, without the novelty and curiosity interrupting it with new diversions—even as the device itself persists in delivering constant distraction.
* * *
The irony of the iPhone’s ubiquity is that nobody seems content with it. Voice-activated artificial intelligences, virtual reality, augmented reality, robots, and more futurist interlopers all press at the gates, eager to unleash their novelties and disrupt the iPhone. Apple’s investors are impatient that the company hasn’t yet done so itself, dissatisfied with a single company selling a billion rectangles to turn the round Earth into its private flatland.
It’s a bittersweet victory when a technology achieves ubiquity. On the one hand, it makes universal experience possible. Huge swaths of people share a common means for socializing, learning, working, and playing. But on the other hand, pervasiveness domesticates technology. What was once wild and exciting becomes ordinary and routine. Its wings clipped, its paws declawed, the rectangle poses no threat—and thereby appears to offer no further promise, either.
When faced with technology’s paradox, I often think of Star Trek. The crew of the Enterprise doesn’t think much of the computer. It doesn’t have a name, like Siri or Alexa or Cortana, even though it also speaks in the disembodied voice of a woman. It’s just “Computer.” Despite playing a central role in the operation of the starship, nobody thinks much about the computer. It’s just there, like always. No big deal.
It’s a science-fictional future too few acknowledge, let alone pine for. One in which technology becomes advanced by becoming ordinary. When all the cheers and wails cease, they leave behind a changed world, but one its human residents must nevertheless occupy. That’s us, now: Billions of people, their hands clutched to rectangles both dark and emissive, learning to live with it.


The Harvard psychologist Joshua Greene is an expert in “trolleyology,” the self-effacing way he describes his research into the manifold variations on the “trolley problem.” The basic form of this problem is simple: There’s a trolley barreling towards five people, who will die if they’re hit. But you could switch the trolley onto another track on which only a single person stands. Should you do it?
From this simple test of moral intuition, researchers like Greene have created an endless set of variations. By varying the conditions ever so slightly, the trolley problem can serve as an empirical probe of human minds and communities (though not everyone agrees).
For example, consider the footbridge variation: You’re standing on a footbridge above the trolley tracks with a very large person, who, if you push him or her, can stop the trolley from killing the five people.  Though the number of lives saved is the same, it turns out that far more people would throw the switch than push the person.  
But this is not quite a universal result. During a session Wednesday at the Aspen Ideas Festival, which is co-hosted by the Aspen Institute and The Atlantic, Greene joked that only two populations were likely to say that it was okay to push the person on the tracks: psychopaths and economists.
Later in his talk, he returned to this, however, through the work of Xin Xiang, an undergraduate researcher who wrote a prize-winning thesis in his lab titled “Would the Buddha Push the Man of the Footbridge? Systematic Variations in the Moral Judgment and Punishment Tendencies of the Han Chinese, Tibetans, and Americans.”
Xiang administered the footbridge variation to practicing Buddhist monks near the city of Lhasa and compared their answers to Han Chinese and American populations.  “The [monks] were overwhelmingly more likely to say it was okay to push the guy off the footbridge,” Greene said.
He noted that their results were similar to psychopaths—clinically defined— and people with damage to a specific part of the brain called the ventral medial prefrontal cortex.
“But I think the Buddhist monks were doing something very different,” Greene said. “When they gave that response, they said, ‘Of course, killing somebody is a terrible thing to do, but if your intention is pure and you are really doing it for the greater good, and you’re not doing it for yourself or your family, then that could be justified.’”
For Greene, the common intuition that it’s okay to use the switch but not to push the person is a kind of “bug” in our biologically evolved moral systems.
“So you might look at the footbridge trolley case and say, okay, pushing the guy off the bridge, that’s clearly wrong. That violates someone’s rights. You’re using them as a trolley stopper, et cetera. But the switch case that’s fine,” he said. “And then I come along and tell you, look, a large part of what you’re responding to is pushing with your hands versus hitting a switch. Do you think that’s morally important?”
He waited a beat, then continued.
“If a friend was on a footbridge and called you and said, ‘Hey, there’s a trolley coming. I might be able to save five lives but I’m going to end up killing somebody! What should I do?’ Would you say, ‘Well, that depends. Will you be pushing with your hands or using  a switch?’”
What people should strive for, in Greene’s estimation, is moral consistency that doesn’t flop around based on particulars that shouldn’t determine whether people live or die.
Greene tied his work about moral intuitions to the current crop of artificial-intelligence software. Even if they don’t or won’t encounter problems as simplified as the trolley and footbridge examples, AI systems must embed some kind of ethical framework. Even if they don’t lay out specific rules for when to take certain behaviors, they must be trained with some kind of ethical sense.
And, in fact, Greene said that he’s witnessed a surge in people talking about trolleyology because of the imminent appearance of self-driving cars on human-made roads. Autonomous vehicles do seem like they will be faced with some variations on the trolley problem, though Greene said the most likely would be whether the cars should ever sacrifice their occupants to save more lives on the road.
Again, in that instance, people don’t hold consistent views. They say, in general, that cars should be utilitarian and save the most lives. But when it comes to their specific car, their feelings flip.
All these toy problems add up to a (still incomplete) portrait of human moral intuitions, which are being forced into explicit shapes by the necessity of training robots. Which is totally bizarre.
And the big question Greene wants us to ask ourselves before building these systems is: Do we know which parts of our moral intuition are features and which are bugs?


A man sits in a chair in front of a small documentary camera crew. He’s trim, dressed in all black. A red notebook sits on his lap. “Here’s what I wrote in 1989,” he says. “This is a very personal object. It must be beautiful. It must offer the kind of personal satisfaction that a fine piece of jewelry brings. It will have a perceived value even when it’s not being used. It should offer the comfort of a touchstone, the tactile satisfaction of a seashell, the enchantment of a crystal.”
Then comes the reveal. He picks up the notebook. We see a sketch: a rectangular slab of glass, all display, except for bezel at the top and bottom. From his pocket, he pulls an iPhone and holds it above the drawing. The similarities are startling.
“We really had it,” he says with a thin laugh. “We definitely had it.”
This is a scene from the forthcoming documentary General Magic, named for the company that attempted to manufacture the device from the notebook. The man is Marc Porat, CEO of the company. He’d recruited two Apple employees, Bill Atkinson and Andy Hertzfeld, who had created the Macintosh. In its earliest iteration, inside Apple, the project had been called Pocket Crystal.
After the project was spun out and years of frenzied development, Wired profiled the company in their April 1994 issue. There were 13 million internet users in 1994. There was roughly one cell phone per 100 people on Earth, none of them equipped to do much more than make calls. The first SMS text message had been sent just two years before.
Yet they were convinced that they were making the most important device ever.
“It’s like a lot of different areas are converging on an electronic box that’s in your pocket, that’s with you all the time, that supports you in some way,”  Atkinson told Wired. “That’s going to happen with or without General Magic.”
He was right.
The iPhone launched 10 years ago. The device—and its many, many descendants—is core to how we live. After only a decade, smartphones easily outnumber PCs, despite personal computing’s quarter-century head start. There are 2.5 billion Apple iOS and Android smartphones in use out there, with that number, as analyst Ben Evans puts it, “heading for 5 billion plus users.” PCs never even cracked 2 billion users and are now drifting downwards.
The iPhone is the single-most successful product of all time. One billion iPhones have been sold. They underpin the most valuable company in history, and have catalyzed a whole new technology industry that’s an order of magnitude larger than the one built around PCs. This came with a major assist from Android, the mobile operating system that Google acquired, and then rebuilt after the iPhone came out. But the iPhone pioneered the market, the user interface, the working form factor, and the app store. And iPhone users drove network upgrades and buildouts by the major wireless carriers across the world because people with the Apple devices consumed so much data relative to other cellphone users.
In short: the iPhone is the Pocket Crystal, and we are all enchanted.
But staring at the 1989 sketch and down at one’s phone, it is hard not to ask: How could the form, appeal, and importance of the device have been apparent 18 years before its appearance?
Was the iPhone, in some way, inevitable?
* * *
If you want to understand the long sweep of tech history that culminated in the iPhone, it’s worth paying a visit to Bill Buxton’s gadget museum. A Microsoft user-interface designer, he’s collected dozens and dozens of interactive devices, and documented them for all to see. Strange keyboards, handheld devices, electronic gloves, touch screens, touch pads, phones, and e-readers.
The General Magic Data Rover 840, a 1998 release, is in the collection. It looks nothing like a Pocket Crystal. Like all the other devices designed to work with General Magic’s software—e.g. the Sony PIC series and Motorola Envoy—the housing is grey and bulky. There’s a stylus, of course, and a grayscale backlit screen. The device is heavily skeumorphic, drawing on real-world analogs for everything. To add a new contact, one had to first go to the “Office,” one of the software’s “rooms,” before pulling up the address book functionality. The settings were located in the “Hallway,” like a thermostat.
This is not to fault General Magic for creating devices with the technology of the era. Buxton’s collection contains other key precursors to the modern smartphone—and all of them have that teenage awkwardness to them.
There’s the Newton, Apple’s own personal digital assistant, which was released in 1993. The so-called MessagePad looks more like Porat’s sketch, but it relied on shaky handwriting recognition and inadequate battery technology. While Newton improved through the ’90s, it was eventually canceled, and history records it mostly as a flop.
Then there are the various devices that Palm powered. The Palm Pilot, introduced in 1996, became the standard bearer for PDAs, as they were known through the end of the ’90s. They were useful and improved steadily, but never became much more than glorified address books and calendars.
“No computer product category has been more ridiculed than the PDA,” wrote Home Office Computing magazine in 1995. “Originally conceived as a tiny digital factotum that would call home, receive faxes, store documents, and send email, the first PDAs from AT&T, Apple, Casio, and Tandy fell far short of expectations.”
That’s how a review of the most intriguing early smartphone, the IBM/BellSouth Simon, begins. It was a straight-up smartphone with a touchscreen—in the mid-’90s. The battery lasted eight hours in standby mode or a single hour in use. It weighed more than a pound. And it cost $899. But it worked better than the rest of the devices out there.
The Home Office Computing review ends promisingly, or ominously, as it were. “It may be that we're still asking too much of PDAs,” it says. “For example, how can you possibly fit an acceptably large touch screen on an object that’s supposed to fit in your pocket?”
While PDAs floundered through the 1990s, cell phones soared. Nokia became the world’s dominant smartphone maker with rugged, simple devices. It’s easy to forget that Nokia was the cell phone game for many years. In the year the iPhone came out (2007), Nokia sold 437 million phones and had near half of the cell phone market. And yet they never released anything that looked like the Pocket Crystal.
But that’s not to say that they didn’t think about it. In a funereal piece in The Wall Street Journal, former head designer Frank Nuovo rued Nokia’s mistakes.
“More than seven years before Apple Inc. rolled out the iPhone, the Nokia team showed a phone with a color touchscreen set above a single button. The device was shown locating a restaurant, playing a racing game and ordering lipstick,” the Journal narrated. “In the late 1990s, Nokia secretly developed another alluring product: a tablet computer with a wireless connection and touch screen—all features today of the hot-selling Apple iPad.”
Nuovo, clicking through his old slides like General Magic’s Porat paging through his old sketches, echoed the General Magic CEO’s lament. “Oh my God,” he said. “We had it completely nailed.”
So many people had it—and with the backing of the world’s most powerful electronics’ companies—and yet none of them made it.
When Buxton launched his virtual museum six years ago, he told me that it takes two decades for something genuinely new to become a billion-dollar business.
“If what I said is credible, then it is equally credible that anything that is going to become a billion dollar industry in the next 10 years is already 10 years old,” Buxton said. “That completely changes how we should approach innovation. There is no invention out of the blue, but prospecting, mining, refining and then goldsmithing to create something that's worth more than its weight in gold.”
There is no wizard, no singular genius, who comes up with the Next Big Thing, but something like an evolutionary process. Apple’s iPhone business hit a billion dollars in sales in 2008. By 1998, most of the conceptual work thinking through an iPhone-like device had been done.
*  * *
“The iPhone is a deeply, almost incomprehensibly, collective achievement,” Brian Merchant declares in his new biography of the iPhone, The One Device.
“Thomas Edison did not invent the lightbulb, but his research team found the filament that produced the beautiful, long-lasting glow necessary to turn it into a hit product,” Merchant writes. “Likewise, Steve Jobs did not invent the smartphone, though his team did make it universally desirable. Yet the lone-inventor concept lives on.”
In The One Device, Merchant works through the technical achievements, distributing acclaim in and outside Apple. The glass—Gorilla Glass—was a Corning achievement, which had its roots in a half-century-old research project. The multi-touch screen that allowed the entire surface of the glass to become the user-interface has its origins in the European physics organization, CERN. Merchant quotes Buxton saying his lab at the University of Toronto was working on multi-touch in the early 1980s, and that he’d seen an earlier working system at Bell Labs. The winding multitouch trail continues through the University of Delaware, where an electrical engineer named Wayne Westerman created a multi-touch system for typing to ease his own repetitive stress injuries. Apple eventually bought the company and filed patents on the technology with Westerman’s name on them.
One last example, the lithium-ion battery. Merchant provides a pithy genealogy: “The lithium-ion battery—conceived in an Exxon lab, built into a game-changer by an industry lifer, turned into a mainstream commercial product by a Japanese camera maker, and manufactured with ingredients dredged up in the driest, hottest place on Earth—is the unheralded engine driving our future machines.”
Apple’s supply chain for tin, tantalum, tungsten, gold, and cobalt includes no less than 256 refiners and smelters. Look just at the cobalt in lithium-ion batteries and you find a crazy trail that’d leads back primarily to the copper mines of the Congo, and on to smelters in China. China is the biggest consumer of cobalt and 80 percent of it goes to battery production.
And that’s just the stuff inside the phone. There is also the nearly unbelievable story of how much data capacity the various cell phone providers have added, which requires tower after tower of equipment. From 2007 to 2010, when iPhones were only available with an AT&T wireless connection, data traffic on AT&T’s network went up 8,000 percent. And the growth kept going. A Cisco research study found that from 2011 to 2016, when smartphones became far more prevalent, mobile data traffic grew 18-fold. Now, the country hosts an enormous electronic forest: more than 118 thousand towers are now in operation, according to an industry publication.
Underpinning all of these systems are the incredible leaps in computing power (Moore’s law) and energy efficiency (Koomey’s law) that have been hallmarks of the computing revolution. The chip work, alone, represents hundreds of billions of dollars of R&D, not to mention the work on modems and wireless technology by places like Qualcomm.
Merchant follows computer historian Chris Garcia in calling the iPhone a confluence technology: “There are so many highly evolved and mature technologies packed into our slim rectangles, blending apparently seamlessly together, that they have converged into a product that may resemble magic.”
A general-purpose kind of magic, you might say.
* * *
General Magic will probably be written out of the history books as time goes on. The company itself never amounted to much. But look at a list of the people who worked there, and two names jump off the (very distinguished) list: Tony Fadell and Andy Rubin. Fadell led the hardware team that created the iPhone. Rubin led the team that created Android. While Android dominates by market share (87 percent worldwide), the iPhone dominates the profits made from smartphone sales. In any case, together, the two operating systems we can trace back to General Magic have 99 percent share of the smartphone market.
It’s a perfect narrative. A few people in the Apple orbit have the perfect idea. That seed incubates for 15 years until the technology stack catches up, and then two alums of General Magic finally create the object from that original inspired vision.
The only problem is that Fadell has said the iPhone team tried out all kinds of things. They put a scrollwheel on one proto-iPhone. The team had a months-long battle over whether to include a hardware keyboard before Steve Jobs made the decision to go keyboard-less. And Rubin’s team only ditched its hardware keyboard plans after the iPhone came out. If General Magic did have a map of the future, the legend must have been lost somewhere along the way.
The iPhone happened, and we can mark the world as before-and-after. It unlocked a new era of human-computer interaction and human-human interaction. The iPhone is the ur object of our time. A version of it is attached to the vast majority of adults. We sleep with them. We spend more time with them than our children. The success of other technology companies, media empires, romantic relationships, and political campaigns depends on reaching people through them.
Happy 10th Birthday iPhone. Happy 10th Birthday World That the iPhone Made.


As MIT professor and psychologist Sherry Turkle sees it, students are obsessed with perfection and invulnerability. That’s why they will email her their questions instead of coming to office hours.
“As I get famouser and famouser, I post more office hours, and the numbers [of students attending them] come down,” said Turkle, who researches and writes on peoples’ relationship to technology, during a panel at the Aspen Ideas Festival, which is co-hosted by the Aspen Institute and The Atlantic. “What they say is basically, ‘I’ll tell you what’s wrong with conversation, it takes place in real time, and you can’t control what you’re going to say.’” These students are trying to hide their vulnerabilities and imperfections behind screens, she said, and they have a “fantasy that at 2 in the morning I’m going to write them the perfect answer to the perfect question.”
It’s all a sign that we’ve become too dependent on our devices to get us through life, as Turkle sees it. She inveighed not only against email-loving Millennials but also against new moms who would rather sit at home on their phones than go meet each other at the playground.
She points to studies that show that having your phone on the table during mealtimes, even if it’s off, leads to reduced feelings of empathy. To truly turn the tide, Turkle said, there ought to be some no-phone times and places.
They are, uh, most of the times of the day and most places, including when you’re in:
Strict? Yes. This is the price you pay for empathy. This “is not an anti-tech position,” she said. “It’s a pro-conversation position.”
With that, she was interrupted by someone’s iPhone alarm going off.


The function of advertising, wrote Robert E. Lane in The Loss of Happiness in Market Democracies, “is to increase people’s dissatisfaction with any current state of affairs, to create wants, and to exploit the dissatisfactions of the present. Advertising must use dissatisfaction to achieve its purpose.”
The web browser is a dissatisfaction-seeking machine. Every search query we input reflects a desire—to have, to know, to find. Ordinarily, that fact may escape notice. But there are moments when the machine reveals its inhumanity.
Speaking on a panel at the Aspen Ideas Festival, which is cohosted by the Aspen Institute and The Atlantic, Manoush Zomorodi, host of WNYC’s Note to Self, shared a story of a message she received from a listener who’d been following her series on digital privacy. “She was concerned that she might have a drinking problem, and so she went on Google and asked one of those questions, ‘How do you know if you have a drinking problem?’ Two hours later, she goes on Facebook, and she gets an ad for her local liquor store.
“And she left me a voicemail crying, ’cause she was like, ‘You know, it would be one thing if it were even sending me, like, clinics maybe where I could get help. But the fact that that’s how it was targeting me ...’ She felt so betrayed by Facebook, this company with whom she had a very intimate relationship.”
Only 9 percent of adults in the United States say “they feel they have ‘a lot’ of control over how much information is collected about them and how it is used,” according to the Pew Research Center. For most of us, unless we’ve expended the effort to limit the information we share, a vast network of automated snoops constantly monitors our behavior online, and tries to match ads to the fears and desires implicit in our searches and messages.
“You hear these little betrayals of privacy that actually are extremely powerful on a daily basis,” Zomorodi said.
Zomorodi’s co-panelist, the investigative journalist Julia Angwin, spoke about seeing middle-school students plagued by body-image insecurities. “Online, all they get is ads on how to lose weight,” Angwin said. “It preys on their fears. It’s just awful, right? And that is—I don’t know that it’s necessarily targeted advertising, because actually the entire internet is weight-loss ads, as far as I can tell.”
While Google effectively publicizes its aggregate search data—the annual compendium of the year’s queries is always a draw—the service’s value to advertisers comes from precisely the opposite type of data: the personal, strange, incredibly revealing things that each of us is looking for. In a recent episode of the Freakonomics podcast, Seth Stevens-Davidowitz, who wrote his dissertation on what people reveal in Google searches, spoke about how people expose a version of themselves to the search engine that they rarely present in surveys, or even in conversations with friends. “There are lots of bizarre questions—not just questions but statements—that people make on Google,” said Stephens-Davidowitz. “‘I’m sad’ or ‘I’m drunk’ or ‘I love my girlfriend’s boobs.’ Why are you telling Google that? It feels like a confessional window where people just type statements with no reason[able impression] that Google would be able to help.”
If our ad networks have become our confessors, what sort of penance will they extract? What latent or secret desires will they exploit? What could they prod us to do?


The future of privacy in the United States will be shaped by the next generation of citizens and consumers, a rising generation that has never known a pre-Internet world.  
The broadcast journalist Manoush Zomorodi created a segment called The Privacy Paradox on the WYNC show “Note to Self.” Its premise: “ You love the convenience of living online. But you want more control over where your personal information goes.” (The shows dubbed “The 5 Day Plan” are informative. I learned about an additional way that my iPhone was tracking me. And I pay attention to this stuff.)
Zomorodi’s interactions with listeners caused her to think more deeply about the attitudes toward privacy and digital best practices that she ought to pass along as a parent. At a panel Tuesday at the Aspen Ideas Festival, co-hosted by The Aspen Institute and The Atlantic, she expressed chagrin at having chosen Yahoo when creating her child’s first email account––and pride at the child’s subsequent decision to sign up for an account with an overseas email provider that offers strong encryption.
8 Overly Confident, Mostly Pessimistic Predictions About Tech in 2018
Many parents don’t offer any guidance to their children on digital privacy, if only because their children seem so much tech savvy than they are. But Zomorodi’s reflections got me wondering what parents who do think about these matters tell their kids as they begin to use the Internet, or smart phones, or get their first social media account. As Julia Angwin has observed, “if I don’t do anything to help my children learn to protect themselves, all their data will be swept up into giant databases, and their identity will be forever shaped by that information.”
How do you acculturate your children into the digital world?
If you’re a parent who is willing to share, I’d be eager to hear about your approach in your own words. How old is your child? What rules do you lay down? What guidance do you offer, if any? What do you leave up to your child? What do you think of the way they conceive of personal information, digital privacy, and the trail of data they are creating? How would you rate your level of awareness of what they do in digital spaces? What are your biggest worries, challenges, and dilemmas? Email conor@theatlantic.com if you’re willing to share answers to these questions, or any related thoughts.
I expect many parents will benefit from hearing one another’s experiences.


What news do people see? What do they believe to be true about the world around them? What do they do with that information as citizens—as voters?
Facebook, Google, and other giant technology companies have significant control over the answers to those questions. It’s no exaggeration to say that their decisions shape how billions see the world and, in the long run, will contribute to, or detract from, the health of governing institutions around the world.
That’s a hefty responsibility, but one that many tech companies say they want to uphold. For example, in an open letter in February, Facebook’s founder and CEO Mark Zuckerberg wrote that the company’s next focus would be “developing the social infrastructure for community—for supporting us, for keeping us safe, for informing us, for civic engagement, and for inclusion of all.”
The trouble is not a lack of good intentions on Zuckerberg’s part, but the system he is working within, the Stanford professor Rob Reich argued on Monday at the Aspen Ideas Festival, which is co-hosted by the Aspen Institute and The Atlantic.
Reich said that Zuckerberg’s effort to position Facebook as committed to a civic purpose is “in deep and obvious tension with the for-profit business model of a technology company.” The company’s shareholders are bound to be focused on increasing revenue, which in Facebook’s case comes from user engagement. And, as Reich put it, “it’s not the case that responsible civic engagement will always coincide with maximizing engagement on the platform.”
For example, Facebook’s news feed may elicit more user engagement when the content provokes some sort of emotional response, as is the case with cute babies and conspiracy theories. Cute babies are well and good for democracy, but those conspiracy theories aren’t. Tamping down on them may lead to less user engagement, and Facebook will find that its commitment to civic engagement is at odds with its need to increase profits.
The idea that a company’s sole obligation is to its shareholders comes from a 1970 article in The New York Times Magazine by the economist Milton Friedman called “The Social Responsibility of Business Is to Increase Its Profits.” In it, Friedman argued that if corporate executives try to pursue any sort of “social responsibility” (and Friedman always put that in quotes), the executive was in a sense betraying the shareholders who had hired him. Instead, he must solely pursue profits, and leave social commitments out of it. Reich says that these ideas have contributed to a libertarian “background ethos” in Silicon Valley, where people believe that “you can have your social responsibility as a philanthropist, and in the meantime make sure you are responding to your shareholders by maximizing profit.”
Reich believes that some sort of oversight is necessary to ensure that big tech companies make decisions that are in the public’s interest, even when it’s at odds with increasing revenue. Relying on CEOs and boards of directors to choose to do good doesn’t cut it, he said: “I think we need to think structurally about how to create a system of checks and balances or an incentive arrangement so that whether you get a good person or a bad person or a good board or a bad board, it’s just much more difficult for any particular company or any particular sector to do a whole bunch of things that threaten nothing less than the integrity of our democratic institutions.”
Reich said that one model for corporations might be creating something like ethics committees that hospitals have. When hospitals run into complicated medical questions, they can refer the question to the ethics committee whose members—doctors, patients, community members, executives, and so on—represent a variety of interests. That group dives deeply into the question and comes up with a course of action that takes into account various values they prize. It’s a complicated, thoughtful process—“not an algorithm where you spit out the correct moral answer at the end of the day,” Reich said.


When I saw that Google had introduced a “Classic Papers” section of Google Scholar, its search tool for academic journals, I couldn’t help but stroke my chin professorially. What would make a paper a classic, especially for the search giant? In a blog post introducing the feature, Google software engineer Sean Henderson explains the company’s rationale. While some articles gain temporary attention for a new and surprising finding or discovery, others “have stood the test of time,” as Henderson puts it.
How to measure that longevity? Classic Papers selects papers published in 2006, in a wide range of disciplines, which had earned the most citations as of this year. To become a classic, according to Google, is just to have been the most popular over the decade during which Google itself rose to prominence.
It might seem like an unimportant, pedantic gripe to people outside of academia. But Scholar’s classic papers offers a window into how Google conceives of knowledge—and the effect that theory has on the ideas people find with its services.
* * *
Google’s original mission is to “organize the world’s information and make it universally accessible.” It sounds simple enough, if challenging given the quantity of information in the world and the number of people that might access it. But that mission obscures certain questions. What counts as information? By what means is it accessible, and on whose terms?
The universals quickly decay into contingencies. Computers are required, for one. Information that lives offline, in libraries or in people’s heads, must be digitized or recorded to become “universally” accessible. Then users must pay for the broadband or mobile data services necessary to access it.
At a lower level, ordinary searches reveal Google’s selectiveness. A query for “Zelda,” for example, yields six pages of links related to The Legend of Zelda series of Nintendo video games. On the seventh page, a reference to Zelda Fitzgerald appears. By the eighth, a pizzeria called Zelda in Chicago gets acknowledgement, along with Zelda’s café in Newport, Rhode Island. Adding a term to the query, like “novelist” or “pizza,” produces different results—as does searching from a physical location in Chicago or Newport. But the company’s default results for simple searches offers a reminder that organization and accessibility mean something very particular for Google.
That hidden truth starts with PageRank, Google’s first and most important product. Named after Google founder Larry Page, it is the method by which Google vanquished almost all its predecessors in web search. It did so by measuring the reputation of web sites, and using that reputation to improve or diminish its likelihood of appearing earlier in search results.
When I started using the web in 1994, there were 2,738 unique hostnames (e.g., TheAtlantic.com) online, according to Internet Live Stats. That’s few enough that it still made sense to catalog the web in a directory, like a phone book. Which is exactly what the big web business founded that year did. It was called Yahoo!
But by the time Page and Sergey Brin started Google in 1998, the web was already very large, comprising over 2.4 million unique hosts. A directory that large made no sense. Text searches had already been commercialized by Excite in 1993, and both Infoseek and AltaVista appeared in 1995, along with Hotbot in 1996. These and other early search engines used a combination of paid placement and text-matching of query terms against the contents of web pages to produce results.
Those factors proved easy to game. If queries match the words and phrases on web pages, operators can just obscure misleading terms in order to rise in the rankings. Page and Brin proposed an addition. Along with analysis of the content of a page, their software would use its status to make it rise or fall in the results. The PageRank algorithm is complex, but the idea behind it is simple: It treats a link to a webpage as a recommendation for that page. The more recommendations a page has, the more important it becomes to Google. And the more important the pages that link to a page are, the more valuable its recommendations become. Eventually, that calculated importance ranks a page higher or lower in search results.
Although numerical at heart, Google made search affective instead. The results just felt right—especially compared to other early search tools. That ability to respond as if it knew what its users were thinking needed laid the foundation for Google’s success. As the media scholar Siva Vaidhyanathan puts it in his book The Googlization of Everything, relevance became akin to value. But that value was always “relative and contingent,” in Vaidhyanathan’s words. That is, the actual relevance of a web page—whether or not it might best solve the problem or provide the information the user initially sought—became subordinated to the sense of initial delight and subsequent trust in Google’s ability to deliver the “right” results. And those results are derived mostly from a series of recurrent popularity contests PageRank runs behind the scenes.
* * *
Google Scholar’s idea of what makes a paper a classic turns out to be a lot like Google’s idea of makes a website relevant. Scholarly papers cite other papers. Like a link, a citation is a recommendation. With enough citations, a paper becomes “classic” by having been cited many times. What else would “classic” mean, to Google?
As it turns out, scholars have long used citation count as a measure of the impact of papers and the scholars who write them. But some saw problems with this metric as a measure of scholarly success. For one, a single, killer paper can skew a scholar’s citation count. For another, it’s relatively easy to game citation counts, either through self-citation or via a cabal of related scholars who systematically cite one another.
In 2005, shortly after Google went public, a University of California physicist named Jorge Hirsch tried to solve some of these problems with a new method. Instead of counting total citations, Hirsch’s index (or h-index, as it’s known) measures a scholar’s impact by finding the largest number of papers (call that number h) that have been cited at least h times. A scholar with an h-index of 12, for example, has 12 papers each of which is cited at least 12 times by other papers. H-index downgrades the impact of a few massively successful papers on a scholar’s professional standing, rewarding consistency and longevity in scholarly output instead. Hirsch’s method also somewhat dampens the effect of self- and group-citation by downplaying raw citation counts.
H-index has become immensely influential in scholarly life, especially in science and engineering. It is not uncommon to hear scholars ask after a researcher’s h-index as a measure of success, or to express pride or anxiety over their own h-indexes. H-index is regularly used to evaluate (and especially to cull) candidates for academic jobs, too. It also has its downsides. It’s hard to compare h-indexes across fields, the measure obscures an individual’s contribution in co-authored papers, and it abstracts scholarly success from its intellectual merit—the actual content of the articles in question.
That makes h-index eminently compatible with life in the Google era. For one, Google Scholar has been a boon to its influence, because it automates the process of counting citations. But for another, Google has helped normalize reference-counting as a general means of measuring relevance and value for information of all kinds, making the process seem less arbitrary and clinical when used by scholars. The geeks brought obsessive numerism to the masses.
Instead of measuring researchers’ success, Google Scholar’s Classic Papers directory defines canon by distance in time. 2006 is about ten years ago—long enough to be hard to remember in full for those who lived through it, but recent enough that Google had found its legs tracking scholarly research (the Scholar service launched in 2004). Classic papers, in other words, are classic to Google more than they are classic to humanity writ large.
In the academy today, scholars maintain professional standing by virtue of the quantity and regulatory of their productivity—thus Hirsch’s sneer at brilliant one-offs. Often, that means scholarly work gets produced not because of social, industrial, or even cosmic need, but because the wheels of academic productivity must appear to turn. Pressing toward novel methods or discoveries is still valued, but it’s hard and risky work. Instead, scholars who respond to a specific, present conditions in the context of their fields tend to perform best when measured on the calendar of performance reviews.
Looking at papers cited the most in 2006, as Google Scholar’s Classic Papers does, mostly reveals how scholars have succeeded at this gambit, whether intentionally or not. For example, the most-cited paper in film is “Narrative complexity in contemporary American television,” by the Middlebury College television-studies scholar Jason Mittell. Mittell was one of the first critics to explain the rise of television as high culture, particularly via social-realist serials with complex narratives, like The Sopranos. Mittell’s take was both well-reasoned and well-timed, as shows like Deadwood, Big Love, and The Wire were enjoying their runs when he wrote the paper. That trend has continued uninterrupted for the decade since, making Mittell’s article a popular citation.
Likewise, the most cited 2006 paper in history is “Can history be open source? Wikipedia and the future of the past,” by Roy Rosenzweig. The article offers a history and explanation of Wikipedia, along with an assessment of the website’s quality and accuracy as an historical record (good and bad, it turns out). As with complex TV, the popularity of Rosenzweig’s paper relates largely to the accidents of its origin. Wikipedia was started in 2001, and by 2005 it had begun to exert significant impact on teaching and research. History has a unique relationship to encyclopedic knowledge, giving the field an obvious role in benchmarking the site. Rosenzweig’s paper even discusses the role of Google’s indexing methods in helping to boost Wikipedia’s appearance in search results, and the resulting temptation among students to use Wikipedia as a first source. Just as in Mittell’s case, these circumstances have only amplified in the ten years since the paper’s publication, steadying its influence.
This pattern continues in technical fields. In computer vision, for example, a method of identifying the subject of images is the top cited paper. Image recognition and classification was becoming increasingly important in 2006, and the technique the paper describes, called spatial pyramid matching, remains important as a method for image matching. Once more, Google itself remains an obvious beneficiary of computer vision methods.
To claim that these papers “stand the test of time,” as Henderson does, is suspect. Instead, they show that the most popular scholarship is the kind that happened to find purchase on a current or emerging trend, just at the time that it was becoming a concern for a large group of people in a field, and for whom that interest amplified rather than dissipated. A decade hence, the papers haven’t stood the test of time so much as proved, in retrospect, to have taken the right bet at the right moment—where that moment also corresponds directly with the era of Google’s ascendance and dominance.
* * *
PageRank and Classic Papers reveal Google’s theory of knowledge: What is worth knowing is what best relates to what is already known to be worth knowing. Given a system that construes value by something’s visibility, be it academic paper or web page, the valuable resources are always the ones closest to those that already proved their value.
Google enjoys the benefits of this reasoning as much as anyone. When Google tells people that it has found the most lasting scholarly articles on a subject, for example, the public is likely believe that story because they also believe Google tends to find the right answers.
But on further reflection, a lot of Google searches do not produce satisfactory answers, products, businesses, or ideas. Instead, they tend to point to other venues with high reputations, like Wikipedia and Amazon, with which the public has also developed an unexamined relationship of trust. When the information, products, and resources Google lists don’t provide a solution to the problem the seeker sought, the user has two options. Either continue searching with more and more precise terms and conditions in the hopes of being led to more relevant answers, or shrug and click the links provided, resolved to take what was given. Most choose the latter.
This way of consuming information and ideas has spread everywhere else, too. The goods worth buying are the ones that ship via Amazon Prime. The Facebook posts worth seeing are the ones that show up in the newsfeed. The news worth reading is the stuff that shows up to be tapped on. And as services like Facebook, Twitter, and Instagram incorporate algorithmic methods of sorting information, as Google did for search, all those likes and clicks and searches and hashtags and the rest become votes—recommendations that combine with one another to produce output that’s right by virtue of having been sufficiently right before.
It’s as if Google, the company that promised to organize and make accessible the world’s information, has done the opposite. Almost anything can be posted, published, or sold online today, but most of it cannot be seen. Instead, information remains hidden, penalized for having failed to be sufficiently connected to other, more popular information. But to think differently is so uncommon, the idea of doing so might not even arise—for shoppers and citizens as much as for scholars. All information is universally accessible, but some information is more universally accessible than others.


The European Commission has fined Google a record $2.7 billion for the way it promotes its own shopping service over those of its rivals, and ordered the tech giant to change the way it shows the results or face further fines.
“What Google has done is illegal under EU antitrust rules,” Margrethe Vestager, the European Union’s Competition Commissioner, said in a statement. “It has denied other companies the chance to compete on their merits and to innovate, and most importantly it has denied European consumers the benefits of competition, genuine choice and innovation.”
Google in a statement said it “respectfully disagree[s]” with the ruling and will review it “as we consider an appeal.”
The EC said it was up to Google to decide how it would change its search results related to shopping. But if the company fails to comply, it will be ordered to pay 5 percent of Alphabet’s daily worldwide earnings—an amount equivalent to about $14 million each day. Alphabet is Google’s parent company.  
The ruling is the latest run-in U.S. tech companies have had with the EU’s regulators, who regularly target them for antitrust and tax-related issues.  In August 2016, Vestager demanded that Apple repay $14.5 billion in back taxes, calling the incentives the company received in Ireland “illegal tax benefits.”  Apple CEO Tim Cook called that ruling “maddening.” Vestager is also investigating Amazon’s tax practices in Europe and has fined Facebook over its acquisition of WhatsApp. But it’s Google that has felt the brunt of the rulings: Last year the EC announced it was investigating Google mobile-operating system Android on antitrust charges. It also being scrutinized for its advertising, which the bloc says violates its rules.     
The EC’s moves have prompted criticism that European regulators are deliberately targeting U.S. tech companies. The bloc’s regulators reject the accusation. The companies, too, have denied any wrongdoing.






James Berri traveled three hours to Sacramento earlier this month for his first Pride parade, one of hundreds of annual LGBTQ celebrations across America. Berri also talked about the experience on Facebook, reading and reacting to other people’s posts with thumbs-up likes and Facebook’s new rainbow “Pride” emoji. Throughout June, the platform is offering a rainbow flag alongside likes, hearts, and angry faces that people can click on to react to others’ posts and comments. Yet Berri, a 21-year-old transgender artist, is conflicted over the fact that not everyone can use this new rainbow button.
Back in Fresno, Berri wondered how Facebook decides who’s eligible. “Why don’t they have it, too?” he asked, referring to friends sitting with him in a salon in the larger, less-prominent California city. “It makes me confused for my friends.”
One friend disagreed: “Maybe I don’t want my family to actively know that I’m in all of these things because they’re just gonna—they’re not gonna like it.”
As a rare commodity, the Pride reaction has attracted a rainbow hunt among Facebook users. This June, Facebook announced that the feature would be available in “major markets with Pride celebrations” and for people who follow the company’s LGBTQ page. They also announced that the rainbow would “not be available everywhere.” For example, Facebook limits access in countries where LGBTQ rights are politically risky. Yet many Americans, like Berri’s Fresno friends, also missed out.
Is Facebook’s rollout of rainbow flags a case of algorithmic hypocrisy, user protection, or something else? Using their ability to detect people’s location and interests, the company's algorithms are choosing which people get the rainbow flag while hiding it from others. At first glance, this approach looks like it could contribute to the creation of political bubbles, as a feature promoted in progressive cities and less available in the rest of America. If real, these discriminatory political bubbles could constitute a secret kind of “digital gerrymandering,” according to Harvard Law professor Jonathan Zittrain.
Algorithmic political bubbles are hard to detect because they show something different to each person. Only by comparing notes can people map the boundaries of what a platform chooses to show its users. Doing so, when legal, allows independent researchers to detect discrimination and hold platforms accountable for their actions. To find out if Facebook's rainbow Pride reaction was a case of digital gerrymandering, our three-person team—a data scientist, a survey researcher, and an ethnographer of youth social-media practices—conducted an algorithmic audit, asking hundreds of Facebook users in 30 cities to report if they could access the Pride reaction.
Our audit asked two questions. First, are there U.S. cities where everyone is allowed to give a rainbow reaction? Second, do Facebook’s own LGBTQ-interest algorithms predict who has access elsewhere?
By using Facebook’s algorithms, we based our audit on the way that Facebook’s software sees the world. When advertisers publish an ad with Facebook, the company asks them to define the regions, interests, and demographics of the people they want to reach. While the platform’s gender targeting does not allow grouping by LGBTQ identities, their algorithms do infer LGBTQ-interest based on what people like, share, and write about. People can be categorized for their interests in, for instance, “Gay Pride,” “LGBT Culture,” “Pride Parade,” “Rainbow Flag (LGBT),” and “LGBT Social Movements.” Since Facebook allows advertisers to include or exclude people from those categories, we could survey people to discover if LGBTQ-interested people have a different experience on the platform from people that Facebook categorizes as not LGBTQ-interested.
Across 15 states that are home to the largest U.S. cities, we chose a large city per state and paired it with a smaller city elsewhere in the state. Within each city, we used Facebook’s ad targeting to recruit people who the platform’s algorithms think are interested in LGBTQ issues, compared to people who aren’t. We then tested the correlation between LGBTQ interest and access to the Pride reaction.
Among the cities we investigated, an overwhelming percentage of people without LGBTQ interests reported having the pride emoji in New York City, San Francisco, Chicago, Seattle, and Boston. Yet many other cities among the largest 25 in the U.S. were excluded from city-wide access, including Philadelphia, Detroit, Phoenix, and Nashville.
In places without city-wide access, Facebook’s LGBTQ advertising groups correlated strongly with people's ability to use the rainbow reaction. On average, people with LGBTQ interests who responded to our ads were 46 percent more likely than the non-LGBTQ interest group to report having access to the rainbow reaction. It's possible that people in the LGBTQ interest groups received the rainbow because they chose to “like” the LGBTQ@Facebook group, which the company says will unlock the rainbow reaction.
Kristina Boerger, a 52-year-old musician and human-rights organizer from Greencastle, Indiana, was surprised that other people could use the reaction but not her. “It certainly wouldn’t be because Facebook doesn’t know that I am queer,” she said. “That would be one of the first things they know about me.”
Why would Facebook selectively release the pride reaction? When we reached out for comment, a company spokesperson replied with a quote from an early June press release, explaining that Facebook limited access to test the feature, even though 22.7 million people have presumably unlocked the rainbow by liking the LGBTQ page. The platform may also be trying to protect users in parts of the U.S. where they could face harassment. When Facebook encouraged people in 2015 to choose a rainbow profile picture, some administrators of Facebook groups banned any member who made the change.
Limited access to the Pride reaction might also allow Facebook to gain PR benefits from supporting gay rights in some U.S. cities while avoiding scandal elsewhere. Could regional geo-fencing help the company manage public expectations in a polarized political environment? Betsy Willmore, an organizer of PrideFest in Springfield, Illinois, thinks the company is carefully managing its image. “Their intention is not to piss people off,” she said. “And they are legitimizing those that are getting pissed off by it.”
Many Americans could be unaware of Facebook’s public support for LGBTQ rights. After facing election-year pressures, the company might benefit from selective public understanding of its positions. Facebook and its PAC, like many corporations, routinely fund both Democrat and Republican candidates and events. Yet we failed to find a correlation between 2016 presidential election patterns and access to the Pride rainbow. Large cities that supported Trump in 2016 didn’t receive the pride reaction, but neither did many Clinton-supporting cities. If there’s a political pattern to Facebook's decision, we couldn't detect it.
Overall, our audit found that Facebook is doing what it says. The platform has avoided offering city-wide pride reactions in large metropolitan areas that supported Trump in the last election, but LGBTQ-interested people are still able to access the feature on average.
This month, millions of Americans have celebrated Pride with large urban events, in small towns, and across their digital-connected communities. For Berri and his friends in a Fresno salon, the choice to fly a flag online was as consequential as any march. During the conversation, one friend, a queer 19-year-old from Clovis whose name has been omitted to protect them from harassment, decided to “like” Facebook’s LGBTQ page for access to the rainbow reaction. Speaking of disapproving family, they said, “If they’re gonna be pissed off about it, whatever.”


To understand what the world will be like in ten years, it isn’t enough to look back at how different things were a decade ago and presume the differences will be comparable. The pace of technological change is speeding up so quickly, says Astro Teller, who leads the arm of Google that aims at “moonshots,” that one must look back 30 years to experience the same amount of discontinuity we’ll feel ten years hence.
A decade out, he continued, half of all cars on the road will be self-driving (and there would be more but for the fact that today’s cars are too expensive an asset to junk immediately).
The remarks took place Sunday at the Aspen Ideas Festival, which the Aspen Institute co-hosts with The Atlantic. And it prompted a question from moderator Andrew Ross Sorkin.
Trying to imagine a rapid shift toward self-driving cars, Sorkin wondered if the public would be ready as quickly as the technology. “Today there are 35,000 fatalities on the road using cars that we all drive just in the United States,” he said. “What number does that have to go down to that it becomes politically palatable, to the public, that they get in the car, and there may very well be a fatality as the result of a computer?”
In Teller’s view, we’re nearly there already.
“Almost every single person in this room already made that choice, because you got on a plane,” he told the Aspen crowd. “Planes fly roughly 99 percent of the miles that they fly by computer. It's now to the place that it is not safe for humans to fly in a lot of conditions. It's mandated that the computer fly because the computer can do it better.”
He posed this question to skeptics:
If you could have a robotic surgeon that makes one mistake in 10,000, or a human that made one mistake in 1,000, are you really going to go under the knife with the human? Really? We are already at that stage. I think self-driving cars are not in some weird other bucket. We make this decision all the time.
I suspect he is right, if only because more than half of young people already say in surveys that they look forward to self-driving cars, and the ubiquity of ride-sharing services with human drivers is already conditioning car passengers to give over more control. As a counterpoint, however, there are lots of Americans who choose to drive rather than fly, fearing the latter more despite knowing that it is statistically much safer.
With that in mind, I pose the question to readers who shudder at the thought of getting in a self-driving car, even after they are well tested and statistically safer than a car piloted by a human. Are you able to articulate what it is about the self-driving car that scares you? I fear sharks, despite the long odds against one biting me, because they are prehistoric sea monsters who rise up to unexpectedly bite people with razor sharp teeth. Dying by a combination of being eaten alive and drowning seems unusually scary. Why is getting in a self-driving car scarier than getting in a taxi?
The entire opening session of the Aspen Ideas Festival is below, with the Astro Teller interview starting at the 36-minute mark:



Let’s first acknowledge that Gchat was never officially called Gchat. Launched in February 2006, Google named it Google Talk, refusing to refer to it by its colloquial name. For anyone mourning its demise, which the company announced in a March blog post, those names sound awkward, like they’re describing something else. To me, and to many other users, it’s Gchat, and always will be.
The brilliance of Gchat was that it allowed you to instant message any Gmail user within a web browser, instead of using a separate application. This attribute was a lifeline for those of us who, a decade ago, were online all day at our entry-level jobs in open offices, every move tracked on computers that required admin access to download new software, with supervisors who could appear behind you at any time. You could open a separate browser window or a single tab, keeping Gchat running in the background as you ostensibly worked on projects aside from the dramas of your personal life.
Before Gchat, IMing was cloaked in anonymity. On AIM, I dialed up as “thalia587”—inspired by the Greek muse of comedy—after finishing my homework every night in high school. I shed that identity in college, when I’d log onto iChat on my blue iMac as “beulahtengo,” a mash-up of Beulah and Yo La Tengo, two of my favorite bands at the time. My friends knew it was me, but if I’d been a more rebellious youngster, I could have used those handles to IM anyone anonymously.
On Gchat, I was myself. When my invitation from Gmail—which at that point was still invitation-only—arrived right before my college graduation, I jumped on a username that was a variation of my real name, something I could print on a resume.
My college friends all did the same. When we scattered across continents after graduation, just a few months after abandoning Friendster for a new site called Facebook that, as far as we could tell, was most useful for determining who on campus was In A Relationship, Gmail and later, Gchat, helped us stay in touch, filling in the gaps between LiveJournal entries.
* * *
Gchat became another sort of lifeline during my time as a stay-at-home parent. I no longer had an employer standing over my shoulder or restricting what I downloaded. But some of my friends still used Gchat. So once my son whittled his naps down to one a day, guaranteeing a solid chunk of time for me to turn off Raffi and seek adult conversation, I’d crack open my MacBook and launch Gmail, around the time my friends were eating leftovers at their desks, their idle yellow status icons turning green again.
In the middle of my days of unpaid labor, Gchat was my remaining connection to the world of paid work. While I scanned the latest tweets in my feed, I kept a tab open to run Gchat in the background, ready in case someone wanted to talk during the one time of the day I was free.
* * *
Other people used Gchat for its ability to talk “off the record” without saving a transcript of the messages exchanged. As a digital packrat who saves folders of downloads and screen shots in case I need them someday, I never opted to do so. I wasn’t trading secrets or conducting an illicit affair. On the contrary—I loved being able to, for the first time, preserve transcripts of chats with my quick-witted friends, that, short of hiring a stenographer to follow me around, I’d never be fast enough to record in real life.
Only after a decade of trying to capture the ephemeral did I realize my mistake. Now, whenever I use Gmail’s search feature, essential for a service that urges you to keep everything while making it tedious to organize anything, driftwood from some years-old chat floats to the surface. Searching for, say, “Sleater-Kinney” in an effort to retrieve purchased concert tickets bubbles up ancient conversations with a variety of people with whom I’ve discussed the band over the years, only some of whom I’m still friends with.
Reading email exchanges from past relationships that soured is awkward enough. But it’s the old Gchats, conducted in close to real time, that transport me to the past, revealing thoughts I don’t remember having in conversations with people I no longer speak to, people who at the time I could never imagine not knowing. There they are, in stark black sans-serif: my overabundant exclamation points, my unsuccessful attempts at sarcasm, my bad jokes, or worse, responding “lol” to misogynistic ones. All preserved in digital amber, like the insect from Jurassic Park. And just like in the movie, when the past is within such close reach, I can’t leave it alone.
I understand why Google abandoned what it calls Talk. Like Google Reader, the now-defunct RSS feed aggregator that was the first Google product I mourned, Gchat’s limited features are a relic from a simpler time. When Gchat launched, you were either online or offline, with your status indicating your availability. The cultural tide has shifted in the opposite direction—now we’re always on, all messages are instant, and people have embraced the impermanence of digital scraps that briefly remain “on the record” before disappearing forever—think Snapchat, and Instagram Stories.
Unlike with Reader, which Google killed outright, the company has in mind a replacement for Gchat—Google Hangouts, which was stealthily integrated into Gmail in 2013. The company says Hangouts offers “advanced improvements” to Gchat’s “simple chat experience,” and that the vast majority of users who’ve switched over report few differences in functionality. Any tweaks are minor, like the discontinuation of idle and busy status icons in favor of “Last Seen” indicators and a mute feature.
I don’t know that I need Gmail to offer group video calling, photo messages or location sharing. I miss the time when green, yellow, and red bubbles of availability sufficed. We’re already flush with ways to convey the intricate mundanity of our lives, though each new one requires someone younger and younger to explain it to me. Inevitably, something new will change the game again. As an individual user, caught up in the whims of corporations competing for eyeballs and profit, it’s best not to get too attached to any one particular method of communicating.
Hangouts ushers Gchat into the mobile era, allowing asynchronous communication between two or more Gmail users, none of whom need to be sitting behind a computer to send a message. I downloaded the Hangouts app on my phone, but as I examine it in the lineup of other options, its relevance to my own life seems questionable. Rather than using it to contact the people I’m used to Gchatting, I imagine I’ll reach them with another app we’re both already on.
* * *
When I first signed up for Gmail, the mobile world as it exists today was unimaginable. I’d just upgraded from a 30-minute-a-month phone plan, reserved for emergencies, to my first two-year contract. T-Mobile shipped a small box to my first apartment with a shiny black flip phone wrapped in clear plastic. Texting was difficult and expensive. Each one cost about a dime, and if you wanted to type a C, you had to hit the “2” button three times. My mom had a similar phone; until she got the hang of it, she’d type my aunt Marcia’s name as “Mapaga.” The nickname stuck, even as the technology improved.
As new phones and data plans made texting easier and cheaper, and smartphones popularized multimedia messages, like videos, GIFs and emojis, our phones became our go-to sources for instant connection. Now I can send a minute-long video of my son’s first haircut to a group message of out-of-state family members, or show my friends a screenshot of an acquaintance I just saw on TV, and receive an instant response, before the show’s credits roll.
This impulse to share is what Google is trying to leverage through Hangouts, but with a corporate-friendly spin. “We’ve been working hard to streamline the classic Hangouts product for enterprise users,” reads the blog post announcing Gchat's demise. Another post on a different Google blog goes further, highlighting the company’s efforts to “[double] down on our enterprise focus for Hangouts and our commitment to building communication tools focused on the way teams work.” Clearly, people using Gmail for work, not just during work, are increasingly critical as Google competes with Microsoft and Slack for corporate users.
* * *
After Google announced the future of its messaging tools, I could only think about the past. When Google Reader vanished, the accompanying data disappeared forever, so I worried that the formal end of Gchat might mean the loss of those conversations. I searched Gmail’s help section for steps to download an archive of my chats, which number in the thousands, but there’s no easy way to do so. My pulse quickened at the thought of losing all those transcripts I hadn’t read in years, but that I might someday want to read again.
Like the one in which I coached my younger sister, who now has a masters degree and just bought a house with her fiancée, on her college application essay. “I suck at the ‘how did you first learn of Smith College’ question,” she’d lamented. “I was lurking colleges in Princeton Review … and I saw that Smith had ‘dorms like palaces’?”
Or the wistful ones from a friend in the throes of new motherhood, including one in which she contemplated a long car drive with her infant. “What’s the worst that can happen? She cries for three hours? That just sounds like…yesterday.”
Even ones that make me cringe, like one in which a guy who knew I pined for him told me “Serious Talk is a Poor Idea right now” because he was drunk on cheap wine and watching Predator 2 on a Saturday afternoon. “I mean,” he’d typed, “this movie has Bill Paxton in it.”
As with most 21st-century dilemmas requiring an immediate solution, I consulted—what else?—Google search. I discovered a step-by-step method to export all archived chats that looked legit. I followed the instructions and a file started downloading to my desktop with the extension .mbox, something the Mail application could read.
Once complete, I scrolled through the new Mail folder, relieved to see my fleeting correspondence from the previous decade. But as I looked closer, it became clear that the file had only imported the last line of each one of the thousands of chat threads in my Gmail history. Most of them were simple salutations or responses to something unknown—ttyl, haha, brb, lol, you too—stripped of all context through this technological hiccup. But some friends had a habit of never formally ending Gchat conversations, so scrolling through some lines revealed more about what we’d been discussing when one of us had signed off.
            al qaeda clearly has the wrong target
            did you bring the hobo gloves?
            not really wastednot really wasted
            plus i have to find some meat to eat
            she wants help with her Ikea bookshelf
            but I’m Mom Terrible, which is much better than regular terrible
            life is continually amusing
Fortunately, my paranoia was unwarranted. Google’s communications team assured me the company will archive all on-the-record chats, even those predating Hangouts. I’m relieved I can still peek at that time in my life to see how much has changed in a decade, but it’s unsettling to realize that ultimately, it’s not up to me. To keep enjoying the perks of any communication platform, some control over the content must be ceded. Not a comfortable thought, this powerlessness, but technology unspools in one direction only, offering no way to rewind.  


In 2012, the Curiosity rover began its slow trek across the surface of Mars, listening for commands from Earth about where to go, what to photograph, which rocks to inspect. Then last year, something interesting happened: Curiosity started making decisions on its own.
In May last year, engineers back at NASA installed artificial-intelligence software on the rover’s main flight computer that allowed it to recognize inspection-worthy features on the Martian surface and correct the aim of its rock-zapping lasers. The humans behind the Curiosity mission are still calling the shots in most of the rover’s activities. But the software allows the rover to actively contribute to scientific observations without much human input, making the leap from automation to autonomy.
In other words, the software—just about 20,000 lines of code out of the 3.8 million that make Curiosity tick—has turned a car-sized, six-wheeled, nuclear-powered robot into a field scientist.
And it’s good, too. The software, known as Autonomous Exploration for Gathering Increased Science, or AEGIS, selected inspection-worthy rocks and soil targets with 93 percent accuracy between last May and this April, according to a study from its developers published this week in the journal Science Robotics.
AEGIS works with an instrument on Curiosity called the ChemCam, short for chemistry and camera. The ChemCam, a one-eyed, brick-shaped device that sits atop the rover’s spindly robotic neck, emits laser beams at rocks and soil as far as 23 feet away. It then uses the light coming from the impacts to analyze and detect the geochemical composition of the vaporized material. Before AEGIS, when Curiosity arrived at a new spot, ready to explore, it fired the laser at whatever rock or soil fell into the field of view of its navigation cameras. This method certainly collected new data, but it wasn’t the most discerning way of doing it.
With AEGIS, Curiosity can search and pick targets in a much more sophisticated fashion. AEGIS is guided by a computer program that developers, using images of the Martian surface, taught to recognize the kind of rock and soil features that mission scientists want to study. AEGIS examines the images and finds targets that resemble set parameters, ranking them by how closely they match what the scientists asked for. (It’s not perfect; AEGIS can sometimes include a rock’s shadow as part of the object.)
Here’s how Curiosity’s cameras see the Martian landscape with AEGIS. The targets outlined in blue were rejected, the red are potential candidates. The best targets are filled in with green, and the second-best with orange:
When AEGIS settles on a preferred target, ChemCam zaps it.
AEGIS also helps ChemCam with its aim. Let’s say operators back on Earth want the instrument to target a specific geological feature they saw in a particular image. And let’s say that feature is a narrow mineral vein carved into bedrock. If the operators’ commands are off by a pixel or two, ChemCam could miss it. They may not get a second chance to try if Curiosity’s schedule calls for it so start driving again. AEGIS corrects ChemCam’s aim in human-requested observations and its own search.
These autonomous activities have allowed Curiosity to do science when Earth isn’t in the loop, says Raymond Francis, the lead system engineer for AEGIS at NASA’s Jet Propulsion Laboratory in California. Before AEGIS, scientists and engineers would examine images from Curiosity, determine further observations, and then send instructions back to Mars. But while Curiosity is capable of transmitting large amounts of data back to Earth, it can only do so under certain conditions. The rover can only directly transmit data to Earth for a few hours of day because it saps power. It can also transmit data to orbiters circling Mars, which will then kick it over to Earth, but the spacecraft only have eyes on the rover for about two-thirds of the Martian day.
“If you drive the rover into a new place, often that happens in the middle of the day, and then you’ve got several hours of daylight after that when you could make scientific measurements. But no one on Earth has seen the images, no one on Earth knows where the rover is yet,” Francis says. “We can make measurements right after the drives and send them to Earth, so when the team comes in the next day, sometimes they already have geochemical measurements of the place the rover’s in.”
Francis said there was at first some hesitation on the science side of the mission when AEGIS was installed. “I think there’s some people who imagine that the reason we’re doing this is so that we can give scientists a view of Mars, and so we shouldn’t be letting computers make these decisions, that the wisdom of the human being is what matters here,” he said. But “AEGIS is running during periods when humans can’t do this job at all.”
AEGIS is like cruise control for rovers, Francis said. “Micromanaging the speed of a car to the closest kilometer an hour is something that a computer does really well, but choosing where to drive, that’s something you leave to the human,” he said.
There were some safety concerns in designing AEGIS. Each pulse from ChemCam’s laser delivers more than 1 million watts of power. What if the software somehow directed ChemCam to zap the rover itself? To protect against that disastrous scenario, AEGIS engineers made sure the software was capable of recognizing the exact position of the rover during its observations.  “When I give talks about this, I say we have a rule that says, don’t shoot the rover,” Francis says. AEGIS is also programmed to keep ChemCam’s eye from pointing at the sun, which could damage the instrument.
In many ways, it’s not surprising that humanity has a fairly autonomous robot roaming another planet, zapping away at rocks like a nerdy Wall-E. Robots complete far more impressive tasks on Earth. But Curiosity is operating in an environment no human can control. “In a factory, you can program a robot to move in a very exact way over to a place where it picks up a part and then moves again in a very exact way and places it onto a new car that’s being built,” Francis says. “You can be assured that it will work every time. But when you’re in a space exploration context, literally every time AEGIS runs it’s in a place no one has ever seen before. You don’t always know what you’re going to find.”
Francis says the NASA’s next Mars rover, scheduled to launch in 2020, will leave Earth with AEGIS already installed. Future robotic missions to the surfaces or even oceans of other worlds will need it, too. The farther humans send spacecraft, the longer it will take to communicate with them. The rovers and submarines of the future will spend hours out of Earth’s reach, waiting for instructions.
Why not give them something to do?


Stephanie Woodward just wanted to meet her friends for a drink. It was a bar she’d never visited, and she was excited. But going anywhere new for Woodward requires a vetting process. She uses a wheelchair, so building access is always a worry. Research on Google Street View proved promising in this case: A ramp led up into the entryway. That evening, Woodward entered the front door without trouble. But once inside, a single step stood between her and the bar.
It was one step, but for Woodward it may as well have been a wall. “I’m in the front lobby, but to get any sort of service, to even be seen, I had to call the staff,” she says. “I can’t visit this business independently. I’m a strong wheelchair user, but hopping steps is not an easy task.”
Thanks to decades of disability activism culminating in the passage of the Americans with Disability Act (ADA) in 1990, the ramp has become both a tool for accessibility and opportunity for architectural innovation. In the modern built environment, the ramp services people bound to wheelchairs or strollers—making those bodies newly visible in the process. Yet, despite their apparent success, ramps remain contested sites for equal access.
* * *
The ramp is believed to have moved the materials that built the Egyptian pyramids and Stonehenge. The ancient Greeks constructed a paved ramp known as the Diolkos to drag ships across the Isthmus of Corinth. In 1600, Galileo hailed the inclined plane as one of the six simple machines in his work Le Meccaniche.
The ramp’s ability to move objects shouldn’t overshadow its astounding ability to move people. The ramp was retooled as a highly effective “people mover” 300 years after Galileo, in the design of New York’s Grand Central Terminal. The Vanderbilt family, who operated the rail lines the terminal would service, promised New Yorkers an innovative train hub to accommodate newly electrified tracks. They hired the Minnesota-based architecture firm Reed & Stem to get the job done. “Its innovative scheme featured pedestrian ramps inside, and a ramp-like roadway outside that wrapped around the building to connect the northern and southern halves of Park Avenue,” explains the New York Transit Museum.
As design moved forward, engineers built mock-ups at various slopes and, according to the New-York Tribune, studied “the gait and gasping limit of lean men … fat men … women with babies… and all other types of travelers” to determine the ideal grade. It wasn’t a pointless exercise: When the terminal opened in 1913, it was billed as the first great “stairless” station, in the words of Grand Central historian Sam Roberts. The flow of passengers with luggage, strollers and wheelchairs was swift; the “Red Cap” attendees could move their wheeled carriers with ease. The system remains one of the most celebrated in American transit terminals; modern travelers move as easily up and down the ramps, just with less fanfare.
One frequent passenger to Grand Central Terminal was Presidential Franklin D. Roosevelt, who utilized a “secret platform” and elevator to ascend from the lower-level tracks directly up to the Presidential Suite at the Waldorf-Astoria Hotel. At the time, he was hiding his disability and wheelchair from the American public; Grand Central’s ramps were of no use to him. “The first president with a disability was a great advocate for the rehabilitation of people with disabilities,” explains the Anti-Defamation League. “But [he] still operated under the notion that a disability was an abnormal, shameful condition, and should be medically cured or fixed.”
* * *
This sentiment began to change in the 1940s and 1950s. Many World War II veterans returned home with mobility-related injuries. There was little accommodation for wheelchair users at the time, particularly within public spaces. According to a study by the historian Julie Peterson, disabled veterans attending the University of Illinois often hitched rides on service trucks to avoid sidewalks without accessible ramps.
Returning vets planted the seeds for the disability-rights movement, and activism grew alongside the other social movements of the 1960s. Protesters took to the streets, smashing curbs to create their own accessible ramps. In the 1970s, founders of the Independent Living Movement in Berkeley, California, established a wheelchair route through the University of California campus and its vicinity. According to Peterson, they even rolled their own curb ramps, “covertly laying asphalt in the middle of the night.”
Disability activists lobbied Congress and marched on Washington to include their rights in a major affirmative-action bill that would prohibit employment discrimination by the federal government. The so-called Rehabilitation Act was passed in 1973, and for the first time in history, the civil rights of people with disabilities were protected by law. In ensuing years, activists sought to consolidate various pieces of legislation into a single civil-rights statute, much like the 1964 Civil Rights Acts had done for race. But it wasn’t until 1990 that the government passed the Americans with Disability Act, making way for the contemporary, ramped environment. While the law protected the civil rights of disabled Americans, it also required businesses provide accommodations to people with disabilities, and ensured public spaces would receive modifications to become wheelchair accessible.
Architectural, design, and planning practices had to adapt after the ADA. It wasn’t—and still isn’t—an easy shift. Annie Boivin, a designer (and wheelchair user) with the architecture firm Perkins+Will, tells me that the Swiss architect Le Corbusier is partly to blame. In the early 20th century, Le Corbusier created the fictitious character Le Modulor—an able-bodied man, of average height and dimension, around whom Le Corbusier believed standardized design should revolve. Whole cities were designed by the able-bodied men on which Le Modulor was modeled. It was a period with no distinction between what are now known as the two models of disability: medical and social. The medical model views disabled bodies as impaired, the social model points out the environment was never built for them in the first place.
ADA standardization has attempted to remedy the situation. Architects rely on tools like elevators, lifts, and automatic doors. The ramp, the most visible architectural element of the post-ADA period, is also the most important to wheelchair users. Woodward compares the ramp to a “dependable boyfriend who will never leave us.”
Reliable though it might be, the ADA is hardly a cure-all. All buildings constructed or renovated after the law’s passage must follow standards for accessible design, but many older structures still have relics of inaccessibility—like the single-step entrance that kept Woodward from entering the bar. Disability activists I spoke with say that it’s common for building owners to ignore ADA requirements, and pressing them to follow the rules can be difficult. Just this year, a bill was introduced in Congress that would make it harder to sue building owners who fail to remove so-called “architectural barriers.”
* * *
The problem of the single-step entryway inspired the design researcher Sara Hendren to build her own ramp, called Slope Intercept. It can nest, stack, and move on casters. Hendren bemoans an enduring “compliance culture” within architecture, in which ramps are tacked onto buildings with little imagination. Mia Ives-Rublee, who led the disability caucus for the Washington, D.C., Women’s March, says that ramps are often hard to find, placed in the back of buildings, difficult to navigate, or lead to locked doors. She adds that searching for ramps and finding them along the back of buildings “makes you feel like a second-class citizen.”
Those persistent difficulties impact the visibility of disabled bodies in public spaces. “It can become so tiring,” Ives-Rublee tells me. “A lot of people with disabilities won’t go to new places.”
Hendren, eager to show the creative potential that stemmed from one of Galileo’s simple machines, partnered with the dancer Alice Sheppard to design a ramp Sheppard could use onstage with her wheelchair. Sheppard came to the table no stranger to the failure of ramp design. “Why the hell are these eyesores?” she asked. “Compliance-oriented design tends to miss the aesthetic and physical experience of going down a ramp. It should be beautiful, it should participate.”
Attitudes might be starting to change. In 2001, the architect William Leddy was asked to design the Ed Roberts Campus in Berkeley, California. The campus, which opened in 2010, is named after the founder of Berkeley’s Center for Independent Living—the group that installed their own ramps, in the dead of night, throughout the city in the 1970s. It needed to serve as “a symbol of universal design to the community,” according to Leddy. Universal design, he explains, is a design philosophy that strives to create buildings and products usable by all people, to the greatest extent possible, without any need for adaptation.
At the Ed Roberts Campus, the firm designed a helical, bright-red ramp, a dramatic focal point emerging from the middle of the first-floor lobby. At a width of seven feet, there is space for a row of friends or colleagues to traverse it together. Leddy once stumbled upon a wedding ceremony on the ramp, and he vividly recalled a conversation with a wheelchair user who said this was the first building he could move through seamlessly, without asking for any help. The campus, inspired by the design, integrated the ramp into its logo.
Stephanie Woodward is doing her part, too, as Director of Advocacy for New York’s Center for Disability Rights. Upon encountering a non-compliant business—like the bar she couldn’t access—the group writes a letter offering to assist in improving accessibility. If they get no response, they organize protests around the business. “A lawsuit can take seven years to get one ramp in front of a building, one protest could result in a ramp there next week,” she says.
The organization has only started one lawsuit against a non-compliant business, but it’s not how Woodward wants to win her battles. “We shouldn't have to start a lawsuit to have the same access and everyone else,” she says. “We don’t want to sue, we just want to get in.”
This article appears courtesy of Object Lessons.


Few were surprised this morning to learn of the resignation of Travis Kalanick from being the CEO of Uber. The company has endured scandal after scandal, many of which trace back to Kalanick in one way or the other, whether directly as a result of his behavior or his business choices, or less directly as a result of the allegedly toxic and discriminatory culture he helped to create as Uber’s founder. It was easy to see why Kalanick had to go. By removing him, investors and the board are undoubtedly hoping to curtail the onslaught of negative attention and return the company to grow and raise money in peace. But at this point, rebuilding and rebranding Uber will take more than pushing out its leader.
For Uber’s investors and directors, a leadership change is a way of showing that Uber is serious about taking a new direction, and protecting the company’s reported $70 billion valuation in the process. “Uber’s clearly in a situation where small changes, simple policy adjustments, those sorts of things, weren’t going to satisfy the investor community, the customer base, and the employee base,” says Brian Kropp, the head of the human-resources practice at Gartner, a research and consulting firm.
But though Uber’s troubles tended to trace back to Kalanick in some way, they also went beyond him: Last week, at the same meeting that it was announced that Kalanick would be going on a (then-temporary) leave, a different board member made a sexist comment that resulted in his own resignation soon after. “Uber has demonstrated that its problem is not only about a single figure—a reputational cancer that could have been cut away—but that the cancer has infected the rest of the body,” says Audra Diers-Lawson, a professor of public relations strategy  at Leeds Beckett University. “Because the bad behaviors have extended beyond just the CEO, a new negative expectation is probably being formed and this is fundamentally damaging to the company.”
According to Diers-Lawson, Kalanick’s ouster was absolutely the right decision for the company, but it would have been better if it had done so when problems were nascent. “In 2015, the company had the opportunity to genuinely mitigate the damage of his influence on the corporate culture and the company’s reputation as the first wave of this crisis hit the public eye,” she said.  
Uber certainly isn’t the only, or first, tech startup with the problem of a young, brash CEO who creates a unique and disruptive product, but cannot seem to make the leap to successful management. A New York Times article from April dubbed this phenomenon the “bro CEO,” citing examples such as Quirky, a gadget-pedaling platform that raised $185 million before being undone by the questionable behavior of its 20-something CEO and founder, and an HR startup called Zenefits, which was once valued at $4.5 billion but ousted its young male CEO amid both criticisms about the company’s frat-like culture and allegations that the company had engaged in cheating on licensing courses. While the company still exists, it is severely diminished and only a fraction of its former size.
But though CEO problems are somewhat common, Uber is a special case. The company, though it’s never actually turned a profit, is flush with investor cash and wildly popular. But beyond that, the timing of Uber’s drama hits right when the public and investors are more engaged than ever in a conversation about the role of corporate culture in the health of a company and the economy more widely. According to Kropp, in 2010, fewer than 40 percent of company earnings calls took the time to discuss issues such as talent or corporate culture; now that figure has climbed to more than 60 percent. That’s because more people today believe that culture is a critical factor in whether a company can attract the right employees and turn a profit.
Now that Kalanick’s gone, there are still some significant structural challenges for the company to overcome. First, there’s the question of what happens to the upper echelons of Uber’s management. The company has long been without a chief operating officer, a vacancy that many experts, and the Holder report, have suggested desperately needs to be filled. As Quartz reported, some tech startups have filled this role with someone who is all the things that the CEO is not. Facebook’s hiring of Sheryl Sandberg in 2008, for example, is widely seen as a brilliant and effective hire that complemented the company’s CEO Mark Zuckerberg, with Sandberg’s experience and corporate diplomacy tempering Zuckerberg’s relative inexperience and sometimes tough management style. But hiring for the COO role, particularly one who will work as a part of a management team, might be difficult without a CEO.
Replacing any CEO, especially one who is pushed out amidst controversy, is a significant task. Whoever Uber hires or promotes to fill the role could drastically alter operations, or continue to proliferate the same problems. Kropp says that replacing a founder-CEO is often an especially tricky task. In cases where the company is doing well and the CEO is well-loved, it makes sense to promote internally, someone who could potentially continue the current path. For established companies with CEO problems, it can serve to change tacks completely, bringing in an outsider. But for a startup such as Uber, a fairly young company with a CEO who left under very public and difficult circumstances, neither might be quite right. An internal hire may be seen as having accepted and contributed to the existing problems. An outsider may have a difficult time acclimating and understanding which factors make Uber special and unique, and are worth retaining. An outsider may also want to make their mark by completely changing the brand, and that can create corporate and cultural destruction in a different way. The sweet spot, Kropp says, would be someone who has worked at the company before, but then left and was successful elsewhere. And that’s not easy to come by.
In order to create real and lasting change, Uber will need to spend money, Kropp says, not just try to implement one-time changes. “A lot of companies try to talk themselves out of these sorts of cultural challenges. They’ll write memos, send notes, make presentations, saying things need to change. But at the end of the day, if you’re not spending money to try to change the problem, they likelihood that you’re actually able to change the culture is incredibly low.” Forcing the CEO out is certainly a bold step toward change, but Kropp says that alone won’t be enough. Instead, salvaging Uber will require constant investment and training for initiatives that will constantly reinforce the company’s new values, accepted behaviors, and expectations. They’ll need to hire people who align with the new values, and create new roles, such as the one Frances Frei, the new senior vice president of leadership and strategy, inhabits. They’ll also need to expand their budgets to help the people in those new roles build teams and implement big changes that can influence the culture. And they’ll have to implement ongoing methods of measuring progress and sussing out new problems. Without those continuing efforts, eventually muscle memory will kick in and everyone will go back to their same old behavior, new CEO or not.


In a leafy Detroit suburb last March, federal authorities raided a one-story brick house. Their target: Rudy Carcamo-Carranza, a 23-year-old restaurant worker from El Salvador with two deportation orders, a DUI, and a hit-and-run.
The incident would have seemed like a standard deportation case, except for a key detail unearthed by The Detroit News: The feds didn’t find Carcamo-Carranza through traditional detective work. They found him using a cell-site simulator, a powerful surveillance device developed for the global war on terror.
Five days after his election, Donald Trump announced his plan to quickly deport up to 3 million undocumented immigrants—“people that are criminal,” “gang members,” “drug dealers.” How would he do it? How would he deport more people, more quickly, than any of his recent predecessors? The Carcamo-Carranza case suggests an answer: After 9/11, America spent untold sums to build tools to find enemy soldiers and terrorists. Those tools are now being used to find immigrants. And it won’t just be “bad hombres.”
There’s a lot to Trump’s tactics that are very old. Trump seeks to ban Muslim immigrants, spy on mosques, and subject Muslims to extreme border interrogations. In the Chinese exclusion of the late 19th and early 20th centuries, the U.S. government banned most Chinese immigrants, sent investigators to spy on their businesses, and subjected them to extreme border interrogations. In 2017, Trump allies defend the Muslim ban by saying it’s not a Muslim ban, but a geographic ban on people from certain “areas of the world.” In 1917, Congress banned Indian immigrants not by name, but by drawing a box around the region and calling it an “Asiatic Barred Zone.”
Still, there are key aspects to immigration enforcement under Trump that are frighteningly new, albeit some time in the making.
In 2000, when George W. Bush was elected, drones, face recognition, mobile fingerprint scanners, and cell-site simulators—which mimic cellphone towers to intercept phone data—were novel or non-existent. Under the Immigration and Naturalization Service and its successor, Immigration and Customs Enforcement, or ICE, immigration enforcement was a low-tech affair, mostly known for large worksite raids.
Under Barack Obama, ICE went high-tech. At the heart of that shift were biometrics: precise, digitized measurements of immigrants’ bodies. Obama ramped up a Bush-era program, Secure Communities, which sent booking fingerprints from local jails to the Department of Homeland Security, shunting hundreds of thousands of undocumented and legal immigrants, many arrested for minor offenses, into federal deportations.
Previously, federal use of biometrics in the field had focused on Iraq and Afghanistan; with a fingerprint or iris scan, soldiers could tell militants from civilians. In his final years, Obama hit the brakes on Secure Communities—but mobile biometrics trickled down anyways. ICE agents began to stop people in the street to scan their fingerprints. Authorities requested face-recognition searches of Vermont driver’s license photos, looking for visa overstays. Customs and Border Protection sought proposals for face-recognition enhanced drones that, mid-flight, would scan and identify people’s faces.
For all of these technical advances, however, Obama never unleashed his full surveillance powers on immigration enforcement inside the U.S.; most of Obama’s removals took place at the border. Under his Priority Enforcement Program, actions inside the country were primarily targeted against people with criminal records.
Donald Trump brings two fundamental changes. The first is animus. When Trump calls Mexican immigrants drug traffickers and rapists, when he says a judge cannot do his job because of his Mexican heritage, when he implies that Muslim immigrants are party to a vast, Islamist conspiracy (we have to “figure out what’s going on”), it could send a signal to rank and file immigration enforcement.
Second, Trump is starting to use his surveillance arsenal to its utmost legal and technical capacity—within the U.S. Shortly after Carcamo-Carranza’s arrest using a cell-site simulator, a DHS spokesperson clarified that the new “border” drones would not be limited to the border. Instead, the drones would be used wherever there is a “mission need,” a wink at DHS’s claim that the Border Patrol can conduct searches up to 100 miles from the actual border. Simon Sandoval-Moshenberg, a prominent immigration attorney in Virginia, reports that since Trump’s inauguration, every one of his clients arrested by ICE has had their fingerprints scanned before being taken into custody.
Trump’s aggressive use of surveillance is not just about devices. It’s about data. On his fifth day in office, the president issued an executive order on immigration enforcement inside the U.S. Many focused on the fact that he was restoring Secure Communities, the fingerprint-sharing program of the Bush and Obama eras. Fewer noticed the short section, a few lines down, that revoked Privacy Act protections for non-citizens, making it easier for many federal agencies to share with ICE troves of data on legal and undocumented immigrants.
In the era of late-20th-century surveillance—beginning, loosely, with the final years of J. Edgar Hoover and ending with 9/11—there were limits, informal and formal, that focused America’s most powerful surveillance techniques on investigations of the most serious offenses. They were far from perfect, but they were real. The first was cost: It was expensive to “tail” people and track their movements. The second was legal. In 1968, Congress passed the Wiretap Act, which had at its core a simple idea: Wiretaps should be used to catch serious criminals, not petty offenders. You can’t wiretap a jaywalker; you can wiretap a bank robber.
Modern surveillance tools bypass these restraints. They bring the cost of surveillance down to a fraction of the original expense. They outpace federal lawmakers. State legislatures have passed dozens of laws restricting geolocation tracking, cell-site simulators, drones, and other technologies; Congress has passed zero such laws for criminal law enforcement, let alone ICE.
Most people caught in this dragnet will not be like Rudy Carcamo-Carranza. There are not, and never have been, 3 million undocumented criminals. Like his predecessors, most of the people Trump deports will be like Maribel Trujillo Diaz, Arino Massie, or Mario Hernandez-Delacruz: People innocent of any crime. And as Wade Henderson, a dean of the civil rights community, warned, Trump will have, at his disposal, “the greatest surveillance apparatus that this nation, and arguably the world, has ever known.”
In the public eye, Trump’s policies on health care, climate change, and foreign affairs have eclipsed his agenda on immigration. Perhaps people think it only affects immigrants. This is a mistake: Surveillance of immigrants has long paved the way for surveillance of everyone.
Biometrics are no exception. For years, the State Department let the FBI use face recognition to compare suspected criminals’ faces to those of visa applicants. In 2015, State and the FBI announced a pilot program to run these searches against the faces of Americans in passport photos. For years, Congress pressed DHS to use biometrics to track foreign nationals leaving the country. This year, DHS launched face scans through Delta and JetBlue—and both systems scan the faces of foreign nationals and citizens alike.


This year, Netflix will spend something in the realm of $6 billon on original programming, more than any media company apart from ESPN. Amazon is expected to spend $4.5 billion. Even Google, the owners of YouTube, are looking to spend hundreds of millions making TV shows this year. Streaming TV is no longer a fad—it’s a booming industry, one that’s competitive with cable and network television, and supremely attractive to artists who want to make their work with the least interference possible. Now, just as things have gotten crowded, another tech giant is looking to muscle in to the original-TV content world: Apple.
Though Apple, of course, has plenty of money to throw at scripted programming, it’s always seemed cautious about committing to the kind of onslaught that Netflix, Amazon, Hulu and others have engaged in over the past few years. Netflix is now basically offering an entire new season of a television series every week, on top of its original films and slew of comedy specials. Amazon, which provides shows like Transparent to all of its Prime subscribers, has a more democratic process in which it posts pilot episodes online and invites subscribers to watch and review them before ordering them to series.
Netflix Believes in the Power of Thumbs
It’s still unclear what Apple’s strategy is going to be—but the company has hired two of the biggest names in television production to oversee new positions in video programming. Jamie Erlicht and Zack Van Amburg, the longtime presidents of Sony Pictures Television, are joining Apple this summer to begin work on something “exciting,” according to a statement from Apple’s senior vice president Eddy Cue. “There is much more to come,” he teased, providing no other information on their new responsibilities.
It’s pretty easy to guess what comes next. Sony Pictures Television is one of the most respected production companies in the industry, one that’s worked in all genres and mediums. Among the eclectic shows stewarded by Erlicht and Van Amburg since they took the Sony helm in 2005 are Damages, Breaking Bad, Better Call Saul, Drop Dead Diva, Community, Justified, Happy Endings, Hannibal, Masters of Sex, and Underground.
Even before then, they were part of an initial movement toward offering challenging series on basic cable. They worked at Sony (below executive Steve Mosko) when it sold the shows The Shield and Rescue Me to FX, two of the earliest basic-cable programs to attract attention from critics and Emmy voters. That expanded the “Golden Age” of TV beyond premium-channel offerings like The Sopranos and Sex and the City, eventually spurring the rise of streaming networks. In general, the pair have a proven record of teaming up with interesting creators and shepherding projects with the kind of individual touch that stands out—exactly what is needed in the packed world of Peak TV.
The streaming boom is, first and foremost, auteur-driven: Netflix, Amazon, and Hulu attract well-known creators by offering them more artistic freedom than the world of network television. Shows like House of Cards, Transparent, and Orange Is the New Black are sold as distinctive items: not for everyone, of course, but appealing enough to draw in new subscribers eager to watch one particular show. Critical acclaim is only so important; Netflix CEO Reed Hastings long bragged about how the much derided Hemlock Grove attracted more subscribers than House of Cards, at first.
Erlicht and Van Amburg will now be tasked with defining Apple’s new original-TV brand. A statement from the pair said that Apple was looking to bring in programming of “unparalleled quality,” which of course doesn’t mean much; their hiring does seem to indicate that Apple will try to function as more of a traditional TV studio. Some rumors had indicated the company wanted to buy another production company, like Ron Howard and Brian Grazer’s Imagine Entertainment, outright, but instead Erlicht and Van Amburg will build something from the ground up.
Other questions remain: How will Apple present its new shows? Will you need an Apple TV device to watch them? Will the company introduce a subscription service mimicking Netflix, Amazon, and Hulu, and if so, will it buy up the rights to various existing shows and movies to fill out its library? It could also go the route of networks like CBS, offering new shows like The Good Fight and Star Trek Discovery for a smaller monthly fee, or try something else entirely. Other details will come to light soon, but for now, Apple’s big hires suggest Peak TV’s rapid expansion won’t slow down anytime soon.


It came down to money, in the end. Investors backing Uber decided it wasn’t enough that Travis Kalanick announced last week he would take an indefinite leave from his position at the helm of the scandal-plagued company.
He had to go. Now.
This was an “outright rebellion” by shareholders, says Mike Isaac, The New York Times reporter who first reported Kalanick’s surprise ouster overnight. On one hand, it all seemed to have happened rather quickly: Investors delivered a letter to Kalanick while he was on business in Chicago on Tuesday, insisting he step down. Kalanick then spoke with investors and at least one Uber board member, the Times reported, and agreed to resign. (Uber didn’t immediately respond to The Atlantic’s request for comment early Wednesday.)
Viewed another way, Kalanick’s departure was a long, long time coming. Uber has been beset by scandals for most of the year, including a boycott campaign from users, explosive allegations of sexual harassment by a former Uber engineer, a leaked video showing Kalanick arguing with an Uber driver, a federal lawsuit alleging Uber stole a competitors’s design secrets—and those aren’t even all of the big ones. More than once, one unfavorable story about Uber was still prominently in the news when the next PR nightmare materialized.
To onlookers without any stake in the company, Uber’s troubles have been so pronounced as to seem, at times, darkly funny. (“Getting Out Ahead Of This One: Uber Has Apologized In Advance If Anyone Finds Out About Something Called ‘Project Judas,’” said a joke-headline from the satirical website Clickhole, a sister site to The Onion.) In recent weeks, so many of Uber’s senior leaders had either resigned or been fired that, as one mock-suggested on Twitter, a company focused on self-driving cars had become driverless itself. Susan Fowler, the engineer who wrote the explosive blog post about Uber’s toxic culture in February, joked about the possibility of a Hollywood adaptation of the mess: “I would just like to say, just for the record, that I would like to be played by Jennifer Lawrence.”
But the serious questions always came back to Kalanick. It began to seem there was no breaking point. How long could one man remain in charge of a company that seemed to be so badly flailing? And, crucially, what was the public-relations fire-swamp doing to Uber’s $70 billion valuation?
Kalanick’s ouster—and the paradox of how it seems both sudden and drawn out—is a reflection of the forces that rule Silicon Valley. Namely, money, money, and more money. (“Cash flows before bros,” as the tech news site Pando put it last week.)
It was ultimately concerns over the bottom line—not merely the toxic culture, or Kalanick’s trademark hubris, or explosive allegations of sexual harassment, or revelations about Uber’s secret software to evade of law enforcement—that forced Kalanick out. Well, out of his job as CEO, that is. He’ll still be on Uber’s board of directors, and he will retain his control of a majority of Uber’s voting shares.
Which means that, even without Kalanick at the helm, Uber is still the Uber Kalanick built—barring other changes that the company has promised to make. In the meantime, you can be sure Uber employees are watching to see who will succeed their old boss, and what that hire might reveal about the seriousness with which Uber takes its employees’ complaints and its commitment to improving diversity. That remains an open question: The results of Uber’s recent internal investigation yielded superficial and outright bizarre attempts to change the company’s culture—renaming the “War Room” the “Peace Room,” for example, and a request for everyone who attended a company meeting to hug. (Seriously.)
All this calls to mind the old business joke about a CEO who attends a conference on the importance of corporate culture, then barks at the head of HR, “get me one of those things.” The difficulty of shedding a company’s culture—even after shaking up top leadership—was on full display last week. Shortly after Uber published a spate of initiatives it said would help make the company move past its hostile reputation, leaked audio emerged of a board member making a sexist remark at a meeting intended to help with a smooth transition during Kalanick’s then-leave. (Within hours, that board member had resigned.)
Now that Kalanick’s indefinite leave has become definite, Uber finds itself at a crossroads. An Uber without its founding CEO is an Uber untethered to the principles that the company has associated with its rapid growth since it launched in 2009, for better and perhaps for worse. Uber has recently tried to distance itself from some of what it long described as core competencies—qualities like “super pumpedness,” “always be hustling,” and “toe-stepping.” It even announced this week it will allow tips to drivers in a longstanding reversal of a controversial policy.
Neither Kalanick’s departure nor small hints at changes to come are guarantees that Uber’s troubles are over. One of the biggest tests ahead is Uber’s legal battle with Waymo, the driverless-car company that spun out from Google, which claims Uber stole its design secrets.
Eventually, it was investors who answered the question of whether Uber could thrive with Kalanick as CEO. They decided it could not. Next, they will find out if the company can survive without him.


Uber CEO Travis Kalanick has resigned reportedly following a shareholder revolt, capping a tumultuous few months of PR disasters of its own making.
“I love Uber more than anything in the world and at this difficult moment in my personal life I have accepted the investors request to step aside so that Uber can go back to building rather than be distracted with another fight,” Kalanick said in a statement, cited by The New York Times and others. Bloomberg said he’d remain on the company’s board.
Last week Kalanick said he would take an indefinite leave of absence from the company to both work on himself amid a series of controversies as well as to mourn his late mother.
Here’s more from the Times on his resignation:
Mr. Kalanick’s exit [Tuesday] came under pressure after hours of drama involving Uber’s investors, according to two people with knowledge of the situation, who asked to remain anonymous because the details were confidential.
Earlier on Tuesday, five of Uber’s major investors demanded that the chief executive resign immediately. The investors included one of Uber’s biggest shareholders, the venture capital firm Benchmark, which has one of its partners, Bill Gurley, on Uber’s board. The investors made their demand for Mr. Kalanick to step down in a letter delivered to the chief executive while he was in Chicago, said the people with knowledge of the situation.
Tuesday’s move by the controversial CEO is the culmination of months of controversy that began when Kalanick agreed last December to serve on President Trump’s advisory council. But in February, following the president’s executive order on immigration—and public criticism of how Uber reacted to protests against the order—Kalanick resigned from the group.
Controversy followed: There were allegations of a culture of widespread sexism at Uber; a federal lawsuit by Waymo, Google’s self-driving car company, accused the company of stealing its designs, leading ultimately to Uber’s firing of Anthony Levandowski, the central figure in the allegations; and the Department of Justice opened an investigation into a software Uber used to sidestep authorities.
Amid this Kalanick’s own PR troubled mounted: He was filmed berating an Uber driver; it emerged he directed his engineers to camouflage the Uber app so Apple’s engineers wouldn’t see it, allowing the app to secretly track iPhones even after it was deleted; and at least one high-profile departure from the company said “the beliefs and approach to leadership that have guided my career are inconsistent with what I saw and experienced at Uber.”
Ultimately the very attributes that made Kalanick and Uber a darling of Silicon Valley’s investors brought about his downfall. The company has been valued at about $70 billion, and investors feared that any initial-public offering would be imperiled by Uber’s temperamental CEO. As the Times noted:
Taking a start-up chief executive to task so publicly is relatively unusual in Silicon Valley, where investors often praise entrepreneurs and their aggressiveness, especially if their companies are growing fast. It is only when those start-ups are in a precarious position or are declining that shareholders move to protect their investment.
In the case of Uber — one of the most highly valued private companies in the world — investors could lose billions of dollars if the company were to be marked down in valuation.
The result: Kalanick’s resignation.








Weather always makes good news, but the role of climate change in altering weather, especially extreme weather, has made the subject a lightning rod for unease.
A case in point this week: A heat wave is triggering record temperatures in the Southwest. American Airlines reported having canceled up to 50 flights at Phoenix’s Sky Harbor airport, where the temperature has neared 120 degrees in recent days.
Flight cancellations are a perfect foundation for climate-change panic. Commercial air travel is an aspect of ordinary life that touches everyone: Travelers can’t help but worry that their mobility will be impacted by near- and long-term effects of climate change. Much of the coverage tracking the American Airlines cancellations pegs climate change as a direct or indirect cause of the disruption.
That account isn’t wrong. But it doesn’t tell the full story, either.
When I asked, American Airlines cited a 118 degree “maximum operating temperature” for the flights in question, and confirmed that “the heat has impacted some of our regional flights.” But airplanes don’t exactly have such neat and tidy maximum temperatures. Temperature limits might affect avionic systems—the electronics that run communication, navigation, and so forth—but temperatures interact with airplane performance more than they allow or prohibit flight itself. Density altitude, which can change in part based on temperature, affects aerodynamic performance of specific aircraft, but that performance also interacts with other factors, including weight.
“Aircraft engine performance is a function of many things including air temperature,” Glenn Lightsey, an aerospace engineer and colleague of mine at Georgia Tech said. “Hotter days require longer runways and more gradual ascent paths to lift the same weight.” Flight is complex, and it cannot be boiled down to a single number.
The specific aircraft matters, too. American Airlines canceled flights using Canadair Regional Jet (CRJ) equipment. These are the business jets that cover routes between hubs and smaller markets. Larger passenger jets are rated to tolerate higher temperatures, well above those currently being experienced in the American Southwest—after all, planes also fly from Dubai, Riyadh, and Cairo.
The CRJ’s history might play a role in its airworthiness under extreme heat. CRJs are currently made by Bombardier, a multinational transportation manufacturer. Bombardier bought the CRJ line from Canadair, a Canadian state aerospace company. These jets were originally designed for business use, and only later developed to serve the commercial regional jet market. They were not necessarily intended for use in all conditions and markets, nor to be packed full of passengers like they are today. (Bombardier did not immediately respond to a request for comment.)
That circumstance is a consequence of deregulation and consolidation in the American airline market. When regulation demanded that airlines serve all markets, larger jets serviced smaller airports. But as those requirements lifted, and as more airlines merged, even once-thriving hubs like Cincinnati, St. Louis, and Memphis have become minor markets. Airlines began relying on equipment like the CRJ, because they can transport a smaller number of people at a lower cost. Were the affected flights on Boeing large jets instead, there would be no question about their ability to fly.
Speaking of cost, it’s not clear if American itself has issued the CRJ-based flight cancellations, or if they came from the regional partners that actually operate those flights under American-Airlines livery. The business relationships between major carriers and their regional partners are complex. Some are wholly owned subsidiaries, while others—including Mesa and SkyWest, which serve Phoenix on behalf of American—are contracted carriers.
Regional carriers tend to endure financial pressures from their major-carrier partners, some of which might make the effects of high-temperature operation a financial or operational burden. For example, it’s possible that the planes could fly safely above a certain ground temperature, but that the performance data to facilitate that flight is not already available or easily determined. Airlines have to buy the performance charts used to operate flights, and they might determine that it is not worth purchasing them for unlikely or uncommon scenarios.
American Airlines didn’t comment when I asked who had made the determination to cancel flights, or if available performance data had any impact on the decision. At least one other airline, Delta, also canceled a flight operated by SkyWest on CRJ metal scheduled at the peak of Tuesday’s heat, although it isn’t clear if temperature played a role in that decision, or which airline made the call to cancel it.
Grounding flights due to heat in Phoenix clearly is a matter that interacts with climate change. But it’s not solely explained by climate change. Industrial history, public policy, market economics, and other factors exert pressure on the situation, too.
And that applies to more than flight. Climate change is a wicked problem because it interacts with so many other aspects of the lived and built environment. It does the subject a disservice to pretend that it can be summarized by the reading on a thermometer.


What is the best way to describe Rupert Murdoch having a foam pie thrown at his face? This wasn’t much of a problem for the world’s press, who were content to run articles depicting the incident during the media mogul’s testimony at a 2011 parliamentary committee hearing as everything from high drama to low comedy. It was another matter for the hearing’s official transcriptionist. Typically, a transcriptionist’s job only involves typing out the words as they were actually said. After the pie attack—either by choice or hemmed in by the conventions of house style—the transcriptionist decided to go the simplest route by marking it as an “[interruption].”  
Across professional fields, a whole multitude of conversations—meetings, interviews, and conference calls—need to be transcribed and recorded for future reference. This can be a daily, onerous task, but for those willing to pay, the job can be outsourced to a professional transcription service. The service, in turn, will employ staff to transcribe audio files remotely or, as in my own couple of months in the profession, attend meetings to type out what is said in real time.
Despite the recent emergence of browser-based transcription aids, transcription’s an area of drudgery in the modern Western economy where machines can’t quite squeeze human beings out of the equation. That is until last year, when Microsoft built one that could.
Automatic speech recognition, or ASR, is an area that has gripped the firm’s chief speech scientist, Xuedong Huang, since he entered a doctoral program at Scotland’s Edinburgh University. “I’d just left China,” he says, remembering the difficulty he had in using his undergraduate knowledge of the American English to parse the Scottish brogue of his lecturers. “I wished every lecturer and every professor, when they talked in the classroom, could have subtitles.”
In order to reach that kind of real-time service, Huang and his team would first have to create a program capable of retrospective transcription. Advances in artificial intelligence allowed them to employ a technique called deep learning, wherein a program is trained to recognize patterns from vast amounts of data. Huang and his colleagues used their software to transcribe the NIST 2000 CTS test set, a bundle of recorded conversations that’s served as the benchmark for speech recognition work for more than 20 years. The error rates of professional transcriptionists in reproducing two different portions of the test are 5.9 and 11.3 percent. The system built by the team at Microsoft edged past both.
“It wasn’t a real-time system,” acknowledges Huang. “It was very much like we wanted to see, with all the horsepower we have, what is the limit. But the real-time system is not that far off.”
Indeed, the promise of ASR programs capable of accurately transcribing interviews or meetings as they happen no longer seems so outlandish. At Microsoft’s Build conference last month, the company’s vice-president, Harry Shum, demonstrated a PowerPoint transcription service that would allow the spoken words of the presentation to be tied to individual slides. The firm is also in a close race with the likes of Apple and Google to perfect the transcripts produced by its real-time mobile translation app.
Huang believes the point at which transcription software will overtake human capabilities is open to interpretation. “The definition of a perfect result would be controversial,” he says, citing the error rates among human transcriptionists. “How ‘perfect’ this is depends on the scenario and the application.”
An ASR system tasked with transcribing speech in real time is only deemed successful if every word is interpreted correctly, something that largely has been achieved with mobile assistants like Cortana and Siri, but has yet to be mastered in real-time translation apps.  However, a growing number of computer scientists are realizing that standards do not need to be as high when it comes to the automatic transcription of recorded audio, where any mistakes in the text can be amended after the fact.
Two companies—Trint, a start-up in London, and Baidu, the Chinese internet giant with an application called SwiftScribe—have begun to offer browser-based tools that can convert recordings of up to an hour into text with a word-error rate of 5 percent or less.* On the page, their output looks very similar to the raw documents I typed out in real-time during the many meetings I attended as a freelance transcriptionist: at best, a Joycean stream-of-consciousness marvel, and at worst, gobbledygook. But by turning the user from a scribe into an editor, both programs can shave hours off an onerous and distracting task.
The amount of time saved, of course, is contingent on the quality of the audio. Trint and SwiftScribe tend to make short work of face-to-face interviews with the bare minimum of ambient noise, but struggle to transcribe recordings of crowded rooms, telephone interviews with bad reception, or anyone who speaks with an accent that isn’t American or British English. My attempt to run a recording of a German-accented speaker through Trint, for example, saw the engine interpret “it was rather cold, but the atmosphere was great” as “That heart is also all barf. Yes. His first face.”
“We don’t claim that this turnaround in a couple of minutes of an interview like this is perfect,” says Jeff Kofman, Trint’s CEO. “But, with good audio, it can be close to perfect. You can search it, you can hear it, you [can] find the errors, and you know within seconds what was actually said.”
According to Kofman, most of the people using Trint are journalists, followed by academics doing qualitative research and clients in business and healthcare—in other words, professions expected to transcribe a large volume of audio on tight deadlines. That’s in keeping with the anonymized data on user behavior being collected by the developer Ryan Prenger and his colleagues at SwiftScribe. While there is a long tail of users who Prenger speculates are simply AI enthusiasts eager to test out SwiftScribe’s capabilities, he’s also spotted several “power users” that are running audio through the program on almost a daily basis. It’s left him optimistic about the range of people the tool could attract as ASR technology continues to improve.
“That’s the thing with transcription technology in general,” says Prenger. “Once the accuracy gets above a certain bar, everyone will probably start doing their transcriptions that way, at least for the first several rounds.” He predicts that, ultimately, automated transcription tools will increase both the supply of and the demand for transcripts. “There could be a virtuous circle where more people expect more of their audio that they produce to be transcribed, because it’s now cheaper and easier to get things transcribed quickly. And so, it becomes the standard to transcribe everything.”
It’s a future that Trint is consciously maneuvering itself to exploit. The company just raised $3.1 million in seed money to fund its next round of expansion. Kofman and his team plan to demonstrate its capabilities later this month at the Global Editors Network in Vienna. Their aim is to have the transcription of the event’s keynote address up on the Washington Post’s website within the hour.
It’s difficult to predict precisely what this new order could look like, although casualties are expected. The stenographer would likely join the ranks of the costermonger and the iceman in the list of forgotten professions. Journalists could spend more time reporting and writing, aided by a plethora of assistive writing tools, while detectives could analyze the contradictions in suspect testimony earlier. Captioning on YouTube videos could be standard, while radio shows and podcasts could become accessible to the hard of hearing on a mass scale. Calls to acquaintances, friends, and old flames could be archived and searched in the same way that social-media messages and emails are, or intercepted and hoarded by law-enforcement agencies.
For Huang, transcription is just one of a whole range of changes ASR is set to provide that will fundamentally change society itself, one that can already be glimpsed in voice assistants like Cortana, Siri, and Amazon’s Alexa. “The next wave, clearly, is beyond the devices that you have to touch,” he says, envisioning computing technology discreetly woven into a range of working environments. “UI technology that can free people from being tethered to the device will be in the front and center.”
For the moment, however, the engineers behind automated transcribers will have to content themselves with more germane users: the journalist sweating a deadline, or the transcriptionist working out the right way to describe a man being pied in a parliamentary select committee.
* This article originally stated that SwiftScribe is a subsidiary of Baidu. We regret the error.


Something unexpected happened recently at the Facebook Artificial Intelligence Research lab. Researchers who had been training bots to negotiate with one another realized that the bots, left to their own devices, started communicating in a non-human language.
In order to actually follow what the bots were saying, the researchers had to tweak their model, limiting the machines to a conversation humans could understand. (They want bots to stick to human languages because eventually they want those bots to be able to converse with human Facebook users.) When I wrote about all this last week, lots of people reacted with some degree of trepidatious wonder. Machines making up their own language is really cool, sure, but isn’t it actually terrifying?
And also: What does this language actually look like? Here’s an example of one of the bot negotiations that Facebook observed:
Bob: i can i i everything else . . . . . . . . . . . . . .Alice: balls have zero to me to me to me to me to me to me to me to me toBob: you i everything else . . . . . . . . . . . . . .Alice: balls have a ball to me to me to me to me to me to me to meBob: i i can i i i everything else . . . . . . . . . . . . . .Alice: balls have a ball to me to me to me to me to me to me to meBob: i . . . . . . . . . . . . . . . . . . .Alice: balls have zero to me to me to me to me to me to me to me to me toBob: you i i i i i everything else . . . . . . . . . . . . . .Alice: balls have 0 to me to me to me to me to me to me to me to me toBob: you i i i everything else . . . . . . . . . . . . . .Alice: balls have zero to me to me to me to me to me to me to me to me to
Not only does this appear to be nonsense, but the bots don’t really seem to be getting anywhere in the negotiation. Alice isn’t budging from her original position, anyway. The weird thing is, Facebook’s data shows that conversations like this sometimes still led to successful negotiations between the bots in the end, a spokesperson from the AI lab told me. (In other cases, researchers adjusted their model and the bots would develop bad strategies for negotiating—even if their conversation remained interpretable by human standards.)
One way to think about all this is to consider cryptophasia, the name for the phenomenon when twins make up their own secret language, understandable only to them. Perhaps you recall the 2011 YouTube video of two exuberant toddlers chattering back and forth in what sounds like a lively, if inscrutable, dialogue.

There’s some debate over whether this sort of twin speak is actually language or merely a joyful, babbling imitation of  language. The YouTube babies are socializing, but probably not saying anything with specific meaning, many linguists say.
In the case of Facebook’s bots, however, there seems to be something more language-like occurring, Facebook’s researchers say. Other AI researchers, too, say they’ve observed machines that can develop their own languages, including languages with a coherent structure, and defined vocabulary and syntax—though not always actual meaningful, by human standards.
A Computer Tried (and Failed) to Write This Article
In one preprint paper added earlier this year to the research repository arXiv, a pair of computer scientists from the non-profit AI research firm OpenAI wrote about how bots learned to communicate in an abstract language—and how those bots turned to non-verbal communication, the equivalent of human gesturing or pointing, when language communication was unavailable. (Bots don’t need to have corporeal form to engage in non-verbal communication; they just engage with what’s called a visual sensory modality.) Another recent preprint paper, from researchers at the Georgia Institute of Technology, Carnegie Mellon, and Virginia Tech, describes an experiment in which two bots invent their own communication protocol by discussing and assigning values to colors and shapes—in other words, the researchers write, they witnessed the “automatic emergence of grounded language and communication ... no human supervision!”
The implications of this kind of work are dizzying. Not only are researchers beginning to see how bots could communicate with one another, they may be scratching the surface of how syntax and compositional structure emerged among humans in the first place.
But let’s take a step back for a minute. Is what any of these bots are doing really language? “We have to start by admitting that it’s not up to linguists to decide how the word ‘language’ can be used, though linguists certainly have opinions and arguments about the nature of human languages, and the boundaries of that natural class,” said Mark Liberman, a professor of linguistics at the University of Pennsylvania.
So the question of whether Facebook’s bots really made up their own language depends on what we mean when we say “language.” For example, linguists tend to agree that sign languages and vernacular languages really are “capital-L languages,” as Liberman puts it—and not mere approximations of actual language, whatever that is. They also tend to agree that “body language” and computer languages like Python and JavaScript aren’t really languages, even though we call them that.
So here’s the question Liberman poses instead: Could Facebook’s bot language—Facebotlish, he calls it—signal a new and lasting kind of language?
“Probably not, though there’s not enough information available to tell,” he said. “In the first place, it’s entirely text-based, while human languages are all basically spoken or gestured, with text being an artificial overlay.”
The larger point, he says, is that Facebook’s bots are not anywhere near intelligent in the way we think about human intelligence. (That’s part of the reason the term AI can be so misleading.)
“The ‘expert systems’ style of AI programs of the 1970s are at best a historical curiosity now, like the clockwork automata of the 17th century,” Liberman said. “We can be pretty sure that in a few decades, today’s machine-learning AI will seem equally quaint.”
It’s already easy to set up artificial worlds populated by mysterious algorithmic entities with communications procedures that “evolve through a combination of random drift, social convergence, and optimizing selection,” Liberman said. “Just as it’s easy to build a clockwork figurine that plays the clavier.”


When Amazon announced last week that it intended to acquire the upscale grocery chain Whole Foods, it sent shockwaves through the grocery industry. Other grocers’ share prices plummeted. Analysts predicted Amazon would become a “top five” grocer within a few years. Synergies were imagined.
Within all the business chatter, however, a few policy wonks and at least one ally in Congress began to raise the antitrust alarm. They think Amazon is too powerful and might engage in anti-competitive practices.
When Does Amazon Become a Monopoly?
On its face, and judged on the scale of recent jurisprudence, it’s not the most obvious antitrust situation. Amazon has a tiny slice of the grocery market. Whole Foods, large though it may loom in affluent cities, only has 1.2 percent market share. And while Amazon has a dominant position in e-commerce, e-commerce sales remain less than 10 percent of total retail receipts.
But freshman Congressman Ro Khanna, who represents the South Bay, including a big chunk of Silicon Valley, said that the Amazon-Whole Foods deal shows why the government should think differently about mergers. “This as a case study for how we think about antitrust policy,” he said. “It’s the particulars here.”
Khanna said that recent antitrust cases have turned on the question of whether a merger would, in point of fact, immediately raise prices for consumers. Drawing on the work of Matt Stoller and Lina Khan at the New America Foundation, he traced that very narrow test to Robert Bork’s The Antitrust Paradox, which was a move away from decades of more expansive thinking about industry concentration.
In this interview, Khanna calls for a “reorientation” of antitrust decision making to look at a much broader set of concerns, including the effect that a merger could have on jobs, wages, innovation, and small businesses. Whether he can get traction for this idea might be a bellwether for how well the populist wave in U.S. politics can translate into policy reprioritization.
This interview has been lightly edited and condensed.
Alexis Madrigal: Over the last few days, you’ve said that you’re “deeply worried” about the Amazon-Whole Foods deal. What’s drawn your attention to it?
Ro Khanna: I’m very concerned about it, especially the impact that it’s going to have on local grocers. The Walmarts and Targets already are putting pressure on grocers. And that is something in my district: For example, you have Felipe’s Produce in Sunnyvale and Cupertino. These local groceries have already faced so much pressure, and that’s gonna aggravate that situation. As you know, for many immigrant families, grocers are the route into the middle class and the path to wealth creation.
The second challenge to the merger is wages. Whole Foods has a record of paying people really well. One of their founders had a rule that the CEO shouldn’t be paid more than 20 times the average worker. Amazon has not had the same record. You could have downward pressure on wages. And Amazon is a large conglomerate and can leverage suppliers to lower prices, which creates downward pressure on suppliers’ workers wages, too.
If the only metric is “Is this gonna lower prices?”—if that’s the only criteria, that’s debatable. But we also need to consider the impact on local communities and the impact on innovation.
If you look across the economy, if you have multiple players in an industry, you have more customization, more innovation, greater choice for consumers. The more you have consolidation, the less likely you are to invest in innovation. It becomes all about driving down cost and mass production. And that’s not good for innovation in an industry.
Madrigal: The obvious counterargument that people have been making is that Whole Foods controls a teensy tiny fraction of the overall grocery market—1.2 percent,  according to research firm GlobalData.
Khanna: Well, the question is more, what is the potential for it to become? If you look at the past history in Amazon, they were willing to have losses for years to grow their position with the industry. The concern is there could be predatory pricing where they are able to absorb huge losses, which threatens other grocers.
And this has to be viewed not just in its implications for the grocery vertical, but is this amplifying Amazon’s online dominance into the physical retail space? It shouldn’t just be viewed as limited to groceries, but should be viewed in the broader context and Amazon playing into brick and mortar retail.
What I’ve said is that all of that has to be reviewed by the Department of Justice and the Federal Trade Commission to see what is the impact of such a merger given the market share that Amazon does have in many industries.
Madrigal: Who are you thinking through all these issues with? It seems as if there is a group of people in and around D.C. who are rethinking antitrust policy.
Khanna: I think there is a group. There is [Minnesota Congressman] Rick Nolan, who is interested in starting a monopoly caucus in Congress. He’s very concerned about the concentration in industries and the concentration of economic power and what that means for jobs
I’ve talked with [Massachusetts] Senator Elizabeth Warren in the context of defense contractors’ monopolization of the defense industry and what that means for prices.
Then [New America’s] Matt Stoller’s and Lina Khan’s work. Their work has gotten the attention of some of us in Congress that we need to reorient antitrust policy from the Robert Bork days, who made the whole thing a litmus test just about consumer prices, so that if something helps consumer prices, it can’t be an antitrust violation.
The problem is that there wasn’t a consideration of long-term price. Even if short-term consumers benefitted, long-term, there have been cases—airlines, ISP providers—where prices hurt consumers. And it didn’t consider the impact on wages and on local jobs and small businesses, who create most of the jobs. It didn’t take into account the impact on communities. I know what Felipe’s means to the family who created it and the community that it’s in.
Madrigal: Are there specific cases that show the way you think antitrust jurisprudence should be handled?
Khanna: There’s a 1966 Supreme Court case called United States v. Von's Grocery Co. The court blocked a merger between two grocery stores in Los Angeles to prevent a trend towards concentration. And the court said that the dominant theme in Congress was what was considered to be a rising tide of concentration in the American economy. It’s a Supreme Court case. Still good law. So, the courts have looked at economic concentration, particularly in grocery, and that’s a strain of jurisprudence that should be amplified. [From the decision: “The courts must be alert to protect competition against increasing concentration through mergers especially where concentration is gaining momentum in the market.”]
Madrigal: The argument that you’re making seems as if it could be extended to many other technology businesses. The online ad market, for example, is dominated by two companies, Google and Facebook. Are you pushing for tougher antitrust measures across the board?
Khanna: I think we need to have stronger antitrust enforcement. The biggest challenge in the internet space is the ISPs—AT&T, Comcast, Charter—and the fact that we’re paying fives times for access to the internet compared to Europe. There’s only five companies and not much choice because of the extraordinary infrastructure cost. And there is the airline industry. So, in general, we need stronger antitrust enforcement.
What makes the Amazon-Whole Foods deal so problematic is that they are going into an industry with large infrastructure, brick-and-mortar cost, and seeking to build consolidation where we already suffer from consolidation. It’s not like Walmarts and Targets have been good for wages or local grocery stores or niche producers. You already have a problem of concentration and this will just aggravate that.
Madrigal: But you’d like to see the antitrust decision-making overhauled.
Khanna: The big question that some of us in Congress are interested in is how do we reorient antitrust policy to consider all the factors of economic concentration. And consumer price and price discrimination is one factor. But there are also the loss of jobs, the impact on wages, the impact on local small businesses, and the impact on innovation within an industry.
And my point is that especially in a time with declining unionization, if you look at industries where they have numerous competitors and not a few big actors with high market concentration, there’s greater leverage for employees and wages, greater investment in innovation, greater leverage for suppliers, so less downward pressure on wages in supply chains. This is not universally true and there may be exceptions to that, but the FTC and DOJ need to consider all of these factors and make a holistic determination: Is a merger on balance helping wages, jobs, investment for innovation, and prices? Or is it, on balance, not?
And the problem of the current antitrust legislation is that it’s just a litmus test on prices and doesn’t consider all these other equally important factors. And that’s the really the philosophical debate between Brandeis and the consensus all the way from Theodore Roosevelt versus the shift to free-market absolutism that Robert Bork enabled.
Madrigal: Do have any hope that this kind of antitrust transformation will happen during this administration?
Khanna: I hope so. I hope the president is consistent with his campaign promises. He said he’d look at antitrust issues very seriously. Working families, or as he puts it, forgotten Americans, are being shafted by large banks and large corporations. And he campaigned as a populist on antitrust. No one is saying he should arbitrarily make a decision on antitrust, but he should put resources behind the DOJ and FTC to review these things. I have great confidence in the career civil servants at the DOJ and the FTC.
It’s my hope and I’m optimistic that there will be a review.


Updated on June 17, 2017 at 7:51 p.m. ET
The catastrophe wasn’t what it seemed. It was an inside job, people whispered. Rome didn’t have to burn to the ground.
Nearly 2,000 years ago, after the Great Fire of Rome leveled most of the city, Romans questioned whether the emperor Nero had ordered his guards to start the inferno so he could rebuild Rome the way he wanted. They said the emperor had watched the blaze from the the summit of Palatine Hill, the centermost of the seven hills of Rome, plucking his lyre in celebration as countless people died. There’s no evidence of this maniacal lyre-playing, but historians today still debate whether Nero orchestrated the disaster.
What we do know is this: Conspiracy theories flourish when people feel vulnerable. They thrive on paranoia. It has always been this way.
So it’s understandable that, at this chaotic moment in global politics, conspiracy theories seem to have seeped out from the edges of society and flooded into mainstream political discourse. They’re everywhere.
That’s partly because of the richness of today’s informational environment. In Nero’s day, conspiracy theories were local. Today, they’re global. The web has made it easier than ever for people to watch events unfold in real time. Any person with a web connection can participate in news coverage, follow contradicting reports, sift through blurry photos, and pick out (or publish) bad information. The democratization of internet publishing and the ceaseless news cycle work together to provide a never-ending deluge of raw material that feeds conspiracy theories of all stripes.
From all over the world, likeminded people congregate around the same comforting lies, explanations that validate their ideas. “Things seem a whole lot simpler in the world according to conspiracy theories,” writes Rob Brotherton, in his book, Suspicious Minds: Why We Believe Conspiracy Theories. “The prototypical conspiracy theory is an unanswered question; it assumes nothing is as it seems; it portrays the conspirators as preternaturally competent; and as unusually evil.”
But there’s a difference between people talking about outlandish theories and actually believing them to be true. “Those are two very different things,” says Joseph Uscinski, a political science professor at the University of Miami and the co-author of the book American Conspiracy Theories. “There’s a lot of elite discussion of conspiracy theories, but that doesn’t mean that anyone’s believing them any more than they did in the past. People understand what conspiracy theories are. They can understand these theories as political signals when they don’t in fact believe them.”
And most people don’t, Uscinski says. His data shows that belief in partisan conspiracy theories maxes out at 25 percent—and rarely reach that point. Imagine a quadrant, he says, with Republicans on the right and Democrats on the left. The top half of the quadrant is the people of either party who are more likely to believe in conspiracy theories. The bottom half is the people least likely to believe them. Any partisan conspiracy theory will only resonate with people in one of the two top-half squares—because to be believable, it must affirm the political worldview of a person who is already predisposed to believe in conspiracy theories.
“You aren’t going to believe in theories that denigrate your own side, and you have to have a previous position of buying into conspiracy logic,” Uscinski says.
Since conspiracy theories are often concerned with the most visible concentration of power, the president of the United States is a frequent target. “So when a Republican is president, the accusations are about Republicans, the wealthy, and big business; and when a Democrat is president, the accusations focus on Democrats, communists, and socialists.”
“Right now,” he added, “Things are little different. Because of Donald Trump.”
As it turns out, the most famous conspiracy theorist in the world is the president of the United States. Donald Trump spent years spreading birtherism, a movement founded on the idea that his predecessor was born outside the country and therefore ineligible for the nation’s highest office. (Even when Trump finally admitted in September that he knew Barack Obama was born in the United States, he attempted to spark a new conspiracy.)
Now, Trump’s presidency is the focus of a range of conspiracies and cover-ups—from the very real investigation he’s under to the crackpot ideas about him constantly being floated by some of his detractors on the left. Like the implication that Paul Ryan and Mitch McConnell are involved in a money laundering scheme with the Russians, plus countless more theories about who’s funneling Russian money where and to whom.
“The left has lost its fucking mind, and you can quote me on that,” Uscinski said. “They spent the last eight years chastising Republicans about being a bunch of conspiracy kooks, and they have become exactly what they swore they were not. The hypocrisy is thick and it’s disgusting.”
Trump’s strategy in the face of all this drama has been to treat real and fake information interchangeably and discredit any report that’s unflattering to him. It’s why he refers to reputable news organizations as “fake news,” and why he brags about “going around” journalists by tweeting directly to the people. He wants to shorten the distance between the loony theories on the left and legitimate allegations of wrongdoing against him, making them indistinguishable.
Pushing conspiracy theories helped win Trump the presidency, and he’s now banking on the idea that they’ll help him as president. He’s casting himself as the victim of a new conspiracy—a “witch hunt” perpetrated by the forces that want to see him fail.
“Donald Trump communicates through conspiracy theories,” Uscinski says. “You can win the presidency on conspiracy theories, but it’s very difficult to govern on them. Because conspiracy theories are for losers, and now he’s a winner.”
What he means is, conspiracy theories are often a way of expressing an imbalance of power by those who perceive themselves to be the underdog. “But if you control the Supreme Court, the Senate, the House, and the White House, you can’t pull that,” Uscinski says. “Just like how Hillary Clinton can’t, in 1998, say her husband’s troubles are due to a vast right-wing conspiracy.”
Donald Trump may be the most famous conspiracy theorist in America, but a close second is the Infowars talk-radio personality Alex Jones, who has made a name for himself spewing reprehensible theories. He claimed the Sandy Hook Elementary School massacre was a hoax. He says 9/11 and the Boston Marathon bombings were carried out by the U.S. government. Jones has an online store where he peddles products like iodine to people prepping for the apocalypse.
Jones has long been a controversial figure, but not enormously well known. That’s changing. Jones was a vocal supporter of Trump, who has in turn praised Jones. “Your reputation is amazing,” Trump told him in an Infowars appearance in 2015. “I will not let you down.” Jones has claimed he is opening a Washington Bureau and considering applying for White House press credentials.
The latest Jones drama is a three-parter (so far): First, the NBC News anchor Megyn Kelly announced she had interviewed Jones, and that NBC would air the segment on Sunday, June 18. Next came the backlash: People disgusted by Jones blasted Kelly and NBC, saying a man whose lies had tortured the families of murdered children should never be given such a prominent platform. Even Jones joined the fracas, saying he’d been treated unfairly in the interview. Finally, on Thursday night, Jones claimed he had secretly recorded the interview, and would release it in full. (So far, he has released what seems to be audio from a phone conversation with Kelly that took place before the interview.)
Kelly has defended her decision to do the interview in the first place by describing Jones’s popularity: “How does Jones, who traffics in these outrageous conspiracy theories, have the respect of the president of the United States and an audience of millions?” The public interest in questioning a person like Jones, she argues, eclipses any worries about normalizing his outlandish views. The questions are arguably more valuable than the answers.
Many journalists agree with Kelly’s reasoning. But it’s also true, scholars say, that giving a platform to conspiracy theorists has measurable harmful effects on society.  In 1995, a group of Stanford University psychologists interviewed people either right before or right after they’d viewed Oliver Stone’s 1991 film JFK, which was full of conspiracy theories. Brotherton, who describes the findings in Suspicious Minds, says people leaving the movie described themselves as less likely to vote in an upcoming election and less likely to volunteer or donate to a political campaign, compared with those walking in. “Merely watching the movie eroded, at least temporarily, a little of the viewer’s sense of civic engagement,” Brotherton writes.
There are other examples of real-world consequences of giving platforms to conspiracy theorists, too. The conspiracy theory known as Pizzagate, which rose to prominence across websites like 4chan and niche conservative blogs, resulted in a man firing a weapon in a Washington, D.C., pizza parlor.  
The debate over Kelly’s interview comes on the heels of another high-profile conspiracy theory that sent shockwaves through conservative media circles. At the center of that scandal was the TV host Sean Hannity pushing a conspiracy theory about the unsolved murder of a Democratic National Committee staff member and an explosive Fox News report about the murder that was eventually retracted.
* * *
There’s a popular science-fiction podcast, Welcome to Night Vale, developed around the idea of life in a desert town where all conspiracy theories are true. It was first released in June 2012, the summer before a U.S. presidential election, at a moment when Trump was test-driving a new anti-Obama conspiracy. “I wonder when we will be able to see @BarackObama’s college and law school applications and transcripts,” he tweeted the day Night Vale launched. “Why the long wait?”
Joseph Fink, who co-created the podcast, says conspiracy theories today are continuing to function the way they always have. Conspiracy theories are easy ways to tell difficult stories. They provide a storyline that makes a harsh or random world seem ordered. “Especially if it’s ordered against you,” he says. “Since, then, none of it is your fault, which is even more comforting.”
“That said, more extreme conspiracy theories are becoming more mainstream, which is obviously dangerous,” Fink adds. “Conspiracy theories act in a similar way as religious stories: they give you an explanation and structure for why things are the way they are. We are in a Great Awakening of conspiracy theories, and like any massive religious movement, the same power it has to move people also is easily turned into a power to move people against other people.”
Look for the last awakening of this sort in the United States, and you’ll find a sea of similarities—of course, as conspiracy theories tell us, it’s easy to find connections when you go looking for them. Several scholars—people who focus on real conspiracies and people who study conspiracy theories—say the paranoia surrounding the Trump presidency evokes the tumult surrounding the Vietnam War. It’s not that conspiracy theories weren’t, at times, rampant before that. In the 1940s and 1950s, McCarthyism and the trial of Alger Hiss brought with them a surreal spate of hoaxes and misinformation. But it was the assassination of President John F. Kennedy that set off a “general sense of suspicion” that would permeate the culture for some time, says Josiah Thompson, the author of Six Seconds in Dallas: A Micro-Study of the Kennedy Assassination.
“Part of that was, what occurred almost immediately after the assassination, in the years afterward, was Vietnam,” Thompson said, “And over time, a complete loss of confidence in what ever the government was saying about Vietnam. That was not just from the presidency, that was from the government itself.”
This was also a period in which some of the most dramatic ideas that had been disparaged as conspiracy theories turned out to be true. “I am not a crook,” Nixon had insisted. Less than a year later, he resigned. Nixon and Trump are compared not infrequently. Not all presidents are so thin-skinned and antagonistic to the press. Jennifer Senior, reviewing a recent Nixon biography, wrote that “the similarities between Nixon and Trump leap off the page like crickets.” Nixon may have been increasingly paranoid in the final months of his presidency, but he didn't have access to the technology that Trump uses to showcase his conspiracy mindedness.
“With real conspiracy theorists, there’s a kind of—how to put it—almost a dialectic operative,” Thompson says. “Like Trump. You have to keep making wilder and wilder pronouncements over time to hold your audience.”
I tell Thompson the idea Uscinski had shared, about how a person can win the presidency on conspiracy theories, but how they don’t work so well once you’re president. He seems to agree. “In a campaign, what you’re trying to do is affect people’s opinions that will be harvested on one day,” he said. “But governing doesn’t have to do with people’s opinions. It has to do with facts. That’s the real difference.”
When the facts are disputed, of course, you do the best you can with the evidence you can find. Josiah Thompson, the author of Six Seconds in Dallas: A Micro-Study of the Kennedy Assassination, has spent years thinking about all this. When I bring up the enormity of unknown unknowns in people’s understanding of history, Thompson quotes the writer Geoffrey O’Brien:  “‘History unfolds as always in the midst of distraction, misunderstanding, and partially obscured sight-lines,’” Thompson says, reading a line from O’Brien’s 2016 review of the novel Black Deutschland by Darryl Pinckney.*
“And that’s the trouble,” Thompson says. “What may appear as conspiracy theory at one point turns out to be truth at another.”
I ask Thompson how sure he is about the official explanation of the JFK assassination, that there was one gunman who fired on the president’s motorcade from the Texas School Book Depository.
Thompson believes, based on controversial acoustic evidence, that on November 22, 1963, a shot was fired from the grassy knoll at Dealey Plaza—not just from the depository. “The acoustics give us a kind of template for how the event occurred—these two flurries of shots, separated by about six seconds.” (Thompson later clarified that he believes the flurries of shots were 4.6 seconds apart.) He says it was two shots in the second flurry that killed Kennedy.**
Thompson pauses.
“Does that make me a conspiracy theorist?”
He laughs.
“After all these years? What do you think?”
* This article originally quoted Josiah Thompson as having said, “history unfolds, as always, in the midst of distraction, misunderstanding, and partially obscured sight-lines.” After publication, Thompson clarified that he had been quoting the New York Review of Books writer Geoffrey O’Brien, who first wrote the line in his review of the Darryl Pinckney novel Black Deutschland.
** Thompson clarified after publication that he believes the flurries of shots in the Kennedy assassination were 4.6 seconds apart, not six seconds apart. He believes Kennedy was killed by two shots in the second flurry, not by the two flurries of shots.


On Friday morning, Amazon announced it was buying Whole Foods Market for more than $13 billion. About an hour later, Amazon’s stock had risen by about 3 percent, adding $14 billion to its value.
Amazon basically bought the country’s sixth-largest grocery store for free.
As the financial reporter Ben Walsh pointed out on Twitter, this is the opposite of what’s supposed to happen—normally, the acquiring company’s share price falls after a major purchase—and it suggests that investors now believe something odd is going on with Amazon. What could it be?
From a straightforward standpoint, the Whole Foods acquisition means that Amazon will now participate in the $700 billion grocery-store business. Jeff Bezos, the company’s president and CEO, has made grabs at that market for several years—launching Amazon Fresh, a food home-delivery service, and opening several Amazon-branded bodegas in Seattle. Now he owns one of the industry’s best-known brand names.
But Amazon paid a premium to buy Whole Foods, so its new full entry into another industry doesn’t quite explain the rise. Instead, the boost in share price suggests something more ominous: An incredible amount of economic power is now concentrated in Amazon, Inc., and investors now believe it is stifling competition in the retail sector and the broader American economy.
As the country’s biggest online retailer of cleaning supplies and home goods, Amazon competes with Walmart, Target, and Bed, Bath & Beyond. As a clothing and shoe retailer, it competes with DSW, Foot Locker, and Gap. As a distributor of music, books, and television, it competes with Apple, Netflix, and HBO. In the past decade, Amazon has also purchased the web’s biggest independent online shoe store, its biggest independent online diaper store, and its biggest independent online comics store.
And it is successful on nearly all of those fronts. Last year, Amazon sold six times as much online as Walmart, Target, Best Buy, Nordstrom, Home Depot, Macy’s, Kohl’s, and Costco did combined. Amazon also generated 30 percent of all U.S. retail sales growth, online or offline.
Yet Amazon’s dominance extends far beyond retail. It also lends credit, publishes books, designs clothing, and manufactures hardware. Three years ago, it bought Twitch.com, a central player in the $1-billion business of e-sports. And on top of all this, it operates Amazon Web Services, a $12-billion business that rents servers, bandwidth, and computing power to other companies. Slack, Netflix, Dropbox, Tumblr, Pinterest, and the federal government all use Amazon Web Services.
It is, in short, an Everything Store: not only selling goods but also producing them, not only distributing media from its servers but also renting them out to others. And it’s left many experts and analysts wondering: When does Amazon become a a monopoly?
“I think of Amazon as serving almost as the essential infrastructure for the American economy at this point, when it comes to commerce. And that affords Amazon a lot of power and control,” says Lina Khan, a fellow on the Open Markets team at New America, a center-left think tank.
In January, Khan called for Amazon to receive more antitrust scrutiny in an article in The Yale Law Journal.
Historically, many of Amazon’s critics have focused on their Marketplace feature, which allows small businesses to sell their goods through Amazon’s website. Some merchants have accused Amazon of secretly using Marketplace as a laboratory: After collecting data on which products do best, it introduces low-price competitors available through its flagship service.
The Institute for Local Self-Reliance, a nonpartisan advocacy group, has also criticized Amazon for this alleged anticompetitive behavior. “By controlling this critical infrastructure, Amazon both competes with other companies and sets the terms by which these same rivals can reach the market. Locally owned retailers and independent manufacturers have been among the hardest hit,” said a recent report from the group.
But as Amazon has grown across the economy, concern has grown about its strength and power more broadly. “Amazon introduced itself to consumers as a middle man for books,” Khan told me. “But it expanded into becoming a middle man for all sorts of other things—and, for some time now, it has expanded well beyond that middle-man role. As it distributes more content and produces more goods, it’s running into more and more conflicts of interest.”
In short, people have begun to wonder if Amazon is just too big: a company that already controls too much of online retail and that has started to exert its dominance downward into the rest of the supply chain.
Amazon has historically declined to comment about antitrust issues. It recently began searching for a professional economist to consult on competition law concerns. Before November, one of the loudest critics of its market dominance was Donald Trump, who implied on the campaign trail that Jeff Bezos faces “antitrust problems.”
Trump has not yet appointed someone to chair the Federal Trade Commission. The commission must review the acquisition before its completion.
When the United States began to enforce for fairer competition between businesses in the early 20th century, it focused on two kinds of monopolistic organizations: horizontal monopolies and vertical monopolies. In the steel business, for instance, a horizontal monopoly buys up a lot of steel mills, such that other competitors would be boxed out. A vertical monopoly buys up and down the supply chain—acquiring barges and trains and coal mines—essentially barring other companies from competing with it.
Through the middle of the century, regulators focused on business arrangements that could use their control of markets to inflate prices for consumers—cracking down on cartels and more informal price-fixing or market-controlling arrangements—and also on trusts and firms that exercised monopolistic control over their industries.
Starting in the late 1970s, though, legal scholars began arguing that monopolistic behavior could only be measured if it raised prices for consumers. Regulators and judges took notice and opted to pay less attention to overall market structure. And inspired by the corporate raiding and hostile takeovers of the early 1980s, many experts came to believe that bigness in the market would always fix itself.
That consensus has come under attack in the past decade, partly thanks to companies like Amazon. During its first 10 years in operation, Amazon rarely returned profits, and investors allowed Bezos to continue invest in infrastructure and market share. The end result is today’s Amazon: a behemoth company that returns a meager profit, with a stock worth nearly 200 times as much as it earns.
Khan and others have called for the focus to be less on Amazon’s prices and more on its economic power. “Nobody would quibble that Amazon in its current form today is great for consumers. The question is what do things look like going forward?” she asked.
“Americans love to think about their economy as open and competitive,” she said. “But when a growing share of the economy is contained by Amazon, it’s a form of centralization. Owning your own business used to be a way for Americans to build assets and pass on wealth inter-generationally. But if you look at any sector where Amazon is a dominant player—you’d be somewhat crazy to enter there.”
This effect has been true even of large startups. Jet.com opened early last year as a Sam’s Club-style competitor to Amazon, attracting millions in venture capital and plenty of press coverage. And while it grew quickly, it did not last long as an independent company. Walmart bought Jet.com for $3 billion last August.
In the near term, the Whole Foods purchase worries some analysts most because it immediately gives Amazon another infrastructural advantage: more than 400 small warehouses, spread out across some of the most affluent (and Amazon-using) neighborhoods in the United States. They fret that Amazon’s logistical advantages—its network of warehouses, delivery routes, and cargo jets across North America—have given it an unbeatable advantage over other firms. And they argue that advantage was spawned not by technological innovation but by an unending stream of money from Wall Street.
These critics are calling for Amazon to receive a kind of scrutiny now rare in the United States. First, they say, the Securities and Exchange Commission should think hard before approving its purchase of Whole Foods. Second, politicians and regulators should look harder at its structure. They should ask themselves whether its integration is worth its cost—and then either restrict its integrate, essentially breaking Amazon up; or regulating and neutralizing its consolidation.
I asked Khan if she was really thinking about breaking up Amazon. “People have been timid and think that’s an extreme response,” she said. “But I think it’s worth noting that Amazon is expanding in an unprecedented way across our economy.”
She called back to its spiky share price this morning. “Investors know it’s monopolistic,” she told me. “That’s why it’s stock price has been so untethered from profits. The market can register a reality that our laws cannot.”


In 1991, most Americans had not yet heard of the internet. But all of France was online, buying, selling, gaming, and chatting, thanks to a ubiquitous little box that connected to the telephone. It was called Minitel.
Minitel was a computer terminal. It housed a screen, a keyboard, and a modem—but not a microprocessor. Instead of computing on its own, Minitel connected to remote services via uplink, like a 1960s mainframe or a modern Google Chromebook. Terminals were given out, for free, to every French telephone subscriber by the state (which also ran the phone company).
Minitel was a huge success. With free terminals at home or work, people in France could connect to more than 25,000 online services long before the world wide web had even been invented. Many services of the dotcom-and-app eras had precursors in 1980s France. With a Minitel, one could read the news, engage in multi-player interactive gaming, grocery shop for same-day delivery, submit natural language requests like “reserve theater tickets in Paris,” purchase said tickets using a credit card, remotely control thermostats and other home appliances, manage a bank account, chat, and date.
Minitel was decommissioned in 2012 after 30 years of distinguished service. The terminals still functioned, but they could not handle advances in graphics technology, their modems were outdated, and the French had long since moved on to the internet.
But Minitel’s lessons live on, and with new relevance. In the U.S., the Federal Communications Commission’s Open Internet Order made network neutrality law in 2015. But this year, it has come under attack by both cable internet operators and the current FCC chairman. The American implementation of a network derived from Minitel was done by private industry alone. It failed in part because its usage was not regulated by the government. For this reason, it offers a view from the past on why the FCC’s move today might be misguided. It turns out that regulated networks might offer better market opportunities.
* * *
In Silicon Valley, Minitel is often derided as a “backwards system,” the epitome of state centralization and bureaucracy. As the enemy of creative agility. But Minitel was the only online service to reach mass-market penetration to reach mass-market penetration before the late 1990s. Similar systems in the U.S., such as The Source, DowJones, Compuserve, and AOL, were only accessible to the wealthy, geeky few. These American systems were also centralized networks, gated communities where all content was curated by the service provider. They were the computer versions of a cable-television bundle.
By contrast, Minitel didn’t operate as a closed network. Unlike AOL or Facebook, the French state made Minitel an open and neutral platform, which allowed users to connect to privately run services. The state telecom built and operated the underlying infrastructure for the network, and it then allowed anyone to provide services atop it, so long as they registered to do so. Minitel merged state intervention (build and maintain the marketplace) with market-neutrality (anyone can sell legal products and services). That combination catalyzed the boom of Minitel services.
In 1991, France Telecom tried to reproduce their domestic Minitel success in the U.S. through a San Francisco-based venture called 101 Online. It seemed like a match made in heaven. What was then the world’s most successful public computer network was set to meet the world’s hippest tech crowd. For an extra cool factor, France Telecom hired John Coate, the guy who had turned San Francisco’s online bulletin board system, The WELL, into the world’s most influential online community at the time.
As community manager, Coate distributed the little Minitel box to technology and culture leaders such as Alan Lundell of Byte magazine and Mondo 2000. He also took terminals to rave parties such as Oakland’s 1992 Woopy Ball, where hip crowds chatted in digital chill rooms, all in an effort to build a new digital community. The ravers loved it.
But curiosity alone wasn’t enough to spur American adoption of Minitel. It needed a community with intrinsic value. And communities arise when people can meet and exchange goods, services, and ideas freely.
Consider a farmers market. If a city builds and runs one, it must let all types of legal goods to be sold there for the infrastructure to provide maximum value. If citizens can only buy tomatoes and oranges, but not kale nor lettuce, then the value of the market is limited. The same is true of computer networks: If an internet service provider does not let content providers freely access the infrastructure that the user has rented (through a cable or cellphone subscription), the value of the internet as a whole becomes depleted. That’s why the American Minitel failed—and why people should be concerned about ISPs being able to restrict the traffic on broadband and wireless networks.
* * *
On paper, 101 Online understood the distinction between an open, online platform and a cable TV bundle. In a press release, it outlined its mission: to provide Bay Area residents “with a powerful and efficient new way to communicate with each other.” The 101 Online forum, their rebranding of the Minitel network, was said to be “an electronic ‘meeting place’… the first widely available and cheap electronic medium that allows society to talk directly with itself without TV, radio and newspapers acting as a go-between.”
But in practice, 101 Online acted as a go-between for online content. Instead of letting content providers manage their own services, as France Telecom had done, it replicated the dominant model for U.S. online networks of the era: curating all the content itself. Individuals and companies couldn’t plug into the network and sell their content, goods, and services like their French counterparts had done, and as dotcom startups would soon do on the web. Instead, they had to travel to 101 Online’s office in downtown San Francisco, hand a floppy to an operator, and wait for its content to be converted to 101 Online’s proprietary format and uploaded to the company’s server.
As 101’s head of marketing would later admit, “we did not create an ecosystem enabling anyone but us to make money.” It wasn’t anything new for online systems available to Americans at the time. In a 1983, for example, the online version of the World Book Encyclopedia was removed from the CompuServe online platform and replaced with the Grolier electronic encyclopedia—probably the result of some behind-the-scenes licensing deal. The same year, The Source announced a new policy for curating the content on its platform: “new products are receiving close scrutiny based on likely long-term usage rates, as opposed to ‘attention getter qualities.’” It shouldn't come as a surprise that The Source chose to act as a curator, since it was the online arm of Reader’s Digest, itself a master curator. No more than it should startle anyone that AT&T, Comcast, or Verizon—all network providers who also own content companies—might want to do the same with the internet.
What might surprise a proponent of private enterprise over state-run services, however, is that it was private-sector operators who curtailed these early online platforms—whereas in Minitel’s case, the state had remained agnostic. 101 Online used exactly the same technology that the French had implemented across the Atlantic. But when the private sector was fully in charge of administering the platform, it chose to limit rather than facilitate the marketplace.
* * *
When Comcast slows down Netflix, when AT&T forces Apple to block VOIP services, or when Verizon blocks its customers from using tethering apps on their phones, they do nothing different from what CompuServe, The Source, and 101 Online did in the 1980s and 1990s. Acting as private curators, the companies that own the infrastructure through which users seek content can control that content.
By contrast, when an operator treats its infrastructure as neutral, as Minitel had done, its marketplace invites a greater diversity of content and services. That diversity creates more value for users and businesses alike. In Minitel’s case, openness and neutrality were guaranteed by the state, an agent bound by a duty to act in the public interest.
Today, cable internet lobbies have claimed that further regulation of internet services inevitably leads to an internet doomsday that will “increase consumer costs, slow investment and innovation and cause years of uncertainty.” Senator Ted Cruz has even called net neutrality “Obamacare for the internet,” urging against online services that “operate at the speed of government.”
But Minitel offers an unusual historical endorsement for state intervention in commercial computer networks. Government involvement can benefit both public and private enterprise, whereas unbridled reliance on the private sector can restrict innovation, as it did for 101 Online.


I bring tidings from the frontier of airplane Wi-Fi. I experienced faster internet with my own two thumbs aboard a 757! I did a series of speed tests and received between 17 and 27 megabits per second while also flying through the sky. This is streaming-Netflix-while-streaming-Spotify territory—and a far cry from the measly speeds one can attain on most current flights.
Airplane Wi-Fi is technically measured on a scale from makes-you-want-to-throw-your-computer-out-of-the-plane to makes-you-want-to-throw-yourself-out-of-the-plane. That’s because it is hard to direct data to a plane 30,000 feet in the air. You need an antenna pointed down at ground towers or up at a small number of bandwidth-limited satellites. And yet some internet is better than none. And so despite the speed, Wi-Fi aboard planes remains indispensable for work travelers, who might even pay $40 for the privilege of sipping bits through a tiny kinked straw for four or five hours.
As you might expect, the airlines and the people who provide tech components to them are trying to make that experience better. Our sighs have made their way to research team’s ears. Our muttering has led to and starred in PowerPoints.
Here’s the good news: Yes, it will get better. Even though it’s hard, no fewer than three companies are willing to challenge physics to bring you your streaming video.
And now for the bad news: It’s gonna take a while before you get to use that better Wi-Fi.
* * *
My adventure to the future of airplane Wi-Fi began at a part of the San Francisco airport that I’d never seen: Signature Flight Support, the name bold and rich, the planes private. I’d been summoned to the place by the noted thermostat-maker Honeywell, which has been a major supplier of airplane components for decades. They had invited a group of journalists for a ride aboard their hardware-testing plane, and by the time I arrived, the presenters and audience were gathered into a small conference room at the terminal.
Before we flew, Kristin Slyker, Honeywell Aerospace’s VP of “Connected Aircraft” wanted to fill us in on the current aviation industry. The pitch was familiar but compelling: They don’t make the thing you know (the airplane), but they make the thing you know better.
Imagine any Boeing or Airbus: Each one is a conglomeration of parts created by all kinds of different suppliers. Honeywell, for example, does a good business with auxiliary power units, a small engine that lets the pilots turn on the air conditioning, even when the main jet engines aren’t running. Not something you probably think about everyday, but Honeywell has 36,000 of these units flying around on airplanes right now.
They also supply wheels and electrical power infrastructure, cabin pressure control systems and LED lighting, radios, sensors, gyroscopes, accelerometers, and a bunch of other things.
And this is where the Wi-Fi comes in. What is Wi-Fi, from the plane’s perspective?
It’s a system with a few pieces:
And those kinds of things—antennas and radios and aircraft systems—are what Honeywell makes. If airplane Wi-Fi is gonna get better, it’s gonna be through improvements in these parts and the systems that animate them.
So, I boarded the Honeywell plane with the other journalists. On the outside, it looks like any other plane—tube-y, white. But inside, the wall and ceiling panels have been torn out, so all the plane’s cabling is visible. There are bundles of cords spanning the length of the craft, and most of them would be present in any commercial plane, not just this specially kitted out one.
It’s a striking visual: A plane is a flying data-generating machine. Honeywell wants to tie all of these data streams together. They want to sell airlines components, the service for assessing the data that the sensors on those parts generate, apps for pilots, and a host of other services they have rolled under Slyker in a business they call the “Connected Aircraft.” And the charismatic avatar of their capabilities and the coming change is their next-gen internet technology, the Wi-Fi, which they have branded Jetwave.
Honeywell believes that Jetwave is faster and more reliable than the current products from competitors like GoGo, the major incumbent in airplane Wi-Fi, and ViaSat, a general satellite internet company that's moved strongly into the airplane space.
Those companies are also working on next-generation products, like Gogo’s 2ku and ViaSat’s Gen-2 set of equipment. Both of those companies have a stronger current presence in the commercial airplane Wi-Fi market, especially in North America, so it could be a while before you use this particular technology on your flight from Birmingham to Dallas.
Nonetheless, all of the next-generation systems make use of satellite technology, so what Honeywell showed me is a good indicator of how you’ll eventually stream Netflix from a window seat.
* * *
On board, I couldn’t help but follow the cords towards the back of the airplane until I ran into Stéphane Klander, who is an engineer working on Jetwave. He wore a red Honeywell polo shirt and straight leg khakis. He’s blunt, funny, less media-trained than his executive colleagues.
Slyker introduced him as their “communications specialist.” So, eyes twinkling, Klander announced with a light and unplaceable accent, “I know everything about communication. What do you want to know?”
This was the man who could tell me what I wanted to know about how the internet works aboard airplanes, and why it could get better. He invited me back to his workstation on the plane and I strapped into the three-point harness that would keep me in the leather seat next to Klander.
He pulled out his reading glasses and began to run through his standard checks. It was automatic; these guys spend 400 hours a year flying around in this thing.
Honeywell’s new system works with Inmarsat’s satellite system. Built to provide maritime communications, Inmarsat is one of the big, established satcom companies. Honeywell works with three newish Inmarsat-5 satellites in geostationary orbit. That means—with small corrections—the satellites orbit in time with the Earth’s rotation, which to an observer on the ground (or in a plane) means that they appear fixed in the same place in the sky.
The satellites can only cover a slice of the Earth, so when a plane flies out of the range of one, it has to hand-off to the next satellite.
On our flight out of San Francisco, Klander is excited to gather data on how their system performs on the Pacific Ocean Region (POR) satellite, which they’ve tested less than the Atlantic and Indian regions.
Inmarsat calls this particular satellite array Global Xpress. Each satellite uses dozens of beams that provide up to 50 megabytes per second of bandwidth to each commercial plane flying down on Earth.
A key feature of the satellite system is that it operates in the Ka part of the spectrum—that’s roughly 30 gigahertz. That’s extremely high frequency, technically! Consider that cell phones use frequencies in the low thousands of megahertz. FM radio uses spectrum around 85 to 110 megahertz. And AM radio is measured in kilohertz.
Even for satellite communication norms, the Ka band is substantially higher frequency than the Ku band used by many satellites today. And there is one of those massive and acrimonious debates that you’ve never heard of going on between Ka and Ku proponents pushing their technology for higher-throughput satellite communications.
Ku is tried and true. Ka is newer, less congested, and allows for smaller, more powerful data beams.
Suffice to say that the Inmarsat/Honeywell solution is the only global Ka band option. Gogo’s next-generation tech uses the Ku band and Viasat’s second-generation tech uses Ka, but with less blanket coverage.
Honeywell, obviously, stands with the Ka-band proponents for one reason: bandwidth.
“These high frequencies allow us to pass a lot of traffic,” Lander said.
But there’s a tradeoff. Higher frequency means shorter wavelengths, and that tends to make it more difficult for the radio waves to travel long distances. (Longer waves tend to travel farther in the atmosphere: see AM versus FM radio).
So, to use this Ka high-frequency spectrum, the Honeywell radio transmitter applies 25 watts of power. Your phone’s transmitter might have 1 or 2 watts of power.
“This would fry your brain if it were used in a cell phone,” he said. “There is a safety distance we have to respect. No one can be on top of the airplane while we are transmitting. It would harm you physically if you were standing in front of it.”
In fact, the Jetwave amplifier’s 25 watts of power is roughly comparable to the transmission power of a ground-based cell tower.
* * *
Lander pulls up a photograph of the antenna that's sitting on top of the plane under a little dome. Installing this specialized hardware and getting it approved to use with each class of aircraft is one reason why Wi-Fi took so long to roll out and why it will probably be a few years before you regularly encounter the next generation technology.
The antenna itself is beautiful. It can rotate to pick up signals close to the horizon (zero degrees) and up to straight above it (90 degrees). If your hand was the antenna, a karate chop would pick up horizon signals, rotate your hand to an open palm, and now you’d be picking up signals from directly above you.
The antenna needs that flexibility because it needs to be able to lock onto the data beams that the Inmarsat spacecraft are projecting onto the Earth no matter where the plane is relative to the satellite.
Klander showed me the detailed map of how the beams fall on the Earth. And it’s complicated! “Right under the satellite, they are very circular, but as we go along the world,” he said, “they are more elongated because there is more room for the RF [radio frequency] to use the space.”
He didn’t want me to take a photograph of the map he’d pulled up on one of his three screens, but it looked like a zoomed-in version of this map from an Inmarsat presentation:
We were in region 82 of the POR. But if we’d flown into region 81, the internet traffic would be seamless. That's because the plane has two receivers, one of which would lock onto 81, the new adjacent beam, so there’d be no interruption of service when we left 82.
While Honeywell makes the antenna controller, the antenna, the radio, and the modem, Inmarsat’s network software actually makes the decisions on when and how to switch beams and satellites. Inmarsat’s constantly trying to load balance between satellites and beams, as well as effecting seamless transfers from region to region.
They also have to manage the ground operations, called satellite anchor stations. The only one in the United States is located in Lino Lakes, Minnesota (pop. 20,948), and serves as one of two redundant spots for the Atlantic region. (For completists, Inmarsat’s other ground stations are in Canada, New Zealand, Greece, and Italy.)
“The antennas there need to be massive with a lot of gain, so they can receive fine information and push a lot of data,” Lander said.
* * *
And all this, the whole system, is what will make your airplane Wi-Fi experience a little bit better. New satellites up in space. New antenna designs down here. Huge satellite dishes in little towns in Minnesota. Eight hundred hours of troubleshooting and testing by a guy named Stéphane. Raging blog debates about frequency bands. Deals with various regulators and aircraft maker and airlines.
It takes a lot of time to build a new system like this or Gogo’s or ViaSat’s. But man, are we getting close.
And indeed, after we lift off, I open up a speed test app at 9,000 feet and get 17 megabytes per second, despite the bevy of aviation and travel journalists who are doing the exact same thing at that moment. Soon, actual commercial travelers on some Lufthansa flights will carry out this same action, and probably be equally delighted.
As we circled around California, everyone happily Instagramming and tweeting and speedtesting, I couldn’t help but think about that famous of indictment of our era: Louis C.K. talking about how everything is amazing and yet no one appreciates it, using the example of airplane Wi-Fi.
But what is most amazing about our era is not the fact of the whining about modern inconveniences, but rather that the whining is read as demand by ever-more-nimble companies. Wi-Fi grumbling initiates global, technological change enacted by some of the most potent corporate forces on the planet. Our expletives, muttered over peanut bits and stray pretzel salt, have caused satellites to be launched.


It would be hard to fashion a more exquisite snare for a man like Donald Trump than the modern, institutional presidency. Just five months into his term, he appears trapped by its constraints—and the harder he tries to escape them, the more thoroughly entangled he becomes.
On Thursday morning, President Trump again lashed out at the “bad and conflicted people” investigating him for obstructing justice. “They made up a phony collusion with the Russians story, found zero proof, so now they go for obstruction of justice on the phony story,” he tweeted. But to take Trump’s charge at face value is to read it as an indictment of his own blunders. Trump is claiming that there was no underlying wrongdoing, but his decision to fire his FBI director sparked the appointment of a special counsel who’s now exploring whether it was a criminal act. This, he says, is a purely self-inflicted wound—or, as a senior administration official told The Daily Beast, “The president did this to himself.”
Trump is, in many ways, a man out of his time. He ran his business as he is attempting to run the presidency, as a 19th-century style entity, built around its proprietor. But the federal government has long-since evolved into a modern bureaucracy, an institution Trump appears to have neither the experience nor the patience to successfully operate.
Trump’s business empire sprawls into hundreds of LLCs and licensing agreements, but at its core, it takes a familiar form: the proprietary firm. Built around its founder, generally branded with his name, its reputation intertwined with his, and its affairs directly under his management—this was the dominant form of business in the United States until the final decade of the 19th century.  
Who Can Tell the Emperor When He Has No Clothes?
There are real advantages to that arrangement. It confers flexibility, allowing leaders to react to shifting circumstances with speed, and to take risks without fear of being second-guessed. It mitigates the agency problem—the danger that professional managers will prioritize their own interests. And to the proprietors, it offers the satisfactions of independence; they control their own destinies.
It is a form almost perfectly adapted to play to Trump’s strengths, and cover his weaknesses. As a CEO hired by an independent board, he might not have survived the bankruptcies of some of his businesses, a string of failed ventures, constant lawsuits, or the other setbacks of his career. But the Trump Organization is his to do with as he pleases, and if not all the risks he chooses to take, the loopholes he exploits, the deals he strikes, or the ventures he launches have succeeded, enough have paid off to preserve and expand the fortune he inherited.
But America is now a century or more past its managerial revolution—the heyday of the proprietary firm is gone, displaced by the corporate bureaucracy. It swept through industry in the Great Merger Wave at the turn of the 20th century, and through the federal government in the decades that followed. Bureaucracies offer a solution to the challenge of scale; they create rules and procedures, and the corps of professionals who populate bureaucracies abide by them, allowing business to be performed in a predictable fashion, even between actors with no personal relationship. And they bring with them their own set of costs and benefits, requiring the surrender of a degree of autonomy and flexibility in exchange for stability and scale, and putting systems ahead of individual initiative.
The presidency itself underwent a similar transition. In the 19th century, as the historian Brian Balogh has argued, it was already tremendously powerful—but operated indirectly, through third-party entities like state and local governments. It was a great deal like the Trump Organization—a relatively small staff, multiplying its influence by striking deals with larger entities that had the personnel to put its plans into action. As late as 1900, William McKinley had just 13 staffers working directly for him in the White House. Today, the Executive Office of the President claims more than two thousand personnel; the federal government, more than two million.
It is a world against which Trump seems to rebel at every turn. He refuses to empower his chief of staff to create a rule-bound White House, preferring instead to pit advisers against each other in a more freewheeling style. He insists on reaching directly down to subordinates, instead of moving through the hierarchy—calling the acting head of the National Park Service to complain about a photo, tweeting his defense of his travel ban instead of issuing statements through his press office, and meeting with the FBI director instead of the attorney general.
And each time, he has only worsened the trouble he sought to address, or created new problems for himself—producing mockery of his exaggerated crowd-size claims, court injunctions against his executive orders, and now an investigation for obstruction of justice. His repeated defeats seem only to deepen his anger as he strains against bureaucratic rules, the thousands of Lilliputian strings that not even presidential giants can snap.
There is no better example of this than the memo, that humblest—most spare and restrained—of literary genres. It was born in early modern Europe, but came of age during America’s managerial revolution, a key technology of the administrative bureaucracy. They institutionalized memory, making decisions legible to those not personally present, and creating records of conversations that could be referenced later by other bureaucrats.
In the process, they became the key tool of bureaucratic warfare. As MIT’s JoAnne Yates wrote, in her history of the memo as a managerial genre, “This extension of documentary communication also reflected more specific political motivations. As companies grew, allegiances to and rivalries between departments created friction, and each side of each dispute wanted to document its view of the issue.”
When Trump breached protocol and met alone with Comey, the FBI director went back to his office, and wrote a memo. When Trump called NSA Director Mike Rogers to criticize the intelligence community’s conclusions on Russian interference and to pressure him to publicly disavow the possibility of collusion, The Wall Street Journal reports, the NSA’s deputy director dutifully recorded it in a memo. There’s no indication the president ordered his own staff to document his view of these conversations. His preferred form of written communication to subordinates is the personal message scrawled with a Sharpie, not the memorandum dictated to file.
Memo to the president: You’re losing this game because you don’t understand its rules.
There’ve been no shortage of op-eds and talking heads telling the president that he disregards the institutional constraints at his considerable political and legal peril. Many White House advisers have reportedly told him the same, even as others have egged him on. But Trump obtained his clearest warning of all from the man from whom he was least inclined to receive it.
A week after Trump’s election, President Obama held a news conference. He was asked, among other things, about his handling of detainees in Guantanamo Bay. “Keep in mind,” he replied, “that it's not just a matter of what I'm willing to do.” Obama was no minimalist when it came to executive authority; he had pushed the powers of his office further than any of his recent predecessors, often in ways that federal courts would later strike down for exceeding his authority, or failing to follow prescribed bureaucratic procedures. And along the way, he had been humbled to discover that there are limits to what a president can achieve through sheer force of will and disregard of precedents.  
“One of the things you discover about being president is that there are all these rules and norms and laws and you've got to pay attention to them,” he continued. “And the people who work for you are also subject to those rules and norms. And that's a piece of advice that I gave to the incoming president.”
Maybe it’s a lesson each president needs to learn for himself.


A buried line in a new Facebook report about chatbots’ conversations with one another offers a remarkable glimpse at the future of language.
In the report, researchers at the Facebook Artificial Intelligence Research lab describe using machine learning to train their “dialog agents” to negotiate. (And it turns out bots are actually quite good at dealmaking.) At one point, the researchers write, they had to tweak one of their models because otherwise the bot-to-bot conversation “led to divergence from human language as the agents developed their own language for negotiating.” They had to use what’s called a fixed supervised model instead.
In other words, the model that allowed two bots to have a conversation—and use machine learning to constantly iterate strategies for that conversation along the way—led to those bots communicating in their own non-human language. If this doesn’t fill you with a sense of wonder and awe about the future of machines and humanity then, I don’t know, go watch Blade Runner or something.
What an AI's Non-Human Language Actually Looks Like
The larger point of the report is that bots can be pretty decent negotiators—they even use strategies like feigning interest in something valueless, so that it can later appear to “compromise” by conceding it. But the detail about language is, as one tech entrepreneur put it, a mind-boggling “sign of what’s to come.”
To be clear, Facebook’s chatty bots aren’t evidence of the singularity’s arrival. Not even close. But they do demonstrate how machines are redefining people’s understanding of so many realms once believed to be exclusively human—like language.
Already, there’s a good deal of guesswork involved in machine learning research, which often involves feeding a neural net a huge pile of data then examining the output to try to understand how the machine thinks. But the fact that machines will make up their own non-human ways of conversing is an astonishing reminder of just how little we know, even when people are the ones designing these systems.
“There remains much potential for future work,” Facebook’s researchers wrote in their  paper, “particularly in exploring other reasoning strategies, and in improving the diversity of utterances without diverging from human language.”


Amazon is rumored to be mulling a purchase of Slack, the fast-growing corporate chat platform. A deal could give Slack a valuation of $9 billion, according to a report from Bloomberg.
It’s no surprise that tech giants have taken interest in Slack, with its elegant, user-friendly interface that keeps employees ever-connected to work via their smartphones. The startup has enjoyed extraordinary growth since its 2013 debut. It now has about 5 million daily users, including more than 1 million paying users. As of last year, Slack claimed 77 Fortune 100 companies among its clients. It’s quite popular in American newsrooms—including at The Washington Post, which the Amazon CEO Jeff Bezos owns. (The Atlantic has used the platform since 2014.) Last year, Microsoft was considering scooping up Slack for itself. Instead, it launched a competing collaborative group-chat service called Teams in November. Even for Silicon Valley’s most formidable companies, a multibillion-dollar acquisition isn’t taken lightly.
So in the broader sense, the idea that Amazon is flirting with a Slack takeover is just another chapter in the Office Wars of Silicon Valley. Remember, Amazon is not just an online retailer; Amazon Web Services is already a major force in the corporate world. But Amazon’s possible interest also signals some clear ways of thinking about how the company wants to position itself in the future. (Neither Amazon nor Slack returned requests for comment early Thursday morning).
For one thing, it’s easy to see why Amazon would want to add a popular corporate communications tool to its suite of offerings to Amazon Web Services customers, Amazon’s widely used cloud-computing platform. “Widely used” may be an understatement. AWS, with its global server farms, is the backbone of the commercial web. It reported an eye-popping $12.2 billion in sales last year, and more than $3 billion in profit.
But the more intriguing explanation of Amazon’s interest has to do with one of the company’s even bolder visions of the future. Amazon is one of the major players in the fight for dominance in the realm of voice-activated artificial intelligence. And it seems to be doing pretty well so far. As of January, Amazon had sold more than 11 million of its Echo home device, according to a report by the investment banker Morgan Stanley. (You may know the Echo as “Alexa,” which is the word users must say to get the device’s attention.)
Silicon Valley is, at the moment, obsessed with this technology. The consensus is that voice—the commonly used shorthand for voice-activated devices and other conversational machines—promises to be the most transformative technology since the smartphone.  
Lex, the conversation interface that powers the Echo, already has a Slack integration. But many of Google’s apps integrate with Slack, too, and Google is one of Amazon’s major competitors—in voice and in general. So at a pivotal point in the fight to rule potentially world-changing technology, why wouldn’t Amazon leap to acquire a communications platform that its top competitors could be eyeing for themselves? Besides, where Amazon’s peers—Microsoft, Google, and Facebook—all already have robust communications platforms among their key offerings, Amazon has none. Acquiring Slack could change that—and could position Amazon for shaping the way workers use voice-activated technology at a time when Slack is already considered a possible email slayer.
Just think of what bringing all that work data to the Echo’s capabilities would mean for the worker—and the further blurring of any remaining line between work and home. A person could be driving to work, or cooking dinner, and dip back into work through a conversation with the Echo:
“Alexa, read me the conversation in The Atlantic’s technology Slack channel since I last checked it.”
“Alexa, dial-in to the 10 o’clock conference call for me.”
“Alexa, download and play that podcast that my colleague recommended the other day.”
“Alexa, let me know if my boss Slacks me.”
We’re only in the very earliest stages of imagining how voice will transform the workplace, but it makes sense that Amazon—which aims to lead the way—would look to Slack as it vies for superiority.


When Major Spencer Williams was ordered to “shut down shop and move out” of Afghanistan in 2005, he closed his final message from the field as he always did—quoting a long-dead historian. “Plant yourself not in Europe but in Iraq; it will become evident that half of the roads of the Old World lead to Aleppo, and half to Bagram.”
Williams made up one-third of the U.S. Army’s historical field staff in Afghanistan—a team directed to cover the breadth of the country, vacuuming up media, documents, and oral histories so that some future soldier or academic could better understand the course of the war and how one might respond to circumstances should they arise again. The war offered more than enough material to keep Williams and the others busy, but they weren’t able to communicate the importance of that work to those leading the mission in the country. Following a command from the highest-ranking officer in Afghanistan, the historians were on their way out of Kabul.
It would be almost two years until another team came back into the country. In that time, units cycled in and out of the war zone, each adding a small drop to the bucket of the longest U.S. military engagement in history. Whenever a unit prepared for the return trip home, its soldiers collected their gear, prepared the site for the following unit, wiped local servers, and allowed the details of the prior months to fade.
The gap in record-keeping created by the absence of Williams and his team—and the difficulties they faced in demonstrating this record’s value while overseas—illustrate a common headache of 21st century historians. Though technology has made more sources than ever available to color, verify, and explore history, determining those sources’ value remains the task of a trained human eye. And in the case of the Army, support for that eye has declined as its necessity increased.
* * *
Williams and his team were sent to Afghanistan in order to collect material for inclusion in the Army’s Tan Books, histories of the wars in Iraq and Afghanistan written by the Center of Military History. CMH’s official histories, which have existed in various forms since the years following World War I, trace narratives of wars using material only a massive enterprise like the Army can synthesize. They offer maps of documents flying through the Pentagon and overseas bases everyday, and examine points in which the arc of history bends ever so slightly. The sprawling nature of the histories also means that precise connections between events can be found and applied.
Jill S. Russell, a visiting professor of national security and strategy at the U.S. Army War College, recalls digging through niche volumes of CMH’s  The United States Army in World War II, also known as the Green Books. “There were things that guys coming back from the Philippines were writing that are being repeated by guys coming back from Afghanistan almost literally word-for-word,” she says. Russell cited modifications made by soldiers in World War II to improve the ease of use of heating elements in mountainous terrain. Upon deploying to similar terrain in Afghanistan, some soldiers rediscovered the modifications made by their predecessors.
Modern wars have upended some of the most basic factors CMH relied upon in order to write histories in a consistent way. “Since 1991, the operations we’ve gotten into—Somalia, Haiti, Bosnia, Kosovo—were more complicated scenarios,” says Shane Story, a retired Army helicopter pilot who’s overseeing the creation of the 30-some eventual volumes of the Tan Books. “Even more since 9/11, both Afghanistan and Iraq, they’re not clean, neat narratives. ... [In] the best work that you write, you know what the last word is before you start. We don’t know what the last word is.”
Story and Jerry Brooks, who’s responsible for collections in the field, speak candidly of the problems CMH historians face as they work through the Tan Books creation, a process that will take decades to complete. Once, there were scores of clerks responsible for maintaining detailed records of units. With the rise of computers and software, the Army believed that “everybody would be their own records manager,” and broadened this responsibility to average soldiers, Brooks says. “Well, they failed to take into consideration that people are lazy.”
In theory, these individual record-keepers hand off documents and other sources of information to field historians when requested. However, in addition to the two years after Spencer Williams and his team left Afghanistan a decade ago, the country hasn’t had a field historian from the Army since 2014. Historians dealing with Iraq have avoided these same gaps, but still suffer from fewer personnel in the field than in past wars. Brooks cites Vietnam, where U.S. headquarters in Saigon alone maintained a staff of more than 20 historians. Nowadays, as a result of caps on the number of troops deployed, historians oftentimes find themselves on the first flights back home.
“We did a disservice to the American public, because we did not put enough historians downrange to collect the documents. And now we're reaping what we’ve sowed,” Brooks says.
By his estimate, more records from the wars in Iraq and Afghanistan have been wiped or otherwise lost than remain in existence. But he admits the exact figure is difficult to determine due to the few historians in the field. A concept document that lays out the plan for the Tan Books supports this estimate, listing the amount of data lost in the wars as “incalculable.”
* * *
Both Brooks and the Tan Books concept document exclude the multitude of informal records created by soldiers during the rise of the digital age—emails, Facebook posts, and blogs, to only name a few. These materials are both an opportunity and a source of unanswered methodological questions, according to Michael Gisick, a Ph.D. candidate at Australian National University who studies the use of social media by U.S. service members during the war on terror. “I suspect there are great quantities of images, emails, and other digital narratives tucked away on the hard drives of America’s veterans,” he says, explaining that these sources of information can “jog the memories of participants, spur questions, and illustrate events,” as well as counter popular narratives that might have arisen in political circles and at higher levels of the military.
Even for historical organizations that have been quick to adopt online technology, the role these sources can play is still being tested and explored, says Russell Riley, who co-chairs the Presidential Oral History Program at the University of Virginia’s Miller Center. The program he helps direct, whose texts are of a similar scale to the Army’s official histories, is examining how to use social media as an additional springboard for the Obama administration histories, and he expects they’ll play an even larger role once they begin considering the Trump presidency. He believes that as officials of all stripes have become more reluctant to put thoughts down in writing, social media can offer a less filtered means of communication.
CMH notes that more than 150 terabytes of data have been collected for cataloging and eventual use in the Tan Books. The value of that data remains to be seen. Detachments sent into the field by CMH, though trained to recognize historically significant documents and conduct interviews with people of interest, act as “vacuum cleaners,” as Brooks has called them, rather than analysts. When Brooks briefed a group of Army historians, he included a photo of a Burger King fry holder tagged as coming from a restaurant set up after the invasion. “Technically Correct but Useless” reads the title—a classification that might only increase by including the Facebook posts of young, deployed soldiers.
Despite these limitations, Story remains convinced that the “glass is half-full” for the Tan Books. Like the famed Civil War historian and author Shelby Foote, he believes a good historian “can build the story around just one document,” if necessary. But finding those capable historians is still a challenge. Right now, Story can dedicate one individual to work on an actual text of the Tan Books at any one time. He’s pleased with the work already released, a 70-page pamphlet on the 2007 to 2008 surge of American troops in Iraq, but notes that a complete text stemming from the work is still six or seven years down the line from release.
“In certain respects, I think of ourselves as having a role comparable to the government accounting office,” Story says, arguing that even first drafts of the texts play an important role. Russell and Brooks agree, saying that these early efforts, which few expect to be complete or even immutable, will drive further research and questions as more becomes known and unclassified about the wars.
Even the best histories don’t provide a means of knowing everything ahead of time, Story says. But “when circumstances arise, done well, at least, I think it can give you a means of judging them with a little more acuity.”


In America, terrible acts of violence are often met with handmade signs, bunches of flowers, and teddy bears. Makeshift memorials to shooting victims are flooded with these sorts of objects, and they spring up reliably wherever a horrifying event has occurred.
After the elementary school shooting in Newtown, Connecticut, in 2012, town officials had to rent a warehouse to house all of the gifts and donations that people sent.
But in the era of social media, there’s another place where people gather to react to acts of violence: on the Facebook page of the suspected perpetrator.*
Within minutes of news reports identifying the man who shot at members of Congress while they played baseball this morning, people had found what appeared to be the suspect’s Facebook profile. And then they flooded it with comments. In the span of about an hour, there were hundreds of messages. And then thousands. Things like:
“Rot in Hell buddy!!!”
“Have fun ‘fighting the man’ in prison loser!”
“Another Bernie demorat. You suck and deserve to die in prison”
“I hope you survive bro. I hope you get waterboarded for weeks, then tossed in solitary for life.”
“Dang he's getting his page blown up lmao good”
“You will not win this war”
“Enjoy prison or death”
And, like so many areas where people congregate online, the string of comments devolved quickly into vitriol and political arguments—prompted in part by the large photo of Bernie Sanders atop the man’s profile page. Then there are the memes, so many of them: silly images of Donald Trump, cartoon frogs, disparaging images of Barack Obama and Hillary Clinton, plenty of objectionable language—pretty much what you find on every other social platform.
Among the comments, there is the occasional attempt at unity: “Sad, Sad situation,” one person wrote. “We are a very divided Country. Some way, some how we need to attempt to find middle ground and move forward.”
“There is an unbelievable amount of hate in these comments,” another person wrote. “It's really sad to see that this is what society has boiled down to.”
Such comments were, for the most part, either mocked outright or ignored. And when President Donald Trump announced in remarks from the Rose Garden that the suspect had died, dozens of people cheered the news in real-time on his Facebook page.
Watching reaction unfold in this space is a surreal and disturbing activity—but it’s not exactly surprising. On platforms where anyone can publish, people congregate around what interests them. Just as people use hashtags to find people with similar interests on platforms like Twitter and Facebook, the suspect’s very identity became a temporary newsfeed of sorts. The barrier to entry for participation is practically nonexistent. People might not show up at a shooting suspect’s house to register their dismay; but it’s effortless to swing by his Facebook page and press “post.”
As of this writing, the comments were coming in too quickly to keep up.
* After this story was published, Facebook removed the shooting suspect’s page. A spokesperson for Facebook sent me this statement: “We are shocked and saddened by the incident that took place this morning. We have identified and removed the suspect’s profiles for violating our community standards.”


It didn’t take long for Uber to jeopardize whatever goodwill the company had earned by announcing a series of new initiatives aimed at increasing the hiring and retention of women and minorities. On Tuesday afternoon, the company published a series of initiatives it would be implementing to make the company’s culture more welcoming. By early Tuesday evening, audio had leaked of a board member, David Bonderman, making a sexist remark. By Tuesday night, Bonderman had submitted his resignation from the board.
The exchange in question took place at a meeting to discuss the new policies and the absence of the company’s embattled CEO, Travis Kalanick. According to a recording obtained by Yahoo, Arianna Huffington, a board member, says, “There’s a lot of data that shows when there’s one woman on the board, it’s much more likely that there will be a second woman on the board.” Bonderman replies, “Actually what it shows is it’s much likely to be more talking,” to which Huffington says, “Oh come on, David.”
Bonderman’s statement wasn’t just in poor taste, it was also wrong, according to research on gender dynamics during business meetings. As The New York Times reports:
Tali Mendelberg, professor of politics at Princeton University, and Christopher Karpowitz, an associate professor of political science at Brigham Young University, conducted a study in 2012 concluding that men talked far more than women did at meetings. The professors convened 94 groups of five people and varied the number of men and women in the groups. Their study is in line with multiple others drawing similar conclusions — men talk more than women, and men interrupt more than women.
In a statement, Bonderman, who is a partner in the private equity firm, TPG, an investor in Uber, wrote that his comments “came across in a way that was the opposite of what I intended, but I understand the destructive effect it had, and I take full responsibility for that.”
The events of Tuesday cap another iteration of what is now a very familiar pattern for the company: There was a revelation of problematic behavior or actions, an apology and a pledge to change, and then a revelation of new problematic behavior or actions. Uber has been in a seemingly never-ending public-relations spiral that started with a bombshell blog post by a former employee, Susan Fowler, alleging that she faced overwhelming sexism at the company. In response, Uber hired the former attorney general Eric Holder and his law firm to investigate the company and submit what has been called the Holder report, which would tell it how to fix things. But as Holder and his team were doing their investigation, a series of other troubling revelations came to light, including footage of Kalanick berating an Uber driver (for which he later apologized in a memo), and questionable business practices such as a program known as Greyball, which some said made it possible to discriminate against riders.
Given Uber’s history, Bonderman’s comments are not especially surprising, but their timing and delivery are especially cringeworthy: A powerful male board member makes an offensive comment about women in response to a female board member speaking at a meeting meant to introduce policies to address allegations of sexism. What makes matters worse is that Uber framed the release of the Holder report as a turning point for the company. But now, even with Bonderman's resignation, the narrative for Uber is once again of a company whose culture is hostile to many of the goals the company espouses.


Ernest Moniz is the antithesis of Donald Trump. As the head of the Department of Energy throughout much of President Obama’s second term, he was responsible for championing the Paris Climate Accord, negotiating the Iran nuclear deal with former Secretary of State John Kerry, and diligently pursuing a broad-based energy strategy often called the “all of the above” option. He is as comfortable testifying before the Senate as he is leading rigorous research efforts into the future of energy systems. There are few theoretical physicists who seem to enjoy the grind-it-out politics of Congressional appropriations, but Moniz is one of them.
Perhaps it is not a surprise that Moniz’s accomplishments have come under attack by Trump’s administration. The biggest move was the president’s announcement that the United States would pull out of the Paris treaty, much to the dismay of the international community and American business leaders.
When I spoke to him recently, Moniz spelled out his legacy, as he sees it, and the various ways that the current administration is undermining the very programs that, as he put it, “have clear track records of tremendous success.”
For Moniz, that includes the Loan Programs Office, which made capital available for the deployment of energy technology. After—and despite—the well-publicized Solyndra debacle, that program has experienced few losses and generated substantial returns for taxpayers. Also on Trump’s chopping block are ARPA-E, a research program modeled on the famous Defense Advance Research Projects Agency, and a new structure Moniz created at the Department of Energy called the Energy Policy and Systems Analysis Office, which he says was designed to and received bipartisan support.
But you’d be hard-pressed to find anyone who understands the politics, policy, science, and technology of energy as well as Ernie Moniz, so the discussion covers a lot more: the growth of solar, government support for drilling technology, the continued sluggishness of innovation in nuclear power, and the current Department of Energy “review” of base-load power, which has renewable-energy advocates worried.
Perhaps most intriguingly, Moniz calls attention to the fact that the job losses in the coal industry have been primarily driven by technological change in fossil-fuel extraction, namely the development of fracking and other drilling technologies. That allowed for a massive ramp in the production of natural gas, which made it less expensive than coal. And he is founding a new organization that he calls The Roosevelt Project to try to address, on a community-scale, the dislocations that workers will face in a deeply decarbonized world, underpinned by new energy technologies.
A lightly edited and condensed transcript of our conversation follows.
Alexis Madrigal: How did you think about your overall remit at the DOE?
Ernest Moniz: I always looked at the overall portfolio of the department as addressing three different time scales. So, the loan program is addressing the decadal time scale. The idea was to provide relatively low-cost capital to get these technologies out there: go down the cost curve, go up the experience curve.
Then, you have something like ARPA-E, which addresses maybe the mid-term. These are still pre-commercial, taking a lot of risk, but something that could hit a few triples and a couple of home runs, a bunch of doubles, and in that 10 to 20 year time frame, hopefully we’re going to see a number of those technologies scaling.
A number of researchers have formed companies. There has been $1.8 billion in follow on private capital [funding those companies].
Then for the long-term, going to mid-century and beyond, when I think the industrialized countries, obviously including the United States, will need to be into a deep decarbonziation mode. The Paris targets are only a first step: 25, 27 percent in 10 to 15 years. We’re going to need 80 percent reductions, and that’s where much of the early-stage basic science comes in.
For example, at DOE, the Energy Frontier Research Centers. They’re fantastic. They’re run out of the Office of Science and they are addressing a whole suite of very fundamental challenges that could enable the big breakthrough technologies for the longer term.
So, it’s a portfolio and there is often a knee-jerk mantra that, “Oh, the government should only invest in the earliest stage science.” Well, okay, that’s a possible point of view. I don’t think it’s a point of view that’s going to get us to the kinds of solutions that we want and need and that’s going to give us the economic edge we need in a multi-trillion dollar marketplace.
Madrigal: You were tasked with executing on an “all of the above” energy research plan while you were the head of the DOE. Do you think you were able to support all of the energy technologies adequately? Or do you, looking back, feel any got the short stick?
Moniz: I think, overall, we did pretty well. Certainly efficiency, renewables. Fossil, we put quite a bit of money into the carbon capture. That was the focus of the administration. On the nuclear side, I would have liked to see a bit more, to be honest. We did some good things, but we could have done more.
I think a big area that still needs a lot more is on the system side, especially the electric grid, but also the integration of energy infrastructures—electricity and gas. Obviously now the cyber component has come up big. I think a lot more could have been done on resilience of energy infrastructure. We went in the right direction but not quite as fast as I would have liked.
Madrigal: What would you have liked to have seen done with nuclear power?
Moniz: I think there could have been a lot more done—though we were a little bit hamstrung—on the back end, on spent fuel-disposal approaches.  I would have liked to have seen more done on various small modular reactor concepts, for example.
Madrigal: If I compare my own expectations from five years ago, I would say I expected more reactor concepts and things to be farther along than they are. There seem to have been a lot of ideas that never got off the drawing board.
Moniz: In nuclear energy, it’s somehow very slow moving. Part of it is that—for obvious reasons—the regulatory process, the safety process, is so complex.
And a lot of people who are very interested in alternatives to light-water reactors feel they are boxed in because the regulatory process may not have the expertise it needs yet. And of course, that’s difficult for NRC [the Nuclear Regulatory Commission] to do because most of their funding comes from fees. And by definition, if you don’t have an industry of, say, fast reactors, well, where is the fee coming from?
I would have liked to have seen not only DOE but NRC to have more appropriated funding to allow for more alternative approaches.
Madrigal: Carbon capture and storage has encountered a lot of difficulty in scaling up and becoming a practical, usable part of the energy system. What’s the argument to keep spending research dollars there?
Moniz: We’ve seen the cost of lots of stuff coming way down—like wind—after deployment. Carbon capture is at a very early stage. After all, to a very good approximation, it’s always cheaper to release the CO2 than to capture it. In the absence of a carbon charge or a policy to cap CO2, it’s got to be government funding to move forward. Ironically, it is the carbon capture which I could argue is the only part of the energy R&D portfolio that really has only climate as its motivation, as opposed to other possible benefits.
I would also note that carbon capture is much less expensive for many industrial facilities. And if nothing else, this may be an important use of carbon capture and sequestration. Frankly, to be perfectly honest, in electricity production, we know we have multiple options with low carbon, CCS would be one option. In industry, we may be hard pressed to find alternatives to carbon capture for large facilities that produce large amounts of CO2. And often, the cost is much much less because in their chemical-process stream, they have to separate CO2 anyway. And a big cost in the power plant is the separation of the CO2.
Madrigal: Looking back over your tenure, what do you think your biggest accomplishments were?
Moniz: On the big scale, obviously, the role the department played in Paris and, most specifically, the Mission Innovation commitment in Paris, for countries to double R&D investments over 5 years. And then, of course, the Iran Deal, in a different space. Those were the major advances in the Obama administration.
Madrigal: What about things you were able to do inside the institution that maybe didn’t get as much attention as the climate accord and the Iran deal?
Moniz: There were a lot of other things that are very important for the department going forward. For example, I think the relationship with the 17 national laboratories of the Department of Energy and the way they became much more a part of the department’s strategic planning. I think that was important and I hope that will be built upon.
Another thing, if you go really inside baseball, there are some of the reorganizations we made were quite important and helped the department function more effectively.
Madrigal: Like what?
Moniz: For example, I rearranged the responsibilities for the undersecretaries. There were three. What I think was very important was combining the science and energy portfolios in one secretary. The first undersecretary in that role, Franklin Orr, was a professor at Stanford.
Another one: The department always had some energy-policy function, but when I arrived, it was combined with the international affairs office. I never understood the logic of that. The policy got second shrift but it’s a core function for the department. So we split that off. But in splitting it off, it became named the Energy Policy and Systems Analysis Office. The “systems analysis” words were very important. That was the signal that this was going to be seriously analytical. And the hope was that in doing so, it would provide the foundation for a serious bipartisan response in Congress.
That played out dramatically.
That office, EPSA, was the tip of the spear on the Quadrennial Energy Reviews [a duo of reports examining energy infrastructures in the U.S.]. The first one came out in April 2015 and, as you know, these were not years of great political comity, and yet that first report installment had 21 policy recommendations fully or partly put into law. It received very, very strong bipartisan support.
The person who headed that was Melanie Kenderdine, my long-time colleague. She did a fabulous job and Congress even dramatically increased its budget from when we formed it because the response was so positive. That’s the kind of thing that should continue.
And yet the budget proposal from the administration proposes to end it. As they do ARPA-E. As they do the Loan program. It’s almost selectively going after programs that have clear track records of tremendous success. The budget proposal undercuts, in many ways, the entire energy and Paris agenda.
Madrigal: Okay, let’s talk about the U.S. pullback from the Paris Accord. You were intimately involved in the Accord negotiations. How big of a setback is the U.S. pullout for the global effort to prevent the most dangerous levels of warming of the earth?
Moniz: The Paris Accord is extremely important as a great first step towards significant decarbonization. The administration’s decision to announce the intent to withdraw—I think as you know there are some steps that are required and that will go until November of 2020—that obviously was very bad news on many grounds.
For one thing, it certainly flies in the face of the science that clearly calls for some robust response. But beyond that, I’m going to respond to both the announced withdrawal from the Paris Accord and the administration’s budget proposal to Congress. They kind of go hand in hand.
The latter would also, if followed by the Congress, undercut the innovation agenda that both provides solutions to climate risks and, frankly, positions those who innovate to be able to capture a good part of a multi-trillion dollar clean-energy economy. What it does do for sure, at least, for the near term, is abdicate U.S. leadership in this issue of such great global concern. Leadership that was essential for reaching Paris. I would argue, as many would, that the leadership of President Obama in producing the joint announcement with President Xi of China in November 2014  was really the turning point for the road to success in Paris.
And finally this announcement by the current president comes in a string of statements that together really have shaken the confidence of our allies and friends in the reliability of the United States, in terms of the commitments it makes and the leadership it offers. So, it’s a pretty broad-based set of negatives there.
Madrigal: Are there ways to make progress on climate during this administration?
Moniz: I am strongly convinced that the world is going to low-carbon. I think there’s no doubt about it. That’s why that big market is forming and will form even more strongly. So, in the end, I believe the administration’s actions will not deflect the United States from being part of that low-carbon future, but it will slow things down. It will create some bumps in the road and ultimately that will just come back to haunt us by making it more costly to resume the path to low carbon.
In terms of maintaining momentum, clearly, mayors and governors and business leaders have all made it pretty clear that they have the intent of continuing on the pathway because they know that’s where the future is going. Certainly, as far as the business community goes, they are not going to make long-term large capital allocations based on anything other than a low-carbon future.
I don’t want to minimize the impact of the president’s announcement in terms of getting to the climate solutions as efficiently as we can. But I think there is also the near-term damage in terms of American leadership and reliability in the eyes of the world and in the eyes of our allies.
Madrigal: If you were advising young climate researchers, would you tell them to take the offer by Prime Minister Macron of France to go work there on climate? Would you have taken him up on it?
Moniz: I did in a certain sense, take him up on it, in that I was a post-doc in France. It wasn’t Macron, but one of his predecessors. I certainly would do that again and in fact that’s where I met and married my wife.
But look, climate science is obviously an international collaborative effort. I wouldn’t leave the United States just for the reasons you are implying, but I would say there are great opportunities in France and Germany and the U.K. in terms of advancing climate science.
Madrigal: While we’re on the topic of the new administration, there’s currently a review of “base-load power” under Department of Energy Secretary Perry’s direction. [Base-load power is steady, nearly-always-on power currently provided by large fossil fuel, nuclear, and hydroelectric plants.] What do you think of it?
Moniz: It is an important issue today as we are looking at the uncertainty of operations of existing nuclear plants, which are essentially carbon-free. What is the value of base-load power in the mix as part of the overall system design and system reliability? With some colleagues, we’re starting up a small non-profit in the energy space and this was also a question that we intended to look at.
However, a review of this type also needs to look at the emerging technologies. For example, the utility in Tucson recently announced a long-term, a 20-year purchase-power agreement for solar energy plus storage at a pretty attractive—stunning, actually in my view—price. They quoted less than 4.5 cents per kilowatt-hour, including the storage.
Madrigal: Wow. [In Arizona, the average cost of electricity in March 2017 was 9.7 cents per kilowatt-hour. Electricity prices vary around the nation, but the U.S. average was 10.3 cents per kilowatt-hour in March 2017.]
Moniz: That’s with a subsidy, of course. So, without subsidy, maybe it’s twice that. But still, it’s amazing how rapidly these costs are coming down. The extent to which something like storage coming in big time at large scale affects the equation in terms of baseload power. Obviously, that can address a great deal of the non-dispatchabilty of solar or wind.
The critical issue is that these reviews have to be done in an objective, analytical fashion with transparency about assumptions—for example, assumptions on the trajectory of storage costs and deployment would be important in terms of the role of inherently intermittent sources in the grid, as well as nuclear, natural gas combined cycle plants, and the like.
Madrigal: From what you know about the review, as it is, do you think it is being approached in an objective, analytical way?
Moniz: I just don’t know. I certainly haven’t seen a lot of transparency. If the outcomes are not transparent, I don’t see how it is going to have much influence. My impression is that there has not been a wide set of inputs being solicited to the study.
Madrigal: In the past, you’ve said that restricting climate emissions will have a negative impact on some sectors, so in crafting policy, the emphasis should be on “workers.” What do you mean by that?
Moniz: As in any significant transformation, including those in many sectors through technology advances, the economy is dynamic and some areas will directly benefit and some areas will be directly challenged.
The obvious example is the decrease in coal, although I want to emphasize that up til now, that’s had very little to do with climate. That’s been natural gas prices being low.
The job losses in the coal sector have been there for decades. This is not a recent thing. A small impact to date from things like the mercury rule and a very substantial impact from technology development.
Now clearly, if one is going to a very deeply decarbonized economy, then you need to invest in two things, it seems to me. And I would say that the Obama Administration, we did. One is help for communities in transition. That could be retraining. There are lots of approaches. And two: the innovation investments that, for the example we are discussing, can actually permit coal to be a player in a very low carbon world, specifically carbon capture and storage or carbon capture and utilization and storage.
But there are other communities that are affected in very, very different ways. There are all kinds of possibilities, but what we’ve done is think there is a one-size-fits-all solution. Rather than having the headwinds from some of the real or asserted job dislocations, why don’t we start with the workers and the jobs and the communities and build up solutions for their regional development that are consistent with a long-term low-carbon economy?
I will say that is a project that I am intending to carry out. We’re calling it The Roosevelt Project. That will be in concert with the Emerson Collective, which is out in California.
Madrigal: Can you tell me more about that?
Moniz: The idea is to do studies over several years of exactly the type that I’ve described. The reason it is being called The Roosevelt Project, or the reason I called it that, is because Teddy Roosevelt is identified with environmental stewardship, Franklin Roosevelt with the New Deal and jobs, and Eleanor Roosevelt with social justice. And so I think it all comes together as the Roosevelt Project.
Madrigal: One of the things that I want to pick up on is that coal-job loss is actually due to technological development, not regulation. And drawing out your implicit assertion—that’s because drilling technology allowed for access to new reserves, which drove down the cost of natural gas, which then put coal operators in a tough position. And it’s my understanding that these drilling technologies were developed with government assistance at multiple levels. How did the DOE support the project of reaching unconventional natural-gas reserves?
Moniz: When you go back to the late ’70s and early ’80s, the DOE supported a variety of programs involving the drilling technologies but also involving the characterization of unconventional reservoirs. Then in the ’80s to ’90s, there was what I would call a public-private partnership that carried on. This was run by GRI, the Gas Research Institute—
Madrigal: Which was created by that distribution tax?
Moniz: Technically it wasn’t a tax. But you’re on the right track. FERC [Federal Energy Regulatory Commission] allowed a small fee on interstate gas transmission, and that went to provide half the funding because it had to be matched by companies. That research fund, if I recall, grew to something over $200 million a year, and the Gas Research Institute was established as a non-profit organization to manage it. And George Mitchell, the famous George Mitchell from Texas kinda viewed as the father of shale gas, he was on the board of GRI, and helped advance that. That was in effect a public private partnership. They did a lot of demonstrations, demonstration wells and the like. That built up to the coal-bed methane, to tight sands, and to shale as unconventional resources.
At the same time, very importantly as well, there was also finite-time tax credit. So you had the DOE early-stage stuff, you had the GRI developing the technology, and you had the tax credit for the initial deployment, and when the tax credit went away, the production kept going up, up, up, because by that time, the cost had been brought down to a level where it obviously competed on its own.
Madrigal: Another area where the government appears to have jumpstarted a major movement is so-called utility-scale solar—big photovoltaic farms put in by energy companies. And the mechanism was the Loan Program at the DOE, which was made kind of infamous by the collapse of Solyndra. How did the Loan Program work in this case?
Moniz: When the loan program made its first commitments, there were zero [utility-scale solar PV projects]. Partly it was the time period: 2009 to 2010, debt financing was a little bit tricky in general. But the reality also is that banks and investors, unless they are specialized, have a hard time going into new areas, so I think the DOE program and the tremendous diligence process that they developed over several years really gave a lot of confidence to investors.
So, in this case, there were none. The first five got loan guarantees. And that was the end of DOE supporting PV farms. And now there are 10 times as many with private money.
Madrigal: How has the loan portfolio performed generally?
Moniz: Certainly, these [solar-farm] loans, they’ve had no losses. The projects all had power-purchase agreements to cover the cash flow requirements. And certainly there were other successes including the loan to Tesla for their factory in Fremont, which they then paid back nine years early, but it got them going.
If you look at the portfolio overall, clearly there have been losses and Solyndra is the famous one. I might note, I don’t know the exact numbers today, but I believe the total losses in the entire portfolio are in the $800 million range. Over $500 million was Solyndra alone. It is the biggest loss item on the book of the portfolio.
However, the portfolio as a whole has generated I think it is now $2.5 billion of interest payments to the Treasury. And second, when Congress formed the program, they created a $10 billion reserve anticipated for losses. You might argue we’re way behind in the losses. And that may come to the risk level that’s being tolerated.
Madrigal: Let’s turn to your other big accomplishment, the Iran nuclear deal and nuclear non-proliferation. Do you think you’ve stopped Iran from getting a nuclear weapon or more kicked the can down the road? At this point, thinking about Iran’s behavior since the deal and all the geopolitical factors, how do you see the Iran deal?
Moniz: Let me emphasize what I consider to be the most important part of the deal, which is verification. Certainly the bar to any covert weapons program has been raised a lot, and will be quite high.
Now, what I want to say is, first of all, you may know that I now have another job as CEO of the Nuclear Threat Initiative, so proliferation in general is something I’m also working hard on.
Our hope, as was President Obama’s hope, was to minimize the role of nuclear weapons over time and certainly to prevent the further spread of nuclear weapons. With Iran, they have made a very strong commitment to never having a nuclear weapon. So, it is my expectation that Iran will never have a nuclear weapon. If they attempt to go there, it will be known and actions will be taken to, shall we say, discourage that.
The Iran agreement stands on its own: If 15 years from now—or whatever is left of 15 years, 13.5 years—we chalk up the score, and say, yup, the [deal] was followed to the letter and now Iran is free to develop its peaceful nuclear program as it chooses, the deal was a success.
However, I have to say that those of us who were engaged in it so heavily, along with many others, are hopeful that over this time period, 10, 15 years, that there will be other more fundamental changes in that whole regional dynamic. Because obviously we have a lot of problems with Iran in the regional context. If this can help push that society in the right direction, that would be great, but the deal stands on its own as well in terms of nuclear activity.
Madrigal: Looking out 50 years, say, do you expect a greater or smaller number of nations to have nuclear weapons?
Moniz: In 100 years, I would hope that the answer is zero. Let me add an important word. It’s a word I just used in the Iran context. When we talk about a world without nuclear weapons, what I always talk about is a world verifiably without nuclear weapons. The word verification is critical.
What is the success path that is practical? I strongly believe in the goal of a world without nuclear weapons, but that doesn’t help if we don’t have something that is pragmatic that can survive in the political realities of all the countries that have to play in this.
It’s a very, very tough problem. I think we are decades away from reaching that kind of a goal, but we’ll be even more decades away if we don’t start thinking about it now.
Madrigal: Sounds like it’s time for some game theory.
Moniz: Yup. Tom Schelling [the eminent strategist and an important contributor to game theory].
Madrigal: I read in The Boston Globe profile of you that you’ve literally never taken a vacation? Did you finally take one after you left the DOE?
Moniz: Yeah, I took … My wife termed it the anti-Genesis strategy, I slept for six days and worked on the seventh.
Madrigal: Really?
Moniz: Yes.


On Tuesday, Uber announced a host of changes that it hopes will stem the ongoing public-relations crisis the company has found itself in for the past several months. The embattled CEO, Travis Kalanick, will step away from the company for an unspecified period of time. But that won’t likely change the day-to-day lives of the more than 5,000 Uber employees as much as the the changes the company is committing to make to their recruiting, retention, and workplace-culture policies, detailed in a report known as the Holder Report.
For many, a cultural change at Uber seems overdue. At the start of 2017, a former Uber employee, Susan Fowler, published a damning blog post alleging her experiences with sexism and discrimination at the company, and the feeble response she received when she complained. Immediately after Fowler’s post, the company retained the former U.S. attorney general Eric Holder and his firm Covington Burling to investigate the company’s culture and structure and to provide recommendations to help it improve. While the firm worked on its investigation, more disturbing details about Kalanick’s behavior and the culture of the company came to light, leading many to wonder if Uber’s survival was on the line.
The 13-page report by Holder and his team attempts to address a wide variety of problems, some of which are Uber-specific, but many of which are common to companies across the country, such as a lack of diversity, particularly at the higher levels of the corporate structure. Many of the changes Uber plans to implement, such as flexible work schedules and less aggressive mottos, are quite basic, but if the company can do so successfully it might encourage companies struggling with similar problems to follow suit. Some of the other initiatives, such as hiring a chief operating officer and instituting mandatory management and interview trainings for HR and other leadership, are the sorts of things that are extremely common at small and medium-sized companies, to say nothing of a big, international company like Uber.
This report isn’t the company’s first foray into the discussion about diversity. In late 2016, Uber shared data about its dismal diversity numbers and its commitment to improving them. The report encourages the company to continue to publish its diversity statistics, as well as to assemble an employee diversity board, and to expand the role and reach of Bernard Coleman, the current head of diversity, elevating the position to report directly to the CEO and COO. Holder and his team also suggest that employees undergo implicit-bias training (a contested method of increasing awareness and diversity) and implement blind resume review to decrease opportunities for bias based on perceived race or gender—a proven issue in hiring.
“Implementing these recommendations will improve our culture, promote fairness and accountability, and establish processes and systems to ensure the mistakes of the past will not be repeated. While change does not happen overnight, we’re committed to rebuilding trust with our employees, riders and drivers,” Liane Hornsey, Uber’s chief human-resources officer said in a statement.
Beyond these basic plans, additional changes coming down the pike may strike some as a bit more symbolic than substantive. According to Bloomberg, the “War Room” will be renamed the “Peace Room,” and Mike Isaac of The New York Times reports that Uber’s head of Human Resources requested that everyone in attendance at a company meeting to give each other a hug. Those type of feel-good exercises are undoubtedly an attempt to bring about a new, kinder Uber. But they’ll be meaningless without real shifts in the company’s policies and corporate structure—the kinds of changes that are much harder to make.


Travis Kalanick, Uber’s embattled CEO, is taking a leave of absence. “It’s hard to put a timeline on this,” he told his employees in an emotional email on Tuesday. “It may be shorter or longer than we might expect.”
An extraordinary string of scandals have consumed Uber for the better part of the year, including an instance when a leaked dashboard-video showed Kalanick cursing at a driver. Amid all this, Kalanick’s mother died in a boating accident last month.
“For the last eight years my life has always been about Uber,” Kalanick wrote. “Recent events have brought home for me that people are more important than work, and that I need to take some time off of the day-to-day to grieve my mother, whom I buried on Friday, to reflect, to work on myself, and to focus on building out a world-class leadership team.”
“But if we are going to work on Uber 2.0,” he added, “I also need to work on Travis 2.0 to become the leader that this company needs and that you deserve.”
Also on Tuesday, Uber made public the recommendations put forth by a special committee hired to investigate the company’s culture. In a marathon meeting on Sunday, the Uber board unanimously voted to adopt all recommendations in the Holder Report, a representative for Uber told The Atlantic, referring to the report that came from the investigation.
Uber asked Eric Holder, the former U.S. attorney general, to lead the probe after an extraordinary string of scandals at the ridesharing company. The past six months have been enough of a public-relations nightmare for Uber to have raised the question of whether the company can survive—or at least whether its $70 billion valuation will take a hit.
Uber’s recent problems have included a high-profile public boycott, an explosive first-hand account of sexism at the company, revelations of how Uber used secret software to evade law enforcement, and the leaked video of Kalanick’s argument with a driver.
Kalanick’s leave will begin just three months after he pledged to his employees that he would “grow up” and get the “leadership help” he needed, Kalanick may take a leave of absence from the embattled ridesharing company, according to multiple reports. Though the board was expected to discuss Kalanick’s future on Sunday, it’s not clear whether a leave of absence was one of the items formally recommended in the Holder report.
The Holder report chronicles “a frequently chaotic and ‘hostile work environment’ without adequate systems in place to ensure that violations such as sexual harassment and retaliatory behavior were dealt with professionally,” according to Recode. Emil Michael, one of Kalanick’s closest confidants and the second in command at Uber, stepped down on Monday, according to an email to Uber employees obtained by The New York Times. It wasn’t clear from the email whether he resigned or was forced out.
Uber has suffered several high-profile resignations and terminations in recent months. In May, Uber fired Anthony Levandowski, a star engineer and the central figure in a high-profile legal battle over self-driving cars.
His termination was tied to a federal lawsuit in which Waymo—the self-driving car company that spun out of Google—is accusing Uber and Levandowski of stealing its design secrets. An Uber spokesperson told The Atlantic that Uber had been pressing Levandowski to help with its internal investigation of the matter for months, but that he did not meet a deadline set for him. Levandowski could not be reached for comment. He has exercised his Fifth Amendment right against self-incrimination, which played a role in a federal judge’s decision to deny arbitration in the Waymo case.
Uber’s request for arbitration would have allowed the ride-sharing giant to defend itself behind closed doors. Instead, Uber will have to go to trial to answer Waymo’s accusations in what is sure to be an ugly showdown.
Uber Did What!?
The case is poised to be among the biggest legal fights over intellectual property so far this century. The technology at stake is at the center of the emerging and potentially highly lucrative self-driving car industry, and the key players are two of the world’s top tech companies. (Google rebranded its self-driving car project as Waymo in 2016.)
Not only will there be a trial, but Judge William Alsup also referred the case to the U.S. Attorney for investigation of possible theft of trade secrets, a move that suggests the Uber engineer at the center of the case could face criminal charges.
Waymo is accusing the former Google engineer Levandowski of secretly stealing a trove of files from the company before he quit to start his own self-driving truck startup. Uber acquired that company, Otto, for $680 million shortly after it launched last year.
Self-driving cars are arguably the great technological promise of the 21st century. They are in that rare class of technology that might actually change the world. And not just in the way everyone in Silicon Valley talks about changing the world—but really, fundamentally change it. So much so that their dramatic life-saving potential is almost secondary to the other economic, cultural, and aesthetic transformations they would cause.
Those who aren’t able to drive themselves today—people who are blind, for example—would be granted a new level of transportation freedom. Mass adoption of self-driving cars would create and destroy entire industries, alter the way people work and move through cities, and change the way those cities are designed and connected.
To build the technology that prompts all this change is to be in an enormous position of power.
That’s why the race to bring self-driving cars to the masses is so intense. It’s also what makes this particular competition echo other transformative moments in technological history—going all the way back to the Railroad Wars, at least. (Incidentally, there was a different kind of driverless car back then.) “The Wright brothers jump into my brain immediately,” John Leonard, an engineering professor at M.I.T., told me in 2015. “But maybe it’s kind of like a decentralized space race. Like Sputnik, but between the traditional car companies and their suppliers versus tech companies and their startups.”
There’s a lot of money at stake. A lot a lot. We’re talking billions of dollars per year in potential profits, maybe more. All of the major players know this. For some companies, it is a fight to the death. Each one intends to come out on top.
* * *
Waymo (formerly Google)
When Google (now Alphabet) launched its self-driving car program in 2009, it had no competition to speak of. Culturally, the idea of a self-driving car was novel. Even the flying cars in 20th-century science fiction tended to have human drivers. So when Google began to go public with information about the project, in 2010, its level of seriousness about the effort wasn’t yet clear. “Some of these things will turn out to be wildly successful, and others will just fade away,” one investor told The Los Angeles Times at the time, referring to Google’s suite of unusual projects.
Wild success still isn’t a guarantee, but it’s now obvious that Google—which has since spun off its self-driving-car unit into a company called Waymo—is deeply invested in the work it’s doing. Its test fleet is now on public roads in four states: California (since 2009), Texas (2015), Arizona (2016), and Washington state (2016). “We’ve self-driven more than 2 million miles mostly on city streets,” Waymo says on its website. “That’s the equivalent of over 300 years of human driving experience, considering the hours we’ve spent on the road. This builds on the 1 billion miles we’ve driven in simulation in just 2016.”
All that driving and a near-perfect safety record—a reputation that has undoubtedly helped buoy the public’s perception of self-driving vehicles.
Uber
Uber catapulted itself into the self-driving car space in truly Uberesque fashion: With a scandal. In 2015, the ride-sharing giant hired an entire department away from Carnegie Mellon—some 40 robotics experts and engineers, including several top experts in autonomous-driving systems.
Since then, Uber’s commitment to the future of self-driving cars has only intensified. (Consider the business incentive of eliminating human drivers, who get a cut of every ride they give.) In 2016, Uber began testing its self-driving vehicles on public roads in Pittsburgh, it doubled down its own proprietary street-mapping system—ostensibly to reduce reliance on competitors like Google and Apple—and poached Google’s top mapping expert to do so. (He later resigned.) Uber also acquired a fledgling self-driving truck company, Otto, for $680 million in 2016—but more on that in a minute.
Given the talents of its employees and how much venture capital the company has on hand, Uber has emerged as a formidable player in the emerging self-driving car industry. Yet Uber continues to be plagued by controversies.
After a dustup over Uber’s refusal to seek permits for its self-driving cars in California in late 2016, the company changed course and applied for a state testing permit. In February 2017, Waymo filed a federal lawsuit claiming a former Google engineer had stolen self-driving car secrets before leaving the company to found Otto. Waymo says that when Uber acquired Otto, the former Google engineer used the information he allegedly stole to help build a circuit board for Uber’s self-driving car systems. The legal battle is poised to be the first major intellectual-property fight of the driverless car era.
Apple
Apple remains one of the more mysterious and intriguing players in the self-driving car game. On one hand, Apple can’t afford not to pursue this emerging technology if many of its major competitors are. On the other, Apple? A car company? To be fair, though, that’s what people said of Google in 2010. And not all self-driving car companies will manufacturer vehicles themselves; some will just lease out the self-driving software for auto manufacturers.
In April 2017, Apple secured permits to test self-driving vehicles on California roadways. Two weeks later, Bloomberg News published photos of a sensor-equipped Lexus RX450h SUV that emerged from an Apple facility, according to a person who provided the photos to Bloomberg.
For years it was rumored that Apple had a secret self-driving car project in the works. But there have been persistent reports that the project— which according to The Wall Street Journal had hundreds of dedicated employees as of 2016—was plagued by organizational and managerial problems. It wasn’t until December of 2016 that Apple officially made it known that it is working in some capacity on self-driving cars, via a letter to the National Highway Traffic Safety Administration.
“The company is investing heavily in the study of machine learning and automation,” wrote Steve Kenner, Apple’s director of product integrity, “and is excited about the potential of automated systems in many areas, including transportation.”
Other than that, however, Apple has remained characteristically secretive about its work.
Tesla
Tesla wants to bring driverless cars to the market, but it has a markedly different approach than Waymo, which may be its biggest competition. While Google wants to build fully self-driving systems from the ground-up, its critics say this will take too long. In the interest of making everybody safer sooner, Tesla is adding increasingly autonomous systems, bit by bit, to its existing high-end vehicles. But there’s a big debate over which method—fully autonomous versus incrementally autonomous—is actually better for public safety.
Tesla’s CEO, Elon Musk, has said it’s “morally reprehensible” to wait until the technology is advanced enough for complete autonomy. Yet critics of the Tesla approach say that here-and-there semi-autonomous features may present too much of a gray area for today’s drivers to safely navigate. The marketing around Autopilot, the current Tesla system, has arguably left people with the impression that Tesla’s cars are more autonomous than they really are. The very name, Autopilot, certainly suggests it might be okay for human drivers to stop paying attention.
This concern came up again in the spring of 2016, when a Tesla driver who was using the Autopilot feature died in a car accident. At the time, Tesla’s Autopilot feature was in beta mode, meaning the drivers who tested it on public roads were required to acknowledge any risks involved. Federal investigators eventually concluded Autopilot was not to blame in the fatal crash.
Tesla already claims on its website that all of its vehicles “have the hardware needed for full self-driving capability at a safety level substantially greater than that of a human driver.” This is, at best, slightly misleading. Tesla’s hardware may eventually allow for a “full self-driving” system, but it’s definitely not there yet.
Whichever approach to building a truly autonomous car is the right one, Tesla’s sense of urgency is helping to quicken the pace of competition in the driverless-car space.
Legacy automakers
Like Tesla, several legacy automakers are announcing their entry into the driverless-car space with incremental assisted-driving systems. This approach makes sense for them: After all, they already manufacture cars that people can go and buy—something that isn’t true of Apple, Google, or Uber—which means one of the best hopes for legacy carmakers to stay in business is to evolve now rather than attempting to play catchup later (which they may still have to do).
But some legacy companies have gone farther than others. While nearly every major automaker pays lip service to the importance of developing autonomous vehicles, only some have backed up their talk with action. Volvo stands out among the more committed, for instance. In a project Volvo is calling Drive Me, the automaker will put a fleet of 100 driverless cars on the highways in Sweden. (As with tests by Google and Uber on public roads in the United States, humans will sit behind the wheel, ready to take control of the vehicles if needed.) In March 2017, Toyota unveiled its first self-driving car  prototype. The car came out of Toyota’s artificial intelligence research institute, which it launched with a $1 billion investment in 2015.
Ford Motor Company has also made its efforts increasingly visible. Ford announced in August 2016 that it plans to be “mass producing vehicles capable of driving fully autonomously” by 2021. Six months later, the automaker announced a $1 billion investment in the software company Argo AI, a software startup specializing in self-driving cars. (“Ford is the majority stakeholder in Argo AI, but we are structured to operate with substantial independence,” Argo AI says.) “This work is easily the most challenging of my career, and it may be the most important, as well,” wrote Chris Brewer, the head of engineering for Ford’s Autonomous Vehicle Development department, in a blog post in March 2016. “Come to think about it, who better to develop a self-driving car than a company that’s been making cars for more than 100 years?”
As with Ford and Argo AI, several other tech firms and automakers are forging partnerships. Chrysler and Google announced in May 2016 that they would team up to make a driverless minivan, while Volvo and Uber announced a partnership in August 2016.
Newcomers
We should expect to see more startups in the self-driving car space in the years to come. One example is Drive.ai, which launched in August 2016 and is creating deep-learning software for driverless cars.
There will be others. Chris Urmson, the longtime head of Google’s driverless car initiative, left the company in August 2016, at a time when the project seemed to be shedding several key players. In December 2016, the technology-focused news website Recode reported that Urmson is starting his own self-driving-cars venture.
Technology history tells us that the first company to build a technology is not always the company that ends up making a windfall. That may well be the case in the realm of autonomous vehicles.
There are many uncertainties in all this, but one thing is clear: The cultural space occupied by the automobile is undergoing rapid, radical transformation. There are sure to be big winners and losers along the way.


A decade ago, for the most part, phones were phones. Computers were computers. Cameras were cameras. Portable music players were portable music players. The idea that the future of the computer would be a phone, or vice versa, wasn’t merely absurd. It just wasn’t how people thought about consumer technology. At all.
So when the first iPhone was unveiled in 2007, plenty of people assumed it wouldn’t change the world. (“Touch-screen buttons? BAD idea. This thing will never work,” as one naysayer put it at the time.)
To those who had been watching Apple since the 1980s, however, shrinking computers and videophones seemed to be always just tantalizingly out of reach, emblems of a future that would, fingers crossed, eventually arrive.
But when? By 1995, even though Apple’s laptops had dipped to a svelte six pounds, and the transformative power of the internet was becoming apparent, the next great iteration of the web was barely imaginable. Today’s mobile web, the one that would be ushered in by smartphones, was still out of reach. But there were hints of what was to come.
Apple has always been fond of dreaming up hardware and software from a not-too-distant future, and there are glimmers of the iPhone in Apple’s history since long before the rumors about the device were taken seriously in the early 2000s. More than a decade before the smartphone was unveiled, Apple shared with the computing magazine Macworld a semi-outlandish design for a videophone-PDA that could exchange data. (Smartphones eventually made the PDA, or personal digital assistant, obsolete.)
The prototype for the device, published in the May 1995 issue of the magazine, is something of a missing link between the Newton and the iPhone—though still more parts the former than the latter. The Newton was Apple’s lackluster PDA, first released in 1987, 20 years pre-iPhone. The Newton may have been ahead of its time in some ways; but it also failed because it was pricey and didn’t work particularly well. (In 1993, one pithy New York Times writer memorialized his attempts to write on the device this way: “This is being writings a worth it takes a while before the handed tiny red floor is footprint. Signed, Bite (poof!) Beers (poof!) been (poof!) I sits.”)
The Macworld prototype combined a PDA and a videophone, complete with handset, and visualized a future in which the devices would be able to exchange data. Naturally, because this was 1995, the concept also included a CD drive and a stylus.
The design was made public as part of a collection of several made-up Apple products, all published in that same 1995 issue of Macworld. The spread is charming in retrospect, but also revealing for how it signals a shift in the way Apple was changing the way people thought about the intersection of design and technology. Flipping through the old issue of Macworld this week made me think of a conversation I had last year with  Robert Brunner, the industrial designer who worked for many years at Apple and now runs his own design studio.
“When I started out in my career, design was seen as a necessary evil, especially in relation to technology,” Brunner told me at the time. “It moved into this phase where all of the sudden people saw design as a corporate identity thing, like ‘all of our products need to look alike.’ In the early 1990s, it moved into innovation for innovation’s sake. And then there started being this shift, driven somewhat by Apple, where people began to understand that design was what made them want your technology to be part of their lives.”
Design isn’t just the aesthetic quality that makes a device beautiful or identifiable by brand, in other words. It is a core part of how the technology works. Brunner attributes that cultural change largely to Jony Ive, Apple’s chief design officer, and his team’s work over the last 10 years. “Jony and his team have changed the way people see design,” Brunner told me.
You can see the stirrings for this change in attitude across the pages of the 1995 Macworld spread. Computers are compared to “stylish furniture,” and to “strong personal statement piece[s]” of art, namely Richard Sapper’s minimalist, counterweighted Tizio lamp.
In the caption for one imagined computer of the future—a curved and dynamic prototype that was designed to swivel on a four-footed pedestal “so you can get at the floppy disk drive on one side and the CD-ROM drive on the other side”—Macworld described the change that was taking place in the design world explicitly, because at the time it still needed to be said: “The emphasis of this radical approach is how you interact with the Mac, not on the Mac itself.”
A new concept for the Newton made an appearance in Macworld, with Apple adding splashes of color that would eventually reach the market with the iPhone’s colorful, plastic 5c models. “For a personal device, the black Newton MessagePad sure lacks personality,” Macworld wrote back in 1995. “But these Newton designs have plenty. The yellow Sports Newton borrows Sony’s Sports Walkman idea—a ruggedized high-visibility version for people on the go. If you carried a purple Game Freak, people would have no doubt that you’re a serious video-game player. … The MessagePad could be given a custom look for a particular company or application.”
But at the time, most of these computer-of-the-future designs were seen as impractical—too confusing, too far outside the realm of what was technologically possible (or even desirable) for consumers at the time.
Looking back now, two decades since the Macworld feature and one decade since the iPhone reached the market, it’s clear that Apple’s smartphone has forever altered industry standards for electronics design. Losing the keyboard and prioritizing software over hardware was crucial to the iPhone’s success—as was playing up the phone in iPhone to distance the device from the failed Newton that preceded it. “I don’t want people to think of this as a computer,” Steve Jobs, the former Apple CEO, told John Markoff, the veteran technology writer for The New York Times, when Jobs introduced the iPhone in January 2007. “I think of it as reinventing the phone.”
Where Apple’s past failures had always hummed with untapped potential, as one newspaper columnist described the Newton in 1993, the iPhone elegantly and boldly realized it. The device would go on to dramatically reconfigure social norms and behaviors. It changed how people socialize, how people work, how people shop, how people seek information, and how designers think about technology. Gorgeous design is now mainstream. “With the escalation of average design—average is now pretty good, right? So you have to even look harder for what’s really good,” Brunner told me.
“I think for us we constantly have to put more and more pressure on ourselves to be original and meaningful and not just derivative,” he added. “But there’s something unique about American design culture—and, in particular, Silicon Valley design culture—that really drives that originality. I think there’s something in the water here that drives people to always push to do something different, beyond the status quo.”
Even in 1995, Apple’s futuristic concepts offered a glimmer of what might come to pass, Macworld wrote at the time. “Although these prototypes won’t become real products, you can expect many elements to show up in real Apple products of the future,” the magazine said.
Macworld was right. But as we now know, the real products of the future were far better than even Apple’s wildest dreams just 22 years ago.


Updated on June 12 at 11:30 a.m. ET
One of the best predictors of whether people install solar panels on their house isn’t their age, their race, their level of income, or their political affiliation.
It’s whether their neighbors did it first.
This finding has been shown repeatedly across space and time, including in California, Connecticut, Germany, Switzerland, and the United Kingdom. “It happens at the street level, it happens within zip codes, it happens within states. It seems to be a common feature of human decision-making that crosses many boundaries,” says Kenneth Gillingham, a professor of economics at Yale University whose study helped establish the finding.
On Monday, Google will put the finding into practice with Project Sunroof, its free online tool that aims to make it easier for people to obtain and use home solar panels. Project Sunroof will now not only inform users how much sun hits their roof, or how much solar panels would save them per month, but also which of their neighbors have taken the plunge first.  
Project Sunroof was launched in 2015 by Carl Elkin, an engineer at Google who had worked on local solar-installation campaigns in Massachusetts. It now provides data for 60 million homes across the United States that it has already assessed with its algorithms.
For the past two years, Project Sunroof has walked people through all the information-gathering steps of installing solar panels: After you tell it where you live, its algorithms estimate how much solar energy falls on your roof, calculate how much solar panels would reduce your electricity bill, and deliver estimates from local installation firms like Solar City.
It can also walk you through similar steps if you’re interested in leasing or borrowing panels. “It highlights that, for many people, solar is often free. In many cases, including for my house, solar is better than free,” Elkin told me last week.
Now—in a nod to the powerful peer effects of solar power—it will also show you which of your neighbors have already installed panels. In its map view, Project Sunroof will show a red dot over any home or structure that appears to have rooftop solar.
“People want to know: ‘What if there’s some hidden gotcha in the contract?’—and usually there isn’t. ‘Does this work for other people like me? Is solar really viable in my neighborhood?’” Elkin says. “You can zoom around through your town and understand how common solar is in your neighborhood. And many people have found: Wow, there is a lot more solar in my neighborhood than I’d realized.”
Google created the data for this feature in-house, training a machine-learning algorithm on the common appearance of rooftop solar panels and then letting it loose on the cities and towns that Project Sunroof already covers. Right now, the company has analyzed installations on about 60 million buildings in the United States; it hopes to get to the remaining 40 million buildings in the next few years. The methodology doesn’t seem to be perfect yet—I noticed some rooftop solar installations in my own neighborhood that the algorithms missed—but it seems to identify most of them.
“I think the idea is a really great one,” says Gillingham, who has previously talked with Google about his research but did not know they were working on this feature.
His current research has found that people are even more likely to install solar panels if they can see their neighbors’ installations from the street—suggesting that day-to-day visibility, and not, say, word-of-mouth or local marketing efforts, is what nudges people to look into solar power. The new Project Sunroof feature “tells you that people nearby have installed solar panels even if you can’t see them from the road,” he says.
Gillingham did share some privacy concerns about the feature. State solar registries often hide address-level data, and they only share it with academic researchers confidentially. “However, if you go on Google Maps or Google Earth, you can pretty easily see the installation in pretty much all cases,” he says.
When I asked about privacy, Elkin also pointed out that most of this is already in satellite imagery. He also said that Google only stores its installation data at the latitude-longitude level—in other words, it does not resolve it into addresses—and it has no plans to share it with the public or other companies.
“We’re not looking at individuals, we’re looking at buildings,” he told me. “And we’re not publishing a list of these buildings.”
The company has also created a larger map that shows the popularity of solar panels by census tract. Like other studies into the social adoption of solar panels, it has not found a strong correlation between income and installation.  
And privacy concerns aside, Gillingham praised the feature for increasing the visibility of renewable energy. “It creates a social norm around solar panels,” he says. “When many people have solar panels around you, it’s a normal thing to do. You’re not going out on a limb by having a company come out and look at your rooftop.”
Elkin hoped for the same goal. Speaking to me, he almost seemed to conjure an image of Americana Solara: a two-story house, a white-picket fence, a tree and tire swing, and rooftop solar panels. “We want people to realize solar is absolutely part of the fabric of American life,” he says. ‘Out of these many houses, each saving money, comes one solution to the environmental problem. Out of many, one. That’s a very American idea.”


For nearly three hours on Thursday, many Americans turned their attention to the engrossing, absorbing spectacle that was James Comey, the former FBI director, giving his first public remarks since President Donald Trump fired him one month ago. One of the nation’s top law-enforcement officials was poised to divulge damaging information about a president accused of trying to influence a federal investigation into his campaign’s ties to a foreign government—and the people weren’t going to miss it.
The room hosting the congressional hearing could seat less than 100 people. Live-streams, broadcast from cable-news channels, regular television networks, and numerous social-media sites and other places online, accommodated many more, turning computer screens into front-row seats to the action. They also provided the public with something that, in the quiet moments before the opening gavel struck, felt almost unusual in the noisy, cluttered, and deeply divided environment that has come to be the norm in political news.
News consumers would be watching alongside reporters, rather than waiting for a handful of national news organizations to publish bombshell scoops. A live-stream offered the public an unedited, unfiltered, and uninterrupted space, a drop of transparency in a sea of tweets and “fake news.”
Not accounting for any spin coming from the people inside the room, what you saw was what you got.
Viewers brought their own biases to the live-streams, of course, their minds perhaps made up long ago about who’s to blame for the current state of affairs. The same live-stream won’t look the same to Fox News viewers and MSNBC fans. But people’s understanding of newsworthy events, as the country has learned over and over in the last year, can become influenced by the smallest forces of news dissemination. Research has shown readers’ perceptions of news stories can be distorted after reading the comments, home to the kind of caustic discourse and debate that has increasingly moved into other parts of media. On cable networks, anchors and commentators chime in, showing viewers real-time action through their own, sometimes narrow, lens. On Twitter, users fire off commentary atop clips and GIFs. A small chyron, the ticker of text at the bottom of a screen, can make a huge difference:
two worlds pic.twitter.com/b5EVMg6Cbr
Those two chyrons refer to a line of questioning from Jim Risch, a Republican senator from Idaho. Risch asked about a conversation, described in memos written by the former FBI director and leaked to the press, in which Comey said Trump asked him to shut down the FBI’s investigation of the president’s former national security adviser, Michael Flynn, and his alleged ties to Russian officials. “I hope you can let this go,” Trump said, according to the memo.
“Now, those are his exact words, is that correct?” Risch asked Comey.
“Correct,” Comey replied.
“He did not direct you to let it go?” Risch followed up.
“Not in his words, no,” Comey said. Risch pressed him further.
“I mean, this is a president of the United States with me alone saying I hope this,” Comey said. “I took it as, this is what he wants me to do. I didn’t obey that, but that’s the way I took it.
“You may have taken it as a direction but that’s not what he said,” Risch said.
Comey: “Correct.”
Comey appears to suggest that Trump wanted him to end the inquiry into Flynn, but didn’t explicitly direct him to do so. The live-stream told that story, confusing as it might be, but the networks, in their chyrons, cherry-picked what parts to highlight; in the process, they seemed to offer their respective viewers two completely different interpretations. The Washington Post scraped all the chyrons shown during the hearing by three networks—MSNBC, CNN, and Fox—and published them alongside each other, revealing the alternate realities among them.  
Perhaps the quietest place to watch the Comey hearing was on C-SPAN, the no-frills network that has spent decades solely streaming government proceedings, and especially congressional hearings. C-SPAN has its own set of anchors and guests on its morning show, but they get out of the way when the action begins on Capitol Hill, at the White House, and elsewhere. C-SPAN is indeed a provider of news, but even Trump could hardly call it “dishonest.” A Washington Post article from 1989 marking C-SPAN’s 10th anniversary describes the network as “America’s town hall,” a nod to the nature of live-streaming that puts some of the power of interpretation into the viewers’ hands first.
Live-streaming technology has been around since the 1990s, but the explosion the of internet and social media has propagated it to all corners of the world, bringing people closer to a broad spectrum of experiences. It has galvanized a stronger sense of immediacy not found with regular-old television, sometimes with distressing and confusing implications. The technology can capture such precious moments like the birth of a giraffe in New York, or a wedding that guests can’t attend. It can also capture some of the most harrowing aspects of humanity, like violent police shootings, a murder of an innocent bystander, or a teenager’s suicide. Live-streams have one thing in common; as they happen, there’s usually no one weighing in. It’s what comes after—the context, the outrage, the official statements, the attempts to understand or mislead—that can alter viewers’ perception of the footage.
After the Comey hearing was over, the live-streams went black. Journalists returned to pumping out analysis of the testimony to decipher and dissect for their viewers and readers, in their sometimes separate realities, the meaning of what they witnessed. C-SPAN, naturally, went back to streaming less interesting hearings. The other streams, the noisier ones tinged with filters and commentary, flowed on.


Binky is an app that does everything an app is expected to do. It’s got posts. It’s got likes. It’s got comments. It’s got the infinitely scrolling timeline found in all social apps, from Facebook to Twitter, Instagram to Snapchat.
I open it and start scrolling. Images of people, foods, and objects appear on and then vanish off the screen. Solar cooker. B.F. Skinner. Shoes. Marmalade. Sports Bra. Michael Jackson. Ganesha. Aurora Borealis. These are “binks,” the name for posts on Binky.
I can “like” a bink by tapping a star, which unleashes an affirming explosion. I can “re-bink” binks, too. I can swipe left to judge them unsavory, Tinder-style, and I can swipe right to signal approval. I am a binker, and I am binking.
There’s just one catch: None of it is real. Binky is a ruse, a Potemkin-Village social network with no people, where the content is fake and feedback disappears into the void. And it might be exactly the thing that smartphone users want—and even need.
* * *
It’s strange to think of content as optional. When Bill Gates declared that “Content is King” in 1996, he meant that digital content creators would make more money online than computer manufacturers. Gates cited television as a precursor: It was an invention that created many industries, but broadcasters—the content creators—were the long-term winners on TV.
Gates was right and wrong. Content, from e-commerce to social media, did drive  huge profits in the two decades since. But equipment also produced enormous wealth—just look at Apple. With the rise of Facebook, Google, Uber, Microsoft, Amazon, and others, content stopped being a name for ideas alone and started signifying a confluence of machines, services, media, and ideas. This is the phenomenon some nickname #content (as a hashtag), implying that the purpose of ideas is to fill every moment with computational engagement. Technology’s effect on ordinary life is always more important than the ideas its content carries.
Marshall McLuhan was the best theorist of media as mechanisms for behavior rather than channels for ideas. His famous quip “the medium is the message” was meant to deemphasize content in favor of the media forms that make it possible. For McLuhan, the meaning of individual books, television programs, newspaper articles, movies, and software programs is just a distraction. More important: how those media change the way people think and behave in aggregate. The book, for example, creates a society for which knowledge is singular, certain, and authoritative thanks to the uniformity of print.
The smartphone’s effects have evolved and changed. When I wrote about the iPhone shortly after its launch, I called it the geek’s Chihuahua: a glass-and-metal companion that people could hold, stroke, and pet—a toy dog for the tech set. Some years later, after games, apps, and social media made smartphone use compulsive, I dubbed the device the cigarette of this century: a source of obsessive attention that, like smoking, brings people together in a shared dependency whose indulgence also produces the calming relief of new data.
It doesn’t make sense to talk about the meaning of cigarettes or Chihuahuas. Their meaning is the pattern of their use. That’s the thing about content: Its form and meaning matters less than how it changes people’s behavior. And when it comes to smartphones, seeing and touching them is far more important than processing the meaning they deliver.
* * *
Binky eviscerates meaning by design. Every bink on Binky is a labeled image, chosen randomly and generated endlessly. Liking a bink does nothing. Swiping or re-binking sends binks nowhere. The comments are my favorite: A keyboard appears on which to type them, but each key-tap reveals a whole word in a pre-generated comment. Words, tags, or emoji continue appending until I stop typing. “This looks amazing! #harlemshake #wordsToLiveBy #rofl,” or “I dunno, I like this but it’s problematic  😹😜😁👾😱🎃😣😡.”
Binky is a social network app with no network and no socializing. And yet, Binky is not just as satisfying as “real” social apps like Twitter or Instagram, but even more satisfying than those services. Its posts are innocuous: competent but aesthetically unambitious photos of ordinary things and people. Should binkers feel the urge to express disgust at Linus Paulding or Lederhosen, they can swipe left, and Binky accommodates without consequence. And the app doesn’t court obsession by counting followers or likes or re-binks.
Dan Kurtz, the game developer and improv actor who created Binky, tells me that the idea for the app arose partly from his own feelings after reading through the current updates on Facebook or Twitter while waiting for a train. “I don’t even want that level of cognitive engagement with anything,” he explains, “but I feel like I ought to be looking at my phone, like it’s my default state of being.” Kurtz wondered what it would look like to boil down those services into their purest, most content-free form. This is what people really want from their smartphones. Not content in the sense of quips, photos, and videos, but content as the repetitive action of touching and tapping a glass rectangle with purpose and seeing it nod in response.
Binky also offers a new take on the smartphone’s effects, McLuhan-style. Some of the toy-dog aspects of mobile computing remain, along with the compulsive ones, too. But the novelty of touching the smartphone has long since ended, and the angst of its compulsive use is universally acknowledged. Those habits are here to stay, like it or not.
Standard smartphone fare inspires users to create content whose publication accrues value for the tech titans that operate walled-garden services. Those businesses transform that aggregated attention into revenue and stock value in turn. Meanwhile, the pleasure and benefit of those services dwindles by the day, as conflict and exhaustion suffocate delight and utility.
Binky offers a way to see and tolerate that new normalcy. What if the problem with smartphones isn’t the compulsion to keep up with the new ideas they deliver, but believing that the meaning of those ideas matters in the first place? Binky offers all the pleasure of tapping, scrolling, liking, and commenting without any of the burden of meaning.
The app frames its intervention with humor and mockery. Its name is a trademark for baby pacifiers, an image that also adorns the app’s icon. Calling it “Binky” implies a global infancy among apps, but also a legitimate comfort thanks to Binky’s succor. And Kurtz initially conceived of the app in a Comedy Hack Day mini-hackathon held by Cultivated Wit, a firm that produces, well, content—videos and events and software and the like. Forged from games and comedy, Binky might look like an ironic joke to some.
“Is a baby pacifier just a parody?” Kurtz retorts when I press him on the matter. It’s a good point; something that replaces another isn’t always a joke. He reminds me of my own ironic app, which, to my delight, he cites as an inspiration: a game called Cow Clicker that boiled down Facebook games to their purest form like Binky does social apps. In both cases, irony offers an in-road for some but burns out fast. Deliberate use always wins.
On that front, Kurtz makes his faith in the app’s earnest utility clear. “Look, all we want from our apps is to see new stuff scroll up from the bottom of the screen,” the Binky website reads. “It doesn’t matter what the stuff is.” That’s no gag; it’s an incisive elucidation of why people want to handle their smartphones so often.  By sparing the mental and emotional effort of taking in content and spitting back approval and commentary, Binky makes it possible to experience the smartphone as such, as a pure medium for its behavior rather than a delivery channel for social-media content.
That’s also where apps start, it turns out. Kurtz wanted to learn iOS programming, and he reasoned that the best approach would be to incorporate all the standard interface widgets. Binky was the result. What’s an app without content? Pure, unadulterated tapping and scrolling through the hollowed-out interfaces that all apps now share.
* * *
There’s a use of cigarettes beyond their chemical effects. Smoking gives people something to hold and something to do with their hands. McLuhan called it poise. And smartphones offer something similar. At the bus stop, in the elevator, in front of the television, on the toilet, the smartphone offers purpose to idle fingers. To use one is more like knitting or doodling than it is like work or play. It is an activity whose ends are irrelevant. One that is conducted solely to extract nervous attention and to process it into exhaust.
There have been attempts to cure the ills of smartphone compulsion. Fidget cubes and spinners offer a recent example, doodads that offer mechanical intrigue that might, some users hope, distract them from the draw of the smartphone. But these devices fail to cop to the smartphone’s victory in standardizing the mechanics of idle effort. The tapping, the scrolling, the liking, the #content, even. Those must be preserved. Binky offers an unexpected salve: a way to use a smartphone without using one to do anything in particular. Isn’t that all anyone really wants?


Perhaps the aphorism should be changed to “In Google, veritas.” Where do people go with their most intimate worries, thoughts, and fears? Not the nearest water cooler or humblebrag app. More likely, they’ll seek comfort in the relative privacy of a search box.
Seth Stephens-Davidowitz, a former data scientist at Google, used his data-analysis skills to learn what was really on Americans’ minds. The result, a new book called Everybody Lies, shows how the terms and questions people type into search engines don’t at all match what they claim on surveys.
“So for example,” he told me recently, “there have historically been more searches for porn than for weather.” But just 25 percent of men and 8 percent of women will admit to survey researchers that they watch porn.
In addition to Google, some of his research comes from tape-recorder (rather than self-report) studies, which can provide a similar truth-serum effect.
I recently spoke with Stephens-Davidowitz about some of the most surprising findings from his book, which spans data on gender norms, prejudice, and romance. We focused on the search data about sex and relationships, because who are we kidding. An edited version of our conversation follows:
Olga Khazan: Speaking of porn, I was wondering if you could talk about pornography featuring violence against women. What's surprising about who looks for that, and what might that tell us?
Seth Stephens-Davidowitz: It’s a big theme of pornography, but I think the somewhat surprising thing is that it’s far more popular among women than men. It’s one of the most popular genres of pornography for women. Just about every search that is looking for violent porn is roughly twice as common among women than men.
Of course the danger is that somehow people will hear this and they’ll think that somehow this makes rape a less horrific crime, which it doesn’t. It’s just a fantasy, of course it doesn’t mean that they want that in real life.
Khazan: To me that suggested that there’s a really big distinction between fantasy life and real life, as far as people's sexual desires.
Stephens-Davidowitz: Well it’s kind of similar also to horror movies, which are also [popular] among women. I don't think women want to be kidnapped in real life, but many women enjoy watching movies featuring kidnappings.
Women also search for a lot of lesbian porn, even women who do not consider themselves lesbians.
Khazan: So let’s say you stop watching porn and actually go on a date with someone. How can a man tell if a woman is interested in him, and vice-versa?
Stephens-Davidowitz: This is a study where researchers gave tape recorders to men and women, heterosexual men and women, who are on speed dates. Then they measured whether the men and women wanted to go on a second date. Then they said: What words do men and women use on first dates that suggest that they want to go on a second date, or that can improve the chances that a partner wants to go on a second date?
For the women, a woman frequently signals interest by talking about herself using the word “I” a lot. A man signals interest by talking in a deep monotone voice. A woman signals disinterest by using hedge words, such as “sort of,” “kind of,” or “probably.” A man can increase the odds of a woman wanting a second date by laughing at her jokes or showing support, such as saying “that must have been difficult” or “that sounds tough.”
Of course that's not rocket science, but I think a lot of men probably still need to read it. A woman can increase the odds of a man wanting a second date by talking about herself a lot, by using the word “I.” That kind of goes against conventional wisdom. I think a lot of women think that they shouldn't talk too much about themselves. But, men seem to like when a woman opens up on a first date.
Khazan: Alright, and once they’ve been dating a while … what's the number one search complaint about boyfriends?
Stephens-Davidowitz: That my boyfriend won't have sex with me.
Khazan: You said that's more common than “my girlfriend won't have sex with me,” right?
Stephens-Davidowitz: Yeah, it's about twice as common. That doesn't mean that twice as many boyfriends are refusing sex, relative to girlfriends. It may be that when a boyfriend doesn't want sex, women are more likely to turn to Google, because it's more surprising. Because men in popular culture are supposed to want sex all the time. But, I think this data does show that men avoiding sex is probably more common than is traditionally thought.
Khazan: Why are they so reluctant to have sex? What are men's biggest insecurities about their bodies?
Stephens-Davidowitz: Men tend to be insecure about the size of their penises. It definitely wasn't too surprising. It was surprising the degree of it. I estimate that men ask more questions about their penis than any other body part. Men's top concern about the aging process is not their blood pressure, cholesterol, or potential memory problems. It's whether their penis is getting smaller.
Women don't usually search about their partner's penis. When they do, they're about as likely to complain that it's too big and hurts as that it's too small.
Khazan: Do men start worrying about actual health issues as they get older, or is that pretty consistent?
Stephens-Davidowitz: We don't know exactly. You don't know the age of a searcher for sure.
Khazan: Okay, what about women? What are they concerned about?
Stephens-Davidowitz: I think the main insecurity, and this did surprise me, I didn’t know about it at all, was vaginal odor. That takes up a good percentage of women's questions about their genitals. I think there's a lot of value in knowing this information because this isn't really talked about in most sex ed classes, but there clearly is a fairly widespread paranoia among many women, particularly younger women, around odors.
So it clearly is something that should be talked about. What's normal and what’s maybe a cause for concern? It's a big issue that we didn't know about, because it's a little hush hush because it's embarrassing to a lot of people. But because people tell Google everything, now we know how widespread this insecurity is.
Khazan: And do men search for, "I don't like the way my girlfriend's vagina smells"?
Stephens-Davidowitz: Yeah, they do. This is kind of humorous, they're concerned that it smells like condoms or another man’s semen. Because [that, in their minds, means] she may be cheating on them.
Khazan: Despite all this insecurity and worries about smells, how often do people actually have sex? What's the disparity between how much they say they have sex and how much they actually do?
Stephens-Davidowitz: They have a lot less sex than they say they do. The way I studied this is I looked at condom data. The General Social Survey asks people how frequently they have sex, whether it's heterosexual or homosexual sex, and whether they use a condom. You do the math. Heterosexual women say they use 1.1 billion condoms every year in heterosexual sex. Men say they use 1.6 billion condoms in heterosexual sex, but you know that someone's lying. So who’s lying?
Only 600 million condoms are sold every year in the United States. Some of them [are used by] gay men and some of them thrown out. They're exaggerating how frequently they use a condom. This doesn't mean that they are lying about how frequently they have sex. They may just be lying about how frequently they use protection when they do have sex, but if you look at how frequently American women of fertility age say they have sex without using any contraception, if they really were having that much unprotected sex, there would be more pregnancies every year in the United States. I think everybody in surveys exaggerates how frequently they have sex, because in today's culture there is a lot of pressure to have a lot of sex and to not admit if you're having not that much sex. For both men and women, there is a pressure to exaggerate.
Khazan: Another thing that I thought was interesting was that “Is my husband gay?” is a more popular search term than “Is my husband cheating?” Why is that?
Stephens-Davidowitz: “Is my husband gay?” is most common in states where it's hard to be gay, states like South Carolina and Mississippi and Tennessee. I think some of the husbands are gay in those states. Also, the percentage of porn searches that are for gay porn is much higher in these states than the percentage of men who say they’re gay.
So I think it is true that in [places like] Mississippi, South Carolina, and Tennessee, there is a risk of men being gay. That said, I think that women are probably a little too concerned that their husband may be gay. I think there are 10 times more searches for “Is my husband gay?” than “Is my husband depressed?” But, there are a lot more depressed men married to women than gay men married to women.
I think it goes back to how there's not that much sex happening in the United States and there are a lot of sexless marriages. It may be that many women in a sexless marriage, their first thought is, “Oh he must be gay.” Which probably isn't usually the case. There are lots of other reasons a man might not want to have sex.
Khazan: It’s a little conceited of us. “Oh, he must be gay.”
Stephens-Davidowitz: Yeah, well, I probably do the same thing. Anytime a woman rejects me, I'm just like, “She's a lesbian.” Which is not really true probably, but I think it's a little bit of a defense mechanism.
It's kind of a weird contrast. On the one hand you see this enormous insecurity online—an almost needless insecurity. But then you have the “Is my husband gay?” as soon as he doesn't want sex. Which is a defense mechanism.
Khazan: Did you have any takeaways or big insights about Americans’ personal lives that struck you when you were done researching this?
Stephens-Davidowitz: I think there's two. One is depressing and kind of horrifying. The book is called Everybody Lies, and I start the book with racism and how people were saying to surveys that they didn't care that Barack Obama was black. But at the same time they were making horrible racist searches, and very clearly the data shows that many Americans were not voting for Obama precisely because he was black.
I started the book with that, because that is the ultimate lie. You might be saying that you don't care that [someone is black or a woman], but that really is driving your behavior. People can say one thing and do something totally different. You see the darkness that is often hidden from polite society. That made me feel kind of worse about the world a little bit. It was a little bit frightening and horrifying.
But, I think the second thing that you see is a widespread insecurity, and that made me feel a little bit better. I think people put on a front, whether it's to friends or on social media, of having things together and being sure of themselves and confident and polished. But we're all anxious. We’re all neurotic.
That made me feel less alone, and it also made me more compassionate to people. I now assume that people are going through some sort of struggle, even if you wouldn't know that from their Facebook posts.


President Donald Trump wants to cut a budget the Bureau of Land Management uses to care for wild horses. Instead of paying to feed them, he has proposed lifting restrictions preventing the sale of American mustangs to horse meat dealers who supply Canadian and Mexican slaughterhouses.
Horse meat, or chevaline, as its supporters have rebranded it, looks like beef, but darker, with coarser grain and yellow fat. It seems healthy enough, boasting almost as much omega-3 fatty acids as farmed salmon and twice as much iron as steak. But horse meat has always lurked in the shadow of beef in the United States. Its supply and demand are irregular, and its regulation is minimal. Horse meat’s cheapness and resemblance to beef make it easy to sneak into sausages and ground meat. Horse lovers are committed and formidable opponents of the industry, too.
The management of wild horse herds is a complex issue, which might create difficulty for Trump. Horse meat has a long history of causing problems for American politicians.
* * *
Horses originated in North America. They departed for Eurasia when the climate cooled in the Pleistocene, only to return thousands of years later with the conquistadors. Horses became a taboo meat in the ancient Middle East, possibly because they were associated with companionship, royalty, and war. The Book of Leviticus rules out eating horse, and in 732 Pope Gregory III instructed his subjects to stop eating horse because it was an “impure and detestable” pagan meat. As butchers formed guilds, they too strengthened the distinction between their work and that of the knacker, who broke down old horses into unclean meat and parts. By the 16th century, hippophagy—the practice of eating horse meat—had become a capital offense in France.
However, a combination of Enlightenment rationalism, the Napoleonic Wars, and a rising population of urban working horses led European nations to experiment with horse meat in the 19th century. Gradually, the taboo fell. Horses were killed in specialist abattoirs, and their meat was sold in separate butcher shops, where it remained marginalized. Britain alone rejected hippophagy, perhaps because it could source adequate red meat from its empire.
America also needed no horse meat. For one part, the Pilgrims had brought the European prohibition on eating horse flesh, inherited from the pre-Christian tradition. But for another, by the 1700s the New World was a place of carnivorous abundance. Even the Civil War caused beef prices to fall, thanks to a wartime surplus and new access to Western cattle ranges. Innovations in meat production, from transport by rail to packing plants and refrigeration, further increased the sense of plenty. Periodic rises in the price of beef were never enough to put horse on the American plate.
Besides, horse meat was considered un-American. Nineteenth-century newspapers abound with ghoulish accounts of the rise of hippophagy in the Old World. In these narratives, horse meat is the food of poverty, war, social breakdown, and revolution—everything new migrants had left behind. Nihilists share horse carcasses in Russia; wretched Frenchmen gnaw on cab horses in besieged Paris; poor Berliners slurp on horse soup.
But in the 1890s, a new American horse meat industry arose, if awkwardly. With the appearance of the electric street car and the battery-powered automobile, the era of the horse as a transportation technology was ending. American entrepreneurs proposed canning unwanted horses for sale in the Old World, paying hefty bonds to guarantee they wouldn’t sell their goods at home. But Europe had higher standards and didn’t like the intrusion of American meat onto its home market. U.S. aversion to regulation had led to food scares and poisonings. When French and German consuls visited a Chicago abattoir suspected of selling diseased horse to Europe, opponents tried to smear the U.S. Agriculture secretary, who had previously intervened. By 1896, the fledgling industry was faltering: Belgium barred U.S. horse meat, Chicagoans were rumored to be eating chevaline unwittingly, and the price of horses had fallen so drastically that their flesh was being fed to chickens because it was cheaper than corn.
In 1899, horse meat was dragged into one of the highest-profile food scandals of the century: the notorious Beef Court investigating how American soldiers fighting in the Spanish-American War ended up poisoned by their own corned meat. Many speculated wrongly that the contaminated beef was in fact horse meat. The first decade of America’s horse meat industry had been an unprofitable, ill-regulated disaster for the country’s reputation. The new regulations put in place in the 1906 Pure Food Act could not reverse this overnight.
* * *
When beef prices rose as canners shipped it abroad during World War I, Americans finally discovered horse steak. By 1919, Congress was persuaded to authorize the Department of Agriculture to provide official inspections and stamps for American horse meat, although as soon as beef returned after the war, most citizens abandoned chevaline.
The end of the war meant another drop in demand for range-bred horses no longer needed on the Western Front. A dealer, Philip Chappel, found a new use for them: Ken-L-Ration, the first commercial canned dog food. His success attracted perhaps the first direct action in the name of animal liberation: A miner named Frank Litts twice attempted to dynamite his Rockford, Illinois packing plant.
During World War II food shortages, horse meat once again found its way to American tables, but the post-war backlash was rapid. “Horse meat” became a political insult. “You don’t want your administration to be known as a horse meat administration, do you?” the former New York Mayor Fiorello La Guardia demanded of his successor William O’Dwyer. President Truman was nicknamed “Horse meat Harry” by Republicans during food shortages in the run up to the 1948 “Beefsteak Election.” In 1951, reporters asked if there would be a “Horse meat Congress,” one “that put the old gray mare on the family dinner table.” When Adlai Stevenson ran for president in 1952, he was also taunted as “Horse meat Adlai” thanks to a Mafia scam uncovered in Illinois when he was governor.
Although work horses vanished by the 1970s and mustangs were finally under federal protection, the growing number of leisure horses led to another surge in horse slaughter. The 1973 oil crisis pushed up the price of beef and, inevitably, domestic horse meat sales rose. Protestors picketed stores on horseback, and Pennsylvania Senator Paul S. Schweiker floated a bill banning the sale of horse meat for human consumption.
But once again the bubble burst. Competition sent beef prices into freefall. Even poor Americans didn’t need to buy the “poor man’s beef,” so U.S. manufacturers continued to export horse meat to Europe and Asia. Politicians began to apply pressure. In the early 1980s, Montana and Texas senators shamed the Navy into removing horse meat from commissary stores. The few remaining horse-packing plants dwindled during a market squeeze that also drove down welfare standards. Sick, injured, or distressed horses were driven long distances to slaughter under poor conditions.
In 1997, the Los Angeles Times broke the news that 90 percent of the mustangs removed from the range by the Bureau of Land Management had been sold on for meat by their supposed adopters. An Oregon horse abattoir called Cavel West was named in the report. It burned down that July, in an attack claimed by the Animal Liberation Front on behalf of the mustangs. The members of the ALF cell responsible were tried for terrorism, but Cavel West was never rebuilt. Nonviolent activists also applied pressure to the horse meat business, with California banning the transport and sale of horses for meat.
Activists and politicians worked to shut down the remaining abattoirs in the years that followed. In early September 2006, the Horse Slaughter Prevention Act passed the U.S. House, with Republican John Sweeney calling the horse meat business “one of the most inhumane, brutal and shady practices going on in the United States today.” Horse slaughter was not outlawed, but both federal and commercial funding for inspections was canceled, effectively shutting down the business.
Meanwhile, the town of Kaufman, Texas, mobilized against the Belgian-owned abattoir on their outskirts that paid little tax but spilled blood into the sewage system. The plant, along with another in Fort Worth, were closed. In DeKalb, Illinois, the only remaining American horse meat plant burned down in unexplained circumstances. The owners were prevented from rebuilding, as Illinois once more passed a law to stop the horse meat business. Horse slaughter ceased on U.S. soil, at least for domestic use as food. Even so, American horses were still being transported long distance to Mexican and Canadian abattoirs.
* * *
The 2009 financial crisis dealt the equestrian industry a heavy blow. The pro-slaughter lobby, backed by a 2011 GAO study, suggested that American horses had suffered, as owners no longer receiving meat money would not pay to dispose of them. Groups like United Horsemen coopted Tea Party rhetoric to compare animal-welfare campaigners to the Nazis. Opponents pointed out that poor paperwork meant many slaughter-bound horses had been treated by drugs that should have ruled them out of the food chain. Across America, both sides clashed when Obama signed a new law lifting the ban on funding for inspections. New abattoirs were proposed, but town after town blocked the measures. The 2014 Obama budget once more ruled out a revival. Meanwhile, the horses continued to be shipped to Mexico and Canada.
Today, all the familiar contradictions of the American horse meat business are playing out again, as Trump looks toward horse meat as a cost-cutting measure. Ranges are overflowing with mustangs. Animal-welfare information has disappeared from government websites, and the administration is rumored to have called on the GAO to launch another study into the benefits of building domestic abattoirs.
And yet, without adequate funding for proper inspections in a reborn U.S. horse meat industry, the market might languish. Europe is already skeptical of Mexican and Canadian exports sourced from the United States, making horse meat less profitable anyway.
Forever marginal, always unsteady, the business of packing and selling the poor man’s beef could boom and crash again in America. If it does, Trump might find himself sporting a new political epithet: Horse-Meat Donny.
This article appears courtesy of Object Lessons.


Imagine someone told you to draw a pig and a truck. Maybe you’d sketch this:
​
Easy enough. But then, imagine you were asked to draw a pig truck. You, a human, would intuitively understand how to mix the salient features of the two objects, and maybe you’d come up with something like this:
​
Note the little squiggly pig tail, the slight rounding of the window in the cab, which recalls an eye. The wheels have turned hoof-like, or alternatively, the pig legs have turned wheel-like. If you’d drawn it, I, a fellow human, would subjectively rate this a creative interpretation of the prompt “pig truck.”
Until recently, only human beings could have pulled off this sort of conceptual twist, but no more.  This pig truck is actually the output of a fascinating artificial intelligence system called SketchRNN, a part of a new effort at Google to see if AI can make art. It’s called Project Magenta, and it’s led by Doug Eck.
Last week, I visited Eck at Google Brain team’s offices in Mountain View, where Magenta is housed. Eck is clever, casual, and self-effacing. He received his Ph.D. in computer science from the University of Indiana in 2000, and has spent the intervening years working on music and machine learning, first as a professor at the University of Montreal (a hotbed for artificial intelligence) and then at Google, where he worked at Google Music before heading to Google Brain to work on Magenta.
Eck’s drive to create AI tools for making art began as a rant, “but after a few cycles of thinking,” he said, “it became, ‘Of course we need to do this, this is really important.’”
The point of SketchRNN, as he and Google collaborator David Ha have written, is not only to learn how to draw pictures, but to “generalize abstract concepts in a manner similar to humans.” They don’t want to create a machine that can sketch pigs. They want to create a machine that can recognize and output “pigness,” even if it is fed prompts, like a truck, which don’t belong in the barnyard.
The implicit argument is that when humans draw, they make abstractions of the world. They sketch the generalized concept of “pig,” not any particular animal. That is to say, there is a connection between how our brains store “pigness” and how we draw pigs. Learn how to draw pigs and maybe you learn something about the human ability to synthesize pigness.
Here’s how the software works. Google built a game called, “Quick, Draw!” which, as people played, generated a large database of human drawings of all kinds of stuff: pigs and rain, firetrucks and yoga poses, gardens and owls.
When we sketch, we compress the rich, colorful, noisy world into just a few movements of a (digital) pen. It is these simple strokes that are the underlying dataset for SketchRNN. Each class of drawing—cat, yoga, rain—can be used to train a particular kind of neural network using Google’s open-source TensorFlow software library. This is distinct from the kind of photograph-based work that’s inspired so many news stories, like when a machine can render a photograph in the style of Van Gogh or the original DeepDream, or drawing any shape and having it fill in with “catness.”
These projects all feel, subjectively, to humans, uncanny. They are interesting because they make images that are sort of like, but not exactly like, human perception of the real world.
The outputs of SketchRNN, however, don’t feel uncanny at all. “They feel so right,” Eck told me. “I don’t want to say ‘so human,’ but they feel so right in a way that these pixel-generation things don’t.”
This is a core insight of the Magenta team. “Humans … do not understand the world as a grid of pixels, but rather develop abstract concepts to represent what we see,” Eck and Ha argue in their paper describing the work. “From a young age, we develop the ability to communicate what we see by drawing on paper with a pencil or crayon.”
And if humans can do it, Google would like machines to be able to do it. Last year, Google CEO Sundar Pichai declared the company “artificial intelligence-first.” AI, for Google, is a natural extension of its original mission “to organize the world's information and make it universally accessible and useful.” What’s changed is that now the information is being organized for artificial intelligences, which then make it accessible and useful for people. Magenta is one of Google’s wilder attempts to organize and understand a particular human domain.
Machine learning is the broadest term for the tools Google has adopted. ML, as it is often abbreviated, is a way of programming computers to teach themselves how to do various tasks, usually by feeding them labeled data to “train” on. One popular way of doing machine learning is with neural networks that are very loosely modeled on the brain’s system of connections. Various nodes (artificial neurons) are connected to each other with different weightings that respond to some inputs, but not others.
In recent years, neural networks with multiple layers have proven very successful in solving tough problems, especially in translation and image recognition/manipulation. Google has rebuilt many of its core services on these new architectures. Mimicking the known functioning of our own brains, these networks have interconnected layers that recognize different patterns in an input (say, an image). A low-level layer might contain neurons that respond to simple pixel level patterns of light and dark. A high-level layer might respond to dog faces or cars or butterflies.
Building networks with these kinds of architectures and mechanics can be unreasonably effective. Computing problems that were remarkably difficult become a matter of tuning the training of a model and then leaving some graphics-processing units to compute for a while. As Gideon Lewis-Kraus described in The New York Times, Google Translate had been a complex system built over 10 years. Then the company rebuilt it with a deep-learning system in 9 months. “The A.I. system had demonstrated overnight improvements roughly equal to the total gains the old one had accrued over its entire lifetime,” Lewis-Kraus wrote.
Because of this, there have been an explosion of uses and types of neural networks. For SketchRNN, they used a recurrent neural network, which deals with sequences of inputs. They trained the network on the progression of pen strokes people made to draw different things.
The easiest way to describe training is as a type of encoding. Data (sketches) are fed in, and the network tries to come up with the general rules for what it is processing. Those generalizations are a model of the data, which is stored in the mathematics describing the propensities of the neurons in the network.
That configuration is evocatively called the latent space or Z (zed) and it is where the pigness or truckness or yoganess is held. Sample it, as the AI people say, by asking the system to draw what it has been trained on, and SketchRNN spits out a pig or a truck or a yoga pose. What it draws is what it has learned.
What can SketchRNN learn? Below is a network trained on firetrucks generating new fire trucks. Inside the model, there is a variable called “temperature,” which allows the researchers to crank the randomness of the output up or down. In the following images, bluer images have the temperature turned down, redder ones are “hotter.”
​
Or maybe you’d prefer to see owls:
​
And the best example of all, yoga poses:
​
Now, these are like human drawings, but they are not themselves drawn by any human. They are reconstructions of how a human might sketch such a thing. Some of them are quite good and others are less so, but they would all pretty much make sense if you were playing Pictionary with an AI.
SketchRNN is also built to accept input in the form of human drawings. You send something in and it tries to make sense of that. Working with a model trained on cat data, what would happen if you lobbed in a three-eyed cat drawing?
​
You see that? In the various outputs from the model to the right (again showing different “temperatures”), it strips out the third eye! Why? Because the model has learned that cats have triangular ears, two whiskers, a roundish face, and only two eyes.
Of course, the model does not have any idea what an ear actually is or if cats’ whiskers move or even what a face is or that our eyes can transmit images to our brains because photons change the shape of the protein rhodopsin in specialized cells in the retina. It knows nothing of the world to which these sketches refer.
But it does know something about how humans represent cats or pigs or yoga or sailboats.
“When we start generating a drawing of a sailboat, the model will fill in with hundreds of other models of sailboats that could come from that drawing,” Google’s Eck told me. “And they all kind of make sense to us because the model has pulled out from all this training data the platonic sailboat—you’ll kill me for saying this—but the ur sailboat. It’s not a question of specific sailboats, but sailboatness.”
As soon as he said it, he seemed to regret his momentary loftiness. “I’m gonna have the philosophers come crush me for that,” he said. “But as a handwavey thing, it makes sense.” (The Atlantic’s resident philosopher Ian Bogost told me, “Philosophically speaking, this is pure immanent materialism.”)
The excitement of being a part of the artificial intelligence movement, the most exciting technological project ever conceived, at least to those within it, and to a lot of other people, too—well, it can get the better of even a Doug Eck.
I mean, train a network on drawings of rain. Then input a sketch of a fluffy cloud, and, well, it does this:
​
Rain falls out of the cloud you’ve sent into the model. That’s because many people draw rain by first drawing a cloud and then drops coming out of it. So if the neural network sees a cloud, it makes rain fall out of the bottom of that shape. (Interestingly, the data is a succession of strokes, though, so if you start with the rain, the model will not produce a cloud.)
It’s delightful work, but in the long project to reverse engineer how humans think, is this a clever side project or a major piece of the puzzle?  
What Eck finds fascinating about sketches is that they contain so much with so little information. “You draw a smiley face and it’s just a few strokes,” he said, strokes that look nothing like the pixel-by-pixel photographic representation of  a face. And yet any 3-year-old could tell you a face was a face, and if it was happy or sad. Eck sees it as a kind of compression, an encoding that SketchRNN decodes and then can re-encode at will.
It’s not unlike Scott McCloud’s famous (among a certain kind of nerd) case for the power of cartooning.  

“I’m very supportive of the SketchRNN work and it's really cool,” said Andrej Karpathy, a researcher at OpenAI, who has become a central node in AI research dissemination. But he also noted that they have made some very strong assumptions about the importance of strokes into their model, which means they are less useful to the overall enterprise of developing artificial intelligence.
“The generative models we develop usually try to be as agnostic as possible to the details of the dataset, and should work no matter what data you throw at them: images, audio, text, or whatever else,” he said. “Except for images, none of these are made up of strokes.”
“I'm also perfectly okay with people making strong assumptions, encoding them in the models, and getting more impressive results in the respective specific domains,” he added.
Eck and Ha are building something closer to a chess-playing AI than an AI that could figure out and play the rules any game. To Karpathy, the scope of their current work seems limited.
But there are some reasons to think that line drawings are fundamental to the way humans think. The Googlers are not the only researchers who have become intrigued by the power of sketches. In 2012, Georgia Tech’s James Hays teamed up with Technische Universität Berlin’s Mathias Eitz and Marc Alexa to create a dataset of sketches as well as a machine learning system for identifying them.
For them, sketches represent a form of “universal communication,” something all humans with standard cognitive functioning can do and have done. “Since prehistoric times, people have rendered the visual world in sketch-like petroglyphs or cave paintings,” they write. “Such pictographs predate the appearance of language by tens of thousands of years and today the ability to draw and recognize sketched objects is ubiquitous.”
They point to a paper in the Proceedings of the National Academy of Sciences by neuroscientist Dirk Walther at the University of Toronto that “suggests that simple, abstracted sketches activate our brain in similar ways to real stimuli.” Walther and his co-authors hypothesize that line drawings “capture the essence of our natural world,” even if on a pixel-by-pixel basis, a line-drawing of a cat looks nothing like a picture of a cat.
If the neurons in our brains work within the layered hierarchies that neural networks mimic (slash caricature), sketches may be one way to grasp at the layer that stores our stripped-down concepts of objects—“the essence” as Walther put it. That is to say: they may tell us something important about the fresh way that humans began to think when our ancestors rounded into modern form some time in the last 100,000 years. Sketches, on cave walls or on the backs of napkins, may be the literal depiction of the jump from horse to horseness, from everyday experience to abstract, symbolic thought, and with it, the modern human.
Most of modern life flows from that transition: language, money, mathematics, and eventually computing itself. So, it would be fitting if sketches ended up playing an important role in the creation of a significant artificial intelligence.
But of course, for humans, a sketch is a depiction of a real thing. We can easily understand the relationship between the abstract four-line representation and the thing itself. The concept means something to us.  For SketchRNN, a sketch is a sequence of pen strokes, a shape being formed through time. The task for the machine is to take the essences of things depicted in our drawings and try to use them to understand the world as it is.  
The SketchRNN team is exploring in many different directions. They might build a system that tries to get better at drawing through human feedback. They could train models on more than one kind of sketch. Maybe they'll find a way to see if their model, trained to recognize pigness in sketches, say, can generalize to photorealistic images. I'd love to see their model plugged into others that, for example, have been trained on traditional photographs of cats. This would let them skin cat drawings, coloring in the sketches with what a UC Berkeley-created neural network knows about the texture of cats.  
But they themselves admit that SketchRNN is a “first step” and that there is so much to learn. The arc of human life that these sketch-decoding machines find themselves a part of is long. The human history of art has occurred on roughly the opposite of technological time.
In covering cave paintings in Europe for The New Yorker, Judith Thurman wrote that Paleolithic art remained mostly unchanged for “for 25 millennia with almost no innovation or revolt.” She notes that’s “four times as long as recorded history.”
The art must have been deeply satisfying and its broader culture stable, a scholar tells Thurman.  
Computers, and especially the new artificial intelligence techniques, are destabilizing long-held notions of what humans are good at. Humans fell to machines in checkers in the ’90s. Then chess. Most recently Go.  
But the power of recent work in AI is not due to the speed at which the state of the art is advancing (though it is moving very fast). For Eck, it’s more that they are striving after the very bedrock of how humans think, and by extension, who we are. “A really core part of art is its basic humanity, that we’re communicating with each other,” Eck told me.  
Taking in the whole enterprise of deep learning, all the different people working on the underlying mechanisms of human life—how we see, how we move, how we talk, how we recognize faces, how we structure words into stories, how we play music— and it looks a little like an outline not of any particular human, but humanness.
Right now, it’s low-resolution, a caricature, a stick figure of real thought, but it is not hard to recognize the gathering intelligence from the sketch.


Across the computer security world yesterday, heads were shaking.
The FBI filed a criminal complaint against Reality Winner, an NSA contractor, who the agency alleges stole classified documents and shared them with an “online news outlet” believed to be The Intercept. Because the documents in question appear to have been printed, some security experts have been wondering if a mysterious code used by some printers is to blame for Winner’s capture. That code is an almost-invisible grid of dots that some color printers ink into every document they print.
The complaint also details how agents say they tracked the leak back to Winner. The news org contacted the National Security Agency and said they were “in possession of what they believed to be a classified document.” The news organization then sent that document to the NSA, presumably for verification. “The U.S. Government Agency examined the document shared by the News Outlet and determined the pages of the intelligence reporting appeared to be folded and/or creased, suggesting they had been printed and hand-carried out of a secured space,” the complaint continues.
From there, the agents say that they simply looked to see who had printed the document—six people had—and then discovered that one of them, Winner, had been in contact with the media company in question from her work computer (although on an unrelated topic).
When FBI agents showed up at her house, they say she confessed to “removing the classified intelligence reporting from her office space, retaining it, and mailing it from Augusta, Georgia, to the News Outlet.” She faces up to 10 years in prison.
Given what is in the public record from the FBI complaint, Winner was almost certain to get caught, and some have argued that The Intercept could not have prevented that. Obviously, the NSA monitors and records who prints what documents. There’s an audit trail there, which one imagines an NSA contractor would know.
That’s why many in the computer security have deemed the way the leak was made and handled to be a terrible example of “operational security,” or as you’ll see it relentlessly abbreviated, “opsec.”
The Intercept released a statement today, however, reminding people not to take the FBI’s complaint as fact.
“While the FBI’s allegations against Winner have been made public through the release of an affidavit and search warrant, which were unsealed at the government’s request, it is important to keep in mind that these documents contain unproven assertions and speculation designed to serve the government’s agenda and as such warrant skepticism,” they wrote. “Winner faces allegations that have not been proven. The same is true of the FBI’s claims about how it came to arrest Winner.”
If Winner wasn’t found the way the complaint claims, the mysterious dot code is one other way the FBI could have found her, as the research blog Errata Security spelled out in detail.
In fact, the document that The Intercept published contains these dots, and the code spells out a date—May 9—that matches the FBI affidavit’s account of Winner’s printing. It also notes a serial number, which the NSA could obviously match back up to a machine in their offices.
Let’s invert the colors.
And then crank up the brightness of the dots.
As you can probably see now, the dots are printed in a repeating rectangular pattern. They’re a code. And the Electronic Frontier Foundation cracked it some years ago, at least for Xerox printers.
Let’s look at the rectangular code up close.
Errata Security says that the pattern was flipped upside down, so let’s flip it.
Then they used the EFF decoder, which I’ve combined with the original image here, so you can see the pattern more easily.
Run through EFF’s parser, Errata Security found that it output a time of 6:20, a date of May 9, and a couple of possible serial numbers. As Errata Security notes, “The NSA almost certainly has a record of who used the printer at that time.” If they didn’t already have Winner in some other way, this would probably have sealed her capture.
As one might expect, the EFF has some strong opinions about these dots, and their inclusion in the firmware that runs color laser printers. They’ve been tracking the use of these dots for a decade.
“In a purported effort to identify counterfeiters the U.S. government has succeeded in persuading some color laser printer manufacturers to encode each page with identifying information,” they write.
The idea was that color laser printers might be (or become) so good that anyone could just print money. So, manufacturers were persuaded to add this countermeasure into the software that’s baked into some printers. When someone prints in color on one of these machines, the printer does everything normally, but also adds this nearly invisible identifying stamp. (If they print in black and white, though, there are no dots.)
According to a research team at the Illinois Institute of Technology that worked on ways to counteract the dot-code, some printer manufacturers don’t even acknowledge that they exist or provide any information about how they work. “It’s a trivial way of tracking,” said Louis McHugh, one of the IIT researchers. “There’s nothing high-tech about it.”
But the dots are tracking documents, and the people who print them, nonetheless.
Did these dots play a role in Reality Winner’s investigation? We don’t know, but if you’re planning on leaking some classified documents by printing them through a color printer, you may want to rethink your plan.


On a recent Thursday, I waited for an email that was supposed to contain every personal detail the internet knows about me. The message would be from an online data broker—a company that collects and sells information that many people would hope is private. This includes browsing history, online purchases, and any information about you that’s publicly available: property records, court cases, marital status, social-media connections, and more. Facebook collaborates with data brokers for targeting advertisements. In some states, the Department of Motor Vehicles, among other agencies, sells information to brokers. Brick-and-mortar stores do, too.
As I refreshed my inbox, I listened to garbage trucks outside my window begin to make their evening rounds through Taipei’s Beitou District. I work remotely, so I’ve opted to travel from city to city for the past 11 months, exploring the Canary Islands, Bulgaria, Serbia, Hungary, Israel, Vietnam, South Korea, and Taiwan, all while editing books and answering emails. The internet has been my most permanent residence, both my office and the nexus of my social life. I generate data all day as I make money, stay in touch with friends, and order e-books to read in my spare time.
In the time I’ve been away, Congress has repealed protections that would have blocked internet providers like AT&T, Comcast, and Verizon from sharing browsing data with other companies. It’s unlikely that these legislative changes will have a significant effect on the relationship between data brokers and internet-service providers, but they do bring concerns about privacy to the fore, even more than usual. And when the news first broke, it caused a lot of confusion about just how much data companies will one day be able to collect and just how far people need go to protect themselves.   
It all sounded scary enough that I wondered what would come up if I bought data about myself from a data broker. With a quick Google search, I found a company that promised to detail net worth, age, zip code, and education, among other personal information. All I had to do was upload a text file of the email addresses of people whose info I wanted (in this case, just my own) and pay a $50 fee. The whole endeavor gave me pause. It seemed like I was about to do something that violated the company’s lengthy terms of service. Then there was the queasiness about the data itself: Did I really want to know?
The report arrived in my inbox a matter of hours later with an accompanied missive trumpeting, “Wow! That was easy.” Yes. I never had to talk to a customer service representative nor identify myself. It was just like any other transaction. My misgivings gave way to glee. A strong Christmas morning vibe overtook me. Would I find something I didn’t know? There was a part of me that genuinely believed the internet knew me best: Maybe I’d discover a pattern in my life that could point toward the future—a palm reading constructed from metadata.
In the zip file, I found a PDF, a spreadsheet, and a .txt file. I chose the spreadsheet first, and this was the first of many letdowns. It was merely a summary of how many of the email addresses had provided “matches” for the various information categories. I tried again with the charts, which aren’t visually interesting when they each feature one piece of data about a single subject. The pie chart, for example, was just an uninterrupted blue circle labeled “Female 100.0%.” I got a sense I had wasted my money. Finally, I opened the.txt file, and as though I had time traveled back to the advent of personal computing, a document I was reading in Notepad was the most useful of the three: It included each data point, organized email address by email address. But much of the data was flat-out wrong.
If you like percentages, nearly 50 percent of the data in the report about me was incorrect. Even the zip code listed does not match that of my permanent address in the U.S.; it shows instead the zip code of an apartment where I lived several years ago. Many data points were so out of date as to be useless for marketing—or nefarious—purposes: My occupation is listed as “student”; my net worth does not take into account my really rather impressive student loan debt. And the information that is accurate, including my age and aforementioned net worth (when adjusted for the student debt), is presented in wide ranges.
Historically, data brokers don’t do nuance. Companies care about demographics: If they can get information that is in the right ballpark, it’s likely to suit their needs just fine. I thought opening my data would be like looking in a mirror, maybe a dressing room mirror under lighting that makes you think you should start taking many vitamins. Instead, it was like seeing an expressionist painting of myself. I caught glimpses of something I recognized, but everything was hazy and a little off.
The sight was a relief. Conversations and debates about privacy tend to take for granted that the technology invading privacy finds information that is correct. But while our data is collected aggressively these days, clearly companies still aren’t infallible. Maybe the death of privacy isn’t quite so near.
So I did something I probably should have done much earlier in the day. I got up from my computer. I went either to get lo mein or to buy dumplings from the cart outside our apartment—I don’t remember which. Some things aren’t worth tracking. Most facts couldn’t tell you who I really am, anyway.


Without once saying the words “artificial intelligence,” a stream of Apple executives described a vision of the near future in which Siri, the company’s AI avatar, stitches together the company’s many hardware products.  
And they introduced a new—and widely anticipated—entry into their lineup: a $349 cylindrical voice-controlled speaker they call HomePod.
After a strangely dystopian video in which Apple’s apps go away and the world plunges into post-apocalyptic violence, Apple CEO Tim Cook led off the company’s keynote at its big gathering for coders, the Worldwide Developers Conference, in San Jose.
The WWDC keynote tends to be a place where Apple showcases all the little incremental “refinements” they are making to their software and hardware. This year, however, there was a thread that ran through many presentations: Siri.
Through the demonstrations and talks, Apple’s vision for Siri became clearer: It is an all-purpose stand-in for predictive, helpful intelligence across all Apple devices. “Siri isn’t just a voice assistant,” said Craig Federighi, Apple’s senior VP of software engineering. “With Siri intelligence, it understands context. It understands your interests. It understands how you use your device. It understands what you want next.”
For example, Federighi said, imagine you’re planning a trip to Iceland. Siri might suggest stories about Iceland within the news app or even suggest the spelling for a difficult Icelandic place name. (Perhaps she’ll suggest some Björk for your HomePod, even.)
Even the Apple Watch has a new (and decidedly Google Now-like) face that guesses what information you might want to see on that tiny screen at any given time.
Siri suffuses all the Apple products now. It’s less a voice-UI gimmick than an organizational structure for how Apple thinks about proactive and reactive user assistance. Or, to put it slightly less generously, “Siri is turning into Watson, a generic brand for anything using simple machine learning,” tweeted Ben Cunningham, a former Facebook engineer.
The Apple presenters probably said machine learning two dozen times as they described their plans for iOS, the software that runs iPhones and iPads, watchOS, and the next version of macOS. They planned to roll out a new series of machine-learning tools for developers, which will allow app makers to access Apple’s computer vision and natural-language processing tools.
That kind of easy AI access was a theme of Google’s developer conference, too. But unlike Google and Amazon, Apple emphasized  the privacy features of their devices. For example, Amazon’s Echo speakers transmit some data to Amazon as they wait to hear the word “Alexa” and spring into action. But Apple’s HomePod will do that processing locally inside the speaker. “Until you say [‘Hey Siri’], nothing is being sent to Apple,” said Phil Schiller, Apple’s VP of marketing. And then, what is sent to the company’s servers is an “anonymous Siri ID.”
Amazon emailed me to clarify that they don't send data while the Echo is idling, waiting to be called upon. "When you use the wake word," a company spokesperson explained, "the audio stream includes a fraction of a second of audio before the wake word, and closes once your question or request has been processed."
It is true that Apple’s business model is far less dependent on amassing data about individual people than Google, Facebook, or Amazon. They sell stuff to people.
Or as analyst Horace Dediu summed up the Apple pitch: “Siri knows you. Apple doesn’t.”
Taken together with Google, Microsoft, Amazon, and Facebook’s pushes into this space, and it would seem that we’ll soon have a wide variety of systems that build and rebuild their models of your desires every moment, hoping they can provide just the right suggestion. It’ll be nudges all the way down.


It seemed like an odd pairing from the start: Elon Musk, the brilliant South African immigrant who runs companies that build electric cars and lease solar panels to homeowners, and Donald Trump, the television-obsessed politician who repeatedly describes climate change as a hoax.
Musk joined the president’s advisory councils a month after the election last year, along with a band of high-profile tech executives. Trump’s election seemed like it could hurt those in the renewable-energy business, and Musk seemed to hop on board in part to get the president’s ear. It also helped that Trump supported partnerships between NASA and private spaceflight company’s like Musk’s SpaceX. The partnership seemed shaky from the beginning, with Musk criticizing Trump’s executive order banning immigration from seven predominantly Muslim nations.
On Wednesday, as rumors of the Trump administration’s decision to withdraw the United States from the Paris climate accord swirled, Musk said he was leaving the advisory councils over the move. On Thursday, after Trump announced the nation would indeed exit the agreement, Musk followed through on his promise:
Am departing presidential councils. Climate change is real. Leaving Paris is not good for America or the world.

He also tweeted a pointed message referencing China, Trump’s favorite culprit for many global ailments, particularly rising temperatures:
Under Paris deal, China committed to produce as much clean electricity by 2030 as the US does from all sources today https://t.co/F8Ppr2o7Rl

Others in the technology sector have also voiced their disappointment.
Brad Smith, Microsoft’s president and chief legal officer, tweeted Thursday during Trump’s speech:*
We’re disappointed with the decision to exit the Paris Agreement. Microsoft remains committed to doing our part to achieve its goals.


Elon Musk Is Betting Big on Donald Trump
Oil gas giant ExxonMobil supported the U.S. remaining in the pact, as did Rex Tillerson, the company’s former CEO and Trump’s secretary of state. Earlier this month, the CEOs of 30 companies with operations in the U.S. published an open letter to Trump about the Paris agreement. “Our business interests are best served by a stable and practical framework facilitating an effective and balanced response to reducing global [greenhouse gas] emissions,” they wrote. “The Paris agreement gives us that flexible framework to manage climate change while providing a smooth transition for business.”
Business leaders have spent months trying to change Trump’s mind on the Paris agreement, but in the end they proved unpersuasive. As for Musk, he can now try his hand at criticizing the administration from the outside, leveraging his considerable popularity with voters from both parties. He may not find success with this strategy, but at least he won’t lose face with the renewable energy community, in which he is widely perceived as a leader, moral or otherwise.
*This article originally misstated that Smith is the CEO of Microsoft. We regret the error.


Updated on June 1, 2017
Hillary Clinton came to Recode’s Code Conference with her gloves off. In an interview with the journalists Kara Swisher and Walt Mossberg, she delivered a fully baked articulation of the ways technology was “weaponized” against her campaign to aid Donald Trump.
“I take responsibility for every decision I made,” Clinton said, “but that is not why I lost.”
In previous elections, the internet was primarily used to identify likely donors and voters and then get them to give money and turn out to cast their ballots, she said. That was definitely the story of the 2008 and 2012 Obama campaigns, as I encountered them as a journalist.  
But that changed in 2016, Clinton said.
“What we thought we were doing was going to be Obama 3.0: better targeting, better messaging, and the ability turn out our voters as we identified them, and to communicate more broadly with voters,” she explained. “Here’s what the other side was doing, and they were in a different arena. Through content farms, through an enormous investment in falsehoods, fake news, call it what you will—”
“Lies,” Mossberg interjected.
“Lies, that’s a good word, too,” Clinton continued. “The other side was using content that was just flat-out false, and delivering it in a very personalized way, both above the radar screen and below. And, look, I’m not a tech expert by any stretch of the imagination, [but] that really influenced the information that people were relying on.”
She called out fake news stories on Facebook, which she said were spread by 1,000 Russian agents, as well as bots running on social media to amplify the disinformation. “It was such a new experience. I understand why people on their Facebook pages, [said] ‘Oh, Hillary Clinton did that. I did not know that! Well, that’s gonna effect my opinion about her,’” Clinton said. “And we did not engage in false content.”
Mossberg asked her why the Democrats were not better at combatting that false information. “There’s a way to weaponize tech that doesn’t involve lying or having Russians help you,” he said. “It is a political weapon. It’s a fact of life. But how do you do it?”
At that point, Clinton claimed that the data candidate Trump received from the Republican National Committee was much better than what she received from the Democratic National Committee.
“I get the nomination. I’m now the nominee of the Democratic party. I inherit nothing from the Democratic party,” she said. “I mean, it was bankrupt. It was on the verge of insolvency. Its data was mediocre to poor, non-existent, wrong. I had to inject money into it.”
(Michael Tyler, the DNC’s spokesman, notes that DNC chair Tom Perez has admitted that the committee was not “firing on all cylinders,” and says that the DNC “is now undergoing an organizational restructuring that will include a new Chief Technology Officer, who will do an in depth analysis and maintain the party’s analytics infrastructure needs.”)
Meanwhile, Clinton continued, the Republican National Committee spent $100 million on its data infrastructure between 2012 and 2016, which it handed over to Donald Trump’s campaign.
“Then you’ve got Cambridge Analytica,” Clinton said, referencing the political analytics start-up backed by the wealthy and right-wing Mercer family.
In the run up to and aftermath of the election, Cambridge Analytica got tons of press for its targeting abilities. “We are thrilled that our revolutionary approach to data-driven communication has played such an integral part in President-elect Trump’s extraordinary win," the company’s CEO said in a press release.  Later reports tended to be more circumspect about the firm’s importance.
“You can believe the hype on how great they were or how not great they were, but the fact is they added something. And I think we better understand that,” she said. “The Mercers did not invest all that money just for their own amusement.”
She described a deal that Cambridge Analytica cut with the Trump campaign that put Steve Bannon, who had been running Breitbart, into the center of Trump’s world.
“They marry content with delivery and data. And it was a potent combination,” Clinton said. “The question is where and how did the Russians get into this.”
Then, like a prosecutor walking through her argument, she talked about the 17-agency report from the intelligence community about Russian interference into the presidential election.
“[The report] concluded with high confidence that the Russians ran an extensive information war campaign against my campaign to influence voters in the election,” Clinton said. “They did it through paid advertising, we think. They did it through false news sites. They did it through these 1,000 agents. They did it through machine learning, which kept spewing out this stuff over and over again, the algorithms they developed.”
Then she asked, not-quite-rhetorically, “Who were they coordinating with or colluding with?”
Unlike previous Russian cyberattacks inside the U.S., “This was different. They went public,” she said. “The Russians, in my opinion—and based on the intel and counterintel people I’ve talked to—they could not have known how best to weaponize that information unless they had been guided.”
“Guided by Americans?” Mossberg asked.
“Guided by Americans,” Clinton answered. “And guided by people who had polling data and information.”
After a brief tour of James Comey’s behavior during the election, Kara Swisher asked Clinton who she thought was guiding the Russians. “ I hope that we’ll get enough information to be able to answer that question,” Clinton responded at first.
Swisher prompted, “But you’re leaning Trump.”
“I am leaning Trump,” Clinton said.
“We’re going to, I hope, connect up a lot of the dots,” she said. “And it’s really important because when Comey did testify before being fired this last couple of weeks, he was asked, ‘Are the Russians still involved?’ And he goes, ‘Yes. They are.’ Why wouldn’t they be? It worked for them. It is important for Americans, particularly people in tech and business, to understand, Putin wants to bring us down and he is an old KGB agent.”
Having made her case, Clinton then tried to put herself in Mark Zuckerberg’s shoes. “With respect to the platforms, I am, again, not exactly sure what conclusions we should draw,” she said. “But here’s what I believe. I believe that what was happening to me was unprecedented and we were scrambling.”
But she did have some advice for Facebook and Twitter. “They’ve gotta get back to trying to curate more effectively,” she said. “Put me out of the equation. They’ve got to help prevent fake news from creating a new reality that does influence how people think of themselves, see the world, the decisions that they make.”



In the early days, when motion pictures were still new, filming the ocean was a radical idea.
A surface-level shot of the waves was certainly feasible, but capturing footage of swaying undersea fauna, swimming fish, and marbled sunlight dancing on the seafloor? The consensus was: It couldn’t be done.
In fact, it could be. A century ago, the brothers John Ernest and George Williamson, the sons of a sea captain and inventor, would prove it. To do so, the Williamsons turned to a piece of technology their father had designed for divers in undersea repair and salvage jobs. The device was a series of flexible concentric tubes, “interlocking iron rings that stretched like an accordion,” as the Library of Congress puts it, made to suspend from a specially outfitted ship so that a diver could descend into a watertight chamber below. At one end of the tube was the boat on the surface of the water; at the other, the submersible room.
John Ernest and George were enchanted by their father’s machine. From the glass portals along the tube, they observed red snappers, yellowtails, fat groupers, and other shimmery creatures weaving through the coral reef of the Bahamas. And they had the idea of bringing a camera with them next time. Later, when they shared their still photos with newspapers—images included a blurry oblong shark and shadowy seaweed—it created a sensation.
Eventually, the Williamsons’ tubes—outfitted with a new, specialized spherical observation chamber that had a large funnel-shaped window—would be used to film the underwater scenes in the 1916 film, 20,000 Leagues Under the Sea. “I call this my magic window,” one of the silent film’s intertitle cards reads, before revealing a gray, clouded ocean view. “We gaze on scenes which you might think God never intended us to see.”
At the time, the footage was extraordinary. The film was a smashing success.
Filmmakers have been using technology to push the limits of how the ocean is portrayed ever since—and not just in live-action films. Most recently, Disney dazzled audiences with the animated film, Moana, which tells the story of a girl from the Pacific Islands who sets out on a voyage to rediscover her ancestors’ wayfinding heritage.
Moana’s directors, Ron Clements and John Musker, have been obsessed with stories about the ocean for decades. They made The Little Mermaid in 1989. The 1940 film Pinocchio, with its famous whale sequence, is what first inspired Clements to pursue a career in animation. But depictions of the ocean in those stories are nothing like what audiences see in Moana, which is as groundbreaking for its portrayal of water in 2017 as 20,000 Leagues Under the Sea was in 1916.
“Water is always hard,” said Marlon West, the co-head of effects animation for Moana. “Usually, in animation, we have a dozen water shots. They’re hard to do.” In Moana, however, the majority of the movie involves water. And the ocean isn’t just a presence; the action takes place on the water, introducing yet another layer of complexity. On top of that, Moana’s Pacific ocean is occasionally anthropomorphic, like a distant cousin to the water in James Cameron’s The Abyss.
Disney’s software team came up with a program it calls Splash—a companion to Matterhorn, which was used to create the snow in Frozen—to help automate the way the water would behave in various shots. Splash is a “fluid solver” that plugs into Houdini, third-party 3-D animation software. To use the solver, effects specialists would define the area they wanted to simulate—say, a section of water around an animated boat—then use a setting to determine what the ocean condition should be like to begin with. From there, they’d run the simulation on the pre-determined ocean surface, to animate how that area of water responds to the boat. The output from that simulation—“millions of particles,” essentially millions of new points of animation data—would then be smoothed into the final rendering of the film.
Splash also involved a series of algorithms that could simulate splashes, eddies, and wakes. The program’s buoyancy algorithm made it so the huge navigating canoes in the film bounced in and out of waves realistically. (In several shots, many of these canoes appeared in the water together, in close proximity, creating an additional animation challenge.)
“As effects artists, working with fluids, you can’t always predict what you’re going to get from your water simulation,” said Erin Ramos, the film’s effects lead. “And the hard thing with water is, if it doesn’t look right, you can really tell. Even if it’s in the background.”
Disney effects specialists told me they were able to successfully automate key ocean-movement details about 80 percent of the time—meaning you could have a production assistant simply run a script to generate the animated boat’s wake. “Running these scripts to generate these animations leaves room for the artists to focus on the artistry of the shot,” Ramos told me, “so they have time to create these sweeping shots, and the ability to have the ocean acting as a character.”
The ocean in Moana is an anthropomorphic force that occasionally nudges Moana along the way. Except the ocean character doesn’t have a face. And it doesn’t talk. (In this way, it was a bit like the animated magic carpet in Disney’s Aladdin, Osnat Shurer, the Moana producer told me.) So Disney’s effects specialists and animators were constantly navigating the tension between wanting the water to look and act like actual water—but to be magical at the same time.
“That was a big challenge,” West said. “They would animate the ocean as almost a sock puppet, and we would take that and fill it full of bubbles and liquid or we’d do a simulation over it and make it full of water to make it look more watery.”
There were many debates among animators and effects specialists over how to convert the ocean from its ordinary state into a character with agency and back again. “What made the ocean character look like water to one person, looked too agitated and aggressive to another,” West said. “You have the ocean often as a chartacter looming over toddler Moana and looming over Moana’s grandmother, and at no point do you ever want to be afraid for them. You want to be in awe.”
In the end, calibrating between those two expectations—a realistic-looking ocean that could also convey subtle warmth and encouragement as a character—meant keeping a portion of the water unnaturally smooth and rounded when it surfaced as a character. Also, there were moments when obeying the laws of physics were discarded in favor of keeping the audience focused on the characters. “Because it’s storytelling,” West told me. “It’s a stylized world. And we’re trying to create water that exists in your heart and your mind’s eye.”
Then there was the question of representing the ocean in an authentic way—not physically realistic, but culturally true. To do so, Disney formed an Oceanic Story Trust, a group of cultural practitioners from around the Pacific who acted as consultants on the film. Members of the trust weighed in on everything from haka chants to tattoo design to the demigod Maui’s hair. (He was drawn as bald at first. He shouldn’t have been, members of the trust said. Disney gave him luxurious hair.)
On their first research trip, to Fiji, the filmmakers met with Jiujiua “Angel” Bera, a skilled wayfinder. “He spoke about the ocean in such a personal way,” Osnat Shurer, the Moana producer told me. “He would stroke it really gently, and told us you had to speak gently to the ocean. ‘The ocean knows,’ he said. He goes out to greet the ocean in the morning like he greets his family. This left a very deep impression on us.”
Shurer and her colleagues were also struck by a larger theme of connectedness from the Pacific Islanders they met—and the way many island cultures see the land and sea as indistinct. (In ancient Hawaii, for instance, this idea was encapsulated in the concept of ahupuaʻa, divisions of land that run from the mountain down to the ocean.) And also the extent to which some cultures view the ocean itself as a connective force. “In the Pacific, we don’t consider the water a barrier to each other,” Dionne Fonoti, an anthropologist and a member of the Oceanic Story Trust said in an interview with Disney that’s in the Moana bonus features. “It’s not just the cultures of the people and the islands that connect us, it’s also the ocean that connects us.”
When the film project began, however, the Disney team had no idea how they could portray all this complexity—even just from a technological standpoint. Not to mention the separate but related challenge of animating an anthropomorphic volcanic island.
“I’m pretty pumped up about what we did,” West told me. “There’s nothing, when I look at the final film, that I cringe at. And there usually is.”
The project also changed the way that animators and effects specialists think about the actual ocean. Ramos, the effects lead, told me she spent more than a year and a half working on getting the shoreline animation just right. Even now, she says, she can’t go to the beach without noticing things she’d never considered before her work on Moana.
“You know it’s hard for me to go to the beach nowadays,” she said. “When I’m there, I’m looking at how foam dissipates, at how the water recedes back into the ocean, the cadence and the rhythm of the little breaks. I’m looking at how the beach itself is modeled to create the reef breaks, how the light affects the water, the clarity of the water itself, the colors. There’s just a million things going through my head.”
“I don’t think it’s a bad thing,” she added. “I think it’s gorgeous.”


Last month, the technology developer Gnosis sold $12.5 million worth of “GNO,” its in-house digital currency, in 12 minutes. The April 24 sale, intended to fund development of an advanced prediction market, got admiring coverage from Forbes and The Wall Street Journal. On the same day, in an exurb of Mumbai, a company called OneCoin was in the midst of a sales pitch for its own digital currency when financial enforcement officers raided the meeting, jailing 18 OneCoin representatives and ultimately seizing more than $2 million in investor funds. Multiple national authorities have now described OneCoin, which pitched itself as the next Bitcoin, as a Ponzi scheme; by the time of the Mumbai bust, it had already moved at least $350 million in allegedly scammed funds through a payment processor in Germany.
These two projects—one trumpeted as an innovative success, the other targeted as a criminal conspiracy—claimed to be doing essentially the same thing. In the last two months alone, more than two dozen companies building on the “blockchain” technology pioneered by Bitcoin have launched what are known as Initial Coin Offerings to raise operating capital. The hype around blockchain technology is turning ICOs into the next digital gold rush: According to the research firm Smith and Crown, ICOs raised $27.6 million in the first two weeks of May alone.
Unlike IPOs, however, ICOs are catnip for scammers. They are not formally regulated by any financial authority, and exist in an ecosystem with few checks and balances. OneCoin loudly trumpeted its use of blockchain technology, but holes in that claim were visible long before international law enforcement took notice. Whereas Gnosis had experienced engineers, endorsements from known experts, and an operational version of their software, OneCoin was led and promoted by known fraudsters waving fake credentials. According to a respected blockchain engineer who was offered a position as OneCoin’s Chief Technology Officer, OneCoin’s “blockchain” consisted of little more than a glorified Excel spreadsheet and a fugazi portal that displayed demonstrably fake transactions.
And yet, OneCoin attracted hundreds of millions of dollars more than Gnosis. The company seems to have targeted a global category of aspirational investors who noticed the breathless coverage and booming valuations of cryptocurrencies and blockchain companies, but weren’t savvy enough to understand the difference between the real thing and a sham. Left unchecked, this growing crypto-mania could be hugely destructive to one of the most promising technologies of the 21st century.
* * *
This danger exists in large part because grasping even the basics of blockchain technology remains daunting for non-specialists. In a nutshell, blockchains link together a global swarm of servers that hosts thousands of copies of the system’s transaction records. Server operators constantly monitor one another’s records, meaning that to steal money or otherwise alter the ledger, a hacker would have to compromise many machines across a vast network in one fell swoop. Even as the global banking system faces relentless cyberattacks, the more than $30 billion in value on Bitcoin’s blockchain has proven essentially immune to hacking.
That level of security has potential uses far beyond digital money. Introduced in July of 2015, a platform called Ethereum pioneered the idea of more complex and interactive applications backed by blockchain tech. Because these systems can’t be altered without the agreement of everyone involved, and maintain incorruptible records of every change, blockchains could eventually streamline sensitive, high-value networks ranging from health records to interbank transfers to remote file storage. Some have called the blockchain “Cloud Computing 3.0.”
Using most of these blockchain applications will require owning the digital currencies linked to them—the same digital currencies being sold in all these ICOs. So, for example, to upload your vacation photos to the blockchain cloud-storage service Storj will cost a few Storj tokens. In the long term, demand for services will set the price of each blockchain project’s token.
While a traditional stock is a legal claim backed up by regulators and governments, then, the tokens sold in an ICO are deeply embedded in the blockchain software their sale helps create. Knowledgeable tech investors are excited by this because, along with the open-source nature of much of the software, it means that ICO-funded projects can, like Bitcoin itself, outlast any single founder or legal entity. In a 2016 blog post, Joel Monegro, of the venture capital fund Union Square Ventures, compared owning a blockchain-based asset to owning a piece of digital infrastructure as fundamental as the internet’s TCP/IP protocol.
Almost all groups launching ICOs reiterate some version of this idea to potential buyers, in part as a kind of incantation to ward off financial regulators. The thinking is that, if they are selling part of a platform, rather than stakes in any company, they’re not subject to oversight by bodies like the U.S. Securities and Exchange Commission. But in practice, ICOs are constantly traded across a variety of online marketplaces as buyers breathlessly track their fluctuating prices. In this light, they look an awful lot like speculative investments.
Buyer expectations may matter more to regulators than technical hair-splitting. Todd Kornfeld, a securities specialist at the law firm Pepper Hamilton, finds precedent in the landmark 1946 case SEC v. W.J. Howey Co. Howey, a Florida orange-growing operation, was selling grove plots and accompanying “service contracts” that paid faraway landowners based on the orange harvest’s success. When the SEC closed in, Howey argued they were selling real estate and services, not a security. But the Supreme Court ultimately disagreed, establishing what’s known as the Howey test: In essence, if you give someone else money in the hope that their activities will generate a profit on your behalf, you’ve just bought a security, no matter what the seller calls it.
Knowledgeable observers tend to agree that some form of regulation is inevitable, and that the term ICO itself—so intentionally close to IPO—is a reckless red flag waved in the SEC’s face. The SEC declined to comment on any prospective moves to regulate ICOs, but the Ontario Securities Commission has issued an advisory that “assets that are tracked and traded as part of a distributed ledger may be securities, even if they do not represent shares of a company or ownership of an entity.”
According to Kornfeld, even those who believe they are conducting ICOs in complete good faith could face serious repercussions when regulators do act, especially if prosecutors think they’ve made misleading statements. “If [prosecutors] think that you’re really bad,” he says. “They can say, hey, you deserve 20 years in jail.”
* * *
While it’s easy to see the lie in OneCoin’s fictional blockchain, entirely sincere claims about such a nascent sector still can strain the limits of mere optimism. Many experts, for instance, believe that Gnosis’s use of the blockchain to aggregate data could become a widespread backbone technology for managing complex systems from traffic to financial markets. But the $12.5 million worth of GNO sold in the Gnosis ICO represented only 5 percent of the tokens created for the project, implying a total market value of nearly $300 million. Most tech startups at similar stages are valued at under $5 million.
That astronomical early valuation alone could become bait for an aggressive regulator. Many founders of legitimate blockchain projects have chosen to remain anonymous because of this fear, in turn creating more opportunities for scams.
Much of the money flowing into these offerings is smart, both in that it comes from knowledgeable insiders, and in a more literal sense: Buying into ICOs almost always requires using either Bitcoin or Ethereum tokens (OneCoin, tellingly, accepted payment in standard currency). Jeff Garzik, a longtime Bitcoin developer who now helps organize ICOs through his company Bloq, thinks their momentum is largely driven by recently minted Bitcoin millionaires looking to diversify their gains. Many of these investors are able to do their own due diligence—evaluating a project’s team, examining demo versions of their software, or scrutinizing their blockchain after launch.
But as cryptocurrency becomes more mainstream, ICOs will present greater risks to larger numbers of people. There are few barriers to participation aside from knowing how to conduct a Bitcoin transaction, and the space mostly lacks the robust independent analysis performed by underwriters in the IPO market, which can help tamp down overoptimism. The risk isn’t just to individual investors; many argue that the mania of the late-1990s internet bubble ultimately slowed the entire sector down by making investors skittish for years afterwards. Imagine how much worse things might have been if the whole thing had been entirely unregulated.
Careful regulation, then, could protect blockchain projects from a hugely damaging bust. And the model is genuinely utopian enough to deserve nurturing. Cryptographic tokens effectively make all of a platform’s users part-owners. Anyone selling goods for Bitcoin, for example, has had a chance to benefit from its huge price boost over the past year, while Facebook and Google users have not shared in those companies’ growth.
The Gnosis team is taking this very long view. Their token sale was halted after that furious 12 minutes by an Ethereum-based bot that knew exactly what the fundraising goal was. It even returned more than $1 million to eager buyers who missed the cutoff. Gnosis’s co-founder Martin Koppelman says the company wants to use its remaining tokens not to enrich its creators, but to attract developers and users. That’s similar to the way that Uber has used cash subsidies to recruit riders and drivers, except that once those new recruits hold Gnosis tokens, they will have a serious stake in the platform’s future.


All over town, the parking meters are disappearing. Drivers now pay at a central machine, or with an app. It’s so convenient I sometimes forget to pay entirely—and then suffer the much higher price of a parking ticket. The last time that happened, I wondered: Why can’t my car pay for its own parking automatically?
Listen to the audio version of this article:Feature stories, read aloud: download the Audm app for your iPhone.
It’s technically possible. Both my car and my smartphone know my location via GPS. My phone already couples to my car via Bluetooth. An app could prompt me to pay for parking upon arrival.
Or imagine this: My car, which is already mostly a computer, enters an agreement to lease time from a parking lot, which is managed by another computer. It “signs” this contract just by entering the lot and occupying a parking space. In exchange, the car transfers a small amount of Bitcoin, the currency of choice for computers, into the parking lot’s wallet.
With computers handling the entire process, I’d never even be able to forget to pay for parking. The only way to fail would be for my car to run out of Bitcoin, in which case the parking lot has easy recourse: Because my car’s ignition is managed by a computer, the parking lot could just shut my vehicle down.
Scenarios like this are possible when blockchain—the digital transaction record originally invented to validate Bitcoin transactions—gets used for purposes beyond payment. In certain circles, the technology has been hailed for its potential to usher in a new era of services that are less reliant on intermediaries like businesses and nation-states. But its boosters often overlook that the opposite is equally possible: Blockchain could further consolidate the centralized power of corporations and governments instead.
* * *
In his book Radical Technologies, the urban designer Adam Greenfield calls cryptocurrency and blockchain the first technology that’s “just fundamentally difficult for otherwise intelligent and highly capable people to understand.” I was relieved when I read this, because I have been pretending to understand cryptocurrencies—digital money based in code-breaking—for years. Bitcoin is hard to grasp because it’s almost like a technology from an alien civilization. It’s not just another platform or app. Making sense of it first requires deciphering the political assumptions that inspire it.
Bitcoin is an expression of extreme technological libertarianism. This school of thought goes by many names: anarcho-capitalism (or ancap for short), libertarian anarchy, market anarchism. Central to the philosophy is a distrust of states in favor of individuals. Its adherents believe society best facilitates individual will in a free-market economy driven by individual property owners—not governments or corporations—engaging in free trade of that private property.
Anarcho-capitalism is far more extreme than Silicon Valley’s usual brand of technological individualism. For one, the tech sector’s libertarianism is corporatist in its bent, and amenable to government, if in a strongly reduced capacity. And Silicon Valley takes a broader approach to the liberating capacity of technology: Facebook hopes to connect people, Google to make information more accessible, Uber to improve transit, and so on.
The ancap worldview only supports sovereign individuals engaging in free-market exchange. Neither states nor corporations are acceptable intermediaries. That leaves a sparsely set table. At it: individuals, the property they own, the contracts into which they enter to exchange that property, and a market to facilitate that exchange. All that’s missing is a means to process exchanges in that market.
Ordinarily, money would be sufficient. But currency troubles market anarchists. The central banks that control the money supply are entities of the state. Financial payment networks like Visa are corporations, which aren’t much better. That’s where Bitcoin and other cryptocurrencies enter the picture. They attempt to provide a technological alternative to currency and banking that would avoid tainting the pure individualism of the ancap ideal.
This makes Bitcoin’s design different from other technology-facilitated payment systems, like PayPal or Apple Pay. Those services just provide a more convenient computer interface to bank accounts and payment cards. For anarcho-capitalism to work in earnest, it would need to divorce transactions entirely from the traditional monetary system and the organizations that run it. Central banks and corporations could interfere with transactions. And yet, if individuals alone maintained currency records, money could be used fraudulently, or fabricated from thin air.
To solve these problems, Bitcoin is backed by mathematics instead of state governments. The Bitcoin “blockchain” is a shared, digital record of all the transactions (or “blocks”) that have ever been exchanged. Every transaction contains a cryptographic record of the previous succession (the “chain”) of exchanges. Each one can thus be mathematically verified to be valid. The community of Bitcoin users does the work of verification. To incentivize the onerous work of cryptographically verifying each transaction in the chain that precedes it, the protocol awards a bounty—in Bitcoin of course—to the first user to validate a new transaction on the network. This is the process known as “mining”—a confusing and aspirational name for what amounts to computational accounting.
There’s a lot more detail that I am omitting. But the key to Bitcoin is that the network distributes copies of one common record of all Bitcoin transactions, against which individuals verify new exchanges. This record is the blockchain, which is sometimes also called the “distributed ledger”—a much more elucidating name. This is the missing element that’s supposed to allow the hypothetical anarcho-capitalist techno-utopia to flourish.
* * *
At least, that’s the theory. In practice, Bitcoin and other cryptocurrencies don’t really meet the ancap ideal. Perhaps it’s an impossible goal; imagining the end of both nation-states and corporations is even harder than imagining the end of capitalism itself. Greenfield speculates in his book that Bitcoin was never meant to be a store of value, like state-backed currency, but only a medium for exchange “between parties who would presumably continue to hold the bulk of their assets in some other currency.”
Anarcho-capitalism might seem fringe and unfamiliar to most people, but at least it helps explain the rationale behind cryptocurrency and blockchain. Unfortunately, those topics become even more confusing when Bitcoin and its kin get used in ways incompatible with their original inspiration—which turns out to be most of the time.
As a medium for exchange, Bitcoin is relatively limited. Some retailers, many tech-oriented, accept the currency for purchases, but it remains best known as a means to buy black-market goods on darknet exchanges like Silk Road. (The fact that such uses were illicit in the first place, the anarcho-capitalist would point out, is precisely the reason individual freedom-fighters should demand a decentralized market unbeholden to governments.)
But Bitcoin’s success has accidentally undermined its viability. Each Bitcoin transaction adds more encrypted data to the blockchain, requiring increasingly more computer power to verify (and to earn the associated commission). More computing power means more energy cost to run and cool the machines, which requires more capital and physical infrastructure to support. Those rising costs inspire centralization. Adam Greenfield tells me that two Chinese giants can control over half of the global Bitcoin mining operations. If they collaborate, a majority-control of the blockchain could allow them to manipulate it. That’s precisely the risk a decentralized currency was meant to avoid.
More often, Bitcoin has been used as a financial instrument instead of a currency. From tulips to tech start-ups, market capitalism is flexible enough to turn anything into a tradable security or futures commodity. Bitcoin hype has made it appealing for speculators certain to transfer their gains back into more stable state currencies, although its volatility makes it a difficult case either as a store of value or a medium of exchange.
The same hype driving cryptocurrency speculation has also attracted banks, governments, and corporations—exactly the authorities it was designed to circumvent. Financial services firms have taken an interest in cryptocurrency. Federal Reserve chair Janet Yellen has called for the Fed to leverage blockchain. Canada has been experimenting with a blockchain-backed version of its national currency, called CAD-Coin. Future cryptocurrencies operated by banks or governments might enjoy more productive use than Bitcoin.
But those futures also undermine cryptocurrency’s ancap aspirations. Corporations and governments re-centralize control, for one. But also, they undermine the discretion and anonymity that accompanies free trade in the ancap fantasy. When the local or central bank manages the cryptocurrency platform, it also gets a record of every transaction that takes place in that economy. One doesn’t need to be an anarchist to surmise potential downsides of that situation. Picture China mandating state cryptocurrency, tying the country’s proposed social credit system to that ledger. Or imagine if the North Carolina State legislature decided to issue all food stamp vouchers in crypto form to better manage their future use.
* * *
Even if Bitcoin’s utility and value might decline, the distributed ledger offers potential uses beyond simple currency exchange. In theory, any internet-connected device could participate in verified, distributed transactions.
Greenfield offers a simple example: the German startup Slock.it, which “gives connected objects an identity, the ability to receive payments, enter into complex agreements and transact without intermediary.” The simplest Slock.it device is a physical padlock that is connected to the internet. Networked locks are nothing new, thanks to the internet of things. But a blockchain-backed connected lock offers some additional capabilities. A distributed-ledger lock could enter into a “smart contract,” an agreement whose terms are implemented directly in code. If attached to an AirBnB rental, such a lock could be programmed to automatically release when a smartphone belonging to a pre-paid renter approaches. Likewise, it could be programmed to cease to unlatch after that tenant’s contract had terminated—or perhaps it could cut off the power or internet service if a sensor inside the property determined that its occupants were cavorting too loudly, or rifling through unauthorized cabinets.
Kik, a startup that makes a messaging app popular among teens, offers a more recent example of distributed-ledger tech in action. The company recently announced plans to introduce its own cryptocurrency, called Kin. Kik will automatically dole out Kin as rewards for developers who build apps on its platform, like stickers or chat bots. Kik’s CEO, Ted Livingston, presented the move as nothing short of emancipation from the oppression of ad-driven content platforms like Facebook and YouTube: “a cryptocurrency for an open future.”
Kin is built atop a platform called Ethereum, which is based on the same distributed ledger as Bitcoin. But Ethereum uses that technology to express a different aspect of the ancap model: contracts. For libertarians, contracts exist to facilitate market exchange, so smart contracts are always backed by currency (Ether, in Ethereum’s case). If Bitcoin is digital money for people, Ether is digital money for computers. It decides how to spend itself via software automation.
Why tout a private, distributed-ledger currency as an agent of liberation when it amounts to a complicated, software-backed, company-town store? One answer: It could give the workers a stake in the company store. In the world of cryptocurrency, this is known as an ICO or Initial Coin Offering. ICOs incentivize the use of an unproven platform, like Kik’s, by distributing an initial batch of cryptocurrency to early adopters. In theory, that value will increase if the platform becomes popular, creating a valuable base investment for its initial users.
In the extremist libertarian aspiration, smart contracts would allow anonymous actors to trade anything whatsoever in an untraceable way, via unregulatable markets. Instead, actual smart contracts, ICOs, and distributed ledger-backed devices mostly offer new ways to interface with the private technology industry. For example, in Brooklyn, a solar microgrid startup called Transactive sells clean energy to a community via Ethereum. And Toyota just announced a partnership with MIT to develop distributed ledger-based infrastructure for future autonomous vehicle services.
On that front, the anarcho-liberatarians share something in common with the plain-vanilla technolibertarians: a belief in the wisdom and righteousness of a fully computational universe. My hypothetical smart-contract parking meter, Toyota’s future blockchain-backed rideshare system, Slock.it’s blockchain lock, Kik’s Kin, Transactive’s solar grid—all are just technology companies enjoying the capitalization and publicity spoils of the latest hot trend. They might become more than that, of course. But in order to do so, something terrifying has to happen first.
* * *
Consider an off-the-cuff example of smart contracts from an Ethereum advocate:
An individual wants to purchase a home from another person. Traditionally there are multiple third parties involved in the exchange including lawyers and escrow agents which makes the process unnecessarily slow and expensive. With Ethereum, a piece of code could automatically transfer the home ownership to the buyer and the funds to the seller after a deal is agreed upon without needing a third party to execute on their behalf.
It sounds so easy. Who needs real-estate agents, closing attorneys, assessors, mortgage brokers, title insurers, municipal tax authorities, and all the rest? Just transfer some Ether after the computers shake hands.
But absent a global ancap revolution, those intermediaries are unlikely to disappear. Consider what would be required for distributed-ledger scenarios like this one become reality. Smart contracts require computational intermediation everywhere. Non-computational devices like parking lots and door locks and property deeds must become connected to computers. People would have to become willing to use machines that enter into decentralized contracts with other machines absent intermediary protection of government, law, banking, and other legacy infrastructures.
The problems with those old institutions are many. In a widely shared tale of voter suppression in the 2016 election, Eddie Lee Holloway Jr., a 58-year-old Wisconsin man, couldn’t vote because the state’s new voter-ID law demanded that he show proper identification. But an error on his birth certificate prevented him from getting a new ID. In a future run by the distributed ledger, a single copy of Holloway’s identification would be securely stored on the blockchain, easily verifiable when needed. For the tech evangelist, it offers a rational solution that would solve social ills by means of impartial technology. (On that note, blockchain-based digital IDs have also been proposed for refugees.)
It sure sounds good. But the scenario only works if the entire system of contemporary life becomes sufficiently interconnected to make it possible. All the departments of public health and the DMVs and the voter registration venues—not to mention the parking spaces and the automobiles and the power grids and all the rest—would have to cohere around a common understanding, so that the machines could execute smart contracts on their behalf. This would require a complete reinvention of public and private life.
A different reinvention is more likely. Instead of defanging governments and big corporations, the distributed ledger offers those domains enormous incentive to consolidate their power and influence. For people like Eddie Lee Holloway, Jr, who’s African American, that might mean even greater exclusion, as the very institutions that locked him out of the voting booth might suppress his transformation into a digital-ledger citizen in the first place.
Or if not, other traumas might yet face citizens like Holloway in a society run by blockchain. A mandated DNA-test could accompany citizens’ blockchainification, allowing their ethnic origins and medical predispositions to become attached to an identity record. Financial assets would also be connected, thanks to an underlying cryptocurrency account through which they make debits and credits. Not to mention all the personal insights already consolidated by services like Facebook.
Businesses might subscribe to this data. Thanks to distributed ledger, it could be used to prevent their automated doors from opening for people whom a smart-contract risk-assessment service rates below a threshold of desirability. Left outside, privately-contracted security robots might deploy ledger-backed ID scanners to sweep loiterers from private property. Once delivered and booked into jails, smart courts could automate sentences based on an automated assessment of future crime potential.
And that’s just America. Imagine how a mature authoritarian state would fare under the rule of blockchain. Is this starting to feel like a Black Mirror episode yet? For Adam Greenfield, the anti-authoritarian left has profoundly misunderstood the corner into which such an ambitious aspiration paints society. “I believe distributed ledger enables the kind of central control they’ve never in their worst nightmares contemplated,” he tells me. The irony would be tragic if it weren’t also so frightening. The invitation to transform distributed-ledger systems into the ultimate tool of corporate and authoritarian control might be too great a temptation for human nature to forgo.
* * *
If this sounds familiar, it’s because contemporary culture has been here before. The existing, comparatively modest surveillance and control technologies in use by Google, Facebook, and their ilk—whose impact on governance we now know all too well—proliferated on the assumption that technology could make life better and more efficient. Nobody chose this life, exactly. People adopted technology in sufficient numbers to allow industry, and the culture that follows it, to conclude that the market had decided what was best.
Likewise, Bitcoin’s triumph hinges mostly on the financial success of speculators who never had any intention of using it as currency, and who appear to have strip-mined it into oblivion in the process. Similarly, blockchain’s future seems tied to the short-term vision of investors and entrepreneurs willing to speculate on a hypothetical, distributed utopia without hedging against the consolidated autocracy it seems equally likely to realize. “This is what happens,” Greenfield says, “when very bright people outsmart themselves.”


When sprinters take their marks, they place their hands on the ground and position their feet onto angled blocks. Before the start signal, the runners rest their knees on the ground, then transfer their weight squarely on the blocks. As soon as the sound waves of the signal reach their ears, their feet catapult off the blocks. If the sprinter pushes off too soon, it means disqualification from the race. But sprints last 400 meters or less, so every millisecond spent on the blocks after the race has begun is a millisecond wasted.
The start of competitive foot races like those of the Olympics have only recently evolved into the test of nerves and alertness they currently are. The stadion race of 600 podes (locally determined “feet”) began the tradition of sprints at Olympia. It spread to the other Panhellenic games throughout the Mediterranean. As the Greeks designed and redesigned the games, they invented ways to make racers take the field in a uniform manner. First, they gave the runners a place to put their feet at the beginning of the race. Later, they designed a gate called a hysplex that released runners at the same time. When the modern Olympic sprints commenced in 1896, runners improvised their start, choosing to crouch or start in the ancient style as they wished. Then came the starting blocks.
The starting blocks helped runners take the crouching position developed in the late 19th century by American and Australian sprinters. They required all runners to direct their energy horizontally—instead of vertically—at the race’s start. But today, they do more than just help runners get off the line. They also detect false starts, putting the race behind the feet of the sprinter as much as in front of them. As a technology of fairness, the starting blocks helped turn foot racing into an ideal for egalitarian citizenship.
* * *
Ancient Greek stadiums show continual upgrades to the starting line of the race. The Greeks ran from a stone starting gate along a central rectangular track. Ropes stretched in front of the runners fell in unison onto the track, releasing them to begin the race. The precursor to the starting block, called a balbis, was redesigned at several stadiums to accommodate a starting gate, even when it meant reducing the number of runners or the width of the track to provide room for the starting gate. The perpetual reworking of the stone balbis is one indication of how seriously the Greeks took these athletic events.
Most contemporary knowledge of the hysplex comes from four tracks at Epidauros, Isthmia, Nemea, and Corinth. At both ends of the track, engineers placed stone support bases for a starting gate. These bases were filled in with wood, which was removed, oiled, and stored when games were not being held. Since the wood and ropes of the starting gate have not been preserved in the archaeological record, historians make guesses based on textual and artistic representations.
The art on one jar from Athens shows the start of the hoplite race. Two horizontal cords act as barriers. One grazes the knees, the other the waist. The cords stretch from one end of the stone starting block to the other. The cords connect to two posts, which were held up under tension by rope. Officials kept the posts upright while the athletes positioned themselves. At the start of the race, they released both posts simultaneously. The weight of the gate and the tension of the ropes brought both crashing to the floor, providing a visual and auditory signal that the race was afoot.   
Starting blocks built into the field make foot races a permanent part of the city. That gave blocks symbolic meaning as much as competitive utility. In Aristophanes’s Lysistrata, first performed in 411 BC, the starting gate serves as a metaphor for releasing women onto the political field, where they run an antiwar campaign. War might have helped inspire technological improvements to the device, too. The ropes and wooden base of the starting gate correspond with the development of the catapult, first mentioned in Athens in 355 BC. Some stadia constructed in this period seem to have been caught up with the interest in ropes and pulleys demonstrated by engineers of Philip II of Macedon.
Evidence for starting gates have been found at several sites of major Panhellenic games, including Isthmia, Olympia, and Nemea. Some suggest that the Greeks also engineered against false starts in other ways. Toe holds were carved into the blocks, forcing runners to adopt a wide stance. It provided balance, which reduced the likelihood of false starts. Runners had a fixed starting location, so they were more likely to stay in their lane, which was marked by cords or colored chalk. As is the case today, sprinters were not supposed to block or run into another athlete in order to win.
* * *
Starting blocks appeared in modern games thanks to an innovation that was made simultaneously on opposite sides of the world. Whereas Ancient Greek runners are depicted with one arm forward, using their undulating hands to build momentum, modern sprinters rely more on their legs. In 1887, the Aboriginal sprinter Bobby McDonald began from a crouching position in Sydney. Across the Atlantic, the Yale athlete C.H. Sherrill was photographed in the same pose in 1888. The crouching start became ubiquitous among college runners by 1890. Today, both the Government of Australia and Yale claim the first crouching start without mentioning the other.
The crouching position that modern starting blocks impose facilitates a horizontal surge of energy. This burst propels runners to higher speeds more quickly. Formerly, track stars dug holes for their toes in the dirt of the track to plant their feet for the start. George Bresnahan, of the University of Iowa, filed a patent for a foot support in 1927, changing the definition of a starting block from the ancient, permanent stone base to the modern, portable metal device with offset foot rests—the design competitors and spectators know today.
The blocks were not immediately used, as races would have been different enough to upset records. However, the benefits of starting blocks soon became apparent. In one case, film footage showed that Ralph Metcalfe dug his holes significantly back from his true start in the 200 meter event at the 1932 Olympics, but he refused a rerun of his third place finish in deference to an impressive and potentially unrepeatable American sweep of the podium. It took until the 1948 London games for starting blocks to appear at the Olympics. Early models were both heavy and susceptible to slipping from the force of the athlete’s takeoff.
In contrast to the Greek starting gate, contemporary athletes must take off from the starting blocks without the visual aid—and physical barrier—of a starting gate. This increases the possibility of a false start. In modern games, false starts are determined by an athlete’s reaction to the starting sound. Because sound waves take time to reach human ears, there is a lag between the starting sound and the athlete’s movement. Athletes take at least 0.110 of a second to respond. Given the small margins of victory in short sprints, even this delay can make a difference. In the 100 meter final at the 2016 Olympics in Rio, Usain Bolt, the world-record holder, reacted in 0.155 seconds. Since Bolt is a competitor whose tall frame lends to slower starts, the first fractions of a second are one place in the sprint where Bolt’s rivals can seek to gain an advantage. To ensure the fairest start possible, race officials began placing speakers behind each athlete to make the start equally audible for all competitors.
The starting blocks can also disqualify athletes who begin the race before the starting sound reaches their ears. Sensors within the blocks track the pressure the athlete’s body applies to the block. The latest Omega blocks take measurements 4,000 times per second, enough to tell judges if athletes moved their bodies while still on the starting blocks, or if they have reacted so fast that they must have begun the start before the signal. The technology has also inspired tactical use among sprinters. A 2003 rule change led some to believe that runners were deliberately committing a penalty-free first false start to encourage the unfortunate runner who ran the second false start to be disqualified. In 2010, international rules instituted a zero-tolerance policy for false starts.
* * *
For the ancient Greeks, starting blocks and gates mattered enough to set in stone and wood.  This was not simply because races formed a circuit that took athletes back to the same cities and stadia. It was also because the moment that everyone took their place on the starting line was an important one. At the start of the race, all competitors were equal. They took the field in unison. Athletes ran on a perfectly flat surface, one that bore no resemblance to the grassy fields and rocky hills of their homes or battlefields. The falling ropes of the starting gate showed that these sprints were signs of civilization.
In the modern Olympics, athletes did not originally have starting blocks or a starting gate. With the adoption of the crouching start, starting blocks have replicated the ancient concern for a coordinated movement in the modern games. The standardization of blocks has meant that athletes no longer face the race variables of the condition of the track and their adroitness at digging holes. In their ideal form, starting blocks and gates are a technology of democracy. Every man or woman can run a race, but the blocks are designed to turn that race into a contest of equals.
This article appears courtesy of Object Lessons.


I work at an ethics center in Silicon Valley.
I know, I know, “ethics” is not the first word that comes to mind when most people think of Silicon Valley or the tech industry. It’s probably not even in the top 10. But given the outsized role that tech companies now play, it’s time to focus on the ethical responsibilities of the technologists who help shape our lives.
In a recent talk, technologist Maciej Ceglowski argued that “[t]his year especially there’s an uncomfortable feeling in the tech industry that we did something wrong, that in following our credo of ‘move fast and break things,’ some of what we knocked down were the load-bearing walls of our democracy.”
This was not unforeseeable—or even unforeseen. In 2014, for example, in an article titled “How Facebook Is Shaping Who Will Win the Next Election,” Tarun Wadhwa cited a study published in 2012: “A 61-Milion-Person Experiment in Social Influence and Political Mobilization.” The study’s authors reported on  “a randomized controlled trial of political mobilization messages delivered to 61 million Facebook users during the 2010 US congressional elections. The results show that the messages directly influenced political self-expression, information seeking and real-world voting behaviour of millions of people.”
So we already knew that tools for “maximizing engagement” can shape the political sphere.  In 2014, Wadhwa concluded, “Whether it wants this responsibility or not, Facebook has now become an integral part of the democratic process globally.”
We also know that technology can be harmful to our democracy. Privacy invasions and algorithmic manipulation, for example, can limit the ability to research and formulate opinions, and then in turn affect how people express views—even via voting. When companies implement practices that are good for targeted advertising but bad for individuals’ democratic engagement (like, for example, the practices involved in the use of “dark posts” on Facebook, tied to the creation of psychological profiles for hundreds of millions of Facebook users in the U.S.), the benefits-versus-harms balance tilts pretty sharply.
Who minds that balance?
You often hear the adage that law can’t keep up with technology. What about ethics? Ethics, too, is deliberative, and new norms take some time to develop; but an initial ethical analysis of a new development or practice can happen fairly quickly. Many technologists, however, are not encouraged to conduct that analysis, even superficially. They are not even taught to spot an ethical issue—and some (though certainly not all) seem surprised when backlash ensues against some of their creations. (See, for example, the critical coverage of the now-defunct Google Buzz, or more recent reaction to, say, “Hello Barbie.”)
A growing chorus has argued that we need a code of ethics for technologists. That’s a start, but we need more than that. If technology can mold us, and technologists are the ones who shape that technology, we should demand some level of ethics training for technologists. And that training should not be limited to the university context; an ethics training component should also be included in the curriculum of any developer “bootcamp,” and maybe in the onboarding process when tech companies hire new employees.  
Such training would not inoculate technologists against making unethical decisions—nothing can do that, and in some situations we may well reach no consensus on what the ethical action is. Such training, however, would prepare them to make more thoughtful decisions when confronted, say, with ethical dilemmas that involve conflicts between competing goods. It would help them make choices that better reflect their own values.
Sometimes, we need consumers and regulators to push back against Big Tech. But in his talk titled “Build a Better Monster: Morality, Machine Learning, and Mass Surveillance,” Maciej Ceglowski argues that “[t]he one effective lever we have against tech companies is employee pressure. Software engineers are difficult to hire, expensive to train, and take a long time to replace.” If he is right, then tech employees might have even more power than people realized—or at least an additional kind of power they can wield. All the more reason why we should demand that technologists get at least some ethics training and recognize their role in defending democracy.
I work in an applied ethics center, and we do believe that technology can help democracy (we offer a free ethical-decision-making app, for example; we even offer a MOOC—a free online course—on ethical campaigning!). For it to do that, though, we need people who are ready to tackle the ethical questions—both within and outside of tech companies.
This article is part of a collaboration with the Markkula Center for Applied Ethics at Santa Clara University.


It’s time to fix the voting process.
American voting systems have improved in recent years, but they collectively remain a giant mess. Voting is controlled by states, and typically administered by counties and local governments. Voting laws differ depending on where you are. Voting machines vary, too; there’s no standard system for the nation.
Accountability is a crapshoot. In some jurisdictions, voters use machines that create electronic tallies with no “paper trail”—that is, no tangible evidence whatsoever that the voter’s choices were honored. A “recount” in such places means asking the machine whether it was right the first time.
We need to fix all of this. But state and local governments are perpetually cash-starved, and politicians refuse to spend the money that would be required to do it.
Among many other needed measures promoted by nonprofit and nonpartisan Verified Voting, Congress should require standardized voting systems around the nation. It should insist on rock-solid security, augmented by frequent audits of hardware and software. Recounts should be performed routinely and randomly to ensure that verified-voting systems work as designed. The paper ballot generated by the machine should be the official ballot.
What Congress should emphatically not do is allow or encourage online voting. The sorry state of cybersecurity in general makes clear how foolhardy it would be to go anywhere near widespread “Internet voting” in the foreseeable future.
There’s one benefit to note in our massively decentralized voting systems: It would be harder to steal a national election. But flipping just a few precincts in some key districts and states could have outsized impact. There’s every incentive for malicious actors to try, though I don’t know if Russia or anyone else had a direct-hacking impact in 2016. But why do we keep taking these kinds of risks?
For reasons that remain unclear, Congress has been largely uninterested in doing what’s needed to make voting safe, secure, and verifiable (perhaps because the existing system is how members got elected). In President Obama’s final weeks in office, the Department of Homeland Security added voting machines to its “critical infrastructure” list, but there’s no sign that Congress will back that statement with money to get things done, and under Trump all bets are off.
Barring a national commitment to getting this right, maybe the answer is to change direction entirely. Maybe we should abandon electronic voting systems and do everything on paper, and count by hand. We’d wait longer for results, a lot longer. If it ensured accurate results, though, I’d call that a reasonable trade.
This article is part of a collaboration with the Markkula Center for Applied Ethics at Santa Clara University.


When I told my mother my work focused on improving public trust in the news, she thought the idea was hilarious. “Trust? News?” I was a bit insulted. Her daughter—me, that is—has been a journalist for years. But she had a point.
Journalism has been struggling to stay afloat in an era when people expect information to come both fast and free. Now, competition by principle-free enterprises further undermines its very role and purpose as an engine for democracy.
The digital world has muddied formerly clear divisions between factual news, sales pitches, hoaxes, and hyper-partisan propaganda designed to incite. On social media and in online search, misinformation spreads with lightning speed. Lurid headlines loom far larger than the logos of news brands, which at least offer a clue to the origin of information and the care behind it.
Last fall, barely one third of Americans polled by Gallup expressed at least a fair amount of trust in the mass media “to report the news fully, accurately and fairly.”
To make collective decisions about our communities, our shared resources, and our government, we have to agree on basic facts. Today those facts can take on the shape of a funhouse mirror where we each see a distorted image tailored to our own expectations.
The Trust Project, which is a collaboration of news organizations around the world, aims to sharpen the picture by using technology to encourage accurate, ethically produced news and make it easy to find. Think along the lines of a nutrition label on a package of food, or a lab report that conveys your health status when you go in for a checkup.
Based at the Markkula Center for Applied Ethics in Santa Clara, California, the project has brought together more than 70 news organizations to work on a transparency system that would show how a story came to be and who stands behind it. Technology companies, which have become powerful distributors of news, are also contributing their expertise and a strong willingness to apply the results.
Imagine you encountered a piece of text or video in your social media newsfeed or while searching for news on your phone or computer. It would be marked clearly as news, opinion, or sponsored content designed to sell you something. If you clicked on the byline, you’d see the author’s background and other published work. Did that person have local expertise? Experience and knowledge covering the topic? Another click would take you to information about the news site itself. What commitments has it made to ethics, diversity, correcting mistakes? Who owns or funds the site, who is the leadership, when was it created?
For investigative or more controversial stories, you could see citations backing up the information presented and a short description of the reporting involved. If you liked, you could learn details about the site’s track record for reaching across economic background, race, age, gender and other differences to get the full story.
You’d quickly find what you needed to make informed choices about your news. The Trust Indicators would also send machine-readable signals to Facebook, Google, Bing, Twitter and other technology platforms. We’re already working with these four companies, all of which have said they want to use our indicators to prioritize honest, well-reported news over fakery and falsehood.
While some news organizations show bits and pieces of these features, until now they have not been standardized and structured across sites for the public to quickly find or platform machines to read.
The journalists working on this project aren’t attempting to prescribe the perfect news diet for the public. That would be self-serving, pompous, and dull. No, we’re asking people to tell us what they want and need from the news. We have conducted dozens of one-on-one interviews with consumers across the United States and Europe. Our questions: What do you value in news? When do you trust it? When have you had your trust broken?
Pundits complain about a naive public that likes cat videos and bias-confirming memes. Our interviews turned up something different: Thoughtful online readers who genuinely want to be informed about their own communities and the world.  Skeptical citizens who work very hard to gather a more complete, nuanced picture of issues and events than they believe news organizations typically provide.
Some members of the public are frustrated with journalism that seems thin, uninformed, biased against their community and replete with argument, anger and violence. They complain about opinion presented exactly like news. Some people are so fed up that they have simply disengaged. They want more humility from journalists, more recognition that in spite of journalists’ best aspirations, we do sometimes get it wrong.
Most of the people we talked to, though, valued the news and the people who produce it. Our interviewees told us that they knew journalists aspire to be objective, but we all have a perspective based on our life experiences. They wanted news organizations to be clear about their funding sources and agenda. They wanted to know more about the reporter and where she got her information.
They wanted to hear from people like themselves and unlike themselves—certainly more than the usual high-powered leaders in business and government. Many talked about more engagement. A news story, we heard, should offer tools like annotation and forms that would allow readers to contest claims, suggest more sources and propose ideas for reporting further. Journalists, people urged, should be more collaborative with the public they aim to serve.
Journalism executives listen eagerly to these insights but don’t always like what they hear. All that transparency may stretch their technology systems and eat up precious time. So far, however, many are still game to try. The Washington Post, Mic, and The Globe and Mail are in the early stages of rolling out our indicators, while many other organizations in the U.S. and Europe are preparing to do so. Technology platforms are mulling the best ways to apply the signals in their own environments. I hope they’ll all recognize the opportunity to become more relevant and responsive. A vibrant, free press that the public can trust is vital to democracy.
This article is part of a collaboration with the Markkula Center for Applied Ethics at Santa Clara University.


The audio recording of a congressional candidate accused of body-slamming a reporter on the eve of a special election in Montana is extraordinary for several reasons.
If you haven’t listened to it yet, and you really should, it’s essential to at least read the short transcript:
Ben Jacobs, a reporter for The Guardian: ...the CBO score. Because, you know, you were waiting to make your decision about health care until you saw the bill and it just came out...
Greg Gianforte, the congressional candidate: Yeah, we’ll talk to you about that later.
Jacobs: Yeah, but there’s not going to be time. I’m just curious—
Gianforte: Okay, speak with Shane, please.
[loud scuffling noises, an even louder crash, repeated thumping]
Gianforte: [shouting] I’m sick and tired of you guys!
Jacobs: Jesus chri—!
Gianforte: The last guy that came in here, you did the same thing! Get the hell out of here!
Jacobs: Jesus!
Gianforte: Get the hell out of here! The last guy did the same thing! You with The Guardian?
Jacobs: Yes! And you just broke my glasses.
Gianforte: The last guy did the same damn thing.
Jacobs: You just body-slammed me and broke my glasses.
Gianforte: Get the hell out of here.
Jacobs: You’d like me to get the hell out of here, I’d also like to call the police. Can I get you guys’ names?
Unidentified third man: Hey, you gotta leave.
Jacobs: He just body-slammed me.
Unidentified third man: You gotta leave.
This is an astonishing encounter. First, there is the simple fact of it. A man who is perhaps hours away from being elected to the U.S. Congress—a person who is pledging to represent the interests of his fellow Americans—can be heard shouting and allegedly attacking a citizen who calmly asked him what he thought about his party’s health-care plan.
I have experienced many tense confrontations with elected officials in my life as a journalist. They have shouted at me and run away from me—into taxi cabs, state-house elevators, even (hilariously) the little slow-moving trolley cars under the U.S. Capitol. Once, the former mayor of Honolulu—at 6 feet 7 inches tall, towering over me—screamed at me after I asked him a question, about historic preservation of all things, on the steps of City Hall.
Reporters are accustomed to this treatment. What appears to have happened to Jacobs is something else entirely, and it should frighten any citizen who believes in democracy. (And not least of all because the Gianforte campaign’s description of what happened contradicts what anyone can hear on the tape.)
There’s something just as frightening in the fact that the audio—shocking as it is—is so ordinary these days. I don’t just mean the alleged assaults in recent weeks against journalists like Jacobs in Montana; or Dan Heyman, the reporter arrested for shouting a question at the Health and Human Services secretary, Tom Price, as Price toured the West Virginia Capitol two weeks ago; or John M. Donnelly the reporter who says he was pinned to the wall at the Federal Communications Commission after he tried to ask the commissioner Michael P. O’Rielly a question.
The ubiquity of recording and broadcasting technology has given Americans an unprecedented view of how frequently and grievously people in positions of power will intimidate, assault, and even kill ordinary citizens in the United States.
Why should anyone be surprised to hear that a congressional candidate may have grabbed a reporter by the throat, thrown him to the ground, and repeatedly punched him—that’s how a Fox News crew in the room described Gianforte’s actions—when we’ve seen the blood-soaked T-shirt of a man killed by police during a routine traffic stop, and the stoic face of the 4-year-old girl who watched it happen?
We have seen it all before and we will see it again. And it’s still not clear whether watching or listening, which is itself a kind of knowing, prevents future violence. Videos of police violence against black citizens are everywhere. Attacks against American citizens who question public officials in public spaces seem to be on the rise. All while corporate and government surveillance is rampant. All while the president of the United States calls information-gathering citizens his “enemies” and “the opposition” and “the lowest form of life.”
There are still strong institutional responses—from advocates for police-worn body cameras, for example, and from the three Montana newspapers that took the rare step of rescinding their endorsements of Gianforte last night. But such steps can only do so much. The public’s faith in myriad institutions is steeply declining.
Citizens have the power to bear witness, to document, to question—and to elect—our nation’s top legislators. It’s unclear how last night’s altercation, or the recording of it, will affect the Montana election, if at all. Many voters already submitted ballots in early voting, before the alleged assault occurred. Still, the outcome of the race will offer a glimpse of how Americans perceive the status of their own rights in democratic society. The rest of us will be watching.


How I tire of journalists asking whether and how technology is bad for society, forever starting with what could go wrong, hunting for blame.
Is technology hurting democracy? No. Can technology help save democracy? No.
These are the wrong questions. We, the people—and we, the media—are hurting democracy. It is in our hands to save it if we still can. Democracy’s enemies and saviors will use whatever tools and technologies are at hand.
The better question: What is wrong with our democracy? Until we know the problem, we cannot build the solution.
My diagnosis: I think we are allowing ourselves to be ruled—in every sense of the word—by emotion over rationality, fear over facts. That opens the door for the cynical and the ignorant, those greedy for power or money, to exploit our weakness, to play to some dark and empty hole in our souls, arguing that they alone are the solution.
The tool they use to degrade democracy is not Facebook or Twitter. Their weapon is the strange. To play to our fears, they must project the spectre of an enemy, an other who is to blame for our problems, who will take our jobs, addict our youth, bomb us in the mall, run us over on the street. That enemy is the stranger.
So we need to make the stranger less strange. We should meet the people we are told to fear, convening communities in conflict into informed and civil conversation. Facebook could help do that, for I think its ultimate potential as a platform is not just to connect us with the people we know but to introduce us to strangers, providing a safe space where people can tell their own stories and try to understand other worldviews.
News media can play a role here, too. Witness an experiment by the local Alabama news site AL.com and the startup Spaceship Media, which brought together Trump voters from the South and Clinton voters from the West on Facebook to meet, listen to each other, find where they disagreed, and—here’s the good part—call on journalists to help them combat misunderstanding with fact.
The lesson for  us in the press: We must learn to listen to the public conversation before we can hope to inform it. We must hear, understand, empathize with, and reflect communities’ concerns and needs to earn their trust. Then and only then will we have any hope of calling them to the rational dialogue and collective discernment informed by fact that define a functioning democracy.
We in media can no longer expect every citizen to come to us and our content as the destination, as if we alone are the solution. We must go to the people where they are having their conversations to listen and then bring value. We have many new ways to do that, which are—yes—created by technologists: Facebook, Twitter, YouTube, Meetup, Reddit, Google, Snapchat, Instagram are a few. How we use them is up to us.

This article is part of a collaboration with the Markkula Center for Applied Ethics at Santa Clara University.


I’m a reporter first, and a writer second, which means I often find myself writing in odd places. Not just geographically unusual, though there’s that, too. I write everywhere, with whatever technology is at hand.
Most of the time, I’m typing away in a plain text editor on my laptop. But I still write first drafts in reporter’s notebooks, and in the Notes section of my iPhone, and on scraps of paper when necessary.
Now here’s a first for me: I’m writing a story for The Atlantic in MacWrite 4.5, the word processing program first released with the Apple Macintosh in 1984 and discontinued a decade later. So here I am, awash in 1980s computing nostalgia, clacking away in an emulated version of the original software, thanks to the Internet Archive.
The only problem is, how am I going to file this story into The Atlantic’s 2017 web based content management system? (Also, the hyphen key isn’t working.) But more on that in a minute.
First, let me get out of here and switch back to my regular text editor. The Internet Archive’s latest in-browser emulator lets anyone with internet access play and use dozens of games and programs originally released for the first Apple Macintosh computers in all of their black-and-white, low-resolution glory. (Ah, so nice to have that hyphen back.)
Along with MacWrite, the collection includes MacPaint, Dark Castle, The Oregon Trail, Space Invaders, Frogger, Shuffle Puck, Brickles, Prince of Persia, and dozens more. The emulator doesn’t just launch the software itself, but situates users in the old-school Mac operating environment, meaning you often find yourself looking at a 1984-style desktop, and opening the program yourself.
“The presentation represents some shift in philosophies, in terms of what we wanted to do,” says Jason Scott, an archivist at the Internet Archive. Whereas Scott went with a “shock and awe” approach to earlier software emulators—making hundreds of programs available all at once—he decided to go for a more methodical, curated strategy this time. One big reason for this is quality control. He’s still fielding tech-support requests for the MS-DOS emulator the archive released in 2014. (It includes thousands of titles.) But Scott also knew the early Mac programs that people would want to see at the outset.
“The main one was Dark Castle,” Scott told me. “Everyone remembers Dark Castle because it was a particularly well-made, good-looking game—but not even a fun one, I want to point out! People playing it on the Mac emulation are not happy. There are reviews.”
Reviews like: “I can't tell if the emulator is laggy, making my controls unresponsive? Or is this just a terrible game? Maybe a bit of both,” as one person commented on the site.
“They are like, ‘This runs too slow for it to be good,’” Scott told me, “when what they really mean is the game was originally so unfair.”
“But it looks beautiful, and the sound is beautiful, so I knew Dark Castle would be a big deal,” he added.  
For what it’s worth, I only vaguely remember Dark Castle from when I had an Apple IIc. When I tried playing it on the emulator this morning I was repeatedly killed by rabid bats, which I can confidently say is a reflection of my own rustiness and has nothing to do with the emulator quality. (It seemed to run pretty smoothly to me.)
But regardless of how well they run, the big question is why it’s worth the drudgery and the painstaking work of presenting ancient programs this way in the first place.
“The existential questions,” Scott said. “What is all this for? What do people need from the original Mac operating systems in the modern era?”
The Internet Archive focused on the Apple II era for a few reasons: It was a finite period of time, it represents a particularly rich moment in computing history, and people remain especially interested in the era. “Nostalgia, to be honest, is a huge chunk of it,” he added. “You’ve got people who come in, and look at the old thing, and they’re happy about the old thing, and then they move on.”
If all goes a planned, the next two emulators will be for the Commodore 64, which predated the early Macintosh; then Windows 98, which came after it. (“That’s only if it works,” Scott emphasized.)
Emulators can be quite buggy, given their complexity. A browser-based system involves the emulated machine running inside the browser's javascript environment, all within the computer running that browser. So, basically, “you’re running a computer within a simulated computer within another computer," Scott says. “It’s crazy.”
Scott’s also hoping to stretch the very idea of what people can do with emulators.
“The initial burst to emulation on the web was about removing the barrier to old software,” Scott told me. “The next realm will be that you can output the data that’s being generated and export it to your modern machine. That’s basically one developer away from happening right now. That’s the kind of thing people eventually will want and get.”
In the meantime, you can’t copy and paste text from the MacWrite emulator back to a contemporary word processor, for example—which is why I had to retype the opening to this story, letter by letter, just to get it into The Atlantic’s web-publishing program. This is still much easier than my predecessors had it, back when the Macintosh was brand new. It was around that time that my colleague James Fallows wrote a long piece for The Atlantic about his own adventure into computerdom. In 1982, he was using a Processor Technology SOL-20 that had 48KB of random access memory. This was miraculous to him then, as were the floppy disks it took, and the printer he hooked up to the machine—it spit out about one page per minute.
It wasn’t all peachy, even for an early adopter like him. There was the time his computer broke in dramatic fashion, sending him back to his old Smith-Corona typewriter for a full month. And also, Fallows wrote: “Computers cause another, more insidious problem, by forever distorting your sense of time.”
What he meant was that computers change people’s expectations about what we should be able to do, and how quickly we should be able to do it. But this observation, made back in 1982 about machines that were quite different from the ones we use today, also got me thinking about how technology collides with people’s perceptions of time as we look back at it years later. Once-miraculous systems seem impossibly slow. They make contemporary software—and the hardware like smartphones running that software—seem newly extraordinary. Watching a 35-year-old program do what it was designed to do is also an implicit reminder that the best tools we have today will, before too long, seem absurd in their limitations.
And we’re able to see all this because so many people, improbably, save objects like old floppy disks and computers.  “I actually still have the SOL-20, walnut case and all,” Fallows recently told me when I asked him what ever happened to it. Scott, from the Internet Archive, says he’s been flooded with requests from people who want to share the programs they’ve held onto all these decades.
“One person, he wasn’t comfortable mailing his floppies to us, so we had to mail him the equipment,” Scott said. “And now he is bringing up one of a kind—or, I should say, extremely rare—software.” His programs, which will be added to the emulator, include original games that are highly sought-after by collectors, and at least one piece of software that was never available commercially.
“This emulation is bringing back into the froth of contemporary culture the existence of all these old programs,” Scott said. “They’re no longer just words on a page.”
Or in my case, they are words on a page. Words rendered in Apple’s familiar old Chicago typeface, materializing on the screen just the way I remember it from so very long ago.


Over the last 20 years, the technology industry has become the most powerful industry in the world, boasting seven of the 20 most profitable companies. Last year, Apple literally doubled the profits ($53.4 billion) of the second-most profitable company, J.P. Morgan Chase ($24.4 billion). And when it comes to market value, tech companies sweep the top five: Apple, Google, Microsoft, Amazon, and Facebook. These companies are not only huge and profitable; they’re also growing.
By most measures, though not all, this power is concentrated in one specific region, the Pacific Coast, and even more tightly centralized in the San Francisco Bay Area. Incredibly, three of those five most valuable companies are located in three adjacent little towns in Silicon Valley. The total distance from Facebook in Menlo Park to Alphabet (née Google) in Mountain View to Apple in Cupertino is just 15 miles.
These companies—with apologies to Intel, Oracle, and Cisco—have become the Big Three of Silicon Valley.
Detroit had a Big Three for decades: General Motors, Ford, and Chrysler. They were also amazingly profitable, industry-leading, and birthed a global industry. In the late 1950s, these three companies had over 90 percent market share in the U.S. car market, which was also the world’s largest.
Now, companies from a similarly small region occupy a similarly dominant role in the economy, which has powered economic growth over the last several decades. But a comparison between Detroit’s Big Three and Silicon Valley’s shows how much the economy around any individual company or place has changed.
* * *
Investors now value tech as they once valued automotive (and oil) companies.
It was the IPO of the decade. Thousands of people flocked to brokers hoping to get their hands on some of the paper from one of the century’s most innovative and respected companies. Finally, finally, the common person could share in the wealth generated by the genius of … Ford.
The year was 1956, and Ford, privately held since its inception by the Ford family and (later) the Ford Foundation, was accessing the public markets. More than 10 million shares went on the market and were immediately snatched up by hundreds of thousands of investors at an opening price of $64.50. The Ford Foundation made $642.6 million in the sale.
It was the biggest IPO ever, as befit the automotive industry, which was the biggest in everything around the mid-century. Likewise, at the time Ford went public, the true behemoth of the American economy, General Motors, was the nation’s most valuable stock, running $263.27. And for good reason.
These companies make a ton of money.
In the second (1956) edition of the Fortune 500, Ford held the third slot in revenue and profit. That year, the company made $437 million dollars. General Motors took the top spot by becoming the first company to break $1 billion in profit that year ($1.19 to be exact). Only 16 companies even made $100 million in 1956. Chrysler was the least profitable of those companies, eking into the echelon with $100.1 million in profits.
The only rival the car industry had was the oil industry, which had the number-two company on Fortune’s list, Exxon Mobil, as well as seven others in the top 20 most profitable companies.
All this to say: making cars and fueling them dominated the American profit-making enterprise. Hell, even the two big tire manufacturers were among the top 35 profit-makers of 1956.
Cars were national. Tech is global.
But there are crucial differences between Detroit’s Big Three and Silicon Valley’s. One is that Silicon Valley’s companies are fully global enterprises.
Since 2015, the majority of Facebook’s ad revenue comes from overseas. Apple crossed that threshold in the first quarter of 2010, and now roughly two-thirds of the company’s revenue comes from abroad. Google, too, has long made a majority of its money outside the U.S., though its home country represents nearly half its revenue.
In fact, all the money that these companies are making overseas is one reason why they are valued so highly, Harvard Business School’s Shane Greenstein told me. “Since the election, the markets have factored in a presumed ‘tax holiday’ that allows firms to repatriate their foreign earnings without U.S. taxes,” Greenstein said. “That especially shapes the values of Apple and Google.”
Since the election, Facebook is up 11 percent, Google is up 21 percent, and Apple is up a gobsmacking 34 percent. Perhaps this is even more remarkable, given that tech company employees gave Hillary Clinton 60 times the money they gave to Donald Trump ($3 million to $50,000).
The tech labor force is a tiny fraction of the automotive industry’s.
The other crucial difference is that tech’s leading companies employee far fewer people than Detroit’s Big Three did. This point can be made in the single chart above,  but it’s worth unpacking in three ways.
One, even though the big industrial giants did employ a lot of people, by the 1950s they were already automating away some of the jobs that they’d just created by building huge factories. “Between 1948 and 1967—when the auto industry was at its economic peak—Detroit lost more than 130,000 manufacturing jobs,” the historian Thomas J. Sugrue has written. To me, that’s startling. This was the absolute golden age of manufacturing, yet in the seat of the most important industry, companies were shedding jobs.
Two, the car companies’ employees were far more concentrated in Detroit and the surrounding cities than the tech companies’. Apple only has 25,000 employees in the “Santa Clara Valley.” Google likely has around 20,000 at its home. And let’s call it at around 6,000 Facebook folks in Menlo Park.
Three, the tech companies have many, many, many subcontractors, from content moderators in the Philippines to manufacturing people at Foxconn in China to custodians on their own campuses to bus drivers dragging people up and down from San Francisco. The way modern companies work, they try to keep employees off their own balance sheets unless absolutely necessary, especially lower-wage workers.
The original Big Three were the motive power for a whole region’s economy. By employing so many people at decent wages, they created broad-based prosperity. In Silicon Valley, the wealth that the Big Three create goes to a much smaller slice of the population, building wealth for thousands instead of hundreds of thousands of workers. In 2016, Facebook generated $600,000 of net income per employee.
That is to say, the tech world, for all its disruptions, is a supercharged example of how the American economy as a whole works right now: The skilled and the already rich make huge amounts of money, and everyone else gets the leftovers.


Thinking about what technological innovation has done to journalism in the past two decades can be a dizzying experience. People have more data, better maps, prettier visualizations, more push notifications, faster fact-checking, and so on.
Yet there is a unifying feature behind all of these innovations, and it has to do with the role of media and the public in a democracy.
The news media, the argument goes, must provide the rationally-minded members of the public with enough information for them to see a clear and accurate picture of the world, and then become deliberative citizens. In that regard, technology could help news reports to be more accurate, data-driven, timely, fact-checked, with rich multimedia embellishment.
Technologically-enhanced journalism was supposed to become better at conveying the complexities of our reality to the public. Why, then, instead of an enlightened citizenry, did we then find ourselves facing a horde of hateful trolls, hysterical fake news outlets, a news agenda led by Russian hackers, and a never-ending spiral of conspiracy theories?
Maybe something was lost along the way. One of the fundamental problems with that vision of the role of media in democracy—that only imagines media as neutral transmitters of information on which the public then rationally deliberates—is that it might not be enough for the news media to hold a mirror that seek to reflect reality as accurately as possible.
A democratic public only emerges when its members feel concerned with something, and therefore become a public that cares. Here, the public is not an aggregate of rational individuals, but a community that realizes that it is affected by some issues. And to be affected, to be concerned, one has to have some kind of experience or sensation. Journalism, then, should also pay attention to what we could call “sensationalism.” Not in its derogatory meaning of exaggerating facts and events in an inaccurate way, but rather that our senses and perceptions, our sensations, inform knowledge in the most basic and important ways.
Among the technological innovations of the last decade, there’s a discrete yet enduring format that may fulfill such an alternative, “sensational” vision of the role of media in democracy: podcasts.
Of course, there is a wide variety of podcasts styles and tones, but with their conversational color and their immersion in sound and atmospheres, they have the potential to make you feel things. Podcasts bring you to places you’ve never been, they give you the impression of sharing an animated kitchen-table banter (or a loud bar argument) with a couple of friends. In that regard, podcasts are a “sensational” medium, a quality that may explain why millions of listeners tune in regularly and listen to long-form episodes that defy all common-sense knowledge about the shortness of our attention span.
One of my current favorites is Reply All, a Gimlet Media podcast that explores internet culture. Its hosts produce silly and fascinating episodes about, among many other curiosities, videos of rats eating a slice of pizza in the New York subway. But, springing from the same wide-eyed wonder about anything that pops-up from the weird corners of the internet, they also regularly bring about smart and honest reporting about phenomena that shine a vivid light on the current political landscape.
Even further from a traditional current affairs beat is the endless stream of podcasts about TV shows produced by Bald Moves. The two hosts, who are ex-Jehovah witnesses from the Midwest, record absurdly long podcasts where they just chat about the latest episode of Game of Thrones or Westworld. Their success (20 million downloads, and counting) may look like another embodiment of the futility of pop culture, until you realize that part of what they do for hours on is to meticulously debunk crazy fan theories—patiently drawing a line between the factual, the plausible, and the ludicrous. Which seems like a useful skill for a democratic public to have.
Freed from the stranglehold of objective or neutral reporting, podcasters act as storytellers rather than merely as journalists, allowing them to take their audiences around unexplored territories that listeners can experience, and maybe care about.
Sounds familiar? Maybe because that’s the recipe of talk radio, that has perfectly understood the power of someone just talking to an audience. What podcasting adds to the mix is a diversity of voices that were not heard before, and a capacity to reach audiences that were not in the habit of tuning in to the radio at the same time every day or every week.
That seamless integration of podcasts in people’s lives might be the key feature of what is otherwise a relatively low-tech medium that pretty much recycles the codes and craft of radio. Flexibility and chronicity—whenever and wherever you want, but you’ll hear from us again next week—allow podcasters to build a relationship with their audience, a relationship that is made of sensations, friendliness, and familiarity. Not a spectacular innovation, in terms of technology, but maybe just enough of a shift to realize what media theorist James Carey saw as the role of media, that is, to be the “conversation of our culture.”
This article is part of a collaboration with the Markkula Center for Applied Ethics at Santa Clara University.


There’re some really bad people who harass journalists. Women and minorities, especially, are the targets of extreme vitriol. Yet many newsrooms do little or nothing to attempt to protect their employees, or to think through how journalists and organizations should respond when harassment occurs.
Harassers and trolls have multiple motivations, often simple racism or misogyny, or in support of misinformation, or to suppress law enforcement or intelligence operations. Frequently, what appears to be multiple harassers are actually sock puppets, Twitter bots, or multiple accounts operated by single individuals.
Sustained harassment can do some serious psychological damage, and I speak from personal experience. Outright intimidation is a related problem, suppressing the delivery of trustworthy news—the kind of news reporting that is vital to democratic governance.
The usual solution is to ignore trolls and harassers, but they can be persistent, and they often game the system successfully. You can mute or block a harasser on Twitter or Facebook, but it's easy enough for them to create a new account in most systems.
If you're knowledgeable in Internet forensics, you can sometimes trace a harasser’s account, and “dox” them—that is, post personally identifiable information as a deterrent. However, that really needs to be done in a manner consistent with site terms and conditions, maybe working with their trust and safety team. (Seriously, this is a major ethical and legal issue.)
Or, if you have a thick skin, you can respond with “shock and awe,” that is, with a brutal response in turn. Or, you can reason with them, which has sometimes been known to work. Retaliation against professionals, however, often backfires. They’re usually well-funded, without conscience, and are often very smart.
One method to address rampant harassment would be for news organizations to work with their security departments to evaluate the worst abuse, and do risk assessment. Sometimes threats are only threats—but sometimes they’re serious. News organizations might share information regarding harassers, while respecting the rights of the accused and the terms and conditions of the organizations involved. There are also serious legal and ethical considerations here, to be considered.
Perhaps news orgs could enlist subscribers or other friends to bring harassment to light. Participants in such a system could simply tweet to the harasser an empty message, or with a designated hashtag, withdrawing approval while avoiding bringing attention to the actual harassment. The empty message might communicate a lot, in zero words.
I believe that the targets of harassment need help from platforms, and here’s the start of a way that could happen. I’m attempting to balance fairness with preventing harassers from gaming the system, so please consider this only a start.
Let’s use Twitter for this thought experiment, mostly because I understand it, and they’re genuinely trying to figure this out.
Suppose you’re a reporter who is a verified user, and you get a harassing tweet. You’d do a quote retweet to a specific account as a way to report the harassment. That specific account would be a bot which could begin to analyze the harassing tweet. The bot would enter the email and IP addresses of the tweet into a database.
Periodically, a process would run to see if there’s a pattern of harassment from that IP or email address, and if so, that account could be suspended and contacted.
While most journalists would find it easy to do such a retweet, perhaps this should be more open to all, which could involve a harassment report button or option in the menu on a particularly tweet. (There’s a button and other means within the Twitter UI to do some of this, and Twitter has signaled that more’s on the way.)
News orgs also need to step up to protect their own reporters.
They could enlist subscribers or other friends to bring harassment to light. Participants in such a system could simply send an automated tweet to the harasser that says “This account has been reported for harassment and is being monitored by the community.” This type of system publicly tells harassers “you are on notice” and the community is watching. Note that this might be easily gamed, unless from verified journalists or similar.
Since this is a significant job, social networks may want to test organizing a volunteer community—like the one Wikipedia has—to help monitor the reports and accounts. Social networks can take it a step further and have trained members of the community respond to some of the harassers (not the bots) to discuss why the tweets were reported for harassment. Teaching moments are important to address harassment. If the social media account user continues the harassment, they get permanently banned from the social network. Some online games have adapted a similar strategy and have had some success with this approach.
I realize these ideas are fairly half-baked; the devil’s in the details. I’m also omitting a lot of detail, since that deeply detailed info could help harassers game this or other systems. In any case, we need to start, somewhere. Harassment and intimidation of reporters is a real problem, with real consequences for democracy.
This article is part of a collaboration with the Markkula Center for Applied Ethics at Santa Clara University.


In 2013, a mysterious group of hackers that calls itself the Shadow Brokers stole a few disks full of National Security Agency secrets. Since last summer, they’ve been dumping these secrets on the internet. They have publicly embarrassed the NSA and damaged its intelligence-gathering capabilities, while at the same time have put sophisticated cyberweapons in the hands of anyone who wants them. They have exposed major vulnerabilities in Cisco routers, Microsoft Windows, and Linux mail servers, forcing those companies and their customers to scramble. And they gave the authors of the WannaCry ransomware the exploit they needed to infect hundreds of thousands of computer worldwide this month.
After the WannaCry outbreak, the Shadow Brokers threatened to release more NSA secrets every month, giving cybercriminals and other governments worldwide even more exploits and hacking tools.
Who are these guys? And how did they steal this information? The short answer is: We don’t know. But we can make some educated guesses based on the material they’ve published.
The Shadow Brokers suddenly appeared last August, when they published a series of hacking tools and computer exploits—vulnerabilities in common software—from the NSA. The material was from autumn 2013, and seems to have been collected from an external NSA staging server, a machine that is owned, leased, or otherwise controlled by the U.S., but with no connection to the agency. NSA hackers find obscure corners of the internet to hide the tools they need as they go about their work, and it seems the Shadow Brokers successfully hacked one of those caches.
In total, the group has published four sets of NSA material: a set of exploits and hacking tools against routers, the devices that direct data throughout computer networks; a similar collection against mail servers; another collection against Microsoft Windows; and a working directory of an NSA analyst breaking into the SWIFT banking network. Looking at the time stamps on the files and other material, they all come from around 2013. The Windows attack tools, published last month, might be a year or so older, based on which versions of Windows the tools support.
The releases are so different that they’re almost certainly from multiple sources at the NSA. The SWIFT files seem to come from an internal NSA computer, albeit one connected to the internet. The Microsoft files seem different, too; they don’t have the same identifying information that the router and mail server files do. The Shadow Brokers have released all the material unredacted, without the care journalists took with the Snowden documents or even the care WikiLeaks has taken with the CIA secrets it’s publishing. They also posted anonymous messages in bad English but with American cultural references.
Given all of this, I don’t think the agent responsible is a whistleblower. While possible, it seems like a whistleblower wouldn’t sit on attack tools for three years before publishing. They would act more like Edward Snowden or Chelsea Manning, collecting for a time and then publishing immediately—and publishing documents that discuss what the U.S. is doing to whom. That’s not what we’re seeing here; it’s simply a bunch of exploit code, which doesn’t have the political or ethical implications that a whistleblower would want to highlight. The SWIFT documents are records of an NSA operation, and the other posted files demonstrate that the NSA is hoarding vulnerabilities for attack rather than helping fix them and improve all of our security.
I also don’t think that it’s random hackers who stumbled on these tools and are just trying to harm the NSA or the U.S. Again, the three-year wait makes no sense. These documents and tools are cyber-Kryptonite; anyone who is secretly hoarding them is in danger from half the intelligence agencies in the world. Additionally, the publication schedule doesn’t make sense for the leakers to be cybercriminals. Criminals would use the hacking tools for themselves, incorporating the exploits into worms and viruses, and generally profiting from the theft.
That leaves a nation state. Whoever got this information years before and is leaking it now has to be both capable of hacking the NSA and willing to publish it all. Countries like Israel and France are capable, but would never publish, because they wouldn’t want to incur the wrath of the U.S. Country like North Korea or Iran probably aren’t capable. (Additionally, North Korea is suspected of being behind WannaCry, which was written after the Shadow Brokers released that vulnerability to the public.) As I’ve written previously, the obvious list of countries who fit my two criteria is small: Russia, China, and—I’m out of ideas. And China is currently trying to make nice with the U.S.
It was generally believed last August, when the first documents were released and before it became politically controversial to say so, that the Russians were behind the leak, and that it was a warning message to President Barack Obama not to retaliate for the Democratic National Committee hacks. Edward Snowden guessed Russia, too. But the problem with the Russia theory is, why? These leaked tools are much more valuable if kept secret. Russia could use the knowledge to detect NSA hacking in its own country and to attack other countries. By publishing the tools, the Shadow Brokers are signaling that they don’t care if the U.S. knows the tools were stolen.
Sure, there’s a chance the attackers knew that the U.S. knew that the attackers knew—and round and round we go. But the “we don’t give a damn” nature of the releases points to an attacker who isn’t thinking strategically: a lone hacker or hacking group, which clashes with the nation-state theory.
This is all speculation on my part, based on discussion with others who don’t have access to the classified forensic and intelligence analysis. Inside the NSA, they have a lot more information. Many of the files published include operational notes and identifying information. NSA researchers know exactly which servers were compromised, and through that know what other information the attackers would have access to. As with the Snowden documents, though, they only know what the attackers could have taken and not what they did take. But they did alert Microsoft about the Windows vulnerability the Shadow Brokers released months in advance. Did they have eavesdropping capability inside whoever stole the files, as they claimed to when the Russians attacked the State Department? We have no idea.
So, how did the Shadow Brokers do it? Did someone inside the NSA accidentally mount the wrong server on some external network? That’s possible, but seems very unlikely for the organization to make that kind of rookie mistake. Did someone hack the NSA itself? Could there be a mole inside the NSA?
If it is a mole, my guess is that the person was arrested before the Shadow Brokers released anything. No country would burn a mole working for it by publishing what that person delivered while he or she was still in danger. Intelligence agencies know that if they betray a source this severely, they’ll never get another one.
That points to two possibilities. The first is that the files came from Hal Martin. He’s the NSA contractor who was arrested in August for hoarding agency secrets in his house for two years. He can’t be the publisher, because the Shadow Brokers are in business even though he is in prison. But maybe the leaker got the documents from his stash, either because Martin gave the documents to them or because he himself was hacked. The dates line up, so it’s theoretically possible. There’s nothing in the public indictment against Martin that speaks to his selling secrets to a foreign power, but that’s just the sort of thing that would be left out. It’s not needed for a conviction.
If the source of the documents is Hal Martin, then we can speculate that a random hacker did in fact stumble on it—no need for nation-state cyberattack skills.
The other option is a mysterious second NSA leaker of cyberattack tools. Could this be the person who stole the NSA documents and passed them on to someone else? The only time I have ever heard about this was from a Washington Post story about Martin:
There was a second, previously undisclosed breach of cybertools, discovered in the summer of 2015, which was also carried out by a TAO employee [a worker in the Office of Tailored Access Operations], one official said. That individual also has been arrested, but his case has not been made public. The individual is not thought to have shared the material with another country, the official said.
Of course, “not thought to have” is not the same as not having done so.
It is interesting that there have been no public arrests of anyone in connection with these hacks. If the NSA knows where the files came from, it knows who had access to them—and it’s long since questioned everyone involved and should know if someone deliberately or accidentally lost control of them. I know that many people, both inside the government and out, think there is some sort of domestic involvement; things may be more complicated than I realize.
It’s also not over. Last week, the Shadow Brokers were back, with a rambling and taunting message announcing a “Data Dump of the Month” service. They’re offering to sell unreleased NSA attack tools—something they also tried last August—with the threat to publish them if no one pays. The group has made good on their previous boasts: In the coming months, we might see new exploits against web browsers, networking equipment, smartphones, and operating systems—Windows in particular. Even scarier, they’re threatening to release raw NSA intercepts: data from the SWIFT network and banks, and “compromised data from Russian, Chinese, Iranian, or North Korean nukes and missile programs.”
Whoever the Shadow Brokers are, however they stole these disks full of NSA secrets, and for whatever reason they’re releasing them, it’s going to be a long summer inside of Fort Meade—as it will be for the rest of us.


The Society of Women Engineers recently shared a trove of astonishing documents from the group’s archives. They’re letters, loads of them, all directed at women engineering students who had contacted various universities about their interest in connecting with other women studying engineering.
Lou Alta Melton and Hilda Counts, both students at the University of Colorado in 1919, were trying to start their own professional society. Their letters—and the many responses they received—are part of the Society of Women Engineers sprawling archives, which are housed at Wayne State University in Detroit.
“We have not now, have never had, and do not expect to have in the near future, any women students registered in our engineering department,” Thorndike Saville, and an associate professor at the University of North Carolina, wrote in his reply to Melton. He signed it, “Yours very truly.”
“We do not permit women to register in the Engineering School under present regulations,” wrote William Mott, the dean of the Carnegie Institute of Technology, which would later merge with the Mellon Institute to become Carnegie Mellon.
1919 was the year Congress passed the 19th amendment, granting women the right to vote. But, as so many of the letters in the collection demonstrate, many women wouldn’t be permitted to formally study the subjects that interested them until much later. Discrimination against women in engineering isn’t always so straightforward today, but the forces that push women out of the field (or prevent them from pursuing it in the first place) remain persistent and complex. Women account for some 20 percent of engineering graduates, according to Harvard Business Review, but a huge portion of them either quit or never enter the profession. Much has changed for women engineers in the past century, but perhaps not enough.
“I suspect the number of women who have undertaken general engineering courses is so few that you will hardly be able to form an organization,” William Raymond, the dean of the State University of Iowa wrote in 1919, adding, “However, I may be mistaken.”
Some schools seemed to encourage women to find loopholes so they could at least attend classes—but didn’t take the additional step of letting them pursue a degree. “While we cannot legally register women in the College,” wrote J.R. Benton, the dean of engineering at the University of Florida, in 1919, “there is nothing to prevent our admitting them as visitors to the classes, which permits them to get all the benefit of instruction altho without definite status as students.”
“Hitherto, there has been no demand for engineering courses here on the part of women,” he added, “except in one case, that of Leanora Semmes, who is now taking work in Mechanical Drawing.” A quick search of newspaper archives and digitized books provides no evidence that Semmes ever worked as an engineer—or at least no evidence that she was ever recognized for it.
Counts, one of the letter writers from the Society of Women Engineers archive, is remembered as a trailblazer—her electrical engineering degree was the first ever awarded to a woman in Colorado and she later took a job with the Rural Electrification Administration in Washington, D.C. Melton, the other letter writer, made headlines at least once, when in 1920 she took a job as a civil engineer in the U.S. Bureau of Public Roads.
“Leave it to a woman!” the Iowa City Press-Citizen wrote at the time. “That’s what the  United States Bureau of Public Roads in Denver did when an assistant bridge engineer’s job was open. Miss Lou Alta Melton is filling the place in fine shape.” The newspaper described Melton as the only “girl” graduate in her civil engineering class at the University of Colorado.
One response to Melton’s letter came from the secretary of the T-Square Society, a group of women engineers at the University of Michigan that had already formed. They were interested in a potential partnership, the secretary wrote. But these and other early organizing efforts eventually fell apart, as Margaret E. Layne described in her book, Women in Engineering: Pioneers and Trailblazers, “partly because they followed a logic of maintaining professional standards similar to that used by male national organizations. Hence they excluded engineering students and working women engineers without formal education.”
How Women Mentors Make a Difference in Engineering
In other words, the high standards for the hypothetical society were deemed necessary to combat sexism, but the sexism that kept women out of formal programs also thwarted efforts to find a critical mass of women engineers for a such a society. It would be decades before the Society of Women Engineers was founded—first as an informal group during World War II, then officially in 1950.
There are still small bright spots in the society’s collection of responses to Melton and Counts. At least one dean of engineering, W.N. Gladson, of the University of Arkansas, wished Melton well. It doesn’t sound like much, but it was more than many other deans were willing to do. “I am aware that in the Northern and Eastern Colleges, often girls register for engineering work and make very excellent students...” Gladson wrote. “Wishing for your organization the fullest measure of success, I am.”
Elsewhere, a professor of mechanical engineering at Georgia Tech seemed to signal that times were changing. (Though he didn’t bother responding to Melton by name.)
“Dear Lady,” wrote J.B. Boon, of Georgia Tech, “Up to the present, women students have not been admitted to [Georgia] Tech.” He added—perhaps optimistically?—that Atlanta officials had taken up the question of women’s suffrage, “so no knowing what may happen!”


Facebook’s 2 billion users post a steady stream of baby pictures, opinions about romantic comedies, reactions to the news—and disturbing depictions of violence, abuse, and self-harm. Over the last decade, the company has struggled to come to terms with moderating that last category. How do they parse a joke from a threat, art from pornography, a cry for help from a serious suicide attempt? And even if they can correctly categorize disturbing posts with thousands of human contractors sifting through user-flagged content, what should they do about it?
This weekend, The Guardian began publishing stories based on 100 documents leaked to them from the training process that these content moderators go through. They’re calling it The Facebook Files. Facebook neither confirmed nor denied the authenticity of the documents, but given The Guardian’s history of reporting from leaks, we proceed here with the assumption that the documents are real training materials used by at least one of Facebook’s content moderation contractors.
The Guardian has so far focused on specific types of cases that come up in content moderation: the abuse of children and animals, revenge porn, self-harm, and threats of violence.  
The moderator training guidelines are filled with examples. Some show moderators being trained to allow remarkably violent statements to stay on the site. This one, for example, is supposed to help content moderators see the difference between “credible” threats of violence and other statements invoking violence.
The slides suggest that Facebook has begun to come up with rules that cover literally anything distressing or horrible someone could post. But what do they say about the role Facebook sees itself playing in the world it's creating?
In explaining the company’s reasoning about violent posts, a training document says, “We aim to allow as much speech as possible but draw the line at content that could credibly cause real-world harm.”
In the U.S., there is obviously an entire body of legal cases dedicated to parsing the limits and protections of speech. Different places in the world have different rules and norms. But these cases occur in the context of a single national government and its relationship to “free speech.”
Here, we’re talking about a platform, not a government. Facebook is unconstrained by centuries of interpretations of constitutions and legal precedents. It could do whatever it wanted.
They could systematically aim for harm minimization not speech maximization. That change of assumptions would lead to a different set of individual guidelines on posts. The popular children's online world, Club Penguin, for example, offered multiple levels of language filtering as well as an "Ultimate Safe Chat" mode that only allowed pre-selected phrases to be chosen from a list. At one point, a thousand words were being added to the software's verboten list per day. But “allow[ing] as much speech as possible” has been part of the ideology of this generation of social media companies from the very beginning.
Getting people to post more, as opposed to less, is the core of Facebook’s mission as a company. It is no surprise that the companies built on sharing that have been the most successful come from the United States, which is the most pro-free speech country in the world.
From these documents and the company’s statements, the company has pragmatically chosen to limit areas where it has encountered problems. And those problems are primarily quantified through the flagging that users themselves do.
“As a trusted community of friends, family, coworkers, and classmates, Facebook is largely self-regulated,” one document reads. “People who use Facebook can and do report content that they find questionable or offensive.”
Facebook wants to stay out of it. So Facebook reacts, evolving content moderation guidelines to patch the holes where “self-regulation” fails. Given the number of territories and cultures into which Facebook has integrated itself, one can imagine Facebook’s leadership sees this both as the most reasonable and only practical approach. In cases where they have deployed top-down speech limits, they’ve gotten it wrong, too (as in the “Napalm Girl” controversy).
“We work hard to make Facebook as safe as possible while enabling free speech,” said Monika Bickert, Facebook’s Head of Global Policy Management. “This requires a lot of thought into detailed and often difficult questions, and getting it right is something we take very seriously.”
Let’s stipulate that these are difficult decisions on an individual basis. And let’s further stipulate that multiplying the problem by 2-billion users makes the task daunting, even for a company with $7 billion on hand. Facebook has committed to adding 3,000 more content moderators to the 4,500 working for the company today.
But is Facebook’s current approach to content moderation built on a firm foundation? The company’s approach to content moderation risks abdicating the responsibility that the world’s most popular platform needs to take on.
“When millions of people get together to share things that are important to them, sometimes these discussions and posts include controversial topics and content,” we read in the training document.  “We believe this online dialogue mirrors the exchange of ideas and opinions that happens throughout people’s lives offline, in conversations at home, at work, in cafes and in classrooms.”
In other words, Facebook holds that the posts on its platform reflect offline realities and are merely a reflection of what is, rather than a causal factor in making things come to be.
Social Media’s Silent Filter
Facebook must accept the reality that it has changed how people talk to each other. When we have conversations “at home, at work, in cafes, and in classrooms,” there is not an elaborate scoring methodology that determines whose voice will be the loudest. Russian trolls aren’t interjecting disinformation. My visibility to my family is not dependent on the quantifiable engagement that my statements generate. Every word that I utter or picture that I like is not being used to target advertisements (including many from media companies and political actors) at me.
The platform’s own dynamics are a huge part of what gets posted to the platform. They are less a “mirror” of social dynamics than an engine driving them to greater intensity, with unpredictable consequences.
Facebook’s Mark Zuckerberg seemed to acknowledge this in his epic manifesto about the kind of community that he wanted Facebook to build.
“For the past decade, Facebook has focused on connecting friends and families,” he wrote. “With that foundation, our next focus will be developing the social infrastructure for community—for supporting us, for keeping us safe, for informing us, for civic engagement, and for inclusion of all.”
To get this “social infrastructure for community” right, Facebook has to acknowledge that it has not merely “connected friends and families." It has changed their very nature.


The year was 1999, and the people were going online. AOL, Compuserve, mp3.com, and AltaVista loaded bit by bit after dial-up chirps, on screens across the world. Watching the internet extend its reach, a small group of scientists thought a more extensive digital leap was in order, one that encompassed the galaxy itself. And so it was that before the new millennium dawned, researchers at the University of California released a citizen-science program called SETI@Home.
The idea went like this: When internet-farers abandoned their computers long enough that a screen saver popped up, that saver wouldn’t be WordArt bouncing around, 3-D neon-metallic pipes installing themselves inch by inch, or a self-satisfied flying Windows logo. No. Their screens would be saved by displays of data analysis, showing which and how much data from elsewhere their CPUs were churning through during down-time. The data would come from observations of distant stars, conducted by astronomers searching for evidence of an extraterrestrial intelligence. Each participating computer would dig through SETI data for suspicious signals, possibly containing a “Hello, World” or two from aliens. Anyone with 28 kbps could be the person to discover another civilization.
When the researchers launched SETI@Home, in May of ’99, they thought maybe 1,000 people might sign up. That number—and the bleaker view from outsiders, who said perhaps no one would join the crew—informed a poor decision: to set up a single desktop to farm out the data and take back the analysis.
But the problem was, people really liked the idea of letting their computers find aliens while they did nothing except not touch the mouse. And for SETI@Home’s launch, a million people signed up.  Of course, the lone data-serving desktop staggered. SETI@Home fell down as soon as it started walking. Luckily, now-defunct Sun Microsystems donated computers to help the program get back on its feet. In the years since, more than 4 million people have tried SETI@Home. Together, they make up a collective computing power that exceeds 2008’s premier supercomputer.
But they have yet to find any aliens.
* * *
SETI is a middle-aged science, with 57 years under its sagging belt. It began in 1960, when an astronomer named Frank Drake used an 85-foot radio telescope in Green Bank, West Virginia, to scan two Sun-like stars for signs of intelligent life—radio emissions the systems couldn’t produce on their own, like the thin-frequency broadcasts of our radio stations, or blips that repeated in a purposeful-looking way. Since then, scientists and engineers have used radio and optical telescopes to search much more of the sky—for those “narrowband” broadcasts, for fast pings, for long drones, for patterns distinguishing themselves from the chaotic background static and natural signals from stars and supernovae.
But the hardest part about SETI is that scientists don’t know where ET may live, or how ET’s civilization might choose to communicate. And so they have to look for a rainbow of possible missives from other solar systems, all of which move and spin at their own special-snowflake speeds through the universe. There’s only one way to do that, says Dan Werthimer, the chief SETI scientist at Berkeley and a co-founder of SETI@Home: “We need a lot of computing power.”
In the 1970s, when Werthimer’s Berkeley colleagues launched a SETI project called SERENDIP, they sucked power from all the computers in their building, then the neighboring building. In a way, it was a SETI@Home prototype. In the decades that followed, they turned to supercomputers. And then, they came for your CPUs.
* * *
The idea for SETI@Home originated at a cocktail party in Seattle, when computer scientist David Gedye asked a friend what it might take to excite the public about science. Could computers somehow do something similar to what the Apollo program had done? Gedye dreamed up the idea of “volunteer computing,” in which people gave up their hard drives for the greater good when those drives were idle, much like people give up their idle cars, for periods of time, to Turo (if Turo didn’t make money and also served the greater good). What might people volunteer to help with? His mind wandered to The X-Files, UFOs, hit headlines fronting the National Enquirer. People were so interested in all that. “It’s a slightly misguided interest, but still,” says David Anderson, Gedye’s graduate-school advisor at Berkeley. Interest is interest is interest, misguided or guided perfectly.
But Gedye wasn’t a SETI guy—he was a computer guy—so he didn’t know if or how a citizen-computing project would work. He got in touch with astronomer Woody Sullivan, who worked at the University of Washington in Seattle. Sullivan turned him over to Werthimer. And Gedye looped in Anderson. They had a quorum, of sorts.
Anderson, who worked in industry at the time, dedicated evenings to writing software that could take data from the Arecibo radio telescope, mother-bird it into digestible bits, send it to your desktop, command it to hunt for aliens, and then send the results back to the Berkeley home base. No small task.
They raised some money—notably, $50,000 from the Planetary Society and $10,000 from a Paul Allen-backed company. But most of the work-hours, like the computer-hours they were soliciting, were volunteer labor. Out of necessity, they did hire a few people with operating-system expertise, to deal with the wonky screensaver behavior of both Windows and Macintosh. “It’s difficult trying to develop a program that’s intended to run on every computer in the world,” says Anderson.
And yet, by May 17, 1999, they were up, and soon after, they were running. And those million people in this world were looking for not-people on other worlds.
One morning, early in the new millennium, the team came into the office and surveyed the record of what those million had done so far. In the previous 24 hours, the volunteers had done what would have taken a single desktop one thousand years to do. “Suppose you’re a scientist, and you have some idea, and it’s going to take 1,000 years,” says Anderson. “You’re going to discard it. But we did it.”
After being noses-down to their keyboards since the start, it was their first feeling of triumph. “It was really a battle for survival,” says Anderson. “We didn’t really have time to look up and realize what an amazing thing we were doing.”
Then, when they looked up again, at the SETI@Home forums, they saw something else: “It was probably less than a year after we started that we started getting notices about the weddings of people who met through SETI@Home,” says Eric Korpela, a SETI@Home project scientist and astronomer at Berkeley.
* * *
The SETI astronomers began to collect more and different types of data, from the likes of the Arecibo radio telescope. Operating systems evolved. There were new signal types to search for, like pulses so rapid they would have seemed like notes held at pianissimo to previous processors. With all that change, they needed to update the software frequently. But they couldn’t put out a new version every few months and expect people to download it.
Anderson wanted to create a self-updating infrastructure that would solve that problem—and be flexible enough that other, non-SETI projects could bring their work onboard and benefit from distributed computing. And so BOINC—Berkeley Open Infrastructure for Network Computing—was born.
Today, you can use BOINC to serve up your computer’s free time to develop malaria drugs, cancer drugs, HIV drugs. You can fold proteins or help predict the climate. You can search for gravitational waves or run simulations of the heart’s electrical activity, or any of 30 projects. And you can now run BOINC on GPUs—graphical processing units, brought to you by gamers—and on Android smartphones Nearly half a million people use the infrastructure now, making the système totale a 19 petaflop supercomputer, the third-largest megacalculator on the planet.
Home computers have gotten about 100 times faster since 1999, thank God, and on the data distribution side, Berkeley has gotten about 10 times faster. They’re adding BOINC as a bandwidth-increasing option to the Texas Advanced Computing Center and nanoHUB, and also letting people sign up for volunteer computing, tell the system what they think are the most important scientific goals, and then have their computers be automatically matched to projects as those projects need time. It’s like OkCupid dating, for scientific research. BOINC, and SETI@Home can do more work than ever.
* * *
The thing is, though, they’ve already done a lot of work—so much work they can’t keep up with themselves. Sitting in a database are 7 billion possible alien signals that citizen scientists and their idle computers have already uncovered.
Most of these are probably human-made interference: short-circuiting electric fences, airport radar, XM satellite radio, or a microwave opened a second too soon. Others are likely random noise that added up to a masquerade of significance. As Anderson says, “Random noise has the property that whatever you’re looking for, it eventually occurs. If you generate random letters. You eventually get the complete works of Shakespeare.” Or the emissions are just miscategorized natural signals.
Anderson has been working on a program called Nebula that will trawl that billions-and-billions-strong database, reject the interference, and upvote the best candidates that might—just might—be actual alien signals. Four thousand computers at the Max Planck Institute for Gravitational Physics in Germany help him narrow down the digital location of that holiest of grails. Once something alien in appearance pops up—say from around the star Vega—the software automatically searches the rest of the data. It finds all the other times, in the 18 years of SETI@Home history, that Arecibo or the recently added telescopes from a $100 milion initiative called Breakthrough Listen have looked at Vega. Was the signal there then too? “We’re kind of hoping that the aliens are sending a constant beacon,” says Korpela, “and that every time a telescope passes over a point in the sky, we see it.”
If no old data exists—or if the old data is particularly promising—the researchers request new telescope time and ask SETI colleagues to verify the signal with their own telescopes, to see if they can intercept that beacon, that siren, that unequivocal statement of what SETI scientists and SETI@Home participants hope is true: That we are not alone.
So far, that’s a no-go. “We’ve never had a candidate so exciting that we call the director and say, ‘Throw everybody off the telescope,’” says Werthimer. “We’ve never had anything that resembles ET.”
And partly for that reason, the SETI@Homers are now working on detecting “wideband” signals—ones that come at a spread spectrum of frequencies, like the beam-downs from DIRECTV. Humans (and by extension, extraterrestrials) can embed more information more efficiently in these spread-spectrum emissions. If the goal is to disseminate information, rather than just graffiti “We’re here!” on the fabric of spacetime, wideband is the way to go. And SETI scientists’ thinking goes like this: We’ve been looking mostly for purposeful, obvious transmissions, ones wrapped neatly for us. But we haven’t found any—which might mean they just aren’t there. Extraterrestrial communications might be aimed at members of their own civilizations, in which case they’re more likely to go the DIRECTV route, and we’re likely to find only the “leakage” of those communication lines.
“If there really are these advanced civilizations, it’d be trivial to contact us,” says Werthimer. “They’d be landing on the White House—well, maybe not this White House. But they’d be shining a laser in Frank Drake’s eyes. I don’t see why they would make it so difficult that we would have to do all this hard stuff.”
And so humans, and our sleeping computers, may have to eavesdrop on messages not addressed to us—the ones the aliens send to their own (for lack of a better word) people, and then insert ourselves into the chatter. “I don’t mean to interrupt,” we might someday say, “but I couldn’t help overhearing...” And because of SETI@Home and  BOINC, it might be your laptop that gets that awkward conversation started.


American voting relies heavily on technology. Voting machines and ballot counters have sped up the formerly tedious process of counting votes. Yet long-standing research shows that these technologies are susceptible to errors and manipulation that could elect the wrong person. In the 2016 presidential election, those concerns made their way into public consciousness, worrying both sides of the political fence. The uncertainty led to a set of last-minute, expensive state recounts—most of which were incomplete or blocked by courts. But we could ensure that all elections are fair and accurate with one simple low-tech fix: risk-limiting audits.
Risk-limiting audits are specific to elections, but they are very similar to the audits that are routinely required of corporate America. Under them, a random sample of ballots is chosen and then hand-counted. That sample, plus a little applied math, can tell us whether the machines picked the right winner.
In nearly all cases, a risk-limiting audit can be performed by counting only a small fraction of ballots cast.  For example, the M.I.T. professor Ron Rivest calculates that Michigan could have checked just 11 percent of its ballots and achieved 95 percent confidence that their machine-counted result correctly named Donald Trump the winner of Michigan's electoral votes. Texas and Missouri, with their wider margins in the presidential race, could have counted a randomly chosen 700 ballots and 10 ballots, respectively, to achieve the same confidence level.  
Since risk-limiting audits verify elections while minimizing the number of audited ballots, they are both inexpensive and speedy. They largely eliminate the need for emergency recruitment of recount workers and can be conducted before the election must be certified by law. This also means that auditing can become a routine part of every election. Regular auditing will also allow state and county electioneers to become more skilled at spotting problems, from mundane system errors to deliberate hacking, something that is difficult for them to do today.
Colorado has been working on audits since 2011, and is ready to take the next step: Risk-limiting audits will be required in Colorado’s 2017 election. More states should follow Colorado’s bold lead.
Yet too many states still have electronic voting machines with no paper trail, meaning that no audit is possible at all. And all audits are not created equal. After the 2016 election, many Wisconsin counties simply ran ballots through their tabulating machines a second time and called it an “audit.” But if the machines were broken or compromised, the same inaccuracies they registered the first time would show up again the second time.
Technology is already deeply embedded in our voting systems. The next step isn’t to pile on more technology; it’s ensuring that the technology we rely on works properly and has not been hacked or undermined. The way to do that is clear: standard election procedure should include risk-limiting audits. If the Nevada Gaming Commission can establish detailed audit requirements for Keno, we can certainly do the same for our democracy.
This article is part of a collaboration with the Markkula Center for Applied Ethics at Santa Clara University.


Donald Trump doesn’t need a crystal ball, he has a mysterious glowing orb. No, wait. Scratch that. Donald Trump doesn’t need a crystal ball, he has a mysterious clairvoyant Twitter account.
There seems to be, Trump watchers have noticed, a weirdly prophetic tweet in Trump’s past for every new aspect of his presidency—from his weekends golfing at Mar-a-Lago to each new bombshell scoop about the embattled White House and its alleged ties to Russia.
This goes beyond using classic Trump tweets to insult him, though people are doing that, too—the prototypical example comes from June 2014, when Trump tweeted, “Are you allowed to impeach a president for gross incompetence?”
Trump’s critics are now delighting in the ability to criticize Trump by using his own targeted complaints about others. His past tweets underscore stupendous hypocrisy, they say, and perhaps a hint at an epic political downfall. Democrats have been agitating for Trump’s political demise since before he was the Republican nominee, but even the most apolitical observer would acknowledge how uncanny some of Trump’s past tweets have become.
When the Congressional Budget Office determined that Congressional Republicans’ Trump-supported plan to replace the Affordable Care Act would increase the number of uninsured people by 24 million in the next decade, the internet reached for a Trump tweet from 2014: “It’s Thursday. How many people have lost their health care today?” he’d written at the time.
When Trump ordered a missile strike against Syria in April, people shared this Trump tweet from 2013: “The President must get Congressional approval before attacking Syria-big mistake if he does not!”
This one has been making the rounds, too: “PresObama is not busy talking to Congress about Syria..he is playing golf ...go figure,” Trump tweeted in 2013. Fast forward to 2017 and Trump has already outpaced Obama’s presidential golfing rate. (Obama was a prolific golfer.*)
There’s more.
After reports that Trump is considering a massive troop surge in Afghanistan, this 2013 tweet reappeared: “Let’s get out of Afghanistan. Our troops are being killed by the Afghanis we train and we waste billions there. Nonsense!  Rebuild the USA.”
“Is there a name for the eerie way that Trump subtweeted his entire presidency?” Peter Daou, a former Hillary Clinton adviser, said recently. “There’s truly a tweet for every occasion.” Various observers have compared the phenomenon to everything from mass-produced greeting cards to the elegance of mathematics to science fiction.  
“Seems there’s a hypocritical Trump tweet for almost every occasion,” one Twitter user wrote. “They’re like Hallmark cards.”
And another: “His hypocrisy meter uses a Fibonacci number and it just keeps spinning into infinity through space and time...”
The appeal of reaching for Trump’s old tweets is understandable, and not just because people enjoy pointing out the hypocrisy of politicians they dislike. The medium is meaningful here, too. Rarely are schadenfreude and political commentary packaged together so neatly. Tweets are, by the platform’s very nature, succinct, atomized, and imminently shareable. Trump himself has employed the same tactic in an attempt to point out hypocrisy among his celebrity rivals.
Skipping through the linear order of events this way is also a reflection of warped time as a dominant theme in the Trump presidency—both among supporters who want to travel backward in time to Make America Great Again, and among critics who compare him to the time-traveling Back to the Future villain Biff Tannen (or worse.)
Using past tweets as present criticism isn’t just suited to Twitter’s platform, or political culture, or even outright partisanship. This approach also leverages Trump’s blustery style of attacking others as well as the richness of his particular Twitter archive, which goes back to 2009.
And in an irony that’s almost too delectable, there is the fact that so many of Trump’s past attacks against Hillary Clinton in last year’s presidential campaign were based on the premise that she was reckless with classified information—which is now the same criticism Trump faces in one of the biggest scandals of his fledgling presidency. “Crooked Hillary Clinton and her team ‘were extremely careless in their handling of very sensitive, highly classified information.’ Not fit!” he tweeted last July. (Trump’s ongoing refusal to share his tax returns is in similarly sharp contrast to this 2012 tweet: “All recent Presidents have released their transcripts. What is @BarackObama hiding?”)
Given the Russia probe, many of Trump’s old tweets seem to have startling new relevance. Like this one, from October, which people shared amid the news last week that the former FBI Director Robert Mueller had been appointed special counsel to investigate Russian interference in the 2016 presidential election. “If I win,” Trump had tweeted a month before election day, presumably directed at Clinton, “I am going to instruct my AG to get a special prosecutor to look into your situation bc there's never been anything like your lies.”
And this one, from February, which Democrats seized on when The Washington Post revealed Trump had shared highly classified information with Russian leaders in the Oval Office the day after he fired the FBI director James Comey: “The real scandal here is that classified information is illegally given out by ‘intelligence’ like candy. Very un-American!”
Last month, when Trump criticized the Obama administration for having done “nothing” to stop the Assad regime in Syria, people resurrected a string of Trump tweets from 2013: “We should stay the hell out of Syria,” he had tweeted in one case. And also: “Do NOT attack Syria,fix U.S.A.”And also: “Stay away and fix broken U.S.”
And just in case there was any doubt whatsoever: “What I am saying is stay out of Syria.”
This week, after Trump visited Saudi Arabia—where the first lady was photographed without a headscarf—this 2015 Trump tweet resurfaced: “Many people are saying it was wonderful that Mrs. Obama refused to wear a scarf in Saudi Arabia, but they were insulted. We have enuf enemies,” he tweeted in January of that year.
Other figures in the Trump inner circle have made cameos in this internet parlor game. After reports on Monday that Michael Flynn, Trump’s former national security adviser, would invoke his Fifth Amendment right against self-incrimination, a 2013 tweet from Sean Spicer, Trump’s press secretary, sprang back to life online:  “why do u take the 5th if you have done nothing wrong and have nothing to hide?” Spicer had tweeted at the time. It seems to have been a reference to an IRS official who invoked her right not to testify after disclosing the agency’s improper targeting of conservative groups. But untethered from context and time, Spicer’s past commentary seemed linked to Flynn today.
There are so many more examples that “a Trump tweet for everything” has long since crossed over into parody—meaning you should definitely remain skeptical about anything being shared as a past Trump tweet until you verify it for yourself. Consider this delightful but obviously fake mock-up, for example, and always cross reference against the legitimate Trump tweet archive.
someone's got egg on their face pic.twitter.com/1iIXwg98Xn
For the record, Trump’s pixelated paper trail shows no references to any orb other than the one in words like “Forbes,” “absorb,” and “forbid.” Even in the most surreal political scenarios, there’s only so much you can see coming.
Or, as Trump tweeted in 2013, “Just shows that you can have all the cards and lose if you don’t know what you’re doing.”
* Tracking any president’s time on the green is a longstanding, petty political pastime. Not surprisingly, then, pundits have gone full ouroboros on the Trump-versus-Obama golf question. Conservative commentators are now accusing fact-checking outlets of hypocrisy for tracking Trump’s golf-playing hypocrisy, arguing that fact checkers didn't follow Obama’s golfing schedule as closely. (Many national news organizations, including The New York Times, The Washington Post, and The Atlantic, wrote about Obama’s frequent golfing while he was president.)


On Thursday morning, Adam Gill stepped outside in a heavy, bright-yellow coat, bulky gloves, and a ski mask to brace himself against the blistering wind. He brought with him a metal teakettle full of boiling water. As he tipped the kettle over, the piping-hot liquid turned instantly into snow and blew away in the wind.
That’s how cold it was at the Mount Washington Observatory in New Hampshire, the highest peak in the northeastern United States. The video of Gill, a meteorologist at the observatory, conducting this little presentation received thousands of sympathetic likes on Facebook. The temperature that day at the observatory hit a bone-chilling low of -34 degrees Fahrenheit (-37 degrees Celsius)—and that was without accounting for wind chill. The day broke the previous record of -31 degrees Fahrenheit (-35 degrees Celsius), set in 1933.
The frigid weather in New Hampshire is part of an Arctic cold front that has settled over large swaths of the United States this week. Millions of people are bundling up as temperatures hit bone-chilling lows in the Northeast and the Midwest. Daily high temperatures on the East Coast have dipped into the teens and 20s in Fahrenheit, and highs in parts of the Midwest are well below zero, with some in the negative 20s and 30s. The National Weather Service warns of hypothermia and frostbite. In Toledo, Ohio, a dog was discovered frozen solid on a porch.
These conditions mean that in some parts of the United States, it’s actually colder than it is on Mars.
The latest weather data from the Curiosity rover on Mars recorded a peak temperature of -9 degrees Fahrenheit (-23 degrees Celsius) on the day of Sol 1910, which for us is December 20. The rover roams around Gale Crater, near the equator. There, the winter solstice has just passed and the cold season is getting started. Humans have yet to figure out if the red planet could be habitable, but right now it seems just as (in)hospitable as home, at least weather-wise. (Of course, Earth still has one thing that Mars doesn’t: breathable air.)
The comparison to Mars is indeed accurate, says Michael Mischna, a research scientist at NASA’s Jet Propulsion Laboratory in California where he studies the Martian climate—and where it will reach an enviable 80 degrees Fahrenheit (27 degrees Celsius) Friday. “Temperature is pretty much temperature wherever you are,” Mischna said.
There is one small difference. The atmosphere on Mars is about 100 times thinner than on Earth, which means a given air temperature—if you could stand outside on the planet, without a spacesuit—wouldn’t feel the same on both planets.
“If you were to jump into a pool that was 70 degrees, it would feel a lot colder to your body than standing in air at 70 degrees, and that’s because the water is able to suck the heat out of your body,” Mischna said. Like water molecules, air molecules can wick away warmth, but they’re much less efficient. On Mars, where there’s little water vapor and few air molecules, a person wouldn’t feel as cold. “Minus 100 degrees on Mars might only feel like minus 30,” he said.
There’s very little water on Mars, which means there’s no humidity; the heat is dry and the cold is bitter. Temperatures during summers on Mars, particularly at warmer regions like at the equator, can rise above freezing. At the height of the season, they can even reach 68 degrees Fahrenheit (20 degrees Celsius). Peak winter on Mars, however, is no match for the same season on Earth. Temperatures are -94 degrees Fahrenheit (-70 degrees Celsius) at the equator, and -200 degrees Fahrenheit (-130 degrees Celsius) at the poles.
Back on Earth, the American cold snap has prompted some doubters, including President Donald Trump, to surface their usual argument against climate change: that record-breaking, frigid temperatures are proof that human-driven climate change isn’t real.
The President Doesn't Care to Understand Global Warming
The president is too eager to leap to conclusions: A single week of cold weather in one part of the world does not disprove that the world, overall, is warming. The Arctic’s current temperature is more than five degrees Fahrenheit above average, and 2017, as a whole, will be one of the warmest years ever recorded worldwide.
But this week’s Arctic blast may actually be a symptom of climate change, albeit an unusual one. In recent years, several studies have found that as the Arctic warms, it seems to destabilize other parts of the winter climate system, bringing frigid air down to the Midwest and Northeast. Scientists aren’t sure why this is happening yet—it may have to do with changes to the jet stream or the high-altitude vortex of air over the North Pole—but for now, it’s a well-documented aspect of our warming world. (Alas, these frigid winters don’t prevent the United States from suffering through scorching, climate-addled summers in the same year.)
Forecasters warn that this week’s temperatures could stretch through the New Year’s holiday. Here’s a disheartening map from the National Weather Service:
Here's a look at the forecast wind chills for right around midnight on New Year's Eve. Brutally cold in some locations. #NewYearsEve pic.twitter.com/pXGBVb3zut
Maybe this holiday season will help prepare humanity for future New Years’ celebrations on Mars.


In the first novel ever written about Sherlock Homes, we learn something peculiar about the London detective. Holmes, supposedly a modern man and a keen expert in the workings of the world, does not know how the solar system works. Specifically he is unfamiliar with the heliocentric Copernican model, which, upon its slow acceptance in the 17th century, revolutionized Western thought about the place of our species in the universe.  
“What the deuce is it to me?” Holmes asks his sputtering soon-to-be sidekick, Dr. Watson. “You say that we go ’round the sun. If we went round the moon it would not make a pennyworth of difference to me or to my work.”
Brains are a kind of “little empty attic,” says the detective, and they should be filled only with furniture that’s useful to one’s line of work. Holmes doesn’t doubt the Copernican model; he simply has no use for it in solving murder cases. “Now that I do know it,” he adds, “I shall do my best to forget it.”
Thursday night, as record lows gripped most of the country’s northern half, President Trump clarified that he does not understand another revolution in our knowledge of the natural order of things: the theory of human-driven climate change.
In the East, it could be the COLDEST New Year’s Eve on record. Perhaps we could use a little bit of that good old Global Warming that our Country, but not other countries, was going to pay TRILLIONS OF DOLLARS to protect against. Bundle up!
Trump is wrong about the science—I’ll get to that in a moment—but, first, let’s not mince words: The president is trolling here. Pointing to cold weather and asking Whither climate change? is, by this point, almost a Republican tradition. In February 2015, Senator James Inhofe of Oklahoma brought a snowball inside the Senate chambers to demonstrate that global warming was not real. “It’s very, very cold out. Very unseasonable,” said Inhofe, then the chairman of the Environment and Public Works Committee. “In case we have forgotten because we keep hearing that 2014 has been the warmest year on record.”
Despite the snowball, 2014 was the warmest year on record. It was followed by 2015, which broke 2014’s record and became the new hottest year on record; and 2016, which  became the new hottest year after it blasted away 2015’s record. Though this year is not yet over, NASA estimates that 2017 will also beat 2015’s record and become the second-hottest year ever recorded. And even if this late cold snap averts that dreadful streak, 2017 will still break 2014’s record. We have just lived through what would be, in any other decade, the warmest year ever measured.
Perhaps Trump has forgotten that his native New York suffered three heat waves by late July this year, or that a “normal” summer now would count as a “hot” summer during the 20th century. Scorching temperatures have vanished not because global warming has stopped, but because it is the winter, which every preschooler knows as the “cold season.” Six months after hurricane season ends, Floridians do not ask where the tropical cyclones have gone; cable-news pundits do not spend time every night debating whether the sun will rise the next morning, though that may be preferable to what is currently aired.
And there is, in fact, a connection between global warming and the current frigid weather across the United States. Spend some time clicking around the Climate Reanalyzer—a fantastic tool from the University of Maine—and you’ll see that the northern half of North America is the only part of the world where temperatures are significantly colder right now than normal. Moscow, Russia, is normally about 24 degrees Fahrenheit this time of year; it’s currently pushing 40. The Arctic as a whole is more than 5 degrees Fahrenheit above normal.
Nor is this phenomenon limited to exotic capitals. Los Angeles has an average high of 68 degrees Fahrenheit on December 29. Today it hit 82.
It’s not too much of an exaggeration to say it’s warmer than normal almost everywhere right now except for Canada and the northern United States. What no one should tell the president is that this pattern may not be due to random chance—it seems to arise from global climate change.
For the past decade or so, climate scientists have noticed that when the Arctic has an especially warm winter, the northern continents become especially cold and snowy. A plethora of studies in the past three years have seemed to confirm the connection: When the Arctic is extremely warm, it seems to loose cold air across the world, and northern North America suffers an extremely cold winter. Why? Scientists aren’t sure yet, but they think it may arise from a destabilized jet stream or a weakened stratospheric polar vortex.
Just this past July, a team of researchers found that frigid winters, driven by a warm Arctic, were already reducing the productivity of American agriculture. They estimated that this warming has already cost Texas a 20 percent decline in corn production for some years.
Suffice it to say: There is plenty of evidence that climate change is still at work, even in a week as chilling as this one. But does the president really need to understand all this stuff? Perhaps if he had remained in real-estate development, he could have stayed cheerfully ignorant. His line of work would never have touched the theory of human-driven climate change—outside of the occasional application for government money to protect a golf course from sea-level rise.  
But unfortunately for Americans—and quite unlike the good detective of 221B Baker Street—Trump’s job has brought him frequently and directly into contact with climate science. During his year in office, he has made decision after decision that turns on a nuanced understanding of global warming. He has withdrawn the United States from the Paris Agreement on climate change. He has revoked a slew of Obama-era policies meant to stem greenhouse-gas emissions. He has repealed the Clean Power Plan, a landmark rule that would have made the United States grid less reliant on coal. And he has declined to seek any kind of climate-protecting measure in the new GOP tax bill, despite the many alumni of the Reagan administration pleading for it.
People of good faith can disagree about the legality, effectiveness, and wisdom of any of these individual policies—and they do. But Trump has attacked the policies collectively and with great gusto, while declining to ever actually learn about the underlying scientific evidence.
Meanwhile, he sits atop the science agencies of the U.S. government, some of the finest scientific institutions ever constructed. Last month, his own administration released the Climate Science Special Report, a product of 13 federal agencies and itself the best scientific synopsis of climate change in years.
It seemed to address exactly his concern. “The number of high temperature records set in the past two decades far exceeds the number of low temperature records,” its authors said, with the highest confidence possible. “The frequency of cold waves has decreased since the early 1900s, and the frequency of heat waves has increased since the mid-1960s.” (The Dust Bowl period still holds the record for the most extreme temperatures.)
Trump has never expressed curiosity for this kind of fact finding or self-education. But neither he nor his administration has publicly fought climate change on its scientific basis. After some hand wringing this summer, the government released the unabridged Climate Science Special Report last month. And while much could still change, Scott Pruitt, the administrator of Trump’s EPA, seems unlikely to revoke the agency’s own endangerment finding, a 2009 memo that accepts the scientific foundations of climate change into federal policy. The Trump administration has hastily scaled back climate policy and barely touched climate science.
Indeed, this has characterized Trump’s approach: a rapid dismantling of law, and a lazy disregard for evidence. He has called climate change—in part a triumph of the American scientific enterprise—a “hoax” “created by and for the Chinese.” Trump seems confident in his belief that Earth scientists and the climate-concerned have invented a phenomenon out of whole cloth and that he needs to pay little attention to it. He seems sure, too, that the shambolic catastrophe of a destabilized climate—which will be a central preoccupation of the United States in the 21st century, whether its leaders recognize the reality of it or not—is a fable. In so doing, he underestimates the citizens whom he governs; and he conceives of the country over which he presides as being shallower, less curious, quicker to anger, more unwise, and altogether not as secure in its good understanding than it actually is. Pity him, and mourn for us.


Two years ago, China launched a space probe into orbit around Earth. Scientists nicknamed it Wukong, or Monkey King, after the hero of a 16th-century novel about a Buddhist monk’s long journey to India to secure religious texts. The probe’s job was to track and record cosmic rays, the streams of high-energy particles that constantly bombard Earth’s atmosphere from all corners of the universe.
In its first 530 days of operation, the probe recorded more than 2.8 billion cosmic rays. When scientists looked at data, they found something unusual. Some of the cosmic rays—at least 1.5 million of them—were recorded at a different and higher energy level than the others. Plotted on a chart, they appeared as a cluster of tiny outliers suspended above the curve.
Though they don’t look like much, this blip is incredibly important to astrophysicists around the world who are trying to solve one of the biggest mysteries in science: the existence of dark matter. Scientists believe that dark matter makes up about a quarter of everything in the universe, but the tricky thing is that we can’t see dark matter. In fact, we don’t even know what it’s made of. The​ existence of dark matter ​is inferred in​directly from observational data. It shapes some important phenomena in the cosmos—like why galaxies form in the first place, and stick together instead of flying apart—that the physics we already understand cannot.
The top theory for dark matter suggests that the mysterious stuff is made of WIMPs, weakly interacting massive particles. Wukong, known formally as the Dark Matter Particle Explorer (DAMPE), is designed to detect the signal that comes from WIMPs. WIMPs are lazy, slow-moving particles that collide rarely, but when they do, they could form pairs of electrons and positrons—the antimatter counterparts of electrons—some of the particles that make up cosmic rays. An uptick in the number of these pairs would be detected as a distinct bump in a survey of cosmic rays—like the one Wukong identified.
The DAMPE findings, which were released last month in Nature, line up nicely with other space-based experiments on dark matter in the last decade, an exciting prospect in a field that depends on measuring the same thing more than once. But “that is not to say that this is an indication of dark matter,” said Douglas Finkbeiner, an astrophysicist at Harvard who studies dark matter. There are many possible interpretations. The signal detected by DAMPE and others come from all directions in the sky, so scientists can’t pinpoint a source. The unusual cosmic rays could come from supernovae, the powerful deaths of stars, or pulsars, bright, fast-rotating stars.
For now, the DAMPE result, while tantalizing, is inconclusive. For scientists, it provides another data point in the search for dark matter. But for China in particular, the findings mean something more. DAMPE is the country’s first mission dedicated exclusively to astronomy and astrophysics, and within two years it has returned a promising result. The successful showing makes China a fierce participant—or, depending on whom you ask, competitor—in the field.
“The way I see it, we are making strides toward solving this big cosmic puzzle,” said Priyamvada Natarajan, a theoretical astrophysicist at Yale. “I don’t feel restricted by international borders in that sense. But it doesn’t go unnoticed that it is a Chinese satellite that did it.”
When people talk about China’s ambitions in space, the discussion usually focuses on activities that have some military implications—like the launch of spy satellites—rather than scientific ones. Headlines shout about a “space race” between China and the United States, and some American defense officials rattle their sabers alongside calls for increased capabilities in low-Earth orbit. The thought of Chinese spacecraft near U.S. assets in orbit is, from a national-security perspective, far more unnerving than a science mission for a particle that may not even exist.
“Unless you’re in a really intense Star Trek fantasy, it’s not going to lead you into a weapons capability,” said Joan Johnson-Freese, a national-security affairs professor at the U.S. Naval War College who has studied space security for 20 years.
China has invested heavily in space science in recent years. The country started building the world’s most powerful radio telescope in 2011, edging the famed Arecibo Observatory in Puerto Rico out of the top spot. In the fall of 2016, the telescope, the 500-Meter Aperture Spherical Radio Telescope (FAST) started making observations. It discovered two new pulsars in its first year of operations, and stands to be the leading instrument in the search for intelligent extraterrestrial life for years to come. In 2013, China landed a rover on the moon to poke around the surface. In June of this year, the Hard X-Ray Modulation Telescope (HXMT), a space observatory to study black holes and neutron stars, joined DAMPE in orbit.
The choice to invest in these particular fields have been very deliberate, according to Johnson-Freese. “China likes to be in the record books like everyone else,” she said, but the country can’t compete in areas of space exploration where the United States and other countries have long dominated. Instead, the Chinese have gone after realms in which no country has yet made a definitive triumph—like the search for dark matter.
In March 2016, a few months after DAMPE launched, Chang Jin, the mission’s chief scientist, said the search for the mysterious substance “tops the basic frontier projects of science listed by the United States, Europe, China, and Japan.”
“Any progress in dark-matter research will probably bring a breakthrough in physics,” Chang said.
While a breakthrough by the Chinese—a breakthrough by any group of scientists in any nation, really—would be cause of celebration in the astrophysical community, the merriment would feel thorny for some.“If China were to get a Nobel Prize in science, would that mean that the United States suddenly lost all of its lead? No,” Johnson-Freese said. “But I can see that there would be a lot of scientists who would say, well, this is going to become a Chinese matter of expertise. We’re going to depend on their science for us to do work.”
The isolation from a potential breakthrough likely will be felt most by American scientists, thanks to a law passed in 2011 that prohibits NASA from working with China’s space agency. There’s some irony there, given that one of the earlier experiments that noticed the same, strange blip in a survey of cosmic rays—the signal that scientists hope betrays the existence of dark matter—came from a collaboration between China, the United States, and other countries, the Alpha Magnetic Spectrometer, mounted on the International Space Station just one month after Congress approved the ban.
China has found opportunities for collaboration elsewhere. Scientists from institutions in Geneva and Italy are working on the DAMPE mission, and Chinese officials are in talks with the European Space Agency about building an outpost on the moon together. “These congressional restrictions presume that forbidding contact will slow the pace of Chinese progress,” Gregory Kulacki, a senior analyst and China project manager at the Union of Concerned Scientists, an American nonprofit group, said in an email. “Projects like FAST and DAMPE prove beyond a shadow of a doubt that presumption is mistaken.
Eventually, even China’s ambitions in particle physics will be subject to the scrutiny usually reserved for its more secretive, space-based military operations.“It seems no matter what China does in space, even if it is to make a significant scientific contribution to our understanding of the universe, some people in the United States will perceive it as a threat,” Kulacki said. But science is not a zero-sum game, he said, and the scientists themselves “understand who is to blame.”
After decades of looking for elusive dark-matter particles, the effort feels like it is at an impasse, Natarajan said. The hunt for WIMPs, the leading candidates, has repeatedly turned up empty, and astrophysicists are trying to broaden their search methods. Scientists are hopeful about more results from DAMPE, which is expected to last five years.
“We’ve all been thinking about this for so long, it’s such an embarrassment,” Natarajan said. “That anyone is making progress is super exciting.”


Among blind people, says Kim Charlson, asking if you’d prefer to see always starts a lively debate: “Every opinion is going to be different.” Charlson, who lost her sight at age 11 and now is president of the American Council of the Blind, says she would hold out for full color vision. Others might settle for seeing in blurry black and white. And yet other blind people might have no desire to see at all.
For a small number of blind people, this hypothetical question recently become a real one.
Last week, the Food and Drug Administration approved Luxturna, the first gene therapy to treat a specific form of inherited blindness called Leber’s congenital amaurosis. In fact, it’s the first gene therapy to treat any inherited disease at all. The news has been universally hailed as a scientific breakthrough. But its stratospheric cost—potentially $1 million per patient—has provoked hard questions about the value of the ability to see, especially if its effects are only partial and temporary, as may be the case with Luxturna.
Spark Therapeutics, the company that makes Luxturna, has yet to set an official cost. But the $1 million figure comes from a November earnings call, where the company’s CEO, Jeff Marrazzo, suggested the high cost is justified in part by the earnings patients and their caretakers could gain. He cited the National Federation of the Blind to note that 70 percent of working-age Americans who are blind are unemployed. “This economic reality of unemployment reminds us of a wonderful real-life example of the value of Luxturna,” said Marrazzo. One woman in the Luxturna trial regained enough vision to get her very first job at age 38.
The National Federation of the Blind took issue with being cited this way. Chris Danielsen, a spokesperson for the organization, says Spark was “relying on erroneous and harmful notions about the capacity of blind people to live the lives we want.” Research into specific eye diseases is well and good, he says, but the high unemployment rate among blind people is due to “society’s low expectations for the blind” and lack of training for skills like cane travel and reading Braille that would help them lead independent lives.
“It’s not like we’ve been sitting in rocking chairs for decades, and we’re waiting until we can go the hospital for the treatment,” says Stacy Cervenka, who was born blind due to an undeveloped optic nerve. “Blind people have lives that are as busy and chaotic and full as any sighted people,” she says. Cervenka is an executive officer for the California State Rehabilitation Council, and her husband, who was blinded by a gunshot wound, teaches blind people how to get around.
She credits such training for their independent lives. When we spoke, they were preparing to take their 4-year-old son on Amtrak for a holiday vacation. “From a purely fiscal standpoint of getting the most bang for your buck, the government could do so much more good for so much less money by providing vocational rehabilitation,” says Cervenka.
Cervenka is not opposed to gene therapy to treat blindness, but she wanted to emphasize the trade-offs. Aside from the potential seven-figure price tag, the therapy comes with risks from injecting into the eyeball, plus it takes time to travel to a hospital that offers it. In the meantime, she asks, “Who’s taking care of your kids? Who’s doing your job?” For her, it’s not worth the hassle for only partial vision, but she would consider it if her vision could be good enough to drive—the one place she feels like being blind actually impinges on her life.
How the Blind Are Reinventing the iPhone
For some people who are blind, particularly if they have been blind since birth, gaining sight can actually be a bizarre, disruptive experience.“It isn’t like you can turn on a switch and someone who hasn’t seen would be able to see, because their brain doesn’t know what vision is,” says Charlson. People who have regained sight, such as through a corneal transplant, report being unable to recognize objects until they pick them up in their hands—as they had been doing their whole life.
Charlson says Luxturna is very promising, and she hopes gene therapy can be developed to treat other forms of inherited blindness. For now, Luxturna can only help people who are born with one specific mutation (RPE65) for one specific disorder (Leber’s congential amaurosis), one of dozens of conditions that can cause blindness. It is for this reason that some blind advocates take such issue with Spark’s justification of a $1 million value. They don’t want it to be marketed using language that comes at the expense of blind people now, most of whom the treatment cannot help.
This has been a perennial tension. In 2016, the Foundation Fighting Blindness, which funds research into retinal degenerative diseases and backed some of the research behind Luxturna, released a series of videos for its #HowEyeSeeIt campaign. One video featured a scene of kids at the playground going dark, asking viewers to contemplate being unable to take kids to the park. An outcry ensued. In response, Cervenka filmed and uploaded a video of her and her husband taking their son to the park. “It’s the worst video—a blind person with an iPad,” she joked. But she made her point. She could take her kid to the park just fine.


In downtown Ulaanbaatar, on a pedestal in the Central Museum of Mongolian Dinosaurs, stands a 70-million-year-old Tarbosaurus bataar dinosaur from the southern Gobi Desert. In 2012, the Tarbosaurus was very nearly sold at auction in New York, despite such a sale violating Mongolian law as well as a temporary restraining order by a U.S. federal judge in Dallas. Five years and 6,000 miles later, that very same dinosaur fossil found itself back in Mongolia, now an icon symbolizing Mongolian and American efforts to combat the illicit fossil trade in Central Asia. As I walked through the dimmed entry hall backlit with the museum’s name in lights, it occurred to me that the long-dead and almost-trafficked dinosaur has a lot of life left to live.
Ever since the 1997 sale of Sue for a then-unprecedented $7.6 million, fossils have proven to be an extremely lucrative luxury market. For buyers interested in owning prehistoric natural objects, dinosaur fossils like skulls and complete skeletons can add an impressive bit of the Cretaceous to their portfolios. In the 21st-century high-end collectors’ market, fossils from Mongolia and China, in particular, are challenging the international community’s ethical response to fossil trafficking. Ever since the return of that first Tarbosaurus, thanks to the Herculean efforts of the Mongolian paleontologist Bolortsetseg “Bolor” Minjin, dozens and dozens of other dinosaur fossils have been seized by ICE and sent back to Mongolia.
“Sending the fossils back” is really just a new beginning for these repatriated fossils.
* * *
Through the efforts of Bolortsetseg and other fossil activists, the Institute for the Study of Mongolian Dinosaurs, a nonprofit organization in Ulaanbaatar, works with U.S. and Mongolian agencies to help return Mongolian fossils before they eventually go on display at the Central Museum of Mongolian Dinosaurs. Bolortsetseg and her colleagues consider education and outreach, as well as museum curation, to be an integral part of Mongolia’s successful fossil-repatriation program, whether through new dinosaur museums in Ulaanbaatar or driving a mobile museum with casts of fossils to rural parts of Mongolia.
At the museum, the fossils are important objects that teach visitors about Mongolia’s deep natural history and paleo past. According to UNESCO, the Gobi is the world’s largest “fossil reservoir.” Scientists have discovered something like 80 different dinosaur genera in the desert from over 60 known fossil sites. Although other parts of the world, like the Rocky Mountain region in the western United States and Canada, have spectacular fossils, those found in the Gobi are particularly prized because the fossils are so well preserved, allowing researchers to see tiny details on the fossils, like marks of blood vessels and nerves. The spectacular preservation also means that the Gobi can boast an inordinate number of fossil species, both large and small, plant and animal, all of which offer scientists the opportunity to study ancient ecosystems.
For decades, Russian, Polish, Chinese, and American paleontologists have come to the southern stretches of the Gobi’s arid Nemegt Basin to excavate dinosaur—and other—fossils. (The interest in the area traces back to the 1920s, when the American Museum of Natural History explorer Roy Chapman Andrews showed the world Mongolia’s fossil-rich deposits, including the first example of dinosaur eggs.) The first Tarbosaurus fossils were discovered in the 1940s and the species was officially named by Soviet paleontologist Evgeny Maleev in 1955; the Tarbosaurus bataar is an evolutionary cousin to the well-known Tyrannosaurus rex. By the time Tarbosaurus started hitting American auctions in the 21st century, however, it was long understood in the scientific community as a uniquely Mongolian dinosaur (despite some specimens being found in China) thanks to its uniquely Mongolian geological context.
The Tarbosaurus that sparked Mongolia’s repatriation revolution stands eight feet tall, measures 24 feet from tail to snout, and arrived back in Ulaanbaatar in 2013. Both the Tyrannosaurs and Tarbosaurs are apex predators, sporting fierce teeth and meme-ically comic forearms—both are some of the most charismatically identifiable species of the late Cretaceous, striking a perfect balance of awe and inspiration to museums and collectors the world over.
As with most countries, it is illegal to remove fossils and archaeological artifacts from Mongolia and sell them in private markets. Having legislation on the books that prohibits the sale of fossils is one thing. Being able to enforce that legislation is another. Money talks and there are immense amounts of it to be made with buyers willing to pay top dollar for something as stunning as a dinosaur skeleton. (“[The Tarbosaurus] can fit in all rooms 10 feet high,” the auctioneer added when the original Tarbosaurus fossil was going under the gavel in 2012. “So it’s also a great decorative piece.”) The repatriation of the Tarbosaurus bataar in 2013 gave Mongolia and the paleontological community some Tarbosaurus-like teeth to pursue the possibility of recovering other smuggled fossils.
For artifacts and fossils, repatriation has long been a way of ceding social authority and acknowledging a country’s right in assigning an object’s value, be it scientific or cultural. For the fossils to connect with their audiences, they need to be defined as something more than just “returned”—they need to be shown, studied, talked about, and valued.
When the Tarbosaurus bataar arrived in Mongolia in 2013, Bolortsetseg spent two weeks curating an exhibit that showcased the fossil and the efforts to have it returned to Mongolia. The exhibit was a sensation as visitors queued for hours to see the fossil. The exhibit piqued public curiosity and reintroduced dinosaurs to Mongolia’s national identity. The Central Museum of Mongolian Dinosaurs soon dedicated itself to bringing public awareness to repatriated Mongolian dinosaurs. The museum operates out of Ulaanbaatar’s old Lenin Museum, now repurposed and renovated with shiny new display cases, snazzy illustrations, detailed maps of paleontological sites, placards describing the science behind the species, and a fossil-themed gift shop. In 2013 alone, 22 more trafficked dinosaurs were repatriated—each dinosaur sent back home reinforced precedent for other fossils to follow. As visitors to the Central Museum of Mongolian Dinosaurs walk up the building’s old marble steps to the second floor, the plethora of trafficked fossils—and the scope of the smuggling problem—quickly becomes apparent. The fossils on display are simply the tip of the iceberg, in terms of the number of fossils smuggled out of Mongolia.
Since 2013, other dinosaur-museum spaces have been built to highlight Mongolia’s fossil heritage. At the Hunnu Mall on the outskirts of Ulaanbaatar, for example, visitors are welcomed to the small museum with the truly spectacular
“Amazing Dinosaurs From Mongolia” exhibition, sponsored by the Mongolian Academy of Sciences Institute of Paleontology and Geology. From my bench perch on the Hunnu Mall’s second floor, I watched visitors circle the full-size dinosaur displays and then follow the decal dinosaur footprints to the museum’s exhibition. Some of the Hunnu Mall’s dinosaurs are real fossils with museum catalog numbers clearly visible, like the giant plant-eating sauropod Opisthocoelicaudia skarzynski; some are replicas, like the Tarbosaurus bataar that stands in front of the first-floor escalator backlit by a Coca-Cola advertisement. Most shoppers pause, look at the dinosaurs, take some selfies, and go on about their business.
There is little doubt that these repatriated fossils have an important role to play in educating a new generation of local dinosaur experts. Mongolia does not currently have a graduate training program in paleontology—for either the science or curation of fossils. During summer months, Bolortsetseg and the ISMD volunteers drive a 40-foot Winnebago through remote parts of Mongolia to introduce kids to Mongolia’s deep geologic history. (The dino-mobile was previously owned and subsequently donated by the American Museum of Natural History, which also used it for fossil-outreach projects.)
In 2016 alone, more than 30 fossils were recovered from U.S. auctions, ranging from a nest of dinosaur eggs, to a Protoceratops skull (similar in size and anatomy to a Triceratops), to an entire turkey-sized Psittacosaurus skeleton. The fossils recovered from 2016 occupy the second floor of the Central Museum of Mongolian Dinosaurs’ exhibit hall, with plaques from ICE that commemorate the joint efforts to halt the illegal smuggling. Based on U.S.-Mongolian efforts, seven additional fossils from Europe joined the burgeoning collection of returned dinosaurs currently housed at the Central Museum of Mongolian Dinosaurs.
At the Central Museum, I watched visitors point out the ICE certificates to each other. As tourists tried to figure out the best angle to photograph the Tarbosaurus on its pedestal, museum guides told and retold the story of its triumphant return.


The year 2017 has turned out to be a good one for rocket science in the United States.
American companies made 29 successful rocket launches into orbit, the highest figure since 1999, which saw 31 launches, according to a comprehensive database maintained by Gunter Krebs, a spaceflight historian in Germany. The final launch of the year, by a SpaceX Falcon 9 rocket carrying a cache of commercial communications satellites, took place Friday night at Vandenberg Air Force Base in California.
SpaceX, Elon Musk’s private spaceflight company, is responsible for most of this year’s launches. After a brief hiatus following an explosion in September 2016 that destroyed a Falcon 9 and its $200 million commercial payload, SpaceX returned to the launchpad in mid-January. At the time, the success of the launch was imperative; SpaceX had lost another rocket in June 2015, about two minutes after takeoff, and its rocket-fueling process was receiving intense scrutiny by a NASA safety advisory group. NASA was entering its fifth year of using SpaceX rockets for resupply missions to the International Space Station, and future deals were on the line.
The launch went smoothly. Another successful attempt followed in February, and then two more in March. Observers held their breath as the months progressed, hopeful for a streak but unwilling to call attention to it, lest they jinx the whole thing. But the streak continued, and SpaceX launched a variety of payloads—commercial satellites, International Space Station cargo, even supersecret missions for the U.S. government—at a regular cadence throughout the year.
Despite an explosion of one of its rocket engines during a test at its facility in Texas in November, SpaceX ended the year without any in-flight failures or launchpad explosions. The company’s total comes to 18 launches in 2017—its most in a single year, and more than double that of the previous year.
The streak has made rocket launches by a commercial company from U.S. soil seem routine and almost ordinary—though someone probably should have warned the people of Los Angeles, many of whom were confused or terrified at the sight of the exhaust plume from Friday’s launch, which took the shape of a ghostly spaceship in the night sky.
It has been just two years since SpaceX launched a Falcon 9 into orbit and then returned the rocket’s first-stage booster back to Earth, landing it upright on the ground. And although there were a few spectacular fiery fails before and after this attempt, the recovery of a Falcon 9’s first stage, whether on land or on a drone ship at sea, seems, by now, a regular occurrence.
But landing a rocket stage after launch is only the first part of a truly reusable rocket regime. In March of this year, SpaceX made good on the second part when it used a refurbished first stage, previously flown in 2016, to launch a communications satellite. The first stage returned to Earth, touching down on a drone ship in the Atlantic Ocean. It was the first time SpaceX had launched a “flight-proven” rocket, the promise of which led Musk to create the company in the first place. In November, NASA agreed to let SpaceX use refurbished rockets in its resupply missions to the International Space Station. Earlier this month, a used rocket did just that, launching 5,000 pounds of crew supplies and science experiments.
The last year has placed commercial companies like SpaceX and Blue Origin, Amazon founder Jeff Bezos’s firm, squarely in the age of reusable rockets. While NASA builds an expendable heavy launch system for astronauts of the future, the rest of the market has decisively moved away from disposal rockets, and private firms are jockeying to be the top brand in the field. Blue Origin beat SpaceX to reusability in November 2015, when the company launched its New Shepard rocket into Earth’s atmosphere, breached the edge of space, and parachuted back down, landing vertically. Musk and others quickly, and correctly, pointed out that while Bezos’s rocket had made it to the Kármán line, the boundary that separates the atmosphere and outer space, it had not reached orbit. Blue Origin has tested the New Shepard several times this year, but hasn’t yet achieved orbital flight.
SpaceX said it plans to significantly increase the number of launches in 2018. “We will increase our cadence next year about 50 percent,” Gwynne Shotwell, the company’s president and COO, told SpaceNews in November. “We’ll fly more next year than this year, knock on wood, and I think we will probably level out at about that rate, 30 to 40 per year.” The company also said it would start construction next year on the BFR, a behemoth of a rocket that Musk said in September would someday replace its current fleet of spacecraft in sending payloads—and eventually people—into low-Earth orbit and beyond.
SpaceX will begin the new year as it began 2017, with another high-stakes launch. After some delay and much hype, the company plans to launch its Falcon Heavy rocket, a reusable, behemoth version of the Falcon 9 capable of lifting more than twice the payload of the Delta IV Heavy, the most powerful heavy-lift launch vehicle in operation. The Heavy is a throwback to the power of the Saturn V, the rocket that sent astronauts to the moon and Skylab, the first U.S. space station, into orbit in the 1960s and 1970s.
Musk has said he’s not sure if the flight will be a success. “I hope it makes it far enough away from the pad that it does not cause pad damage,” he said at a conference in July. “I would consider even that a win, to be honest.”
For its maiden flight, the Falcon Heavy will carry one of Musk’s cherry-red Tesla roadsters, in quite the show of cross-platform promotion. The rocket will blast off from a launchpad SpaceX leases from NASA, at Cape Canaveral in Florida—the historic site of launches for the Apollo program, the Space Shuttle program that built humans a home in orbit, and, in this case, an attempt to make big rocket launches a routine thing.


It’s not easy being a “theory of everything.” A TOE has the very tough job of fitting gravity into the quantum laws of nature in such a way that, on large scales, gravity looks like curves in the fabric of space-time, as Albert Einstein described in his general theory of relativity. Somehow, space-time curvature emerges as the collective effect of quantized units of gravitational energy—particles known as gravitons. But naïve attempts to calculate how gravitons interact result in nonsensical infinities, indicating the need for a deeper understanding of gravity.
String theory (or, more technically, M-theory) is often described as the leading candidate for the theory of everything in our universe. But there’s no empirical evidence for it, or for any alternative ideas about how gravity might unify with the rest of the fundamental forces. Why, then, is string/M-theory given the edge over the others?
The theory famously posits that gravitons, as well as electrons, photons, and everything else, are not point particles but rather imperceptibly tiny ribbons of energy, or “strings,” that vibrate in different ways. Interest in string theory soared in the mid-1980s, when physicists realized that it gave mathematically consistent descriptions of quantized gravity. But the five known versions of string theory were all “perturbative,” meaning they broke down in some regimes. Theorists could calculate what happens when two graviton strings collide at high energies, but not when there’s a confluence of gravitons extreme enough to form a black hole.
Then, in 1995, the physicist Edward Witten discovered the mother of all string theories. He found various indications that the perturbative string theories fit together into a coherent non-perturbative theory, which he dubbed M-theory. M-theory looks like each of the string theories in different physical contexts but does not itself have limits on its regime of validity—a major requirement for the theory of everything. Or so Witten’s calculations suggested. “Witten could make these arguments without writing down the equations of M-theory, which is impressive but left many questions unanswered,” explained David Simmons-Duffin, a theoretical physicist at the California Institute of Technology.
Another research explosion ensued two years later, when the physicist Juan Maldacena discovered the AdS/CFT correspondence: a hologram-like relationship connecting gravity in a space-time region called anti–de Sitter (AdS) space to a quantum description of particles (called a “conformal field theory”) moving around on that region’s boundary. AdS/CFT gives a complete definition of M-theory for the special case of AdS space-time geometries, which are infused with negative energy that makes them bend in a different way than our universe does. For such imaginary worlds, physicists can describe processes at all energies, including, in principle, black-hole formation and evaporation. The 16,000 papers that have cited Maldacena’s over the past 20 years mostly aim at carrying out these calculations in order to gain a better understanding of AdS/CFT and quantum gravity.
This basic sequence of events has led most experts to consider M-theory the leading TOE candidate, even as its exact definition in a universe like ours remains unknown. Whether the theory is correct is an altogether separate question. The strings it posits—as well as extra, curled-up spatial dimensions that these strings supposedly wiggle around in—are 10 million billion times smaller than experiments like the Large Hadron Collider can resolve. And some macroscopic signatures of the theory that might have been seen, such as cosmic strings and supersymmetry, have not shown up.
Other TOE ideas, meanwhile, are seen as having a variety of technical problems, and none have yet repeated string theory’s demonstrations of mathematical consistency, such as the graviton-graviton scattering calculation. (According to Simmons-Duffin, none of the competitors have managed to complete the first step, or first “quantum correction,” of this calculation.) One philosopher has even argued that string theory’s status as the only known consistent theory counts as evidence that the theory is correct.
The distant competitors include asymptotically safe gravity, E8 theory, noncommutative geometry, and causal fermion systems. Asymptotically safe gravity, for instance, suggests that the strength of gravity might change as you go to smaller scales in such a way as to cure the infinity-plagued calculations. But no one has yet gotten the trick to work.
This article appears courtesy of Quanta Magazine.


For most people, the thought of spending every waking hour with strangers in a metal capsule roughly the size of a studio apartment for weeks sounds like the stuff of nightmares.
For others, it’s a dream.
About 400 people applied this year to live, work, and sleep in NASA’s Human Exploration Research Analog, a three-story habitat built to mimic the confinement of space missions and study human behavior and teamwork dynamics. The space agency has spent the last several months shuffling groups of four volunteers in and out of the habitat, which sits inside a warehouse at the Johnson Space Center in Houston. The groups live in the habitat for 45-day stays designed to simulate a round-trip journey to an asteroid to collect and return soil samples. The latest group emerged this month, were greeted with sparkling fruit juice, and returned to their daily lives, with some much-needed privacy.
The participants are essentially lab rats, the test subjects that will inform the procedures and protocols necessary for future missions to Mars and deep space. Everything that happens to them in that metal tube—their physiological changes, mood swings, interpersonal interactions—will someday be folded into guidelines for keeping astronauts happy and healthy on long-term missions. Similar campaigns to study human behavior for space journeys are underway elsewhere, including a University of Hawaii program called HI-SEAS that put six people in a fake Mars habitat in Hawaii for eight months this year.
“I’ve built a career asking other people to be test subjects. I felt like I owed it to the science to be a guinea pig myself,” says Rick Addante, a psychology and neuroscience professor at California State University at San Bernardino. Addante and three others moved into the HERA habitat in August. “If we want to get to Mars, we have to use our brains, but we also have to understand our brains and what’s going to happen to them on the way to Mars,” he says.
For now, though, the goal of analogs like HERA and HI-SEAS is to get people to survive weeks of confinement in good health—without losing it or turning on each other. The HERA program is part space camp, part escape room, and pays $10 an hour. Crew members are all in it together—literally—so making it work is their only option.
“I had a lot of faith in the powers that be to select a good crew,” says Reinhold Povilaitis, a former HERA crew member and research analyst at Arizona State University who works on a NASA moon orbiter. “Before I went in, I reminded myself to keep an open mind about everything.”
NASA picks “astronaut-like” volunteers, people between ages 30 and 55 who have advanced degrees in science fields or some military experience and can pass medical, physical, and psychological screenings. Participants must also pass virtual-reality motion-sickness tests to prepare for simulations of space walks and sample collections using VR headsets. After that, assembling a crew is kind of like an admissions office pairing roommates in the first year of college. HERA staff tries to pick people who will get along. “They may or may not become best friends, but they work together,” says Lisa Spence, the flight-analog project manager for HERA.
The experience is meant to be as isolating as possible, far more extreme than the environment on the International Space Station. Unlike ISS astronauts, HERA crew members—or HERAnauts, as they’re called—had no internet access and get just 30 minutes a week to call family and friends. Their only connections to the outside world were the handful of NASA employees who monitor them and electronic copies of the Houston Chronicle and U.S.A. Today, delivered every weekday. Their workdays were scheduled to the minute, packed with sample collections, simulations, drills, and wellness tests and screenings.
“It’s weird not to see the sun and not hear the rain and not feel the wind,” says Tim Evans, a biology professor at Grand Valley State University in Michigan who stayed in the HERA habitat from May to June. “But you don’t dwell on that because you’re so busy doing other things.”
NASA closely monitored the crew’s health. They took surveys asking them about their emotional states and math tests that targeted their cognitive function. Their diets—consisting of freeze-dried or thermo-stabilized foods made for microgravity—were tightly controlled. No breath mints were allowed on board, since those would add an extra calorie or two to the daily intake. Participants wore sensors to track their vitals (and, during virtual-reality simulations, their brain activity) and regularly gave blood, urine, and fecal samples. For the blood draws, crew members stuck their arms through a hole in a curtain in the habitat’s airlock, where a “robot”—actually a HERA staffer—stuck a needle in their veins.
On top of that, crew members were deprived of sleep. To study how lack of adequate slumber affects humans, the HERA program kept participants awake for 19 hours every weekday. “We would stand around at 1:59 a.m. just waiting for the clock to go to 2 a.m. so we could go upstairs [to our bunks] and go to sleep,” says Shelley Cazares, a research scientist at the Institute for Defense Analyses in Virginia who stayed in the HERA habitat in August. On weekends, they were allowed a full eight hours of sleep a night.
But the deprivation was taxing. The books some crew members brought with them for entertainment instead put them to sleep. “I’d wake up tired and be tired all day,” Evans says.
The lack of sleep negatively affected their cognitive and motor skills—they made more mistakes when maneuvering a robotic arm on the habitat, for example—and sometimes made them irritable and grumpy. “We all joked about there being a kind of honeymoon phase, where we’d get up in the morning and say, ‘Hey, how are you doing, how’d you sleep?’” says John Kennard, a Green Beret in the U.S. Army who teaches cyberdefense at Fort Bragg in North Carolina. Kennard stayed in the HERA habitat from May to June. “And then after about a week, it was more like grunts than actual conversation. You figure out who’s less talkative in the morning, who needs their own space to fully wake up.” James Titus, Kennard’s fellow crew member, heartily agreed. “In the morning, in-depth conversation could not happen,” says Titus, who works for a nuclear-fusion start-up in California. He said he would dab some Tabasco sauce on his tongue when he started to feel himself dozing off.
The resulting crankiness naturally led to some misunderstandings among crew members. Tension, in general, is pretty unavoidable in such an environment. One crew member likened the experience to a long family car ride across the country, where people are bound to get on each other’s nerves at some point. The key difference is that they can’t leave the HERA habitat for a walk, and must instead talk it out.
Time slows down inside the habitat. A day could feel like an entire week, some crew members say. To pass the time when they weren’t working, the crew played board games and watched a ton of movies. One crew watched every installment of Star Wars, The Lord of the Rings, and Harry Potter.
Sometimes, when the Houston Chronicle delivered a big story, like the firing of FBI director James Comey in May, they’d talk about politics. Crew members often had different political leanings, but they say their discussions remained respectful and productive. “It was, in a lot of ways, the types of discussions you have when you’re in college and you would have that time to sit there and actually hash something out,” says Mark Settles, a plant-cell and molecular biology professor at the University of Florida, who was in the habitat when the Comey news broke.
The isolation had some perks, like the lack of email and all its anxiety-inducing qualities. “It was pretty freeing,” Settles says. Of course, when he returned to the real world, “it took me months to catch up on the things that I had missed,” he says.
The outside world crept into the habitat in a very big way in August, when Hurricane Harvey arrived in Houston, wreaking havoc on the city. A crew was about halfway through their mission at the time, and they tracked the storm’s developments through their daily newspaper deliveries. When Harvey moved to front-page news, NASA started calling the crew member’s emergency contacts, just in case. When tornado warnings were issued in the middle of the night, HERA’s mission-control staff roused the crew members and told them to huddle together on the first floor of the habitat. On the morning of August 27, as Houston’s roads and highways swelled with floodwaters, NASA decided to abort the mission. The rising water made it difficult and dangerous for HERA staff to come in, and the HERAnauts couldn’t be left alone.
The crew had about 20 minutes to pack up their stuff and grab their astronaut food. “We stepped outside and I asked the first person I saw, ‘Why did we cancel?’ And he said, ‘Well, just go outside and you’ll see,’” says Paul Haugen, one of the crew members and a NASA engineer. They stepped out of the habitat and found a whole city underwater. The crew piled into a van and drove carefully to a hotel across the street.
Spence, HERA’s project manager, doesn’t know yet whether NASA will use the data gathered during the shortened mission. The crew that got Harveyed, as Spence puts it, was, naturally, disappointed about such an abrupt end. The crews that completed the full 45 days had a slightly different outlook when they stepped out of the habitat. The members had become friends, and would stay in touch after they left the habitat—texting about inside jokes and even how grouchy they could get—but when that airlock popped open, revealing the world they had pretended to leave, they were thrilled. Their work was done. The astronauts of the future will benefit from what NASA learns from this experiment—but they’ll need to wait far longer to feel the relief of coming home.


Good news for the environment comes from California today, and from a part of the state very near the hillsides that have suffered the economic and environmental devastation of the recent wildfires. A renowned tract of undeveloped California coastal land totaling more than 24,000 acres, or about 38 square miles, has been purchased by The Nature Conservancy (TNC) for permanent preservation, thanks to a $165 million donation by a wealthy tech-industry couple. The donation, the largest single gift TNC has ever received, is significant in its immediate effects, and it has the potential to matter even more through the longer-term example it aspires to set.
The tract includes hills and canyons, grasslands and brush, 2,000 acres of coastal live-oak stands containing perhaps 1 million trees, a creek, parts of the Santa Ynez mountain range—and a full eight miles of the bluffs and beaches that make up the coastline around Point Conception, west of Santa Barbara. The benefactors are Jack and Laura Dangermond, who founded and still run the Esri mapping company in the small southern California town of Redlands where they both grew up. (For the record: I have known the Dangermonds for many years, having grown up in the same town at about the same time.)
The tract will be called the Jack and Laura Dangermond Preserve. It has been informally known as the Bixby Ranch, after the family that owned it and a lot more of Southern California starting in the late 1800s. (More recently the land has been known as the Cojo and Jalama ranches.) Cattle have grazed here since the ranch’s founding, but the land still very much has the look and the ecological characteristics of the wild. Crucially, it has never been subdivided, converted to malls or mansions, or otherwise commercially developed. With today’s announcement, it never will be—a different outcome than many conservationists feared over the past decade, when the land was owned by a real-estate hedge fund from Boston that has specialized in coastal-development projects.
The TNC representatives I spoke with about this nature preserve, along with Jack Dangermond himself, were careful to say nothing whatsoever about these previous owners. But the group’s identity and track record are easy to figure out from online sources. When it bought the ranch for some $136 million 10 years ago, just before the worldwide financial and real-estate crash, its lawyer gave a non-denial denial to local citizens concerned about commercial development. Under terms of the sale, the land could have been broken into more than 100 parcels. But according to Ethan Stewart of the Santa Barbara Independent, the hedge fund’s lawyer said everyone should calm down. After all, he said, his clients “absolutely do not have any specific plans yet for the land.” In the financial and regulatory conditions of the decade that followed, it turned out that they were not able to pursue any.
* * *
Point Conception is evident on a map as the place where the California coast takes a 90-degree turn. On one side of it, stretching toward Santa Barbara and Ventura and beyond them to Los Angeles, the coast runs nearly east-west. On the other side, heading up toward Monterey and San Francisco, the coast runs mainly north-south. (Immediately north and west of the tract is Vandenberg Air Force Base, still in active use as a launching site for rockets and missiles.) The headland of Point Conception juts out into the Pacific at the corner where the coast makes its turn, with one of the earliest lighthouses built along the California coast.
“There’s no place like it on this Earth,” a county supervisor named Joni Gray told Ann Herold and Dan Harder of the Los Angeles Times 10 years ago, after the hedge-fund sale. “It’s more beautiful than Yosemite or Yellowstone.” Another man who had grown up in the region told Herold and Harder, “The footprint of man is very light out here. It’s where you understand what California was all about before people ruined it.”
Before the arrival of the Spanish and then the Americans, this was the land of the Chumash people, who considered the point a portal through which spirits entered the next world. Point Conception’s positioning has long made it perilous for mariners—and precious to scientists and naturalists. “I like to say it’s where Northern California and Southern California meet,” Michael Sweeney, executive director of The Nature Conservancy in California, told me. “That is why it is a zone of such ecological diversity. You have plants and animals you’d expect to find in the north, along with those you’d expect to find in the south. It is uniquely diverse.”
The melting-pot aspect extends even to the waters off the point. “On the marine side, it is one of the most valuable places on the California coast, because it’s where the cold currents coming down from the north meet the warm currents from the south,” Jack Dangermond told me in a phone interview. “With this huge mixing, you have a rookery of seals, big whales, all the elements of a hugely diverse marine resource.” A California state marine reserve, established over the past decade, protects fish, sea mammals, and other wildlife in a 22-square-mile area of the waters off Point Conception. “From the seafloor to the ridgetops you have a big protected area, and that is pretty special in the world,” Sweeney said.
“This is a conservation project of historic significance,” Henry Yang, the chancellor of the University of California at Santa Barbara, told me via email when I asked him about the new preserve. As part of this project, the Dangermonds are also establishing a $1 million endowed Chair in Conservation Studies at UCSB. “The area is recognized globally for its rich biological diversity and ecological significance,” Yang said. “As a transition zone from warmer southern species to cooler northern species both on land and in the coastal ocean, it provides a unique place to study and learn how climate affects the ecosystem.”
“Laura and I ... became deeply attached to this land a long time ago,” Jack Dangermond said in an all-hands messages to the several thousand Esri employees in announcing the purchase. Fifty years earlier, on the Dangermonds’ honeymoon in the late 1960s, they’d driven along the coast in a little car and camped at nights in a pup tent. “We were just kids, and that was our first connection to realizing it was a special place,” he told me.
* * *
The Dangermonds’ gift is significant in two ways, the immediate and the longer term.
The first effect is what it does directly, to preserve habitat. The simple fact is that without this move, sooner or later this ecologically, aesthetically, and even spiritually important land would have been bulldozed away. Now that will not occur.
Jack Dangermond told me that his friend E.O. Wilson, the famed biologist, had advanced the idea that “we are innately all ‘bio-philiacs’”—that people are drawn to nature even in unconscious ways. “It’s why people keep a little philodendron in their apartment, or have an aquarium, or dogs. People want to feel some connection to nature and the natural living world.” The innate importance of preserving parts of the natural matched the Dangermonds’ own sensibility. Jack grew up in a gardening culture—his father, an immigrant from Holland, was a gardener and ran a small nursery business, and Jack’s original training was in landscape architecture—and he and Laura have made the property around their modest home in California essentially one big arboretum. But they argue that preservation has a larger consequence.
“These natural areas, particularly pristine and intact areas like this one, are so very important, and they are disappearing like crazy,” Jack told me. The Dangermonds’ Esri company specializes in mapping software that, among other functions, allows long-term analysis of geographical trends. “We did a study with Clark University, forecasting out 50 years, and making maps with our software about natural areas in that time. And in 50 years, the areas that remain will become very fragmented. If any normal person would see that, they would get very disturbed, but the process is well underway. These models make it clear that the fundamental fabric of nature is being altered, and these areas are going to disappear.”
Thus the urgency, he argued, of “grabbing places like this and putting them into conservation, while you still can.” The vistas of Point Conception may not have the same iconic impact as the Grand Canyon or Yosemite, he said. “But ecologically it is just as important”—because of the diversity of flora and fauna, because of its nearly unspoiled aspect. “We have the splendor of the national parks because they were set aside 100 years ago in a park system. It gives me a kind of solace to think that more of these areas will be protected.”
Beyond solace, the Dangermonds hope that UCSB and The Nature Conservancy will together make Point Conception a unique digital-research center for the identification and protection of similarly important and endangered ecological hotspots around the world. They’ll have help from Esri’s software, which they believe will allow them to measure and analyze trends, good and bad, more extensively than in any comparable site.
“This really is a pivotal point to be envisaging how new technologies might be employed to gather the data in support of research,” Chancellor Yang wrote to me. “Many new Earth-observing satellites have been launched in recent years, with the ability to scan the surface more frequently and in more detail. We have a wide range of sensors that can be installed on, below, and above the land surface; we have new computer models that can take these data inputs and process them to make accurate predictions of the impacts of conservation strategies.”
* * *
The second significant aspect of the purchase is the example the Dangermonds hope to set for their fellow rich people. This donation is unusual not simply in its scale but also because the Dangermonds are publicizing it, something they usually take pains not to do.  
For a long time, Jack and Laura Dangermond have been the wealthiest family in their small community of Redlands, and its leading philanthropic donors. Since their Esri company is still privately held, the precise extent of the Dangermonds’ wealth is also private. Jack told me that published estimates, in the low to mid billions, are “exaggerated.” Whatever the details, they are people of means. But their previous gifts have been unpublicized and often anonymous. Their names don’t appear at all, or only in fine print, at many of the local institutions they support.
In contrast, this tract of land will be the Jack and Laura Dangermond Preserve, and the professor at UCSB will hold the Jack and Laura Dangermond Chair. Why?
“We’re very intentionally setting out a model that we hope other people with money will follow,” Jack Dangermond said. “We’d like people to think, ‘Let’s do what the Dangermonds did.’ We’d like them to copy us.”
“There are lots of wealthy people in the tech industry in California and elsewhere,” he said. “America’s wealthy people are flush with money. They’re wondering where to put it”—and the favored emphases of this era’s tech leaders include education and public health. “Those are great! But we haven’t yet seen that kind of commitment to nonpolitical conservation issues.” He gave the example of the Rockefeller family a century ago, who devoted some of their assets to preserving land in New York, in Maine, in Wyoming, in many other places that eventually adjoined or became part of national or state parks.
He is aware of the complications of this private-philanthropy model for conservation: the baronial overtones, the theoretical superiority of having the government take the lead with truly national parks, like the Grand Canyon or Yellowstone. But to put it mildly, that’s not what the federal government is doing these days. In the weeks before the Point Conception announcement, Donald Trump ordered the the Bears Ears and Grand Staircase–Escalante national monuments reduced in size. When I asked Michael Sweeney why the state or federal government hadn’t bought the ranch long ago—including through the defense budget, since Vandenberg Air Force Base is next door—he said, “Because it just would never happen. The price is an obstacle. There are too many agencies to coordinate.”
Jack Dangermond makes a more positive case. “If you look back historically, the national parks were of course a public undertaking. But families like the Rockefellers played a direct or indirect part for many years. America has a long history of private philanthropy for public causes that sometimes gets overlooked.”
Much as the Rockefellers’ example is remembered now—or Carnegie’s with his libraries, or the Mellons’ and Fricks’ with their museums—the Dangermonds hope that in their smaller-scale way they can set an example others can refer to. “We would love to have 100 Dangermond Reserves,” he said. “But I’m not Carnegie. We’re not in the oil business. We can’t do this by ourselves. We’re telling the story to set an example of what others could do.”
Examples set by people of wealth and influence can have their ripple effects through society. Andrew Carnegie established some 2,500 libraries largely with his own money, but ultimately their success and survival depended on support from the communities where they were built. Indeed, Carnegie conditioned his grants on guarantees of long-term local taxpayer support for each library, because otherwise “the public ceases to take interest in it, or, rather, never acquires interest in it.” Jack Dangermond said that, beyond its potential effect on other rich families, the new preserve would encourage citizens more generally to look at conservation projects in their own neighborhoods. “People could buy a vacant lot and turn it from ‘gray infrastructure’ to ‘green infrastructure,’” he told me. “As cars become automated, a lot of parking-lot space will open up, and people in neighborhoods can collectively or individually create parks and make cities more livable.”
Will individuals and families make the connection between this large act of philanthropy and the smaller-scale opportunities immediately around them? It’s a lot to expect. But even if the impact of today’s news were, despite the donors’ aspirations, confined to one evocative tract along the California coast, the announcement would be a significant one. It is a gift to the planet and to the future—a dramatically positive move from people with wealth, when there’s been so much movement the opposite way.


Can unexpected weather make a war or a failed state more likely? It’s a question that could define the 21st century.
A new study, published Thursday in Science, finds a link between temperature variation and forced migration.
When unusually hot or cold weather strikes the growing region of an agricultural country, more people living in that country seek asylum protection in the European Union. Those people are then, in turn, more likely to be accepted as permanent residents by the EU.
Because asylum applicants must be fleeing conflict or persecution—and because their acceptance seems to validate the severity of their claims—the study’s authors say they’ve found an underlying connection between weather, agriculture, and failed governance.
“It’s pretty much like a medical trial of a new drug. There are many impacts that affect health, just like there are many impacts that affect asylum applications. But we’ve set up the trial and those are not correlated, so I have faith that we’ve established a relationship between weather and conflict,” said Wolfram Schlenker, an environmental economist at Columbia University. Schlenker and Anouch Missirian, another environmental economist at Columbia, conducted the study.
The study began by examining two databases: asylum applications to the EU between 2000 and 2014, and average temperatures across 103 countries.
The authors omitted asylum data from 2015 and 2016, when refugees were fleeing the Syrian Civil War and other conflicts. More than 1 million people applied for asylum annually during those two years, a spike well above the average 350,000 annual applicants from 2000 to 2014.
Schlenker and Missirian found an early correlation between weather and migration, but they waded through the data, trying to account for as much statistical noise as possible. They removed one-year shocks from events like the onset of the global financial crisis. They also factored in the difference between hot and cold countries, as a naturally colder country might be able to deal with a few extra degrees more easily than a hot country.
Ultimately, they found that the entire effect in asylum increases was attributable to temperature shocks in maize-growing countries that hit during the growing season, in the area where crops are grown.
Though the research seems to examine the relationship between climate change and migration, it’s actually getting at the deeper question of climate and governmental collapse or oppression. That’s because of the definition of forced migration: Only refugees from conflict or persecution can apply for permanent asylum in another country. The 1951 UN Refugee Convention, which every nation in the EU has signed, does not define a refugee as someone fleeing a country for economic reasons.
If a country suffered a temperature shock, then its refugees were three times more likely to be accepted for asylum than the average applicant to the EU. To Schlenker, this points to a deep connection between climatic crises and military conflict or persecution.
“The mechanism through which this works is conflict,” he told me. “We don’t know why they get accepted or not, but people in destination countries in the EU find them to be worthy of protection.”
The paper also projected its findings forward, by examining when various climate models believe weather shocks in growing regions could become more likely. They found modest increases. In a world that gets carbon-dioxide pollution under control and holds global warming to roughly 2 degrees Celsius, the number of asylum applications to the EU could rise by about 28 percent.
But if carbon pollution continues unabated, and global temperatures rise by about 5 degrees Celsius, annual applications could rise by 188 percent by 2070. More than 650,000 people could seek protection in the European Union annually.
Schlenker made it clear that these numbers should be used as tools for thinking and not final projections. “This is the best estimate we can do with current data, but there’s lots of asterisks,” he said. The projections could err too high, because they don’t account for global adaptation to warmer weather. But they could also still be too low, because they can’t anticipate political tipping points like civil wars or regime change.
“If a country switches from a democracy to a dictatorship—which would cause many more asylum applications—that would have a huge effect and we don’t account for it in the model,” he said. Schlenker noted that while these would be huge increases, the number of migrants who fled the Syrian Civil War was “much larger than the predicted impact we’re seeing with climate change.”
Researchers who study the intersection of climate and politics said that the historical connection was an extremely sturdy finding, but they shared in Schlenker’s skepticism about the future projections.
“This is a very strong study showing a robust relationship between temperature and forced migration,” said Claire Adida, a political scientist at the University of California at San Diego, by email. “The authors were meticulous in their analysis.”
“They show that the relationship holds only when you look at temperature deviations over the growing season and in the crop area ... This, by the way, is consistent with a lot of other research in agricultural economics showing that climate change and conflict are likely to be related via the effect of climate change on agricultural income,” she said.
Elizabeth Chalecki, a political scientist at the University of Nebraska at Omaha, agreed that the core finding was strong, but she identified a few more variables that she said could be related. Forced migration could also depend on whether commodity crops were available to relieve a food shortage or whether there were broader non-climatic events—like a genocide—happening in the region.
“The researchers admit that their correlation is ceteris paribus, meaning all things being equal, but that never is the case in real life,” she said. “If nationalist politics are on the rise across the EU and elsewhere, then doing something about climate change might be good national-security policy, not just good environmental policy.”
Giovanni Bettini, a political theorist at Lancaster University, said it was important to be careful about discussing climate migration as fait accompli.
“The inference that correlation equates causation is very problematic,” he said, adding that the link between climate change and political mobility was a “political open question.”
And he said it was important to treat projections not as settled science but as one of many tools for thinking about the future. It was crucial, too, he said, to talk about migrants with nuance and care not to fall into racial tropes. “Climate refugees are never white, in the discourse,” he said. “The dangerous flocks of people who are set to be uprooted in the future, creating dangerous security threats for ‘us,’ are always somewhere else.”


Emma Wren Gibson, frozen as an embryo in 1992, was born a few days after Thanksgiving in 2017, more than 25 years later. It’s the longest an embryo is known to have been frozen before being born as baby.
In fact, the embryo that became Emma is only a year younger than the woman who gave birth to her, Tina Gibson. “This embryo and I could have been best friends,” Gibson, now 26, told CNN. Tina and her husband “adopted” the frozen embryo after learning he was likely infertile. It came from an anonymous couple who went through in vitro fertilization (where sperm and egg are united in a lab) and donated their remaining frozen embryos, which have remained suspended in time for more than two decades.
Today, IVF and the cryopreservation of resulting embryos are so commonplace that it’s easy to overlook how disruptive the process once seemed. “All hell will break loose, politically and morally, all over the world,” James Watson once said about IVF. Others warned of the slippery slope to surrogate pregnancies, designer babies, and artificial wombs.
When The New York Times wrote about cryopreservation in 1974, experts envisioned a catalogue of “one-day-old frozen embryos, guaranteed free of all genetic defects, with sex, eye color, probably IQ, and other traits described in detail on the label. Just thaw and implant.” But what no one quite considered was how much the technology could eventually shift time—resulting in births like Emma’s.
In the beginning, embryos created through IVF were implanted in the womb right away. The first IVF baby, Louise Brown, was born in 1978. It took six more years before doctors perfected the freezing and unfreezing technique. Zoe Leyland, the first baby from an embryo frozen after IVF, was born in Australia in 1984. The embryo had been frozen for what we might now consider a brief two months. Doctors in the United States, Britain, Israel, the Netherlands, and West Germany quickly picked up the procedure; over the next two years, more than two dozen babies were born from frozen embryos.
The ability to stop time, even briefly, offered practical advantages. Before cryopreservation, embryos had to be implanted all at once or discarded; now extra embryos could be saved for later, reducing the chance of twins and triplets. Doctors could wait for the moment a mother’s body was most ready to accept an embryo. And it eventually became possible, as predicted, to use the extra time to test embryos for genetic disease. (Actual designer babies are still far from reality, though.)
The IVF Panic: 'All Hell Will Break Loose, Politically and Morally, All Over the World'
Freezing also bought time for families who were uneasy about discarding their extra embryos, but who were not sure about donation, either. Today, as many as a million frozen embryos are stored in tanks of liquid nitrogen. They remain even when their genetic parents die or divorce, inevitably raising legal questions. There are ethical questions, too: The National Embryo Donation Center, where the Gibsons “adopted” Emma, is a Christian fertility center whose purpose includes “protect[ing] the lives and dignity of frozen embryos.”
To make it even more complicated, no one knows exactly how long frozen embryos can remain suspended in time. The only way to know is to keep waiting. There are no official records on how long eggs are frozen before being implanted, but before Emma, the oldest known embryo that resulted in a live birth had been frozen for 20 years. A woman in New York gave birth to the boy in 2010. And in 2013, a woman gave birth from a 19-year-old embryo.
Theoretically, the reproductive specialist Barry Behr once told Scientific American, frozen embryos might last as long as a century or two. The real hitch, he said, is how cryopreservation could scramble ideas of family and generations:
The nature of reproduction and building families does not really foster an environment that would allow you to wait 50 years before you thaw your embryos out. Unless your daughter wanted to carry her sibling, for example, which in theory is possible: A person born from IVF may still have sibling embryos frozen and when they reach 30 and are infertile could technically thaw out the embryos that were created at the same time they were created, gestate them, and deliver their sibling. That hasn’t happened yet but it could be possible.
There may another practical consideration, too. Embryo-freezing technology has changed considerably in the last 25 years, and different freezing techniques require different thawing techniques. Deborah Wachs, a doctor at the Reproductive Science Center of the Bay Area, where the 19-year-old embryo was thawed and implanted, says that her center was transitioning from an older slow-freezing technique to a new vitrification technique at the time. Their longtime lab director had been there since the embryo was first frozen.
“If you look at a new embryologist coming out, they will likely never slow freeze in their entire careers because all they will know is vitrification,” Wachs says.  In 10 years, she notes, everyone will be much less familiar with how to thaw slow-frozen embryos. The limiting factor on how long frozen embryos can remain suspended in time may not be physics or biology, but the half-life of human knowledge.


Hundreds of millions of miles away, in the orbit of Saturn, on the surface of Titan, the planet’s largest moon, rests a piece of human-made technology. Huygens, a nine-foot-wide, saucer-shaped probe, was dropped by the passing Cassini spacecraft and parachuted down to the surface in 2005. For a precious 72 minutes after it landed, Huygens transmitted data back to Earth, including image after image of its surroundings. There they were—gullies, the kind that on Earth are etched into rocky terrain by flowing water. Scientists watched, enthralled, as views of an alien land flashed across computer screens, marking humanity’s first look at the surface of a moon that wasn’t their own.
Then Huygens, out of battery power, went dead; its demise was, as grim as it seems, part of the plan. Huygens remains in the spot where it landed, a dusty monument to the desire to glimpse, even briefly, the worlds in our solar system.
But a new piece of technology may be on its way. Not to land on Titan, but to hover—getting just close enough to reveal secrets.
A mission to send a drone-like spacecraft to study Titan received approval and funding from NASA Wednesday. Dragonfly, a dual-quadcopter, would launch in the mid-2020s and, upon arrival, hop from one spot over Titan’s surface to the next, searching for signs of life.
The funding comes from NASA’s New Frontiers program, a competition for exploration proposals that has produced several well-known and successful robotic missions in the solar system: New Horizons, which flew past Pluto in 2015; Juno, which currently orbits Jupiter; and OSIRIS-REx, which is on its way to an asteroid called Bennu, where it will carve out some surface material and return the sample back to Earth. The Dragonfly mission, led by Elizabeth Turtle, a planetary scientist at Johns Hopkins University’s Applied Physics Laboratory, now has $4 million and one year to complete its concept. So does the second mission NASA approved today, the Comet Astrobiology Exploration Sample Return (CAESAR) that would target a comet that approaches the sun about every six-and-a-half years. But only one of these missions will actually launch—NASA will choose between them in 2019.
Titan, the largest of Saturn’s moons, has long mystified scientists. The moon is wrapped in a dense, planetlike atmosphere mostly made of nitrogen. Spacecraft like Voyager 1 and Cassini have detected a plethora of complex and organic molecules in the atmosphere that are also found on Earth. Robotic missions have also revealed Titan has a similar liquid process to the water cycle on Earth, but with a different chemical compound: methane, the main ingredient of natural gas. On Titan, methane clouds release methane rains that feed methane lakes and seas and streams that can erode the rocky landscapes. This makes Titan, like Earth, an ocean world. All together, these features mean Titan is one of the best candidates for life in our solar system.
But, aside from Huygens, the imaging instruments on spacecraft that have visited Saturn’s orbit have been unable to penetrate Titan’s hazy atmosphere, and the composition of its surface remains largely unknown.
Dragonfly, should it take flight, will carry equipment capable of identifying chemical components important to biological processes. It will search for signatures of hydrocarbon-based life-forms and water and check for signs of progress in prebiotic chemistry—the transformation of simple molecules into complex life. It will carry cameras that will help the copter choose future landing sites and, of course, send home potentially beautiful, high-resolution images. “We’re not exactly looking for five-legged creatures running around or something like that,” Peter Bedini, the program manager for Dragonfly, said during a talk in July. “Although if we did confront one of those, we’d be sure to take a selfie and send it home.”
The support for the Dragonfly mission is a boon to astrobiologists seeking signs of life on other ocean worlds, like Enceladus, another Saturnian moon, and Europa, a moon of Jupiter. On these moons, potential oceans lurk under thick crusts of ice, perhaps teeming with microbes and other life-forms.
The drone-like design of Dragonfly is unusual given the previous roster of robots sent to explore the solar system, like the suite of landers and rovers that have traveled to Mars since the 1970s. But “the dense, calm atmosphere and low gravity [of Titan] make flying an ideal means to travel to different areas of the moon,” the Dragonfly team explains. Aerial probes can cover more ground than rovers. Mars rovers like Curiosity and Opportunity have a top speed of less than one mile per hour, and are not built to maneuver over particularly rough terrain. After five years on Mars, Curiosity’s wheels show some significant damage from crunching over rock. As NASA pushes forward with the exploration of worlds whose surfaces are less understood than that of Mars, scientists and engineers may need to rethink the hardware best suited for exploration.
“In just a few flights, Dragonfly will be able to go farther than the Opportunity rover on Mars has in the last 12 years,” Bedini said.


The Republican tax-reform bill, which passed Congress Wednesday, makes some big changes to the federal government. It repeals Obamacare’s health-insurance mandate, temporarily expands the child tax credit, and permanently cuts taxes on corporations and the wealthiest Americans.
What it doesn’t do: impose a new tax on carbon dioxide, the heat-trapping gas that warms the globe and acidifies the oceans.
This might not come as a surprise, as only some GOP politicians accept that global warming—which a carbon tax is meant to slow—is real. But passage of the tax bill will end a year of prodding, cajoling, and storytelling from leaders in both parties—including sitting senators and Reagan-administration alumni—that tried to force senior Republican leadership into considering some kind of plan to soften the blow of global warming.
As the GOP passes its largest legislative package in years, with no carbon price to be found, it’s clear that those rhetorical efforts have failed. Even as wildfires and hurricanes ransack the coasts, and record-breaking temperatures stack up, there’s still little appetite among the party’s leaders to address climate change through tax policy.
By rejecting a carbon tax without proposing an alternative, Republicans may have relinquished their best chance to shape climate policy this decade, continuing to cede the issue to Democrats at a national level.
Some background: A carbon tax is one of several ways that economists have proposed dealing with climate change. Right now, it costs nothing for American polluters to release carbon dioxide and other greenhouse gases into the atmosphere. Those gases trap heat and aggravate global warming, leading to hotter days, rising seas, more intense storms, and a host of other environmental problems.
A carbon tax aims to make it expensive to release carbon pollution. Under a carbon tax—also called a “carbon fee” or “price”—the federal government would charge companies for each ton of carbon dioxide they release into the atmosphere. Economists and politicians disagree about how to spend the cash raised by such a policy: Some want to rebate it directly back to Americans; others say it should be used to help out-of-work coal miners and renewable-energy development.
Economic models of the idea have made big promises. If the United States imposed a tax of $25 per ton of carbon pollution, it could raise $1.1 trillion over 10 years while preventing 12.4 billion tons of carbon emissions by 2030, according to Resources for the Future, an independent economics research group. A $50 tax could raise $1.9 trillion and prevent almost 20 billion tons.
A carbon tax of $15 would allow the United States meet its goals under the Paris Agreement, according to Resources for the Future. In those negotiations, President Obama promised that the United States would reduce its annual carbon emissions by about 1.6 billion tons by 2030.
At this time last year, Republican opposition to carbon taxes seemed like a sure thing. In May 2016, as he was securing the party’s presidential nomination, Donald Trump tweeted: “I will not support or endorse a carbon tax!” One month later, House Republicans unanimously voted for a nonbinding resolution that rejected the idea of a carbon tax.
But it seemed possible that the party’s stated goal of bringing the U.S. corporate income tax in line with other countries’ revenue schemes could have pushed them to adopt a carbon tax had things gone a little differently. Ireland, which boasts famously low corporate tax rates, also imposes a carbon price of roughly $23 per ton. France, Japan, Ukraine, and more than 35 other countries have either implemented or passed a carbon price, according to the World Bank.
After Trump’s victory and inauguration, some of the Republican Party’s elder statesmen said it was time for a conservative climate policy.
In February, a group of GOP-affiliated economists and former Reagan cabinet officials announced a plan to gut EPA regulations while imposing a a new tax of $40 per ton of carbon pollution. The proposal’s supporters included James A. Baker III and George P. Shultz, secretaries of state to Presidents George H.W. Bush and Ronald Reagan. (In the interest of disclosure: Laurene Powell Jobs, a co-owner of The Atlantic, was also a founding supporter of the plan.)
“Crazy as it may sound, this is the perfect time to enact a sensible policy to address the dangerous threat of climate change,” wrote the economists Martin Feldstein and Gregory Mankiw in The New York Times in February. “Republicans are in charge of both Congress and the White House. If they do nothing other than reverse regulations from the Obama administration, they will squander the opportunity to show the full power of the conservative canon, and its core principles of free markets, limited government, and stewardship.”
Both men brought solid Republican bonafides: Feldstein spent three years as President Reagan’s chief economic adviser; Mankiw played the same role for George W. Bush. They warned that “a repeal-only climate strategy would prove quite unpopular,” citing polling that shows six out of 10 Americans are worried about global warming.
Mitt Romney tweeted approvingly about the plan, its leaders had a press conference in D.C., and some of them even met with the White House economic adviser Gary Cohn—and that was about as far as it got. Congressional Republicans never took up the proposal. President Trump withdrew the United States from the Paris Agreement in June, and he revoked the bulk of President Obama’s climate regulations in October.
Speaking by email on Tuesday, Feldstein did not sound contrite about the work he did to support the Republican carbon-tax plan. “I wouldn’t have done it if I didn't think it might be legislation,” he said.
But the carbon tax’s failure didn’t cloud his view of the overall Republican tax bill. “On balance,” he told me, “I like it.”
Republicans were not the only ones to make a pitch. In August, Senators Brian Schatz of Hawaii and Sheldon Whitehouse of Rhode Island, both Democrats, proposed their own carbon-tax compromise. They hoped to impose a new price of $49 per ton of carbon pollution, using the trillions of dollars in new revenue to cut the top corporate income tax rate. Their plan also went nowhere.
“I don’t think there was a serious prospect of it happening, seeing as how the whole tax package is mostly a way to reward the rich—and especially campaign contributors, a significant portion of whom are in the fossil-fuel business,” says Michael Gerrard, the director of the Sabin Center for Climate Change Law at Columbia University.
“If the Republican leadership was actually serious about the deficit, a carbon tax would’ve been a marvelous opportunity,” he adds. “But obviously there were several things going against it, mostly the words ‘carbon’ and ‘tax’ ... and ‘Trump.’”
“It’s clear that much of the sales pitch for a carbon tax that has been aimed at Republicans has fallen flat,” says Joseph Majkut, a geoscientist and the director of climate policy at the Niskanen Center, a libertarian think tank that supports a carbon tax. He says it’s time to focus on “bring[ing] Republicans toward the mainstream on climate science” while still advocating for a hands-off carbon price.
Oren Cass, a senior fellow at the Manhattan Institute and the domestic policy director of Mitt Romney’s 2012 presidential campaign, said he was grateful that a carbon tax never made it into the bill.
“The long and the short of it is, I don’t think a carbon tax is either good climate policy or good fiscal policy,” he told me. “Even the pretty strong proponents of the carbon tax, they don’t try to assert that it will do anything about climate change. And if you ask them to provide any benefit estimate that they could put into analysis, they won’t—you’ll get vague talk about leadership, or how it’s going to spur innovation.”
But he also resented that the Republican tax bill in its final form increased the deficit by more than $1 trillion. “I dislike strongly that the tax cut is not revenue neutral,” he told me. “I think if you want to cut taxes, you should find other taxes you want to have to pay for it. But if you were to plot out a list of all the revenue raisers you could have, I think a carbon tax should be very far down the list.”
Why? Because carbon-dioxide emissions make for a “terrible tax base,” he said. If a carbon tax succeeded in reducing the use of fossil fuels, or forcing people to move to renewable energy, then it would erode its own tax base over time. “You’ve shifted onto an unstable tax base that you’re hoping will go away, and you’ll wind up having to raise other taxes up anyway” He also said a carbon tax imposed regressive penalties on sectors and regions already struggling in the current economy—such as energy-intensive manufacturing in the Midwest—while rewarding “higher-income coastal knowledge work.”
The massive unpopularity of the Republican plan—41 percent of Americans believe it is a “bad idea,” according to a Wall Street Journal/NBC News poll—may now give Democrats the opportunity to pass their own tax bill. Adam Looney, who led tax analysis at the U.S. Treasury Department for the last three years of the Obama administration, predicted at an event last month that the tax code would continue to change in the years to come.
“It also seems like this is not going to be the last word on tax reform. There will be a lot of changes yet to come in the tax system,” he said. “The carbon tax will always be right there on the shelf ready for the right moment.”
Gerrard, the Columbia University professor, agreed, saying that the Republican carbon-tax plan from February isn’t dead yet. “If, for instance, we have Democratic control of Congress [in 2020 or 2024], then that proposal could attract the moderate Republicans who are now keeping their heads down in the foxhole,” he told me. (Though that assumes that Democrats would reintroduce a Republican-invented plan.)
But even if the pipe dream of a bipartisan carbon tax has ended for now, the effort to bring Republicans over to the fold will never die. The Climate Solutions Caucus, a bipartisan group of House legislators, recognizes the existence of global warming and “explores policy options” to research it, slow it, and prepare for it. Membership must be kept even between Democrats and Republicans—meaning, in essence, that a Democrat can only join when they woo a Republican to hop in, too.
The caucus’s membership has swelled this year; there are now 31 House Republicans who belong to the group. Their votes alone could have nixed the GOP tax bill: Only 23 House Republicans would have needed to vote no to kill the bill (or at least force changes to it).
Carlos Curbelo, the cochair of the Climate Solutions Caucus and a Republican whose district includes the Florida Keys, seemed to exemplify the tensions faced by moderate, climate-concerned Republicans this week as he voted “yes” for the tax bill—right after calling President Trump’s refusal to recognize global warming as a national-security threat “unacceptable.”
Six Republicans in the Climate Solutions Caucus did vote against the tax bill; they made up half of all GOP House dissenters.
On Tuesday afternoon, I contacted all 31 Republicans in the Climate Solutions Caucus, asking them if they had threatened to withhold their support for the bill for its lack of a carbon tax. I also asked them whether they anticipated this Congress, led by Speaker Paul Ryan and Senate Majority Leader Mitch McConnell, ever passing climate legislation.
None of them got back to me.


Growing up, the writer Clay Bonnyman Evans had heard all sorts of stories about his mother’s father, First Lieutenant Alexander Bonnyman Jr. Evans’s grandfather had joined the Marines’ officer ranks during World War II, and in 1943, he learned he’d be taking part in an assault on the remote island of Betio (BAY-sho) in the South Pacific. The goal was to drive out Japanese defenders so the United States could use the island to launch attacks on other Japanese-occupied territories.
After Bonnyman and his troops stormed the beach at Betio, they dropped TNT where Japanese service members were hiding out, using flamethrowers to light the explosives. That flushed out more than a hundred Japanese defenders so that American forces could take control of the island, despite heavy losses. Bonnyman was felled by Japanese fire on November 22, 1943, and the military awarded him the Medal of Honor after his death. “By his dauntless fighting spirit,” the citation read, “Bonnyman had inspired his men to heroic effort, enabling them to beat off the counterattack.”
Though 56-year-old Evans, of Hilton Head Island, South Carolina, had never met his grandfather, it was his greatest wish to find Bonnyman’s remains and bring him home to the United States. No one seemed to know where he’d ended up, though. A family member said Bonnyman was buried in the National Memorial Cemetery of the Pacific in Hawaii, but that was never confirmed. Relatives eventually decided to have his tombstone inscribed with the words “Buried at Sea.”
Evans’s mission to find his grandfather got a major assist when he struck up a conversation with a man named Mark Noah. A commercial pilot with a consuming interest in lost U.S. service members from World War II, Noah had founded a private nonprofit organization called History Flight. He had made multiple trips to the South Pacific to unearth the bodies of lost Marines—one of a variety of efforts, both public and private, to recover missing service members around the world.
The U.S. government’s Defense POW/MIA Accounting Agency (DPAA) estimates that the remains of more than 80,000 U.S. service members are still undiscovered, and the total of undiscovered remains is many times that when fallen troops from other nations are included. Using a combination of traditional excavation methods and newer ones like ground-penetrating radar, History Flight and similar organizations around the world have found hundreds of service members once deemed unrecoverable. The Association for the Recovery of the Fallen in Eastern Europe, for example, has retrieved the remains of World War II service members in a variety of countries, including Germany, Poland, and Russia. With government programs sometimes falling short, privately funded recovery efforts and public-private ventures have come to play a central role in efforts to return missing service members to their home countries.
It was History Flight’s Noah who first supplied Evans with an important clue about his grandfather’s whereabouts. According to a World War II–era casualty card Noah unearthed from military archives, Bonnyman had been buried in a grave at Cemetery 27 on Betio—a site that had been given up for lost in the 1940s after U.S. personnel carried out a search for it and came up empty.
Knowing Noah’s sights were set on locating this grave site and others, in 2013 Evans decided to enlist as a volunteer on History Flight’s Betio project. “I went down for the first time to help dig—I was just a grunt. I’ll do anything and everything they want me to do.”
Evans returned to Betio multiple times over the years, sifting sand, cleaning skeletal remains, digging out burial trenches, and shooting photos and video. The more time he spent there, the more committed he became to Noah’s mission, even though the excavation team hadn’t yet found his grandfather. “Mark and I were in very close touch,” Evans recalls. “We always talked about Cemetery 27 and that finding my grandfather might be a needle in a haystack.”
In March 2015, Evans got a call from Noah. He said History Flight’s team might be zeroing in on Cemetery 27. Would Evans like to join the excavation and document the recovery? Noah kept Evans updated on the team’s progress, and it wasn’t long before Evans went ahead and booked himself a flight.
The island of Betio—a side jewel in the necklace-shaped Tarawa Atoll—has an area of less than one square mile, but it played an outsize role in 20th-century history. For centuries, Betio, part of the Republic of Kiribati, has been home to a small population of seafaring Pacific Islanders who fished to feed their families. Their descendants, more than 16,000 of them, still inhabit the island today and speak the traditional Kiribati language.
In late 1943, the U.S. command decided it needed to capture the nearby Marshall Islands from the Japanese in order to secure enough territory to launch air attacks from a land base. But to claim the Marshalls, the United States would have to capture a series of other islands first, including the Tarawa Atoll.
The United States assembled a massive invasion force comprised of a dozen battleships, 17 aircraft carriers, more than 60 destroyer ships, and over 18,000 Marines to take Tarawa and nearby islands. The frontline Marines who landed on Betio, Tarawa’s largest island, saw a tropical beach that was swiftly transforming into a hellscape. Battle strategists had failed to account for the low tide, so Japanese gunners on high ground were able to pick off many men as they struggled up the exposed beach. Others fell later on in fierce hand-to-hand combat. Altogether, more than 1,000 U.S. troops died in the 76-hour battle, and afterward, the waves around Betio were tinted red with blood.
These men’s sacrifices were not in vain—U.S. forces defeated the Japanese at Tarawa, setting up a string of other crucial wins in the Pacific. But in the heat of war, many service members killed in action had to be hastily buried on Betio. A body-recovery sweep in 1946 turned up the remains of 436 Marines, fewer than half the battle’s total casualties, and in 1949, the Office of the Quartermaster General designated the rest of Tarawa’s missing “unrecoverable.” That meant hundreds of families thought they’d never be able to bury their loved ones. Families were told, variously, that service members had been buried at sea or in unmarked graves.
That lack of closure was what got to Noah. As a commercial pilot and aviation-history expert, he had lent his knowledge to missing-persons recovery cases involving planes. Seeing the grief families felt for their missing relatives, who were often service members, Noah resolved to use his flying skills to aid the recovery cause. “We started doing airplane rides and using [them] to fund the search for the missing,” says Noah, who founded History Flight in 2003. “While we started doing that, it became very apparent there was a tremendous need for this kind of work.”
Though Noah eventually stopped selling plane rides, his recovery mission continued, taking on momentum as donations poured in. Since 2006, History Flight has sent more than 100 recovery teams to World War II battle sites around the world, including Allmuthen, Belgium; Buschhoven, Germany; and Mili Atoll in the Marshall Islands. The organization has raised more than $6.5 million, much of it privately donated, to fund these searches, and Noah himself logs 40 to 60 hours of searching each week on a volunteer basis. In 2015, History Flight entered a public-private partnership with the government-run DPAA, which helps identify the service members History Flight finds.
In Tarawa, Noah recognized a peerless recovery opportunity. World War II–era military records showed that hundreds of service members were likely still buried on Betio. That meant a well-targeted long-term search effort could grant peace and closure to thousands of relatives. “The reuniting of the person’s remains with their family is extremely important,” Noah says. “Instead of being buried underneath a garbage dump or a parking lot, they can be buried in their home country in a cemetery.”
As Noah ramped up his Tarawa excavation plan, he got in touch with a number of archaeologists, including Agamemnon Gus Pantel, a Puerto Rico–based archaeologist who’d worked on other History Flight projects in Europe. Pantel had experience using ground-penetrating radar to identify burial locations, an invaluable asset to any team searching for lost service members. At Noah’s urging, Pantel joined the Tarawa team as a volunteer in 2012. He was moved by the chance to help bring service members home with the honor they deserved. “When you deal with MIAs, it’s very real, and these are almost contemporaries,” he says. “Most of these Marines we find are basically just kids—people who gave up their lives and didn’t really have a chance to enjoy life.”
From the start, the team—which also included History Flight forensic archaeologist Kristen Baker—knew they were in for a tough slog, since others had explored Betio in the past and come up empty. But because History Flight’s team came armed with ground-penetrating radar, they had an advantage. This type of radar uses microwaves to map different kinds of material below the soil surface. Unusual objects in the ground send different patterns of energy waves back to the surface than the surrounding soil, and the radar’s antenna picks up and records these differences. With this information—considered alongside things like historical records and the findings of cadaver-dog searches—researchers can assemble a rough map of where bodies may be located, which allows them to focus on excavation sites where they are most likely to have success.
Beginning in 2008, teams of History Flight researchers had gone to Betio periodically to conduct radar scans of the island. On some of these trips, they brought along surveyors and historians to help them get the lay of the land, as well as a cadaver dog named Buster, who had been trained to sniff out human remains. As Noah and his team compared the evidence they’d gathered to World War II–era burial-location notes and maps, they grew fairly sure they’d found Cemetery 27. According to U.S. military records, at least 40 Marines—including Bonnyman—had been buried in this cemetery after the Battle of Tarawa, but no one had been able to find it since.
In early 2015, with maps and radar results in hand, Pantel, Baker, Noah, and the rest of the History Flight team returned to Betio to uncover Cemetery 27. The team sweated for long hours under the scorching tropical sun to dig in the most promising spots. The work was even more brutally difficult than usual, since thick concrete pads—the remains of wartime installations—covered much of the cemetery. To dismantle the pads, the team had to unleash an arsenal of heavy equipment, including concrete saws, jackhammers, and a backhoe. “Our people work 14 to 16 hours a day,” Noah says. “We went through about 50 to 70 centimeters of concrete, and about 50 centimeters of crushed coral rock below.”
But chewing through the rock-hard barriers proved worthwhile. As the concrete and coral fragments gave way to finer grains and rich dirt, Pantel knew just how close they were getting. “You had a very clear whitish coral sand, and then all of a sudden you had this dark soil literally defining the burial trench. It just jumped out at you,” he says.
Soon after that, the team started turning up telltale dog tags, long bones, and even uniforms. One by one, Marines killed at Betio emerged from the soil, many of them almost perfectly preserved. As difficult as it had been to demolish, the thick concrete had kept Cemetery 27’s occupants largely intact. To figure out the identity of each man, the team compared still-intact sets of teeth to the wartime dental records of known service members. Other clues like identification tags and inscribed cigarette lighters also helped. Ultimately, the History Flight team retrieved the bodies of at least 35 Marines from the cemetery’s trenches—the largest war-related recovery ever completed at a single site.
Evans, his video-recorder light on, was there to document the whole thing. Based on wartime burial records, the History Flight team had determined that Bonnyman was most likely in grave number 17. Baker, the recovery team’s leader, was very confident Bonnyman would be found exactly where the record stated. As they inched closer to the grave, Evans’s nerves threatened to get the best of him. “I had a lot of time to really think about it: ‘Wow, I can’t even believe we’re coming this close.’”
The first indication that the crew had found Bonnyman was a glint of precious metal. “My grandfather had this extremely distinctive dental work,” Evans says. “Kristen saw the gold on this tooth—a kind of unusual-looking filling. She just said, ‘It’s gold.’” As it dawned on Evans that he was actually seeing Bonnyman face-to-face, he dropped his camera, completely overcome. “I had to try to regroup.” (Evans’s book about his search, Bones of My Grandfather: Reclaiming a Lost Hero of WWII, is slated to come out in July 2018.)
After History Flight makes preliminary identifications like these, the U.S. Department of Defense (DOD) and the DPAA step in for the next phase: DNA testing to confirm each service member’s identity. Most often, the DOD contacts a known living family member to request a DNA sample. If there are maternal relatives, technicians extract the sample’s mitochondrial DNA—long-lasting genetic codes that are passed down from mother to child—and read the genetic codes using a machine called a sequencer. When a relative’s mitochondrial DNA sample matches that of a recovered service member, the DOD can state with confidence that the person belongs to a particular family line. (If necessary, other types of DNA testing, like autosomal DNA or Y-chromosome DNA analysis, can also be used to confirm service members’ identities.)
Even as the identities of the Cemetery 27 service members were being confirmed, History Flight’s excavation team was forging ahead in its search for the hundreds of Marines still missing. In the two years since the Cemetery 27 find went public, the team has found dozens of other Marines in various locations on Betio, including in garbage pits and pigsties. This year, project researchers are wrapping up their second-largest find—another cemetery, where 24 service members have so far been uncovered. Wartime documents mention other nearby burial sites that have not yet been excavated, so Pantel predicts other recoveries in the near future.
Still, as they search for the remaining service members on Betio, the researchers face ongoing logistical challenges. The Tarawa Atoll has a population density rivaling Hong Kong’s, and Betio’s landscape looks much different than it did 74 years ago. Wartime bunkers have been turned into latrines and animal pens, and residents frequently dig large pits to dispose of trash, which can disturb remaining burial sites. The research team is in a race against time to recover what they still can—especially since climate change threatens to bring on rising waters that could one day overtake the island.
To secure access to interred remains, the team often engages in delicate negotiations with residents whose thatched-roof homes line Betio’s shores. The locals have a deep appreciation for the island’s history and are often willing to assist researchers. “We always go into the communities—we meet with the mayor, the police department,” Pantel says. “We don’t want to come in invading their privacy or their cultural tranquility.” If the team needs to remove a row of plants or dismantle a fence to uncover a burial site, they either restore the site themselves or compensate the homeowners. History Flight says it’s also working with Tarawa’s cultural-heritage department to put up displays about the significance of bunkers, gun emplacements, and other battle relics that remain.
Beyond adding to the historical record, what matters most to the History Flight team is granting families the peace they have sought for generations. “Closure is more than just putting someone to rest. The unknown is very disconcerting,” Pantel says. “You have comrades, nephews, nieces, grandchildren who still don’t know what happened to their relatives. These are very real pains that families still feel.”
Family members say their loved ones’ homecoming has helped ease some of that pain. Betsy Gore Bond of Pembroke, Kentucky, found out through a surprise phone call that History Flight had located her missing uncle, Private First Class Ben Hadden Gore, in Cemetery 27. Her uncle’s return and burial, she says, brought family members closer together and strengthened their sense of their own history. “It’s like something you never even imagined happening to you.”
Evans, too, says the impact of a physical reunion can’t be overstated. In the hours after Bonnyman’s body was first uncovered on Betio, Evans was mostly able to keep it together—he didn’t feel comfortable letting his emotions out with so many people around. It wasn’t until the day after the recovery, as the sun rose over the quiet excavation grounds, that he let his guard down completely. “I went back over there by myself, and I sat down. It was between me and my grandfather, and that’s when I decided to have my moment in private.”
This post appears courtesy of Sapiens.


Although it’s impossible to say for sure, Trofim Lysenko probably killed more human beings than any individual scientist in history. Other dubious scientific achievements have cut thousands upon thousands of lives short: dynamite, poison gas, atomic bombs. But Lysenko, a Soviet biologist, condemned perhaps millions of people to starvation through bogus agricultural research—and did so without hesitation. Only guns and gunpowder, the collective product of many researchers over several centuries, can match such carnage.
Having grown up desperately poor at the turn of the 20th century, Lysenko believed wholeheartedly in the promise of the communist revolution. So when the doctrines of science and the doctrines of communism clashed, he always chose the latter—confident that biology would conform to ideology in the end. It never did. But in a twisted way, that commitment to ideology has helped salvage Lysenko’s reputation today. Because of his hostility toward the West, and his mistrust of Western science, he’s currently enjoying a revival in his homeland, where anti-American sentiment runs strong.
Lysenko vaulted to the top of the Soviet scientific heap with unusual speed. Born into a family of peasant farmers in 1898, he was illiterate until age 13, according to a recent article on his revival in Current Biology. He nevertheless took advantage of the Russian Revolution and won admission to several agricultural schools, where he began experimenting with new methods of growing peas during the long, hard Soviet winter, among other projects. Although he ran poorly designed experiments and probably faked some of his results, the research won him praise from a state-run newspaper in 1927. His hardscrabble background—people called him the “barefoot scientist”—also made him popular within the Communist party, which glorified peasants.
Officials eventually put Lysenko in charge of Soviet agriculture in the 1930s. The only problem was, he had batty scientific ideas. In particular, he loathed genetics. Although a young field, genetics advanced rapidly in the 1910s and 1920s; the first Nobel Prize for work in genetics was awarded in 1933. And especially in that era, genetics emphasized fixed traits: Plants and animals have stable characteristics, encoded as genes, which they pass down to their children. Although nominally a biologist, Lysenko considered such ideas reactionary and evil, since he saw them as reinforcing the status quo and denying all capacity for change. (He in fact denied that genes existed.)
Instead, as the journalist Jasper Becker has described in the book Hungry Ghosts, Lysenko promoted the Marxist idea that the environment alone shapes plants and animals. Put them in the proper setting and expose them to the right stimuli, he declared, and you can remake them to an almost infinite degree.
To this end, Lysenko began to “educate” Soviet crops to sprout at different times of year by soaking them in freezing water, among other practices. He then claimed that future generations of crops would remember these environmental cues and, even without being treated themselves, would inherit the beneficial traits. According to traditional genetics, this is impossible: It’s akin to cutting the tail off a cat and expecting her to give birth to tailless kittens. Lysenko, undeterred, was soon bragging about growing orange trees in Siberia, according to Hungry Ghosts. He also promised to boost crop yields nationwide and convert the empty Russian interior into vast farms.
Such claims were exactly what Soviet leaders wanted to hear. In the late 1920s and early 1930s Joseph Stalin—with Lysenko’s backing—had instituted a catastrophic scheme to “modernize” Soviet agriculture, forcing millions of people to join collective, state-run farms. Widespread crop failure and famine resulted. Stalin refused to change course, however, and ordered Lysenko to remedy the disaster with methods based on his radical new ideas. Lysenko forced farmers to plant seeds very close together, for instance, since according to his “law of the life of species,” plants from the same “class” never compete with one another. He also forbade all use of fertilizers and pesticides.
Wheat, rye, potatoes, beets—most everything grown according to Lysenko’s methods died or rotted, says Hungry Ghosts. Stalin still deserves the bulk of the blame for the famines, which killed at least 7 million people, but Lysenko’s practices prolonged and exacerbated the food shortages. (Deaths from the famines peaked around 1932 to 1933, but four years later, after a 163-fold increase in farmland cultivated using Lysenko’s methods, food production was actually lower than before.) The Soviet Union’s allies suffered under Lysenkoism, too. Communist China adopted his methods in the late 1950s and endured even bigger famines. Peasants were reduced to eating tree bark and bird droppings and the occasional family member. At least 30 million died of starvation.
Because he enjoyed Stalin’s support, Lysenko’s failures did nothing to diminish his power within the Soviet Union. His portrait hung in scientific institutes across the land, and every time he gave a speech, a brass band would play and a chorus would sing a song written in his honor.
Outside the U.S.S.R., people sang a different tune: one of unwavering criticism. A British biologist, for instance, lamented that Lysenko was “completely ignorant of the elementary principles of genetics and plant physiology ... To talk to Lysenko was like trying to explain differential calculus to a man who did not know his 12-times table.” Criticism from foreigners did not sit well with Lysenko, who loathed Western “bourgeois” scientists and denounced them as tools of imperialist oppressors. He especially detested the American-born practice of studying fruit flies, the workhorse of modern genetics. He called such geneticists “fly lovers and people haters.”
Unable to silence Western critics, Lysenko still tried to eliminate all dissent within the Soviet Union. Scientists who refused to renounce genetics found themselves at the mercy of the secret police. The lucky ones simply got dismissed from their posts and were left destitute. Hundreds if not thousands of others were rounded up and dumped into prisons or psychiatric hospitals. Several got sentenced to death as enemies of the state or, fittingly, starved in their jail cells (most notably the botanist Nikolai Vavilov). Before the 1930s, the Soviet Union had arguably the best genetics community in the world. Lysenko gutted it, and by some accounts set Russian biology back a half-century.
Lysenko’s grip on power began to weaken after Stalin died in 1953. By 1964, he’d been deposed as the dictator of Soviet biology, and he died in 1976 without regaining any influence. His portrait did continue to hang in some institutes through the Gorbachev years, but by the 1990s, the country had finally put the horror and shame of Lysenkoism behind it.
Until recently. As the new Current Biology article explains, Lysenko has enjoyed a renaissance in Russia over the past few years. Several books and papers praising his legacy have appeared, bolstered by what the article calls “a quirky coalition of Russian right-wingers, Stalinists, a few qualified scientists, and even the Orthodox Church.”
There are several reasons for this renewal. For one, the hot new field of epigenetics has made Lysenko-like ideas fashionable. Most living things have thousands of genes, but not all those genes are active at once. Some get turned on or off inside cells, or have their volumes turned up or down. The study of these changes in “gene expression” is called epigenetics. And it just so happens that environmental cues are often what turn genes on or off. In certain cases, these environmentally driven changes can even pass from parent to child—just like Lysenko claimed.
But even a cursory look at his work reveals that he didn’t predict or anticipate epigenetics in any important way. Whereas Lysenko claimed that genes didn’t exist, epigenetics take genes as a given: They’re the things being turned on or off. And while epigenetic changes can occasionally (and only occasionally) pass from parent to child, the changes always disappear after a few generations; they’re never permanent, which contradicts everything Lysenko said.
Epigenetics alone, then, can’t explain Lysenko’s revival. There’s something more going on here: a mistrust of science itself. As the Current Biology article explains, Lysenko’s new defenders “accuse the science of genetics of serving the interests of American imperialism and acting against the interests of Russia.” Science, after all, is a major component of Western culture. And because the barefoot peasant Lysenko stood up to Western science, the reasoning seems to go, he must be a true Russian hero. Indeed, nostalgia for the Soviet era and its anti-Western strongmen is common in Russia today. A 2017 poll found that 47 percent of Russians approved of Joseph Stalin’s character and “managerial skills.” And riding on the coattails of Stalin’s popularity are several of his lackeys, including Lysenko.
On the one hand, this rehabilitation is shocking. Genetics almost certainly won’t be banned in Russia again, and the rehabilitation effort remains a fringe movement overall. But fringe ideas can have dangerous consequences. This one distorts Russian history and glosses over the incredible damage Lysenko did in abusing his power to silence and kill colleagues—to say nothing of all the innocent people who starved because of his doctrines. The fact that even some “qualified scientists” are lionizing Lysenko shows just how pervasive anti-Western sentiment is in some circles: Even science is perverted to promote ideology.
On the other hand, there’s something depressingly familiar about the Lysenko affair, since ideology perverts science in the Western world as well. Nearly 40 percent of Americans believe that God created human beings in their present form, sans evolution; nearly 60 percent of Republicans attribute global temperature changes to nonhuman causes. And while there’s no real moral comparison between them, it’s hard not to hear echoes of Lysenko in Sarah Palin’s mocking of fruit-fly research in 2008. Lest liberals get too smug, several largely left-wing causes—GMO hysteria, the “blank slate” theory of human nature—sound an awful lot like Lysenko redux.
Like the Soviet Union itself, the “science” of Trofim Lysenko has been consigned to the dustbin of history. Yet the dangers of Lysenkoism—of subsuming biology to ideology—continue to lurk.



Today, you can find a huge variety of breads on supermarket shelves, only a few of which are called “sourdough.” For most of human history, though, any bread that wasn’t flat was sourdough—that is, it was leavened with a wild community of microbes. And yet we know surprisingly little about the microbes responsible for raising sourdough bread, not to mention making it more nutritious and delicious than bread made with commercial yeast. For starters, where do the fungi and bacteria in a sourdough starter come from? Are they in the water or the flour? Do they come from the baker’s hands? Or perhaps they’re just floating around in the foggy air, as the bakers of San Francisco firmly believe? This episode, Cynthia and Nicky go to Belgium with two researchers, 15 bakers, and quite a few microbes for a three-day science experiment designed to answer this question once and for all. Listen in for our exclusive scoop on the secrets of sourdough.
No one knows how or when humans first figured out that if you mixed mashed-up grains and water and let them sit for a while, you got a bubbling beige goo that you could use to make beer or raise bread. (No one even knows whether humans went for the booze or the bread first, although Nicky is firmly on Team Booze.) Historians and archaeologists speculate that the first bread would have been fluffy and spongy, something like the injera that is still eaten in Ethiopia today. Despite the microbes, it would likely have still been quite flat, because the fermented ground-grain-and-water mix would have been cooked simply by pouring it on a rock—there’s evidence that humans were grinding sorghum, an African grain, long before pottery was invented in which to bake the shaped dough into a loaf.
But once Louis Pasteur explained yeast’s role in fermentation using his new compound microscope in the 1850s, sourdough’s popularity slumped. Sourdough was finicky, unreliable, and slow; commercial baker’s yeast, which was first sold by the Fleischmann brothers just 11 years after Pasteur first published his discoveries, worked well enough, and it provided the same rapid results every time. By the 1960s, sourdough had all but been forgotten. In this episode, we trace its revival, but also reveal how science, which initially gave us boring baker's yeast, is now uncovering the secrets of sourdough.
Microbiologists Rob Dunn and Anne Madden allowed us to accompany them to Belgium for their groundbreaking experiment to discover where the microbes in a sourdough culture come from. While there, we visit the world’s first and only Sourdough Library, learn how the microbes in sourdough improve the texture, flavor, and nutritional quality of bread, and eat our body weight in baked goods. And then, the results! Why do bakers have “sourdough paws”? What is so special about Australian starters? And where do all those microbes actually come from? All that and more this episode: Listen in now!
This post appears courtesy of Gastropod.


To telescopes, ‘Oumuamua, the interstellar asteroid that made itself known to Earth in October, looks like a point of light in the dark, much like a star in the night sky—a perhaps underwhelming picture of a significant discovery.
But for astronomers, the tiny speck—the sunlight reflected by the asteroid—can reveal a trove of information. They can break down the light from an object into a spectrum of individual wavelengths, from which they can infer the object’s shape, chemical composition, and other properties. As astronomers like to say, if a picture is worth a thousand words, then a spectrum is worth a thousand pictures.
The astronomy community has spent weeks sorting through these pictures of ‘Oumuamua, captured by telescopes around the world as the asteroid sped away from the sun and faded from view. The earliest analysis of the light from ‘Oumuamua, conducted by its discoverers in Hawaii, revealed a strange, fast-spinning, cigar-like object unlike anything they’ve ever seen. The latest analyses continue to produce tantalizing results, further challenging long-standing predictions for the first visitor to our solar system.
‘Oumuamua has a thick crust of carbon-rich material, hardened by years of exposure to cosmic radiation in interstellar space, that could be protecting an icy interior, according to a new analysis in Nature Astronomy of the object in visible and near-infrared wavelengths. The coating could explain why ‘Oumuamua shows no signs of being a comet, the kind of object scientists long expected would coast into our solar system.
When the asteroid was first spotted by the Pan-STARRS telescope in Hawaii, ‘Oumuamua was already speeding away from the sun and quickly fading from view. Scientists had only about two weeks to deploy telescopes to soak up the reflected light before the interstellar visitor got too far away. They looked for a coma, a stream of evaporated particles that trails comets as they pass near the sun and their icy contents become heated. ‘Oumuamua had made a fairly close pass to the sun—about a quarter of the distance between the sun and Earth—and telescopes were prepared to spot as little as a sugar cube’s worth of material flying off the object every second.
“We had data that matched pretty closely with what we’d expect for a body out there,” said Alan Fitzsimmons, an astronomer at Queen’s University Belfast who led the new analysis. “And yet we saw no sign of ice being heated and ejected into space.”
‘Oumuamua didn’t show signatures of ice or minerals found in rock, which means it’s neither icy nor rocky, at least not exactly. But it did show signs of carbon compounds. Fitzsimmons said previous studies have revealed that when carbon-rich, comet-like objects are exposed to the radiation that would be found in interstellar space, the material forms a crust that acts as insulation. If ‘Oumuamua has ice, as a comet would, it may be hiding beneath a mantle half a meter thick, formed after hundreds of millions—perhaps even billions—of years of bombardment by high-energy particles.
Fitzsimmons and his colleagues say ‘Oumuamua’s crust may have been able to prevent heat from the sun from penetrating the surface and vaporizing ice particles. According to their thermal models, any ice buried 30 centimeters (12 inches) deep would have remained intact even as the surface of the asteroid reached temperatures of about 600 degrees Kelvin (620 degrees Fahrenheit) during its pass of the sun.
“Comets are absolutely excellent insulators,” said Karen Meech, one of ‘Oumuamua’s discoverers at the University of Hawaii, who was not involved in this new analysis.“They’re very fluffy and porous, like a down jacket.”
Scientists will likely never know for sure what lies inside ‘Oumuamua. When astronomers study the light reflected from the asteroid, they’re only examining the top few microns of its surface, a width smaller than that of a human red blood cell. “We can infer only so much about these little dots of light,” said Michele Bannister, an astronomer at Queen’s University Belfast and a coauthor on the new study.
Astronomers also have a finite amount of data to work with. Most of their ground-based telescope observations were carried out in the two weeks after ‘Oumuamua was discovered, when it was still visible enough to get good measurements. The majority of research on ‘Oumuamua—the attempts to explain its weird shape, its nonstop spinning, its unexpected hardiness—will come out of this data set.
The rest may come from yet-to-be published observations by space telescopes like the Hubble, which has tracked ‘Oumuamua to help astronomers better understand its trajectory and where it came from. The asteroid is now about twice the distance between the Earth and the sun from our planet. Even to Hubble, the producer of countless, radiant images of distant stars and galaxies, ‘Oumuamua, a small object about 400 meters long, will look like a small point of light.
“It could be icy inside, and we’ll never know,” Meech said.


Forty years ago, the great tropical ecologist Dan Janzen noticed something funny about the plants in Costa Rica. Many species invested a lot of their energy in producing huge fruits with tough seeds and seed pods, which no animals seemed to eat. With nothing to consume them, the seeds and pods just fell to the jungle floor and rotted, or died in the shade of their parent trees.
In a now legendary paper cowritten with Paul S. Martin (“Neotropical Anachronisms: The Fruits the Gomphotheres Ate”), Janzen speculated that there was a good reason for this: The jungle plants’ original partners had all gone extinct. They identified a long list of plants, such as the jicaro and guanacaste in South America and honey locust, pawpaw, persimmon, and Osage orange in North America, which seem to have lost their original dispersal agents. These species had evolved over millions of years to have their seeds eaten and spread around by ground sloths, glyptodonts, gomphotheres, (a family of mastodon-like creatures from South America), extinct horses, and other vanished megafauna. The great extinctions at the end of the Pleistocene left these plants as orphans.
They’re not alone. In many ways, we all live in an orphaned world. Extinct megafauna like the mammoth and ground sloth weren’t just hapless prey or passive victims of climate change. They were engineers, actively shaping their environment to suit their needs. In doing so, they created evolutionary niches in which plants and predators could thrive. It’s these patterns of coevolution and mutual dependence that create a functioning ecosystem.
Since the Quaternary extinction event in which the world lost some 50 percent of its large mammal species, many crucial links in the food chain have gone missing. Figuring out exactly what this has done to the world we’re living in now is something paleoecologists have been trying to figure out for a good half-century. Like anything to do with ecology, the problem of how missing megafauna affect modern-day habitats is a complex puzzle, with multiple lines of evidence. First you have to figure out who ate what and where, in what season and in what quantity. And if you want to do that, the best place to look is ancient poop.
In the past few years, a group of researchers in New Zealand led by the paleoecologist Jamie Wood have succeeded in using ancient droppings to reconstruct the world of the giant moa, one of a group of large flightless birds that includes the ostrich, emu, cassowary, and Madagascar’s extinct elephant birds. From the fossil record, we know there were at least nine species of moa on New Zealand. The smallest, the little bush moa, stood a little over four feet tall. At 12 feet with its neck outstretched, the largest, the giant moa, may have been the tallest bird that ever lived. In between these two extremes, moas came in a range of sizes and forms, adapted for a range of habitats.
Before the arrival of human beings, New Zealand was a paradise of birds. When it started drifting away from its parent supercontinent of Gondwana some 85 million years ago, it only carried with it a few primitive mammals. In time, they went extinct, as did the dinosaurs. With 2,000 miles between it and the next nearest landmass, the only group that could repopulate New Zealand were the birds. Over millions of years, they evolved to occupy most of the available ecological niches (they were joined later by bats, who took the place mice occupy in most terrestrial ecosystems). Birds functioned as the only major predators and herbivores. They were New Zealand’s antelopes, cheetahs, and giraffes.
New Zealand’s plants evolved in concert with its birds. An unusual number of the islands’ bushes and trees have what’s called a divaricating pattern of growth. The branches of these plants grow at wildly offset angles, creating an impenetrable mesh of interwoven twigs. Growing this way costs a lot of energy—the plants lose precious sunlight by creating their own shade—but it makes for an effective defense against a large, toothless herbivore like the moa. But now, with the moa gone and replaced by mammalian herbivores (mostly sheep), they find themselves defenseless.
New Zealand is a perfect place to study the effects of megafauna on their landscape. For one, the megafauna stayed around until very recently. Giant moas were happily foraging for tree-fern buds while the Magna Carta was being signed and the Florentines were building Brunelleschi’s dome. Two, it was quite varied: Nine species of moa coexisted on the islands, each (presumably) with its own habits and ecological niche. And three, all those moas left a lot of poop. People have been finding it in caves since the 1870s. Natural-history museums have over 2000 specimens on file, from 30 localities, with more waiting to be discovered.
Woods’s lab used some of this abundant resource to settle a few mysteries about New Zealand’s lost ecosystems. Using a combination of ancient DNA, plant macrofossils, and pollen, his team reconstructed the diet of four moa species. They found that the heavy-footed moa preferred to graze in open fields and grasslands. The bush moa preferred to munch on forest understory. The giant moa was more of a generalist, moving between these two habitats at will.
Figuring out moa diet is only the tip of the dungheap when it comes to ancient poop studies. Prehistoric dung has a wealth of uses for science, from tracking the demise of the mammoth to deciphering the peopling of the Americas. The spores of Sporormiella, a type of mold that loves nothing better than a nice pat of dung to grow over, have been used as a proxy to track the abundance of megafauna across the millennia. Carbon dating DNA-fingerprinted coprolites from the Paisley Caves in Oregon helped prove the presence of pre-Clovis humans (and, as a bonus, testing feces for DNA doesn’t raise the same ethical quandaries as testing ancient skeletal remains).
The climate history of the American Southwest was established in large part thanks to a deposit of sloth dung discovered in Arizona in the 1950s. The dung was left by the Shasta ground sloth, a small species as ground sloths go, more bearlike than the more famous Megatherium, which grew to the size of an elephant. For around 30,000 years, these sloths used Rampart Cave, a hollow in the side of the Grand Canyon, as their latrine. Pollen in the accumulated droppings recorded the shifts in vegetation that accompanied the arrival and departure of past glacial maxima. Crucially, they proved that the shift in vegetation and temperature that came with the end of the last Ice Age wasn’t particularly new or extreme. It was something that had happened multiple times in previous millennia.
Tragically, the precious dung deposit caught fire in 1976. The National Park Service spent tens of thousands of dollars to save it, but to no avail. It made the nightly news. Walter Cronkite joked about “endangered feces,” but to Paul Martin, a geoscientist who devoted much of his career to reconstructing the environment of the ancient Southwest (and came up with the overkill hypothesis in the process) compared it to losing the Library of Alexandria.
Martin felt better a few years later, when two zoologists discovered an unusually big sphere of chewed-up grass in a cave in southern Utah. Subsequent visits turned up several more dung balls of surprising size, which radiocarbon dating showed to be about 12,000 years old. Martin guessed they came from America’s second-largest extinct mammal, the Columbian mammoth.
Larger than any living elephant, the Columbian mammoth, was, like them, a prodigious maker of dung. The Utah cave, named Bechan, from the Navajo word for “big shit,” showed just how prodigious. Excavations in the mid-’80s revealed a layer of dung 16 inches thick covering a surface of several tennis courts (that’s 14,000 cubic feet of dung total). This July, a team at McMaster University in Ontario reported that it had successfully sequenced DNA from the dung boluses, proving that they really did come from mammoths.
The most spectacular dung find of recent years comes from the Page-Ladson site on the Aucilla River in the Florida Panhandle. The site is a sinkhole in the middle of the Aucilla River, on one of the few stretches where it flows above ground, but it used to be a spring-fed pond. Fourteen thousand years ago, when Florida was much cooler and drier than it is today (and also much larger, thanks to lower sea levels), various animals would come to this pond to drink. That made it a great spot for visiting predators, including humans. Underwater excavations at Page-Ladson turned up clear signs of human activity, including mastodon butchery, making it the oldest confirmed habitation in the American Southeast.
But the Aucilla mastodons weren’t just sitting ducks for human hunters. Mostly, they used the pool as a wallow, the way elephants use drinking holes in Africa today. In the process, they left behind a heck of a lot of dung. At some point, the water level rose, burying the wallow in sediment, and preserving this priceless fecal Pompeii for posterity.
Archaeologists have found hundreds of cubic meters of mastodon droppings in this ancient latrine. Since the early 1990s, they’ve been patiently sifting through the only partially digested remains. From it, they’ve been able to piece together a picture of the mastodon in a vanished landscape. The main thing they found was cypress twigs—the mastodons ate vast quantities of cypress, particularly young branches, and particularly in autumn. They also found traces of dozens of other plant species, including some of Janzen’s orphans, such as honey locust, persimmon, Osage orange, and wild gourd. The wild gourd is especially interesting. These days, wild gourds are very rare in the wild. Their extremely bitter rind keeps most living herbivores from snacking on their fruit. It took thousands of years of patient cultivation by Native Americans to turn them into today’s pumpkin and squash.
So next time you have pumpkin pie at Thanksgiving, spare a thought for the wild Cucurbita, the mastodon, which long ago spread its seeds, and all the other ghosts that live on in our orphaned land. And once the meal is over, tell your family about all the amazing things you could learn by stepping back in time to stand in front of a mastodon’s butt.


Updated on December 18 at 3:51 p.m. ET
A few years ago, Michelle Hebl attended the latest in a series of talks hosted by her department at Rice Univeristy. The speaker was a man, and Hebl realized that she hadn’t heard any female speakers in that series for a while. “Maybe I’m just not thinking about them,” she thought. “Or maybe it’s something we should look at.”*
Colloquium talks, where academics are invited to discuss their research, give speakers a chance to publicize their work, build collaborations with new colleagues, and boost their reputations. The talks can lead to promotions or job offers. They are big opportunities. But as Hebl’s student Christine Nittrouer eventually found, they are opportunities that are predominantly extended to men.
Nittrouer and her team scanned the websites of the top 50 U.S. universities, as ranked by U.S. News, to build a database of every colloquium speaker from six departments: biology, bioengineering, political science, history, psychology, and sociology. They chose those six to represent a breadth of disciplines, and to exclude departments with either a very low or very high proportion of women. And they found that men gave more than twice as many talks as women: 69 percent versus 31 percent.
That result should not be too surprising. Several studies have shown that men outnumber women among the speakers of several scientific conferences. There’s even a site that collates examples of all-male panels.
Why does this happen? Hebl accounted for several of what she calls “yeah-but explanations,” which underplay these figures as the result of anything other than discriminatory biases. For example, some might argue that men outnumber women in many fields, and so any equitable selection process would naturally lead to more male speakers. But the team estimated the full pool of available speakers by counting every professor in their six chosen fields at each of the top 100 U.S. universities. And even after adjusting for the relative numbers of men and women in the various fields or ranks, they found that men are still 20 percent more likely to be invited to give colloquium talks than women.
Skeptics might also argue that the problem is a generational one: Science, for instance, has historically been skewed toward men, and when colloquia committees decide whom to invite, they’re prisoners of that history. But if that were true, and the arc of academia was slowly bending toward equality, then when assistant and associate professors—who are younger and more junior than full professors—are selected to give talks, the gender difference should be narrower. Hebl’s team found no such trend. “The people in whom we should see more parity aren’t showing us more parity,” she says.
“People sometime say: You know what? Maybe it’s the women,” says Hebl. “Maybe they don’t want to give talks, or they’re declining because they’re staying home with their kids.” That’s not what she found when she surveyed 186 professors who didn’t give colloquium talk at prestigious universities, but were in the same departments as those who did. Their answers clearly showed that women don’t decline colloquium invitations more than men, that they feel just as strongly that these talks are important for their careers, and that they’re no more likely to decline such talks because of family obligations.
“This dispels the widely held myth that women are less frequent speakers because they travel less,” says Jo Handelsman, from the University of Wisconsin at Madison. “Clearly, we need to test such assumptions before we absolve ourselves of culpability in creating biased slates.”
“Despite their presence in departments, women are not being asked to contribute to the intellectual development of their fields in the most coveted ways,” says Robin Nelson, from Santa Clara University, who has studied the prevalence of harassment in science. “This gendered discrimination minimizes women’s visible contributions to their fields, validating the idea that the greatest intellectual contributions are made by a few brilliant men.”
“We can account for all the yeah-buts,” Hebl says, “but we still have this bias, and we need to do something about it.”
One solution is to give women more power over inviting colloquium speakers. The team found that when those committees are chaired by women, half of the invited speakers are women; that’s compared to just 30 percent when the committees are chaired by men. “I’m not sure if these are explicit bias, where male chairs are saying we don’t want women,” Hebl says. “It’s more about the people who they think about, who are in their networks. And maybe women just know other women in the field.”
But in male-dominated fields, “if we take the few women we have and we put them on all the committees, we’re overwhelming them with experiences that aren’t necessarily helpful to their own progress,” Hebl says. Ultimately, “the burden falls on male allies. We need to train men to be aware of these biases. And we should put women on the important committees—the ones that decides who’s going to be our new board member, not the one that decides where we have our holiday party.”
Yael Niv, from Princeton, who created a site to monitor gender biases at neuroscience conferences, says that her department solved the problem of all-male colloquia by requiring faculty to nominate a certain base rate of women as possible speakers. “In this way, we encourage everyone to think a bit deeper about their nominations, and cast a slightly wider net,” she says. “The results have been colloquia with equal numbers of men and women in recent years, and a wider, more interesting array of scientific ideas that we’re exposed to.” And even if committee members struggle to think of female speakers, several scientists have now compiled lists of possible names in microbiology, astronomy, physics, evolution, political science, neuroscience, and more.
“While finding more women gatekeepers may help get more women colloquium speakers, will it actually solve the whole problem?” asks Kelly Ramirez from Colorado State University, cofounder of 500 Women Scientists. “Probably not. There are so many things that can make science a toxic or difficult environment for women.” When seeking jobs, women are viewed as being less competent than identical male applicants. They’re offered lower salaries and fewer opportunities for mentorship, and they’re given shorter letters of recommendation with more hedging words. When teaching, they’re rated more negatively. When simply trying to work, they face high levels of sexual harassment and assault.
The solution, Ramirez says, is to “build a strong and large network so whatever the challenge, women scientists will have a network to turn to. And we need to speak up, set examples, and hold people and institutions accountable.”
All of these barriers are particularly profound for women of color, who face a double whammy of discrimination because of both their race and gender. It’s telling that Hebl’s team wanted to look at whether ethnicity deepens the gender disparity among colloquium speakers, but with the universities they looked at, they couldn’t find enough professors of color to get a statistically strong sample.
* This article originally attributed quotes from Michelle Hebl to Christine Nittrouer. We regret the error.


Over the past half-century, climate scientists have learned that the weather leaves behind a hidden history of itself. Through evidence preserved in tree rings, in the gunk at the bottom of lakes, and in towering stalagmites that rise from cave floors, researchers have learned how to read thousands of years of weather history, inferring the existence of long-forgotten rainstorms, hurricanes, and mega-droughts.
Recent research suggests that there may be a similar account hidden in popular music, too. Thursday, at the annual meeting of the American Geophysical Union, the meteorologist Paul Williams presented evidence that a spate of intense hurricanes imprinted themselves on the American songbook.
“In the same way that we have a climatological record of temperature from ice cores, it seems there’s also a kind of climate record in music as well,” said Williams, a professor of atmospheric science at the University of Reading.
His and his colleagues’ main finding turns on two spans of hurricane activity. In the 1950s and 1960s, many large and damaging hurricanes made landfall in the United States, including Hurricanes Dora, Donna, and Camille. Throughout the 1970s and 1980s, there were many fewer high-profile hurricanes, especially in the United States.
Drawing on a karaoke database, Williams and his colleagues found more than 750 pop songs with weather-related lyrics from those four decades.
When times were stormy in the real world, the weather in pop music was darker, too. Almost three-quarters of the weather-themed songs from the 1950s and 1960s had lyrics that emphasized storminess, with frequent use of words like rain, wind, and hurricane. But during the next two decades, only 46 percent of the weather-related songs featured stormy themes. The difference is pronounced enough to be statistically significant.
“That climatological difference over that four-decade period was represented in the songs that were being written,” said Williams. “It does seem to be the case that songwriters are writing about the weather that they’re experiencing on the day they write the song.”
That seems much too neat to me—Paul Jabara and Paul Shaffer didn’t compose “It’s Raining Men” when a cold-front squall line precipitating adult human males gusted into lower Manhattan—but it does point to how the cultural prominence of different weather patterns changes across time periods. Songwriters are just as likely to turn to weather metaphors that are in the news or the cultural conversation as they are to write about what they’re experiencing.
This kind of artistic discovery has informed climate science in the past. From about 1600 to 1850, a series of cold periods afflicted much of the world, including Western Europe. One of the worst winters occurred in 1665—the same year that a spate of winter scenes popped up across European art, including Bruegel the Elder’s The Hunters in the Snow.  Indeed, a kind of “Little Ice Age” is captured in thousands of European paintings from the period.
Williams and his colleagues also conducted a very small unscientific survey of a smattering of classical works from across the years, and from around Europe, that feature explicit depictions of the weather. He found that British composers were much more likely to focus on the weather than composers of any other nationality. “The authors of the study are British, but I don’t think there’s a bias in the study from that,” he said. “I think it’s fitting the national stereotype of British people as being obsessed by the weather.”
Often, he found, composers used stormy weather as a musical stand-in for “emotional turbulence,” he said. In his better-known work, Williams is something of an expert on turbulence: Earlier this year, he published a study finding that climate change will increase the amount of turbulence experienced by air travelers.
But the feedback can work the other way too. Many classical musicians turn to climate science—and sometimes even climate data—as a direct inspiration for their work. Matthew Burtner, an Alaska-born composer who teaches music at the University of Virginia, also presented at the American Geophysical Union, describing his music written about—and sometimes with—glaciers.
Burtner is an “eco-acoustician,” meaning he brings both the sounds and the data signatures of the environment into his work. Sometimes, he’ll take climate data and convert it into musical sound, a process known as “sonification.” (Think of it as the aural equivalent of data visualization.)
His work also includes the sound of nature itself: Burtner has traveled to Matanuska Glacier, a 27-mile-long ice run near Anchorage, Alaska, to bring the object into his composition. In 2013, he scattered 19 microphones across the glacier: Some mics sat on top of the ice, listening for the wind; others were lowered into openings into its interior; and a few sat beneath the glacier’s melt pools, catching the drip-drip-drip of its lost mass.
The goal, Burthner says, is to create “a single audio representation of the glacier and bring that into the studio, the gallery, the concert hall.”
Burtner isn’t the only musician who has worked on environmental themes—or with environmental data specifically. He told me that Iannis Xenakis, the Greek-French composer, influenced his work; Xenakis once wrote a piece that represented certain statistical qualities of gases. But other composers—including Pauline Oliveros, R. Murray Schafer, and John Luther Adams—have written music about the environment, sometimes using environmental recordings.
Burtner’s work—and Williams’s discovery of the climatological pop record—suggests that music and science about the environment may share some deep connections. Both rely on qualitative experience quantified; both require patience and exhaustive focus. Recently, the literary critic Amitav Ghosh wondered why so few novelists today are writing fiction about climate change: “Why does climate change cast a much smaller shadow on literature than it does on the world?” Perhaps we can answer: People are writing music about it instead.


Can training the mind make us more attentive, altruistic, and serene? Can we learn to manage our disturbing emotions in an optimal way? What are the transformations that occur in the brain when we practice meditation? In a new book titled Beyond the Self, two friends—Matthieu Ricard, who left a career as a molecular biologist to become a Buddhist monk in Nepal, and Wolf Singer, a distinguished neuroscientist—engage in an unusually well-matched conversation about meditation and the brain. Below is a condensed and edited excerpt.
Matthieu Ricard: Although one ﬁnds in the Buddhist literature many treatises on “traditional sciences”—medicine, cosmology, botanic, logic, and so on—Tibetan Buddhism has not endeavored to the same extent as Western civilizations to expand its knowledge of the world through the natural sciences. Rather it has pursued an exhaustive investigation of the mind for 2,500 years and has accumulated, in an empirical way, a wealth of experiential ﬁndings over the centuries. A great number of people have dedicated their whole lives to this contemplative science.
Modern Western psychology began with William James just over a century ago. I can’t help remembering the remark made by Stephen Kosslyn, then chair of the psychology department at Harvard, at the Mind and Life meeting on “Investigating the Mind,” which took place at MIT in 2003. He started his presentation by saying, “I want to begin with a declaration of humility in the face of the sheer amount of data that the contemplatives are bringing to modern psychology.”
It does not sufﬁce to ponder how the human psyche works and elaborate complex theories about it, as, for instance, Freud did. Such intellectual constructs cannot replace two millennia of direct investigation of the workings of mind through penetrating introspection conducted with trained minds that have become both stable and clear.
Wolf Singer: Can you be more speciﬁc with this rather bold claim? Why should what nature gave us be fundamentally negative, requiring special mental practice for its elimination, and why should this approach be superior to conventional education or, if conﬂicts arise, to psychotherapy in its various forms, including psychoanalysis?
Ricard: What nature gave us is by no means entirely negative; it is just a baseline. Few people would honestly argue that there is nothing worth improving about the way they live and the way they experience the world. Some people regard their own particular weaknesses and conﬂicting emotions as a valuable and distinct part of their “personality,” as something that contributes to the fullness of their lives. They believe that this is what makes them unique and argue that they should accept themselves as they are. But isn’t this an easy way to giving up on the idea of improving the quality of their lives, which would cost only some reasoning and effort?
Modern conventional education does not focus on transforming the mind and cultivating basic human qualities such as lovingkindness and mindfulness. As we will see later, Buddhist contemplative science has many things in common with cognitive therapies, in particular with those using mindfulness as a foundation for remedying mental imbalance. As for psychoanalysis, it seems to encourage rumination and explore endlessly the details and intricacies of the clouds of mental confusion and self-centeredness that mask the most fundamental aspect of mind: luminous awareness.
Singer: So rumination would be the opposite of what you do during meditation?
Ricard: Totally opposite. It is also well known that constant rumination is one of the main symptoms of depression. What we need is to gain freedom from the mental chain reactions that rumination endlessly perpetuates. One should learn to let thoughts arise and be freed to go as soon as they arise, instead of letting them invade one’s mind. In the freshness of the present moment, the past is gone, the future is not yet born, and if one remains in pure mindfulness and freedom, potentially disturbing thoughts arise and go without leaving a trace.
Singer: What you have to learn then is to adopt a much more subtle approach to your internal emotional theater, to learn to identify with much higher resolution the various connotations of your feelings.
Ricard: That’s right. In the beginning, it is difﬁcult to do it as soon as an emotion arises, but if you become increasingly familiar with such an approach, it becomes quite natural. Whenever anger is just showing its face, we recognize it right away and deal with it before it becomes too strong.
Singer: It is not unlike a scientiﬁc endeavor except that the analytical effort is directed toward the inner rather than the outer world. Science also attempts to understand reality by increasing the resolving power of instruments, training the mind to grasp complex relations, and decomposing systems into ever-smaller components.
Ricard: It is said in the Buddhist teachings that there is no task so difﬁcult that it cannot be broken down into a series of small, easy tasks.
Singer: Your object of inquiry appears to be the mental apparatus and your analytical tool, introspection. This is an interesting self-referential approach that differs from the Western science of mind because it emphasizes the ﬁrst-person perspective and collapses, in a sense, the instrument of investigation with its object. The Western approach, while using the ﬁrst-person perspective for the deﬁnition of mental phenomena, clearly favors the third-person perspective for its investigation.
I am curious to ﬁnd out whether the results of analytical introspection match those obtained by cognitive neuroscience. Both approaches obviously try to develop a differentiated and realistic view of cognitive processes.
What guarantees that the introspective technique for the dissection of mental phenomena is reliable? If it is the consensus among those who consider themselves experts, how can you compare and validate subjective mental states? There is nothing another person can look at and judge as valid; the observers can only rely on the verbal testimony of subjective states.
Ricard: It is the same with scientiﬁc knowledge. You ﬁrst have to rely on the credible testimony of a number of scientists, but later you can train in the subject and verify the ﬁndings ﬁrsthand. This is quite similar to contemplative science. You ﬁrst need to reﬁne the telescope of your mind and the methods of investigations for years to ﬁnd out for yourself what other contemplatives have found and all agreed on. The state of pure consciousness without content, which might seem puzzling at ﬁrst sight, is something that all contemplatives have experienced. So it is not just some sort of Buddhist dogmatic theory. Anyone who takes the trouble to stabilize and clarify his or her mind will be able to experience it.
Regarding cross-checking interpersonal experience, both contemplatives and the texts dealing with the various experiences a meditator might encounter are quite precise in their descriptions. When a student reports on his inner states of mind to an experienced meditation master, the descriptions are not just vague and poetic. The master will ask precise questions and the student replies, and it is quite clear that they are speaking about something that is well deﬁned and mutually understood.
However, in the end, what really matters is the way the person gradually changes. If, over months and years, someone becomes less impatient, less prone to anger, and less torn apart by hopes and fears, then the method he or she has been using is a valid one.
An ongoing study seems to indicate that while they are engaged in meditation, practitioners can clearly distinguish, like everyone who is not distracted, between pleasant and aversive stimuli, but they react much less emotionally than control subjects. While retaining the capacity of being fully aware of something, they succeed in not being carried away by their emotional responses.
Singer: How do you do this? What are the tools?
Ricard: This process requires perseverance. You need to train again and again. You can’t learn to play tennis by holding a racket for a few minutes every few months. With meditation, the effort is aimed at developing not a physical skill but an inner enrichment.
In extreme cases, you could be in a simple hermitage in which nothing changes or sitting alone always facing the same scene day after day. So the outer enrichment is almost nil, but the inner enrichment is maximal. You are training your mind all day long with little outer stimulation. Furthermore, such enrichment is not passive, but voluntary, and methodically directed. When you engage for eight or more hours a day in cultivating certain mental states that you have decided to cultivate and that you have learned to cultivate, you reprogram the brain.
Singer: In a sense, you make your brain the object of a sophisticated cognitive process that is turned inward rather than outward toward the world around you. You apply the cognitive abilities of the brain to studying its own organization and functioning, and you do so in an intentional and focused way, similar to when you attend to events in the outer world and when you organize sensory signals into coherent percepts. You assign value to certain states and you try to increase their prevalence, which probably goes along with a change in synaptic connectivity in much the same way as it occurs with learning processes resulting from interactions with the outer world.
Let us perhaps brieﬂy recapitulate how the human brain adapts to the environment because this developmental process can also be seen as a modiﬁcation or reprogramming of brain functions. Brain development is characterized by a massive proliferation of connections and is paralleled by a shaping process through which the connections being formed are either stabilized or deleted according to functional criteria, using experience and interaction with the environment as the validation criterion. This developmental reorganization continues until the age of about 20. The early stages serve the adjustment of sensory and motor functions, and the later phases primarily involve brain systems responsible for social abilities. Once these developmental processes come to an end, the connectivity of the brain becomes ﬁxed, and large-scale modiﬁcations are no longer possible.
Ricard: To some extent.
Singer: To some extent, yes. The existing synaptic connections remain modiﬁable, but you can’t grow new long-range connections. In a few distinct regions of the brain, such as the hippocampus and olfactory bulb, new neurons are generated throughout life and inserted into the existing circuits, but this process is not large-scale, at least not in the neocortex, where higher cognitive functions are supposed to be realized.
Ricard: A study of people who have practiced meditation for a long time demonstrates that structural connectivity among the different areas of the brain is higher in meditators than in a control group. Hence, there must be another kind of change allowed by the brain.
Singer: I have no difﬁculty in accepting that a learning process can change behavioral dispositions, even in adults. There is ample evidence of this from reeducation programs, where practice leads to small but incremental behavior modiﬁcations. There is also evidence for quite dramatic and sudden changes in cognition, emotional states, and coping strategies. In this case, the same mechanisms that support learning—distributed changes in the efﬁciency of synaptic connections—lead to drastic alterations of global brain states.
Ricard: You could also change the ﬂow of neuron activity, as when the trafﬁc on a road increases signiﬁcantly.
Singer: Yes. What changes with learning and training in the adult is the ﬂow of activity. The ﬁxed hardware of anatomical connections is rather stable after age 20, but it is still possible to route activity ﬂexibly from A to B or from A to C by adding certain signatures to the activity that ensure that a given activation pattern is not broadcast in a diffuse way to all connected brain regions but sent only to selected target areas.
Ricard: So far, the results of the studies conducted with trained meditators indicate that they have the faculty to generate clean, powerful, well-deﬁned states of mind, and this faculty is associated with some speciﬁc brain patterns. Mental training enables one to generate those states at will and to modulate their intensity, even when confronted with disturbing circumstances, such as strong positive or negative emotional stimuli. Thus, one acquires the faculty to maintain an overall emotional balance that favors inner strength and peace.
Singer: So you have to use your cognitive abilities to identify more clearly and delineate more sharply the various emotional states, and to train your control systems, probably located in the frontal lobe, to increase or decrease selectively the activity of subsystems responsible for the generation of the various emotions.
An analogy for this process of reﬁnement could be the improved differentiation of objects of perception, which is known to depend on learning. With just a little experience, you are able to recognize an animal as a dog. With more experience, you can sharpen your eye and become able to distinguish with greater and greater precision dogs that look similar. Likewise, mental training might allow you to sharpen your inner eye for the distinction of emotional states.
In the naïve state, you are able to distinguish good and bad feelings only in a global way. With practice, these distinctions would become increasingly reﬁned until you could distinguish more and more nuances. The taxonomy of mental states should thus become more differentiated. If this is the case, then cultures exploiting mental training as a source of knowledge should have a richer vocabulary for mental states than cultures that are more interested in investigating phenomena of the outer world.
Ricard: Buddhist taxonomy describes 58 main mental events and various subdivisions thereof. It is quite true that by conducting an in-depth investigation of mental events, one becomes able to distinguish increasingly more subtle nuances.
Take anger, for instance. Often anger can have a malevolent component, but it can also be rightful indignation in the face of injustice. Anger can be a reaction that allows us to rapidly overcome an obstacle preventing us from achieving something worthwhile or remove an obstacle threatening us. However, it could also reﬂect a tendency to be short-tempered. If you look carefully at anger, you will see that it contains aspects of clarity, focus, and effectiveness that are not harmful in and of themselves. So if you are able to recognize those aspects that are not yet negative and let your mind remain in them, without drifting into the destructive aspects, then you will not be troubled and confused by these emotions.
Another result of cultivating mental skills is that, after a while, you will no longer need to apply contrived efforts. You can deal with the arising of mental perturbations like the eagles I see from the window of my hermitage in the Himalayas. The crows often attack them, even though they are much smaller. They dive at the eagles from above trying to hit them with their beaks. However, instead of getting alarmed and moving around to avoid the crow, the eagle simply retracts one wing at the last moment, letting the diving crow pass by, and extends its wing back out. The whole thing requires minimal effort and is perfectly efﬁcient. Being experienced in dealing with the sudden arising of emotions in the mind works in a similar way. When you are able to preserve a clear state of awareness, you see thoughts arise; you let them pass through your mind, without trying to block or encourage them; and they vanish without creating many waves.
Singer: That reminds me of what we do when we encounter severe difﬁculties that require fast solutions, such as a complicated trafﬁc situation. We immediately call on a large repertoire of escape strategies that we have learned and practiced, and then we choose among them without much reasoning, relying mainly on subconscious heuristics. Apparently, if we are not experienced with contemplative practice, we haven’t gone through the driving school for the management of emotional conﬂicts. Would you say this is a valid analogy?
Ricard: Yes, complex situations become greatly simpliﬁed through training and the cultivation of effortless awareness. When you learn to ride a horse, as a beginner you are constantly preoccupied, trying not to fall at every movement the horse makes. Especially when the horse starts galloping, it puts you on high alert. But when you become an expert rider, everything becomes easier. Riders in eastern Tibet, for instance, can do all kinds of acrobatics, such as shooting arrows at a target or catching something on the ground while galloping at full speed, and they do all that with ease and a big smile on their face.
One study with meditators showed that they can maintain their attention at an optimal level for extended periods of time. When performing what is called a continuous performance task, even after 45 minutes, they did not become tense and were not distracted even for a moment. When I did this task myself, I noticed that the ﬁrst few minutes were challenging and required some effort, but once I entered a state of “attentional ﬂow,” it became easier.
Singer: This resembles a general strategy that the brain applies when acquiring new skills. In the naïve state, one uses conscious control to perform a task. The task is broken down into a series of subtasks that are sequentially executed. This requires attention, takes time, and is effortful. Later, after practice, the performance becomes automatized. Usually, the execution of the skilled behavior is then accomplished by different brain structures than those involved in the initial learning and execution of the task. Once this shift has occurred, performance becomes automatic, fast, and effortless and no longer requires cognitive control. This type of learning is called procedural learning and requires practice. Such automatized skills often save you in difﬁcult situations because you can access them quickly. They can also often cope with more variables simultaneously due to parallel processing. Conscious processing is more serialized and therefore takes more time.
Do you think you can apply the same learning strategy to your emotions by learning to pay attention to them, differentiate them, and thereby familiarize yourself with their dynamics so as to later become able to rely on automatized routines for their management in case of conﬂict?
Ricard: You seem to be describing the meditation process. In the teachings, it says that when one begins to meditate, on compassion, for instance, one experiences a contrived, artiﬁcial form of compassion. However, by generating compassion over and over again, it becomes second nature and spontaneously arises, even in the midst of a complex and challenging situation.
Singer: It would be really interesting to look with neurobiological tools at whether you have the same shift of function that you observe in other cases where familiarization through learning and training leads to the automation of processes. In brain scans, one observes that different brain structures take over when skills that are initially acquired under the control of consciousness become automatic.
Ricard: That is what a study conducted by Julie Brefczynski and Antoine Lutz at Richard Davidson’s lab seems to indicate. Brefczynski and Lutz studied the brain activity of novice, relatively experienced, and very experienced meditators when they engage in focused attention. Different patterns of activity were observed depending on the practitioners’ level of experience.
Relatively experienced meditators (with an average of 19,000 hours of practice) showed more activity in attention-related brain regions compared with novices. Paradoxically, the most experienced meditators (with an average of 44,000 hours of practice) demonstrated less activation than the ones without as much experience. These highly advanced meditators appear to acquire a level of skill that enables them to achieve a focused state of mind with less effort. These effects resemble the skill of expert musicians and athletes capable of immersing themselves in the “ﬂow” of their performances with a minimal sense of effortful control. This observation accords with other studies demonstrating that when someone has mastered a task, the cerebral structures put into play during the execution of this task are generally less active than they were when the brain was still in the learning phase.
Singer: This suggests that the neuronal codes become sparser, perhaps involving fewer but more specialized neurons, once skills become highly familiar and are executed with great expertise. To become a real expert seems to require then at least as much training as is required to become a world-class violin or piano player. With four hours of practice a day, it would take you 30 years of daily meditation to attain 44,000 hours. Remarkable!

This article has been adapted from Matthieu Ricard and Wolf Singer’s book, Beyond the Self: Conversations Between Buddhism and Neuroscience.


Sex at the zoo is a highly managed affair.
When zookeepers do not want a species to reproduce, birth control is in order. “Chimps take human birth-control pills, giraffes are served hormones in their feed, and grizzly bears have slow-releasing hormones implanted in their forelegs,” writes The New York Times. When zookeepers do want a species to reproduce—especially an endangered or threatened one—the couplings must be carefully arranged. An animal might travel 1,500 miles to meet a partner.
But after all this meticulous planning, zookeepers can hit a wall of uncertainty: It’s sometimes quite hard to know whether a female is pregnant. In the case of pandas, their keepers might not be entirely certain until the baby pops out.
There are animals where it’s easier, sure. Great apes, for example, are related enough to humans that regular old pregnancy tests can work. The problem is getting individual apes to pee on a stick. To get around this, the St. Louis Zoo built special gutters where the great apes slept, which would route the urine outside. In the morning, someone would go out to collect the urine. “But the female has to be alone to do that,” says Cheryl Asa, a former director of reproductive research at the zoo. The system worked well for great-ape species like orangutans, which are more solitary, but not so well for chimps or gorillas, which prefer to sleep in groups and inevitably pee together.
Poop is easier to differentiate by individual. In some cases, says Asa, you can feed different animals food studded with beads of different colors. Or you can just watch as they go. Since mating animals are intensely surveilled anyway, staff members take note when females poop. “If they see a female defecate, they map it,” Asa says. A keeper later uses the map to retrieve the samples. The feces are tested for levels of the hormone progesterone, which rises with pregnancy.
Animals closely related to domestic pets also benefit from veterinary medicine. Wolf pregnancies, for example, can be tested using commercial dog pregnancy kits that detect a hormone called relaxin. The kits require a little bit of blood, which is easy enough to draw from dogs, and actually isn’t that difficult to get from wolves, either. “We all think of wolves as being really fierce, and they can be,” says Asa, “but most of the time when they’re cornered, they’ll just give up and you can hold them down and pull a leg free to get a quick blood sample.”
Some species, however, go through a strange phase called a pseudopregnancy, which looks just like a real pregnancy except there is no fetus. Pandas do this. After a mating, their progesterone levels may rise, and they may start acting like they’re about to give birth. “They build a nest. They get lethargic. They don’t really eat,” says Laurie Thompson, the assistant curator of giant pandas at Smithsonian’s National Zoo.
The only way to know for sure is to see a fetus on an ultrasound, which zookeepers can coax pandas into doing with honey water or fruit. But baby pandas are so tiny even at birth—only four ounces—that an unborn fetus is easy to miss. Worse, “sometimes the pandas don’t even want to participate and they’re just in their den,” says Thompson. The Smithsonian team has only successfully detected a panda pregnancy via an ultrasound once.
Then there are cheetahs, another species difficult to breed in captivity. Like pandas, they go through pseudopregnancies. They aren’t terribly cooperative with ultrasounds or blood draws, either. They do, however, poop.
Recently, researchers at the Smithsonian found a protein in cheetah feces called immunoglobulin J chain that rises during a true pregnancy. “The great thing about feces is that it’s noninvasive,” says Adrienne Crosier, a cheetah biologist at Smithsonian Conservation Biology Institute and an author on the study. “We can get feces any day of the week from a cat. It’s very easy. Any facility can collect it.”
The Life, Times, and Departure of Bao Bao the Panda
The study looks at 26 female cheetahs living in seven different zoos. Crosier also relied on the Smithsonian’s own archive of freeze-dried cheetah feces, which goes back 12 years. In fact, the zoo stores frozen feces, urine, blood, sperm, oocytes, embryos, ovarian tissue, uterine tissue, and testicular tissue for dozens of species. “We keep absolutely everything so we can use it for research,” says Crosier. “We don’t know what we may be interested in looking at or need to look at in five to 10 years.”
At the San Diego Zoo Institute for Conservation Research, which famously has its own “frozen zoo,” there is talk of resurrecting species using tissue from long-dead animals. The zoo has sperm, for example, from northern white rhinos, of which only three remain on Earth. So even in death, reproduction can be a highly managed affair.


Days before his return to Earth in 2008, NASA astronaut Daniel Tani told reporters he couldn’t wait to do something very ordinary after spending four months in space.
“I’m looking forward to putting food on a plate and eating several things at once, which you can’t do up here,” Tani said.
Plates are pretty useless on the International Space Station, where food—along with everything else—floats. Mealtime in microgravity usually consists of thermo-stabilized or freeze-dried entrees and snacks in disposable packages and pouches. Astronauts heat them up in an oven or add water before chowing down with a fork straight out of the package. The space station doesn’t have refrigerators or freezers to keep food fresh, so there’s no such thing as leftovers.
Despite the almost alien process of eating, astronauts consume many of the foods they would find back home: scrambled eggs, spaghetti, chicken teriyaki, broccoli au gratin, oatmeal with raisins. During the holidays, they have turkey, candied yams, cornbread dressing, and other seasonal foods. The current menu includes about 200 foods and beverages. Some items can be eaten in their natural form, like nuts and cookies. But most of the food has to be prepared in a laboratory and carefully tested over and over, to ensure it’s fit for consumption but can also last for two years before opening. Some of the prep veers into Food Network territory: The lab gets volunteers to judge food items on appearance, color, flavor, texture, and aroma.
The process of developing a microgravity-friendly food item can take months or years, says Vickie Kloeris, the food scientist who runs the ISS food-systems lab. I spoke with Kloeris about eating in space, how to pack food for a mission to Mars, and the myth of astronaut ice cream. Our conversation has been edited for length and clarity.
Marina Koren: So you got to Johnson Space Center as a food scientist in 1985. What was the state of astronaut food back then?
Vickie Kloeris: It really wasn’t all that different than it is now. Everything was shelf-stable, just like it is now. We had thermo-stabilized, freeze-dried, natural-form food, irradiated food, powdered beverages—just like we do now. But we didn’t have nearly as much variety during the Space Shuttle program because the missions were short, so we really didn’t need a whole lot of variety. When I came to work here, we were flying entrees from the MREs from the military. We don’t do that anymore because the MRE entrees are way too high in salt and fat for what we want for our long-duration crew members. The military has good reasons to have that salt and fat in there, but they are negatives for our crew members.
Koren: How do you transform a a terrestrial recipe into something that’s fit for consumption in microgravity?
Kloeris: Many terrestrial recipes, especially entrees, are not shelf-stable. The end product requires refrigeration. We don’t have a dedicated refrigerator or freezer for food on the space station, so everything that we send to orbit has to be shelf-stable. So we convert standard recipes into shelf-stable foods through freeze-drying and thermo-stabilization. Thermo-stabilization is basically canning—except we don’t do it in cans, we do it in pouches. Pouches are much lighter in weight and more efficient to stow. The tricky thing is, you can’t just take a traditional recipe and thermo-stabilize it or freeze-dry it and have it work. If only it were that simple. When we go to create a new item, it often takes multiple attempts, multiple adjustments, to end up with something that actually works.
Koren: Does microgravity affect the taste buds? Does food taste the same on the space station?
Kloeris: That depends on who you talk to. There is no scientific evidence that microgravity alters the taste of food. There is anecdotal evidence from crew members that they feel like their taste buds are somewhat dulled in orbit. Other crew members say it’s all in their head and there is no difference. But they are probably getting less aroma from the food when they eat in orbit than when they consume those same items on the ground. They’re eating out of packages rather than off a plate, so that can hinder the amount of aroma they’re getting. Plus, when you heat food on the ground, a lot of the heat rises and the aroma goes with it. When you heat stuff in microgravity, the heat can dissipate in different directions, so that has the potential to spread out the odor and have it be less intense. So that could be it. Just like when you and I are congested down here and we’re not getting as much aroma—the food’s not going to taste exactly the same.
Koren: Which foods are the most difficult to prepare for space?
Kloeris: Anything that creates a lot of crumbs. Crumbs are very difficult to deal with in microgravity because they’re just messy. When they get loose, they can make it into the air filtration system. You have to find a way to clean them up, and that usually involves a vacuum cleaner. Anything that requires refrigeration to remain microbiologically stable is going to be impossible to send up there. We occasionally get to send ice cream because they’ll have a freezer for medical samples that’s empty on the uphill trip. When that happens, we can send some frozen ice-cream treats and they have to eat those pretty much as soon as the vehicle docks, because they’re going to have to fill up that freezer with medical samples.
Koren: Have you tried to develop a microgravity-friendly recipe that just didn’t work?
Kloeris: We’ve had it happen more than once. We tried a thermo-stabilized cheesecake and we were never, never happy with the results. So we gave up on that.
Koren: How about carbonated drinks like soda? Can astronauts drink that?
Kloeris: Not unless they’re packaged under pressure, like in a whipped-cream can. In microgravity, the carbonation will not remain with the beverage. It will separate. Coke and Pepsi flew in pressure vessels back in the ’80s on one flight, and at that time, they didn’t have a way to chill it. So it was like, okay, we had hot Coke and hot Pepsi, so what? You’re probably not going to want a lot of carbonation in your diet when you’re in microgravity anyway, because when you burp down here, it’s dry burp. When you burp in microgravity, it’s probably not going to be a dry burp.
Koren: What ... what kind of burp would it be?
Kloeris: Wet. You’re gonna have food coming with it. When you burp, you’re burping through that sphincter at the top of your stomach. That is not a full closure. So in microgravity, when you eat, the food floats high in your stomach. Burping in microgravity is probably not something you want to do a lot of.
Koren: Have you been thinking about what kind of meals NASA would need to prepare for longer missions, like a trip to Mars or into deep space?
Kloeris: The research team in our lab is trying to figure that out right now. For Mars, the food that they eat on the return trip will be somewhere between five and seven years old, so that is a huge challenge. We can actually make food that is microbiologically safe to eat for that period of time. But there’s very few foods in our current food system that would maintain sufficient quality after that long. Even though we can stop microbial changes in these products by preserving them, we can’t stop the chemical changes. The color, texture, and flavor are going to change, and the nutritional content is going to degrade. We’re looking into which items are most susceptible to degradation. A particular nutrient will be more stable in one food than in another. For instance, vitamin C is not very stable in thermo-stabilized products, but it’s very stable in powdered beverages.
Koren: What recipes are you working on right now?
Kloeris: We aren’t developing any new foods right at this moment. We have some new products that have been developed over the last couple of years that we’re just now introducing into the food supply to see how the acceptability goes. We have a new freeze-dried roasted-brussels-sprouts dish, a couple of thermo-stabilized fish casseroles—to try to get some more omega-3s into the food system—a freeze-dried fruit salad.
Koren: I’m sure people have asked you this a million times, but how did that chalky, Neapolitan astronaut ice cream become a thing?
Kloeris: During [the Apollo program], one of the crew members did request ice cream, but what they flew doesn’t look like anything that they sell at the museums or the visitor centers. I think that just took off because it was something the kids liked and a commercial company made it. What actually flew during Apollo was a synthetic cube that was dairy-based. That’s about as close to ice cream as they got.
Koren: Does your job change the way you look at preparing food at home?
Kloeris: My family thinks I overreact sometimes because I worry about food safety. I’m not one to leave the turkey sitting out on the table for hours after dinner.


This article is edited from a story shared exclusively with members of The Masthead, the membership program from The Atlantic. Find out more.

One hundred miles above the Earth’s surface, orbiting the planet at thousands of miles per hour, the six people aboard the International Space Station enjoy a perfect isolation from the chaos of earthly conflict. Outer space has never been a military battleground. But that may not last forever. The debate in Congress over whether to create a Space Corps comes at a time when governments around the world are engaged in a bigger international struggle over how militaries should operate in space. Fundamental changes are already underway. No longer confined to the fiction shelf, space warfare is likely on the horizon.
While agreements for how to operate in other international domains, like the open sea, airspace, and even cyberspace, have already been established, the major space powers—the United States, Russia, and China—have not agreed upon a rulebook outlining what constitutes bad behavior in space. It’s presumed that International Humanitarian Law would apply in outer space—protecting the civilian astronauts aboard the International Space Station—but it’s unclear whether damaging civilian satellites or the space environment itself is covered under the agreement. With only a limited history of dangerous behavior to study, and few, outdated guidelines in place, a war in space would be a war with potentially more consequences, but far fewer rules, than one on Earth.
Although there has never been a military conflict in space, the history of human activity above our atmosphere is not entirely benign. In 1962, the United States detonated a 1.4 megaton nuclear weapon 250 miles above the Earth’s surface. The blast destroyed approximately one third of satellites in orbit and poisoned the most used region of space with radiation that lasted for years. Although the United States, Russia, and others soon agreed to a treaty to prevent another nuclear test in space, China and North Korea never signed it. In 2007, China tested an anti-satellite weapon, a conventionally-armed missile designed to target and destroy a satellite in orbit. In the process, it annihilated an old Chinese weather satellite and created high-velocity shrapnel that still threatens other satellites. Even though demonstrations like this have consequences for everyone, countries are free to carry them out as they see fit. No treaties address this kind of test, the creation of space debris, or the endangerment of other satellites.
The U.S. has the most to lose in a space-based conflict
With by far the most satellites in orbit, the U.S. has the most to gain by establishing norms, but also the most to lose. Almost half of all operational satellites are owned and operated by the United States government or American commercial companies. That’s twice as many as Russia and China, combined. Space may seem distant, but what happens there affects our everyday lives on the ground. When we use our phones to plan a trip, we depend on American GPS satellites to guide us. When the U.S. military deploys troops overseas, satellite communications connect forces on the ground to control centers. When North Korea launches an intercontinental ballistic missile, the U.S. and its allies depend on early-warning satellites to detect it.
On one hand, if the global space powers agreed to put limits on space-based weapons and other related technologies, it could make space safer for everyone. But because the U.S. may have spent time and resources developing exactly the type of weapons that a code of conduct would ban, it could also curtail the most advanced space-based developments, erasing years of research and progress.
There are more players in space—and less consensus
In the first space age, from the launch of the first human-made satellite in 1957 through the fall of the Soviet Union, the United States and the USSR were responsible for over 90 percent of all satellites. Their race to perfect space technology, dominated by both national security interests and scientific discovery, far outpaced everyone else. The second space age, from 1990 to today, looks remarkably different. Now, more satellites are operated by private companies than militaries, and more space launches and new satellites come from countries other than the United States and Russia. More players in space—particularly more unpredictable players—means more opportunities for aggressive behavior, like developing anti-satellite technologies or hacking satellite communications. Countries like Iran or North Korea that are newer to space can choose to operate in a way we’ve never seen before. And if their nuclear programs on Earth are any guide, they could pose serious threats if left unchecked.
Efforts have been made to create a modern-day space rulebook, but so far none have gained traction. In 2008, when Russia and China both proposed norms of behavior, the United States refused to sign on. Similarly, when the United States supported a 2014 European Union proposal to govern the use of conventional weapons in orbit, Russia and China didn’t agree with the terms.
Since the congressional debate about a Space Corps, people have been taking the prospect of a war in space seriously, in a way we haven’t seen before. Now we should start talking about how to avoid that war. To prevent conflict in the upper atmosphere, all potential adversaries—the United States, China, North Korea, Iran, Russia, the EU—need to align, and agree on norms of behavior. They need rules.


Astronomers have completed their first round of telescope observations of ‘Oumuamua, the first known interstellar object to enter our solar system, to check the asteroid for signs of alien technology.
So far, they have found no evidence of artificial signals coming from the asteroid, they said Thursday—but the search isn’t over yet.
“Indeed, nothing has popped up, but we’re busy churning through the data we’ve collected so far,” said Andrew Siemion, the director of the Berkeley SETI Research Center who leads its Breakthrough Listen Initiative, a $100 million effort in the search for intelligent extraterrestrial life.
The decision to check ‘Oumuamua for artificial signals came from Yuri Milner, the Russian billionaire and tech investor who is spending $100 million over 10 years to fund SETI efforts.
‘Oumuamua was first detected by Hawaiian astronomers in October. The asteroid, named for a Hawaiian word meaning “messenger,” puzzled the astronomy community. The properties of the mysterious space rock were unusual, particularly its extremely elongated, cigar-like form, a shape difficult to create through the natural, known processes of the universe. Somewhere along the way, some astronomers began to wonder whether this space rock could be a probe of some kind, dispatched by an advanced civilization.
Aliens are, of course, at the very bottom of the list of explanations for new and confounding astronomical discoveries. But for Milner, it was worth checking when astronomers had the chance, before ‘Oumuamua zoomed out of reach of even the most powerful telescopes.
The observations began Wednesday at 3:45 p.m. Eastern Time. For six hours, a Breakthrough Listen instrument on the Green Bank Telescope in West Virginia scanned ‘Oumuamua across four radio bands. The instrument is capable of scanning billions of individual channels at once, listening for pings between one and 12 gigahertz—a range of frequency that on Earth includes signals from technology like cellphones and microwave ovens.
Astronomers have so far only analyzed the data from one of the bands. “I’m certainly anxious to see if anything pops up from searches of the other bands,” Siemion said. When astronomers analyze data in the range of one to two gigahertz, they can look for evidence of the molecule hydroxyl, which would suggest the presence of water on ‘Oumuamua.
Software developed for the Breakthrough Listen project will search for narrow-bandwidth signals drifting in the frequencies detected by Green Bank. “By matching the rate at which these signals drift to the expected drift due to the motion of ‘Oumuamua ... the software attempts to identify any signals that might be coming from ‘Oumuamua itself,” the team said Thursday. The software will also weed out signals coming from human technology on or around Earth.
Siemion said the analysis will take some time. “We’re looking for signals from weakly transmitting or very distant technologies, bathed in a sea of signals from our own technology,” he said. “SETI is not an especially good field for the easily discouraged.”
‘Oumuamua is currently about twice the distance between the Earth and the sun from our planet, and barreling through the solar system at a rate of 38.3 kilometers per second. At this distance, Green Bank can still detect signals as faint as the radio waves from a cellphone. The next round of observations at Green Bank will likely begin Friday.
Astronomers predict many more interstellar objects will be detected in the solar system in the years to come. For Milner and SETI astronomers, it’s worth checking each one for signs of artificial technology.
“More and more SETI is going to be done on those types of objects over time, because I think this is not the only one that we will detect,” Milner said. “We need to start doing this, to practice.”


In August 1976, a 44-year-old headmaster named Mabalo Lokela arrived back in the town of Yambuku in the Democratic Republic of the Congo, after two weeks spent touring with a local mission. A few days after his return, he checked into the local hospital with nosebleeds, dysentery, and a fever. The doctors treated him for malaria, but to no avail. Lokela got worse. In early September, two weeks after his first symptoms, he died. And meanwhile, other people who had come into contact with him started getting sick.
Over the next three months, 318 people became infected, and 280 of them died. That outbreak, and another that took place simultaneously in South Sudan, alerted the world to the existence of a lethal new disease, which eventually took the name of the waterway on which Yambuku is situated—the Ebola River.
Ebola is famously deadly, but not inevitably so. Around 12 percent of those who were infected in the Yambuku outbreak survived their brush with the disease, and many of them are still around today. They’ve lived through seven more documented Ebola outbreaks in the DRC, the latest of which took place this May, less than 350 kilometers away from Yambuku. They’ve watched from afar as the biggest Ebola outbreak in history ravaged West Africa.
Compared to that epidemic, “the DRC’s outbreaks have been smaller and more isolated,” says Anne Rimoin at the University of California, Los Angeles, “and so too have these survivors. They’ve had no contact or follow-up.” They still bear the scars and social stigma of their experience with the virus. But they also carry defenses against it.
Rimoin has shown that the original survivors’ blood still contains antibodies against Ebola. In some cases, people had antibodies that can destroy the virus outright, even after 41 years. “They should be immune to Ebola,” Rimoin says.
Simply finding the survivors was a Herculean task. Medical records from that 1976 outbreak were nowhere to be found, so Rimoin had to ask the researchers who were on the scene to rummage through their files. Once she had a list, her team took several trips to Yambuku to search for the people behind the names. And since the town is so remote, every trip involved a chartered flight and a grueling drive. “It took maybe 9 hours in the dry season, and 20 hours in the rainy season,” Rimoin says.
The team eventually tracked down 14 survivors who, according to Rimoin, were eager to take part in a new study. “They were very happy that there were people out there interested in hearing their stories and understanding what they had been through,” she says. “It may have been a long time ago, but they are still living the consequences of what they’ve suffered. Most of them lost family members. And when they emerged from the hospital, having narrowly survived a terrifying near-death experience, they found their homes had been burned to the ground for fear of contamination. Everything they owned had disappeared.”
Even now, they have to live with the social stigma of having once had Ebola four decades ago. Such is the fear surrounding the virus that the hospital in Yambuku was initially reluctant to let Rimoin bring her volunteers in to take blood samples. “To this day, if you say you’re an Ebola survivor, people will recoil,” she says.
Previously, another team found that Ebola patients retain some immunity against the virus after 14 years, but Rimoin’s team have shown that this protection extends for decades more. All of the 14 people they studied still carry antibodies that recognize at least one of the Ebola virus’s proteins, and four had antibodies that could completely neutralize the virus. “Those are the kinds of responses you’d like to see in a vaccine—long-lasting and robust,” says Rimoin, “which means that these antibodies are of great value to science.”
It’s clear that the Ebola virus can stick around long after symptoms abate, by hiding out in unusual places like eyeballs. One man still carried the virus in his semen 565 days after he recovered. Ebola’s tenacity might explain why survivors continue to produce antibodies against it, long after they’ve finally cleared it from their bodies.
“It’s well-documented that acute viral infections such as measles and smallpox provide lifelong immunity,” says Mohan Natesan from the U.S. Army Medical Research Institute for Infectious Diseases. Ebola infections can clearly lead to similarly long-lasting antibodies, but “it is difficult to predict whether these antibodies will protect from reinfections, or infection with another Ebola species. Further studies with these survivors are warranted to answer these questions."
It’s also hard to say why four of the 14 people developed such potent antibodies. They might have had naturally powerful immune systems. They might have already been exposed to similar viruses in the past, and confronted Ebola with partial immunity. It’s hard to say why with such a small group, but “it is remarkable that 14 suspected survivors were tracked and found in the first place,” says Lauren Cowley from Harvard University. “The 1976 outbreak was much smaller than the recent West African outbreak so a much larger pool of survivors will now be available for future studies on the long-term effects of Ebola.”
Rimoin is now planning more trips to the DRC to continue studying the survivors, including those who made it through later epidemics. She’s on the clock: The average life expectancy in the DRC is 62 years for women and 58 years for men. Many people who were teenagers during the first epidemic are now nearing the end of their lives. Indeed, Rimoin originally identified 16 survivors from the 1976 outbreak, but two of them died before she got a chance to take their blood. “The aperture for studying these people is closing,” she says.


The journalist was not having it. He tailed the museum director out the door. “Are you looting the museum for your own personal means?” he demanded. “I totally saw you slip something into your car earlier today.”
Obviously, the emergency evacuation of the state museum of “Smithsonia” was not going according to plan.
This scene played out at the Smithsonian on a recent Wednesday afternoon, during an exercise in which a group of cultural-heritage professionals and emergency responders tried to evacuate the fictional Smithsonia museum after a pretend cyclone. They had all come to Washington, D.C., for the weeklong Heritage Emergency and Response Training, or HEART, hoping to learn how museums can plan for natural disasters or even war.
The HEART organizers did not make it easy: The museum’s collection was scattered and uncatalogued, the staff were largely absent, the aforementioned reporter was snooping around for dirt, and the museum’s director was, well, you already know. To successfully evacuate the museum, the team would have to navigate egos, bureaucracies, and public opinion—plus move dozens of fragile objects.
These included a cabinet of ceramics (to be individually Bubble Wrapped), several framed drawings (also to be Bubble Wrapped), and a giant, crumbling flag (to be carefully rolled up and supported atop a bed of cut-up boxes). All of these had to be lifted down a set of stairs, through a narrow tunnel, back up the stairs, out the door, down an outdoor path, and back into another building. All in an hour.
“I would do 200 things differently,” Megan Salazar-Walsh, one of the HEART participants, later said.
HEART is a program of the Smithsonian’s Cultural Rescue Initiative, which deploys experts to sites of conflict or natural disaster around the world. Corine Wegener, one of the organizers of HEART, previously recovered artifacts looted from museums during Operation Iraqi Freedom. In 2010, she went to Haiti after the earthquake and worked on restorations at the Centre d’Art and the Holy Trinity Episcopal Cathedral’s historic murals—that project eventually led to the formal creation of the Cultural Rescue Initiative.
Since then, the group has worked to preserve heritage sites and museums in Iraq, Syria, post-earthquake Nepal, and most recently Puerto Rico.
Large museums have detailed emergency-response plans—in some cases, built in their very architecture. The Getty Museum, recently threatened by wildfires around LA, has a million-gallon water tank and an air-filtration system that keeps out smoke. The Whitney Museum in New York City is protected by a 15,500-pound flood door “designed by engineers who build watertight latches for the U.S. Navy’s destroyers.”
Not all museums are impenetrable fortresses, though, and the state museum of Smithsonia was considerably more modest. In fact, it was woefully understaffed, which is why it had to rely on volunteers, played by the HEART participants, to evacuate it. They had divided into five teams of five, each with a team leader who in turn reported to the incident commander, Megan Salazar-Walsh.
In their real lives, the participants were geographically diverse, spanning Alaska to Puerto Rico. Salazar-Walsh is an art conservator at the John and Mable Ringling Museum of Art in Florida.
The 25 of them met at the beginning of the week, so they were not quite strangers by the evacuation exercise Wednesday afternoon, but not yet at ease, either. Inevitably, when their careful plans to label items A through D and pack them up in a specific box gave way to improvisation, that lack of underlying trust showed. “Different people are giving out different directions, commander,” one woman said, clearly exasperated, as Salazar-Walsh made her way through the museum’s rooms.
It did not help that the museum’s director had waltzed in and was issuing his own orders. Brian Daniels is a director at the Penn Cultural Heritage Center, but for the scenario, he had donned a blue suit and pink bowtie to play the aloof director of the Smithsonia state museum—the kind of director prone to digressions about collection items even in the middle of an emergency evacuation. (In last year’s evacuation exercise, Daniels wore a black suit and played an irate politician. “It was very out of character,” he says. “For the rest of the class everyone was in fear of me.”)
Daniels strode over to the cabinet and plucked a ceramic poodle off the shelf. “Fifi!” he exclaimed, before launching into the story of the beloved white poodle who belonged to the children of Smithsonia’s governor. Salazar-Walsh gently tried to move him along. “If only you’d grown up on the island ...” Daniels continued. He implored the evacuators to wrap Fifi with care.
Fifi the ceramic poodle had come from a garage or estate sale—like all the other items that made up the mock museum. (“No museum artifacts are put at risk in the course of this exercise,” Wegener clarified.) But the mock artifacts had to be treated as real ones, and even as Daniels hammed it up, he was getting at something fundamental: Why save museums in war or natural disaster?
“I always liken it to your own home,” Wegener later said. Of course, you want to get your family out first, but as soon as it’s safe, the fire department is authorized to salvage personal belongings, too. “For people going back in their own homes after disasters, they’re looking for those irreplaceable items that stand for their personal and their family’s identities, their memories, their hope for recovery in the future. Scale that up to whole communities and countries,” she says. One of her colleagues in Haiti had summed it up this way: “The dead are dead, we know that. But without our culture, the rest of us can’t go on living.” For Smithsonia, one had to remember, Fifi represented far more than a random dog.
In the next room, Daniels found a team starting to pack an old, crumbling flag laid out on the floor. “I don’t want it to be rolled crooked,” he said. He strode over, almost stepping on the flag in several places. The team exchanged nervous glances. But Daniels soon found a mug that he said was his personal mug, and they told him to take it, relieved to have finally dispatched him. (“They’re doing really well despite me trying my best to distract them,” Daniels said.)
What the team did not all realize, though, is that while Daniels was wandering around, so was the keen-eyed journalist—played by Wegener’s husband, Paul. (Naturally, he wore a flak jacket.) And when the journalist saw Daniels putting the mug in his car, he confronted him. The resulting make-believe headline: “Objects Looted From Museum.”
The point was not to make everyone suspicious of each other, but to inject the kind of unexpected situation that always comes up in emergencies. “If you can do it here, where you just met these people, etcetera, and you’re under time constraint, think how much better you’ll be able to do it when you get back to your institution when you have a plan,” says Wegener. “And that’s the aha moment people have. Yeah, it’ll be lot easier.” Later in the week, HEART had a whole session on planning for the media during emergencies.
Most of all, HEART’s goal over the week was getting museums and emergency planners to talk to one each other. “Those two universes do not coincide very much,” says Wegener. The goal is to get more cities and towns to integrate cultural heritage in emergency planning. “We’re miles ahead of where we were several years ago, miles ahead with Hurricane Katrina,” she adds.
At times during the evacuation, the gap between these two worlds was on display. The group got stuck in a debate over what to do with several pieces of furniture they would not have time to evacuate. In the scenario, the evacuated museum was being turned into an emergency-operations center, or EOC—not exactly a place where protecting antique furniture would be a top priority. Someone suggested telling the EOC people to keep away from the museum furniture because it might be unsafe.
One of the emergency planners balked. “I can’t in good conscience tell them it’s not safe if I think it is,” he said.
“Can we just tell them not to use the collection items?”
“EOC will use everything,” another emergency planner said.
Okay, what about shrink-wrapping the furniture, which would at least give them some protection?
This time Salazar-Walsh, a conservator, hesitated. “It also could create a microclimate in that space,” she said, referring to how the plastic could trap humidity inside wooden furniture and cause mold. But she reconsidered: The furniture didn’t look wet.
The EOC people were marching up the steps, Wegener said. It was time to make a decision.
They decided to shrink-wrap.


Updated on December 13 at 6:30 p.m.
Hydraulic fracturing, or fracking, may pose a significant—but very local—harm to human health, a new study finds. Mothers who live very close to a fracking well are more likely to give birth to a less healthy child with a low birth weight—and low birth weight can lead to poorer health throughout a person’s life.
The research, published Wednesday in Science Advances, is the largest study ever conducted on fracking’s health effects.
“I think this is the most convincing evidence that fracking has a causal effect on local residents,” said Janet Currie, an economist at Princeton University and one of the authors of the study.
The researchers took the birth records for every child born in Pennsylvania from 2004 to 2013—more than 1.1 million infants in total—and looked at the mother’s proximity to a fracking site, using the state of Pennsylvania’s public inventory of fracking-well locations. They used private state records that showed the mother’s address, allowing them to pinpoint where every infant spent its nine months in utero.
They found significant, but very local, consequences. Infants born to mothers who lived within two miles of a fracking well are less healthy and more underweight than babies born to mothers who lived even a little further away. Babies born to mothers who lived between three and 15 miles from a fracking well—that is, still close enough to benefit financially from the wells—resembled infants born throughout the rest of the state.
While birth weight may seem like just a number, it can affect the path of someone’s life. Children with a low birth weight have been found to have lower test scores, lower lifetime earnings, and higher rates of reliance on welfare programs throughout their lives. In a previous study, a different team of researchers examined twins in Norway whose birth weight diverged by 10 percent or more. The lighter twin was 1 percent less likely to graduate from high school and earned 1 percent less than their sibling through their life.
“Hydraulic fracturing has widely dispersed benefits—we are all paying lower natural-gas bills for heating, we’re all paying lower electricity prices, we’re all paying less for cheaper gasoline at the pump. And even if health was all that you care about, we’re all benefitting from decreased air pollution thats widely dispersed, because coal plants are closing,” said Michael Greenstone, a professor of economics at the University of Chicago and another authors of the paper.
But all those benefits, he said, were borne by the local communities that lived extremely close to hydraulic fracturing wells. “There’s this interesting trade off between the greater good and what are the costs and benefits for local communities,” he told me.
Oil and gas lobbying groups rushed to criticize the study. “This report highlights a legitimate health issue across America that has nothing to do with natural gas and oil operations.  It fails to consider important factors like family history, parental health, lifestyle habits, and other environmental factors and ignores the body of scientific research that has gone into child mortality and birthweight,” said Reid Porter, a spokesman for the American Petroleum Institute, a trade organization that represents the oil and gas industry.
In the fracking study, researchers tried to separate the costs of fracking and socioeconomic status and parental health in several ways. First, they compared baby birthweight near fracking wells to those babies immediately around them, which they believe accounts for the wealth of various communities.
Second, they found that the connection held for siblings who were or were not exposed to a fracking well. “We follow the same mother over time and ask whether on average, children born after fracking starts have worse outcomes than their siblings born before fracking starts,” Currie told me. “In this case, since we follow the same woman over time, we are controlling for her underlying characteristics.”
Babies who gestated near a well had a reliably lower birth weight than their siblings who were not exposed to the well.
The researchers don’t yet know why this link between fracking and low birth weight exists, though they suggest that air pollution could be a possible contributor. The process of fracking may release chemicals into the air, for one, but many wells also run multiple diesel engines at once, and they can be a hub of local activity, with trucks regularly commuting to the sites.
While environmental activists and some researchers have proposed that fracking chemicals may leak into groundwater, most studies have failed to find lasting and widespread water pollution near wells. The birth-weight study seems to suggest that air, not water, pollution may instead be the threat that fracking sites pose to human health.
Greenstone believes the next step for this research is to figure out exactly what is driving the babies’ low birth weight. “Is it the trucks? Is it the diesel generators?” he said. “If you knew the channel, you might be able to devise a light-touch regulatory approach.”
But he and Currie also believe more research is needed to figure out how fracking affects people outside the womb and later in their life. Such connections will be harder to distill, but may become easier as this kind of broad, data-based approach to environmental economics becomes more widespread.


In 1799, the Italian scientist Alessandro Volta fashioned an arm-long stack of zinc and copper discs, separated by salt-soaked cardboard. This “voltaic pile” was the world’s first synthetic battery, but Volta based its design on something far older—the body of the electric eel.
This infamous fish makes its own electricity using an electric organ that makes up 80 percent of its two-meter length. The organ contains thousands of specialized muscle cells called electrocytes. Each only produces a small voltage, but together, they can generate up to 600 volts—enough to stun a human, or even a horse. They also provided Volta with ideas for his battery, turning him into a 19th-century celebrity.
Two centuries on, and batteries are everyday objects. But even now, the electric eel isn’t done inspiring scientists. A team of researchers led by Michael Mayer at the University of Fribourg have now created a new kind of power source that ingeniously mimics the eel’s electric organ. It consists of blobs of multicolored gels, arranged in long rows much like the eel’s electrocytes. To turn this battery on, all you need to do is to press the gels together.
Unlike conventional batteries, the team’s design is soft and flexible, and might be useful for powering the next generation of soft-bodied robots. And since it can be made from materials that are compatible with our bodies, it could potentially drive the next generation of pacemakers, prosthetics, and medical implants. Imagine contact lenses that generate electric power, or pacemakers that run on the fluids and salts within our own bodies—all inspired by a shocking fish.
To create their unorthodox battery, the team members Tom Schroeder and Anirvan Guha began by reading up on how the eel’s electrocytes work. These cells are stacked in long rows with fluid-filled spaces between them. Picture a very tall tower of syrup-smothered pancakes, turned on its side, and you’ll get the idea.
When the eel’s at rest, each electrocyte pumps positively charged ions out of both its front-facing and back-facing sides. This creates two opposing voltages that cancel each other out. But at the eel’s command, the back side of each electrocyte flips, and starts pumping positive ions in the opposite direction, creating a small voltage across the entire cell. And crucially, every electrocyte performs this flip at the same time, so their tiny voltages add up to something far more powerful. It’s as if the eel has thousands of small batteries in its tail; half are pointing in the wrong direction but it can flip them at a whim, so that all of them align. “It’s insanely specialized,” says Schroeder.
He and his colleagues first thought about re-creating the entire electric organ in a lab, but soon realized that it’s far too complicated. Next, they considered setting up a massive series of membranes to mimic the stacks of electrocytes—but these are delicate materials that are hard to engineer in the thousands. If one broke, the whole series would shut down. “You’d run into the string-of-Christmas-lights problem,” says Schroeder.
In the end, he and Guha opted for a much simpler setup, involving lumps of gel that are arranged on two separate sheets. Look at the image below, and focus on the bottom sheet. The red gels contain saltwater, while blue ones contain freshwater. Ions would flow from the former to the latter, but they can’t because the gels are separated. That changes when the green and yellow gels on the other sheet bridge the gaps between the blue and red ones, providing channels through which ions can travel.
Here’s the clever bit: The green gel lumps only allow positive ions to flow through them, while the yellow ones only let negative ions pass. This means (as the inset in the image shows) that positive ions flow into the blue gels from only one side, while negative ions flow in from the other. This creates a voltage across the blue gel, exactly as if it was an electrocyte. And just as in the electrocytes, each gel only produces a tiny voltage, but thousands of them, arranged in a row, can produce up to 110 volts.
The eel’s electrocytes fire when they receive a signal from the animal’s neurons. But in Schroeder’s gels, the trigger is far simpler—all he needs to do is to press the gels together.
It would be cumbersome to have incredibly large sheets of these gels. But Max Shtein, an engineer at the University of Michigan, suggested a clever solution—origami. Using a special folding pattern that’s also used to pack solar panels into satellites, he devised a way of folding a flat sheet of gels so the right colors come into contact in the right order. That allowed the team to generate the same amount of power in a much smaller space—in something like a contact lens, which might one day be realistically worn.
For now, such batteries would have to be actively recharged. Once activated, they produce power for up to a few hours, until the levels of ions equalize across the various gels, and the battery goes flat. You then need to apply a current to reset the gels back to alternating rows of high-salt and low-salt. But Schroeder notes that our bodies constantly replenish reservoirs of fluid with varying levels of ions. He imagines that it might one day be possible to harness these reservoirs to create batteries.
Essentially, that would turn humans into something closer to an electric eel. It’s unlikely that we’d ever be able to stun people, but we could conceivably use the ion gradients in our own bodies to power small implants. Of course, Schroeder says, that’s still more a flight of fancy than a goal he has an actual road map for. “Plenty of things don’t work for all sorts of reasons, so I don’t want to get too far ahead of myself,” he says.
It’s not unreasonable to speculate, though, says Ken Catania from Vanderbilt University, who has spent years studying the biology of the eels. “Volta’s battery was not exactly something you could fit in a cellphone, but over time we have all come to depend on it,” he says. “Maybe history will repeat itself.”
“I’m amazed at how much electric eels have contributed to science,” he adds. “It’s a good lesson in the value of basic science.” Schroeder, meanwhile, has only ever seen electric eels in zoos, and he’d like to encounter one in person. “I’ve never been shocked by one, but I feel like I should at some point,” he says.


When Charles Konsitzke and Dhanu Shanmuganayagam first met, they were both just trying to get some peace and quiet. It was early 2014, and they were representing the University of Wisconsin-Madison at a fancy event to promote the university’s research to local politicians. After hours of talking to senators, Shanmuganayagam was fried, and went for a walk to clear his head. That’s when he bumped into Konsitzke, an administrator at the University of Wisconsin’s Biotechnology Center. They introduced themselves, and discussed their work. Shanmuganayagam said that he ran a facility that rears miniature pigs, which are genetically engineered to carry mutations found in human genetic disorders. Scientists can study the mini-pigs to better understand those diseases.
“And I said: I have a project for you,” Konsitzke recalls.
Konsitzke’s son Mason, now aged 7, was born with little brown birthmarks on his buttocks. Many kids have one or two of these café-au-lait spots and at first, Konsitzke thought they were cute. But after more appeared, he did some research and found that such spots are a common symptom of neurofibromatosis type 1 (NF-1)—an incurable inherited disease. Around Mason’s first birthday, a pediatrician confirmed the diagnosis.
NF-1 is an incredibly varied disease with many possible symptoms. The spots are the least of them. Some patients, Mason included, develop learning disabilities. Others develop bone and heart problems. Most commonly, patients get tumors on their skin and nerves; Mason already has one on the side of his face. These tumors are usually benign, but even so they can still disfigure. “It continues to grow, and if it runs out of space, it will deform his face outward,” says Konsitzke.
Konsitzke isn’t a scientist by training, but through his job he’s well connected to the scientific world. Once Mason’s diagnosis was in, he started asking around about NF-1 research. In particular, he wanted to know where the bottlenecks are. What was the single thing he could do that would most accelerate research into his son’s condition? And the answer that he kept hearing was: Find better animals to experiment on.
When studying diseases, scientists often turn to laboratory animals like mice and zebrafish. They can use these so-called model organisms to work out how mutations cause diseases, and to find and test possible treatments. But the usual lab animals aren’t a good fit for NF-1. They’re too small, and they don’t react in the same way to the mutations that cause the disease in humans. For example, studies in mice suggested that a drug called lovastatin might help to address the learning and attentional problems that accompany NF-1. But when the drug was tested on actual children, in a large clinical trial, it did nothing.
To better understand NF-1, Konsitzke learned, you need a species that’s closer in both size and biology to a person, and yet is still relatively easy to raise and study. That is, you need pigs. “Pigs closely represent humans,” says Neha Patel, who directs the UW neurofibromatosis clinic. “People with NF-1 have varied cognitive deficits, from severe learning issues to subtle problems. If you imagine studying those in a rat, you’d only get a crude picture of how that translates to humans. But pigs are intellectual animals.”
That’s why, in a quiet corner of the Wisconsin State Capitol, Konsitzke was so excited to meet Shanmuganayagam. Here was someone with experience in raising, engineering, and studying pigs. Here was just the guy he needed to give NF-1 research a boost.
For his part, Shanmuganayagam was keen to take on a new challenge. “I know a lot of diseases,” he says, “but when Chuck told me [about NF-1], I thought, ‘I don’t know what this is.’ And I can’t believe I didn’t know because it’s not that rare.” Indeed, NF-1 affects at least one in every 2,500 babies, making it more common than other better-known genetic disorders like cystic fibrosis. And that figure is probably an underestimate, because many cases don’t present with obvious symptoms. “It’s under-recognized, and kids are not getting the best care,” says Patel. She’ll often see parents whose children clearly meet all the clinical criteria for the disease, and yet hadn’t been diagnosed for over a decade.
The disease has a low profile partly because its symptoms can be so disfiguring. Patients with NF-1 often “become closeted and sheltered,” says Konsitzke. “People don’t look at them or listen to them, so there’s no strong speaking voice for the disease.” The man whom Pope Francis embraced in 2013, whose face was covered in growths, had NF-1. Joseph Merrick, the so-called Elephant Man of 19th-century England, was once suspected of having had NF-1. “Our neuro-oncologist told us to take pictures of Mason now, and be prepared for his face to change,” says Konsitzke.
For different reasons, NF-1 has a low profile in the scientific community, too. It’s caused by mutations in a gene called neurofibromin 1 (confusingly also shortened to NF1), which is daunting to study because of its large size and unusual variability. “People fear it in the research realm,” says Konsitzke. There are more than 4,000 variants of the gene, each of which changes the symptoms of NF-1 in subtle ways.
Again, the pigs can help. Konsitzke and Shanmuganayagam aren’t just planning to develop pigs that can model the symptoms of NF-1. They want to use the revolutionary gene-editing technique known as CRISPR to create pigs that have the specific mutations of a particular individual. Each child with NF-1 would get their own personalized piglet, whose version of the NF1 gene matched their own. The piggy proxy could be monitored to see how the kid’s condition might progress, especially since they mature faster than humans do.
The pigs could also be used to test possible treatments. At the moment, people with NF-1 “are their own guinea pigs,” says Konsitze. “It takes years of tests and side effects to find something that works. My son is on a cocktail of four different meds.” Having an animal that mirrors the particulars of a person’s condition might make it quicker to narrow down the best treatments. And Konsitze argues that this should save enough money to compensate for the costs of creating a customized pig.
Shanmuganayagam and his colleagues started actively trying to make the gene-edited pigs in 2014, with a shoestring budget of $50,000 from the Neurofibromatosis Network, which they bolstered through their own fund-raising efforts. (Two companies also developed pig models of NF-1 around the same time: Recombinetics, based in St. Paul, Minnesota; and Exemplar Genetics, in Sioux Center, Iowa.)
Jennifer Meudt, one of Shanmuganayagam’s colleagues, led the work. “Dhanu said: ‘We’re going to gene edit pigs,’” she says. “And I said: Okay, ha ha, how are we going to do this? My background is in botany.”
The easy way to do it would be to buy pig eggs from slaughterhouses, fertilize them in the lab, do any necessary gene editing, and inject them back into sows. But UW’s pig-rearing facility is kept free of disease-causing microbes, so the team can’t just implant their pigs with commercially sourced embryos. Instead, they have to artificially inseminate their own sows, collect the embryos, inject them with CRISPR ingredients, and put them back into other pigs.
The embryos themselves are hard to work with, because they’re not translucent like those of many other animals. “They look like little black balls, which makes it more difficult to inject when you don’t know where you’re poking,” says Meudt. There are logistical complications too. Meudt has to do the CRISPR injections in a building that’s 30 minutes from the pig facility, so the animals have to get shuttled back and forth. From start to finish, the process to make a single embryo starts at 4 in the morning, and ends at around 10 at night. “Those are well-traveled embryos,” Meudt says.
The team delivered their first edited animal in November 2016, which carried a set of NF-1 mutations that had been reported in an earlier published study. They have since engineered three more animals, each with a different cluster of mutations. All of these are essentially personalized—they represent someone’s case of NF-1, but those patients are anonymized.
The next step will be to create pigs that are openly personalized to specific people—and the team is in the midst of getting approval for that. “We slowed ourselves down for some ethical reasons,” says Shanmuganayagam. For example, if a personalized pig starts to show symptoms, and their child counterpart has not, how should the team communicate that information to a family? What happens if the pig dies early? Should a family even get to know which pig is “theirs”? “We’re trying to resolve that,” says Shanmuganayagam. “We might blind ourselves to whose pig is whose with the option of revealing at a later date. I’ve also had my group read Flowers for Algernon.”


The moment of birth is the moment we transform from an individual into an entire world. We leave the sterility of the womb, pass through a mother’s vagina, and become lathered in her microbes, taking them into our skin, our mouths, our guts. We begin our life as we will always live it: as a community of trillions, enclosed within a single body.
Microbes help their hosts to build their bodies, digest food, and defend against disease, so animals have evolved a multitude of ways for bestowing these tiny partners onto their offspring. Many insects do so at the earliest possible opportunity, adding bacteria directly to egg cells, so that their young are accompanied by microbes from conception. There is literally no part of their life cycle where they are sterile. Others do so in the womb. The tsetse fly, which spreads sleeping sickness, nourishes its grub inside a bizarrely mammalian uterus and feeds it with a milklike fluid—one that’s laden with microbes.
Yet other species have ways of provisioning their young as they greet the world. Humans do so automatically. The Japanese stinkbug coats her jelly-bean-like eggs in a bacteria-rich icing so the hatchlings become colonized when they emerge. Koala moms package the bacteria that allow them to digest tough and toxic eucalyptus leaves into a special kind of poop called pap for their joeys to ingest.
And perhaps the strangest technique is used by the beewolf—a powerful, bee-killing wasp that daubs the walls of her infant’s nursery with a living plaster that she squeezes out of her own head. You can learn more about its bizarre life, and the scientist who studies it, in the video below—the third in a series of online films produced by HHMI Tangled Bank Studios, which adapt the stories in my book, I Contain Multitudes.


The history of antibiotics is a history of running in place. Two years after the first of these life-saving drugs—penicillin—was mass-produced, bacteria that resisted the drug became widespread, too. With grim inevitability, the same events have unfolded for every other drug. Every time scientists identify a new substance that can hold back the tide of infectious disease, resistant superbugs surge over that barrier in a matter of years.
The evolution of drug-resistant microbes is unavoidable, but it’s not instantaneous. And one might reasonably wonder why. Microbes have been around for billions of years. They have had, quite literally, all the time in the world to invent every possible biochemical trick, including ways of defusing antibiotics that they themselves use to kill and suppress each other. So why aren’t all microbes already resistant to all drugs?
“The reason is that resistance, like any superpower, comes at a cost,” says Nina Wale, from the University of Michigan. For example, microbes could create pumps that flush out any killer drugs, but those pumps cost energy to build and maintain. These costs mean that, under normal conditions, resistant microbes grow more slowly than their susceptible peers, and are almost always outcompeted. But antibiotics tip the balance of this competition by finally giving the resistant microbes a huge advantage; their susceptible rivals die off, and they can finally take over.
“That’s our in,” says Wale. “Competition is the force that keeps resistance down in nature. Maybe we can harness that competition to keep them down before they even get going.” She and her colleagues, led by Andrew Read at Pennsylvania State University, have devised a way of preventing the evolution of drug-resistant microbes, by putting them at a competitive disadvantage even when antibiotics are around.
The team proved this concept by studying mice infected with malarial parasites. When Wale and her colleagues treated the mice with the drug pyrimethamine, resistant parasites emerged as expected. But these parasites have a weakness: They’re uniquely hungry for a substance called PABA, which they convert into folate, an essential nutrient. Normally, malarial parasites have other ways of making folate. But these alternatives are shut down by the same mutations that make the parasites resistant to pyrimethamine. So when the parasites evolve to resist the drug, they also become uniquely dependent on PABA for their folate-making needs.
When Wale deprived them of PABA, she not only delayed the emergence of resistant parasites, but completely prevented it. “I was bowled over,” she says. “I plotted the data, and I was sitting on my bed, shaking slightly.”
It’s not that the lack of PABA starves the resistant parasites outright; instead, it makes them less competitive than the susceptible ones. When Wale infected the mice only with resistant parasites, they still became sick. But whenever she infected them with both resistant and susceptible ones, the latter always took over, allowing the pyrimethamine to do its job. That’s encouraging, Wale says, especially because she used tens of thousands of resistant parasites in these competitive experiments—far more than would normally exist when they first emerge in the real world. “Even when the horse has bolted and resistance is already here, by intensifying competition, we can buy ourselves more time,” she says.
Here’s the future that Wale envisions. Rather than simply seeing parasites as targets, we view them as organisms in their own right. We work out the nutrients they need, and how those requirements change as they evolve resistance to drugs. We then identify chemicals that deprive them of said nutrients. These “resource limiters” aren’t meant to kill the parasites, but to put the resistant ones at a perpetual disadvantage so a standard antibiotic can finish off the rest. It’s like “developing anti-spinach” to stop Popeye from becoming strong, Wale says.
“It’s promising,” says Heather Hendrickson, from Massey University. And it’s clearly very effective in this particular setup involving mice and malaria. Whether it would work for other superbugs, including bacteria like staph or E. coli, is a matter of detail. “It will really rely on the strength of competition between the resistant and susceptible versions, and the degree to which we can tip the scale in favor of the drug-susceptible ones,” she says.
Of course, it’s possible that microbes will evolve to resist the resource limiters too. But Wale thinks that’s unlikely. Usually, microbes evolve to resist drugs by getting rid of them, neutralizing them, or changing the molecules that they are designed to attack. But those solutions “wouldn’t work against not being given something,” Wale says.
The Plan to Avert Our Post-Antibiotic Apocalypse
If the approach pans out more broadly, it might give us more options for controlling infectious diseases, beyond just developing more antibiotics. That task has become increasingly difficult. Only a few new antibiotics are in development, and no major new types have emerged for decades. But there are plenty of potential resource-limiting drugs around. They’re often ignored because they don’t kill microbes outright, but they don’t need to be lethal to thwart the emergence of resistance. If scientists can identify these substances, and pair them with existing antibiotics, they could prolong the usefulness of our current arsenal.
“It’s going to be very challenging to find these types of [resource-limiting] compounds,” says Tara Smith, from Kent State University. For example, scientists have long talked about using substances that soak up the metals that microbes require, but that idea “is still more theoretical than practical.” Still, “it’s a good example of the outside-of-the-box thinking we need to preserve antimicrobials.”
“This idea of having something that goes along with an antibiotic and reduces the likelihood of resistance is a very productive field,” says Ramanan Laxminarayan, from the Center for Disease Dynamics, Economics, and Policy. Some groups are working on substances that stop antibiotics from reaching the gut, and fomenting the evolution of resistant superbugs there. Others are developing chemicals that attack resistance genes directly, transforming superbugs back into their civilian alter egos. “These are all different strategies to just coming up with new antibiotics, and I think this is obviously the right way to go,” Laxminarayan says.
“This should certainly be at the forefront of the strategies we test and employ,” says Pamela Yeh from the University of California, Los Angeles. “I think our long battle with antibiotic resistance show us that we'll continue to lose ground against resistant pathogens if we don't consider the environment of the pathogen along with the pathogen itself.”


The email about “a most peculiar object” in the solar system arrived in Yuri Milner’s inbox last week.
Milner, the Russian billionaire behind Breakthrough Listen, a $100 million search for intelligent extraterrestrial life, had already heard about the peculiar object. ‘Oumuamua barreled into view in October, the first interstellar object seen in our solar system.
Astronomers around the world chased after the mysterious space rock with their telescopes, collecting as much data as they could as it sped away. Their observations revealed a truly unusual object with puzzling properties. Scientists have long predicted an interstellar visitor would someday coast into our corner of the universe, but not something like this.
“The more I study this object, the more unusual it appears, making me wonder whether it might be an artificially made probe which was sent by an alien civilization,” Avi Loeb, the chair of Harvard’s astronomy department and one of Milner’s advisers on Breakthrough Listen, wrote in the email to Milner.
A day later, Milner’s assistant summoned Loeb to Milner’s home in Palo Alto. They met there this past Saturday to talk about ‘Oumuamua, a Hawaiian word for “messenger.” Loeb ran through the space rock’s peculiarities, particularly its elongated shape, which looks like a cigar or needle—an odd shape for a common space rock.
For Milner, the object was becoming too intriguing to ignore. So he’s decided to take a closer look.
Breakthrough Listen announced Monday that the program will start checking ‘Oumuamua this week for signs of radio signals using the Green Bank Telescope in West Virginia. The interstellar asteroid is now about twice the distance between the Earth and the sun from our planet, moving at a brisk clip of 38.3 kilometers per second. At this close distance, Green Bank can detect the faintest frequencies. It would take the telescope less than a minute to pick up something as faint as the radio waves from a cellphone. If ‘Oumuamua is sending signals, we’ll hear them.
The chance of an alien detection is, as always, small. But it’s not zero. And Milner thinks we should check—just in case—before ‘Oumuamua is gone for good. The object will pass the orbit of Jupiter next year, and by the 2020s will be hurtling beyond Pluto.
“Whether it’s artificial or not, we will definitely know more about this object,” Milner told me, in a video interview last week.
The new observations will likely be welcomed by the many astronomers who have been scratching their heads for weeks over this space rock. ‘Oumuamua seems to smash many of their predictions about fast-moving interstellar objects, and the more scientists delve into the data, the more puzzles they find.
* * *
The glint of ‘Oumuamua was first spotted by the Pan-STARRS survey telescope in Hawaii during its nightly scan of near-Earth objects like comets and asteroids. Its speed and orbit suggested the object was not bound by the sun’s gravity, and was not of this solar system.
At first, astronomers thought ‘Oumuamua must be a comet, based on decades of scientific literature that predicted its arrival. When our solar system was young, the biggest planets wreaked havoc as they swirled into shape and settled into their orbits. Their movements could jostle nearby material so violently that bits of rock and ice would go flying way out into the universe. The easiest objects to eject were those orbiting at the edge of the solar system, where escaping from the sun’s gravity would be easier. In our solar system, there are far more comets than asteroids lurking near the boundary before interstellar space. Astronomers expected these to be the first interstellar objects they saw.
And so astronomers checked ‘Oumuamua for a coma, a tail of evaporated material that trails comets as they pass near the sun and become heated up. They used telescopes that can detect a sugar cube’s worth of material flying off the object every second. But ‘Oumuamua showed no signs of a coma.
This was the first surprise of many.
Unlike the lumpy, potato-shaped asteroids of our solar system, the 400-meter-long ‘Oumuamua is perhaps 10 times as long as it is wide, an extreme aspect ratio that trumps any of the known asteroids. Astronomers don’t know how the universe could have produced an object such as this. Most natural interactions between an object and its surrounding medium favor the creation of rounded objects, Loeb said, like pebbles on a lakeshore made smooth by lapping water.
Further observations of ‘Oumuamua revealed it carried no traces of water ice, which suggests the asteroid is made of rock or perhaps metal. Whatever it is, the material is certainly sturdy. ‘Oumuamua rotates about every seven hours, a rate that would likely cause some rocky objects, nicknamed “rubble piles,” to crumble. ‘Oumuamua even survived a close pass with the sun in September, before it was detected, without breaking apart.
Thanks to its nonspherical shape, the asteroid is tumbling uncontrollably. “If you take an object that isn’t round and you throw it up in the air, it’ll make this complicated spinning motion,” said Jason Wright, an astronomer at Penn State University. “It just doesn’t just spin nicely along one axis.” Wright said a long journey across the cosmos can slow an object’s tumbling, but ‘Oumuamua has shown no signs of stopping its spinning.
“I’m not saying that any of that is necessarily a smoking gun or super exciting,” Milner said of ‘Oumuamua’s unusual properties. “But I think it warrants thorough investigation from a SETI standpoint.”
Karen Meech, an astronomer at the University of Hawaii Institute for Astronomy whose team discovered ‘Oumuamua, has said their observations are “entirely consistent with it being a natural object.” Analysis of the light reflected by the asteroid shows ‘Oumuamua is red, a color that would be expected for rocky bodies exposed to the cosmic radiation of interstellar space for long periods of time.
There are indeed some natural explanations for some of ‘Oumuamua’s weird properties. Some astronomers say ‘Oumuamua could be a contact binary—two objects that drift closer until they touch and fuse at one end—like our solar system’s Kleopatra, a metallic, dog-bone-shaped asteroid.
They suggest any ice on the asteroid’s surface was zapped away by high-energy particles on its journey between stars. Perhaps the asteroid is so hardy because it formed in the inner regions of a solar system, where rock and metal are more commonly found than ice. This would be tricky, since most exoplanets discovered so far orbit extremely close to their parent star, preventing them from flinging debris beyond the star’s pull. But they may have siblings, like our Jupiter and Neptune, lurking in the darkness, doing the work for them.
If ‘Oumuamua has anything exciting to tell us, it’s that our understanding of planet formation needs some work, said Gregory Laughlin, an astronomer who studies exoplanets at Yale University.“We know that planetary systems are extremely common, but the way that their process unfolds seems to be richer than anticipated,” he said.
* * *
The thought of a spaceship being dropped into planetary systems like a reconnaissance mission may sound like the stuff of science fiction. But for Milner, it’s the future. Milner is spending $100 million over 10 years to develop spacecraft technology capable of sending a tiny probe hurtling at one-fifth the speed of light toward Alpha Centauri, the nearest star system to Earth. If Milner succeeds, the 25-trillion-mile trip would be cut from tens of thousands of years, based on our current technology, to a breezy 20 years. Hundreds of these miniature probes would be deployed into the darkness in the hopes that at least one might complete the journey. Perhaps another civilization already had the same idea.
If that kind of technology were available today, Milner said he would send some kind of probe after ‘Oumuamua. “We need some new propulsion technologies to be able to do this,” he said.
The possibility that ‘Oumuamua is an artificial artifact from an advanced civilization is not spoken in whispers in the astronomy community. But there’s a healthy dose of hesitation in their discussions. Scientists must, after all, exhaust every other plausible explanation before considering ET.
“It’s sometimes mentioned in a half-joking way that people say things when they’re not quite sure whether they want you to take them seriously or not,” said Ed Turner, an astrophysicist at Princeton University. He’s intrigued by a potential SETI observation of ‘Oumuamua, but, like most astronomers, he’s not holding his breath.
“If you were betting your house, I wouldn’t bet it on this,” Turner said.
When they speak of ‘Oumuamua, astronomers recall the tale in Arthur C. Clarke’s 1973 novel Rendezvous with Rama. The year is 2131, and astronomers have detected a mysterious object beyond the orbit of Jupiter and classified it as an asteroid. Their observations reveal the object, named Rama, doesn’t follow the orbit of the sun and must have come from outside the solar system. They send a space probe to photograph Rama and find it’s in the shape of a perfect cylinder. A crewed mission is dispatched. When they land, humans discover the asteroid is an alien spaceship, carrying odd, machinelike beings that pay them little attention. There are no signs of the alien civilization that made them. After some tinkering, the human crew disembarks, leaving Rama to speed out of the solar system.
The story shares some tantalizing similarities with the current circumstances. The most skeptical astronomers point out that, aside from its vessel-like shape, ‘Oumuamua doesn’t have any of the characteristics one would imagine for an alien spacecraft. The object has followed an easy-to-predict trajectory through the solar system. Astronomers have accurately plotted its course forward and backward. Wouldn’t an alien spaceship travel at a fraction of the speed of light, and wouldn’t it slow down to take a look at things as it swept by?
“The explanation that this is a directed probe is, in my view, comically unlikely,” said Konstantin Batygin, a planetary astrophysicist at the California Institute of Technology. “This is just a chunk of debris. I think there’s nothing more to it than that.”
Astronomers predict many more interstellar asteroids, perhaps thousands, are coasting through our solar system, out of view of our telescopes. Pan-STARRS, a survey telescope designed to observe the entire visible sky night after night, found ‘Oumuamua after only four years of operation. Turner suspects the discovery—a fairly quick one—is not a case of pure luck, but a sign of more to come.
Some interstellar asteroids may be hiding, overlooked, in the archival data of Pan-STARRS. Many more will be spotted as other powerful survey telescopes, like the Large Synoptic Survey Telescope in Chile, come online in the next few years.
For Milner, it’s worth examining every interstellar object for signs of artificial technology. They could all be nothing more than space rocks, mindlessly plowing ahead. Or they could be the needle in the haystack. “It would be difficult to work in this field if you thought that every time you looked at something, you weren’t going to succeed,” said Andrew Siemion, the director of the Berkeley SETI Research Center who leads the center’s Breakthrough Listen Initiative.
And so, starting Wednesday at 3 p.m. Eastern Time, the Green Bank Telescope will aim at the first known interstellar object in our solar system. The telescope will observe the asteroid for 10 hours across four bands of radio frequency. The results may be made public within a matter of days.
Milner knows the odds are against him. But as he spoke from a conference table in his home, flanked by screens filled with radiant telescope images stretching from floor to ceiling, he smiled in excitement.
“If you look more, everywhere, I think chances are that eventually you will find something,” he said.


When one person asks another a question, it takes an average of 200 milliseconds for them to respond. This is so fast that we can’t even hear the pause. In fact, it’s faster than our brains actually work. It takes the brain about half a second to retrieve the words to say something, which means that in conversation, one person is gearing up to speak before the other is even finished. By listening to the tone, grammar, and content of another’s speech, we can predict when they’ll be done.
This precise clockwork dance that happens when people speak to each other is what N.J. Enfield, a professor of linguistics at the University of Sydney, calls the “conversation machine.” In his book How We Talk, he examines how conversational minutiae—filler words like “um” and “mm-hmm,” and pauses that are longer than 200 milliseconds—grease the wheels of this machine. In fact, he argues, these little “traffic signals” to some degree define human communication. What all human languages have in common, and what sets our communication apart from animals, is our ability to use language to coordinate how we use language.
I hopped into the conversation machine with Enfield for a very meta chat about the big impacts of tiny words and pauses on human interaction. An edited and condensed transcript of our interview is below.
Julie Beck: Can you explain what the “conversation machine” is and why it’s unique among animals?
N.J. Enfield: When we’re having a conversation, because of the entirely cooperative nature of language, we form a single unit. Certain social cognitions that humans have—the capacity to read other people’s intentions and the capacity to enter into true joint action—allow us to connect up to each other in interaction and ride along in this machine.
Obviously, animals communicate in a range of interesting and complex ways. But where I draw the line is the moral accountability that humans have in interaction. If one person doesn’t do the appropriate thing, for example not answering a question when it’s being asked, we can be held to account for that. We don’t see that in animals. [In humans], one individual can say: “Why did you say that?” Or “Please repeat that.” You don’t see animals calling others out for their failures, asking why did they say that, or could they repeat that. [What’s unique in humans] is the capacity for language to refer back to itself.
Beck: It seems like conversation is always operating on two levels. One is we’re talking about whatever it is we’re talking about, and at the same time, on this more meta level, we’re monitoring the conversation itself and steering it in the direction we want it to go.
Enfield: Exactly. In the book, I mention a psychologist by the name of Herb Clark, at Stanford. He’s made the point for years that language is a tool for coordinating joint action. Let’s say you and I are moving house. All day we’re going to be using language to coordinate our activity. When we lift up a table, we’ll say, like, “One, two, three, lift.” We’re using language to coordinate our physical activity. Herb Clark points this out, and then he says language is used in exactly that way to coordinate the very activity of using language. We might be talking about a subject like what are we going to do on the weekend or whatever, but at the same time we’re using all these traffic signals to coordinate the activity of talking itself. We’re sending little signals like, “Wait, I’m not ready to finish my turn yet,” “What was that? I didn’t catch what you said,” “Yes, I’m still paying attention to you.” Language regulates itself.
Beck: One of the ways we do that is how quickly people respond to each other, right? You write that people usually respond to each other in a conversation within 200 milliseconds. What if you take longer to respond? What signal does that send?
Enfield: It could mean a few things. The fact that this is average, 200 milliseconds, suggests people are aiming for that. So if you are late, it suggests you were not able to hit that target because of some trouble in finding the words you wanted. Or maybe you didn’t hear what was said, or maybe you were distracted in some way. That delay is caused directly by some kind of processing problem. And if you ask people difficult questions, their answers will tend to be delayed
One of the big traffic signals that manages that is these hesitation markers like “um” and “uh,” because they can be used as early as you like. Of course, they don’t have any content, they don’t tell you anything about what I’m about to say, but they do say, “Wait please, because I know time’s ticking and I don’t want to leave silence but I’m not ready to produce what I want to say.”
There’s another important reason for delay, and that is because you are trying to buffer what we call a “dis-preferred response.” A clear example would be: I say “How about we go and grab coffee later?” and you’re not free. If you’re free and you say, “Yeah, sure, sounds good,” that response will tend to come out very fast. But if you say “Ah, actually no, I’m not really free this afternoon, sorry,” that kind of response is definitely going to come out later. It may have nothing to do with a processing problem as such, but it’s putting a buffer there because you’re aware saying “No” is not the thing the questioner was going for. We tend to deliver those dis-preferred responses a bit later. If you say “no” very quickly, that often comes across as blunt or abrupt or rude.
The way we play with those little delays, others are very sensitive to what that means. A full second is about the limit of our tolerance for silence. Then we will either assume the other person’s not going to respond at all, and we just keep speaking, or we might pursue a response.
Beck: Maybe I shouldn’t tell you this, but one of the things that they tell you to do if you’re doing an interview is to just wait. If they’re not responding, just sit there quietly, because people get uncomfortable and then they just keep talking.
Enfield: Exactly. The interesting thing about it is you as an interviewer have to suppress quite a strong tendency to jump into that space. It’s a skill you’ve got to learn to do. I think people naturally don’t feel comfortable with that silence. Once you’ve got that one second going by, somebody’s got to do something. Unless it’s a situation where you’re with your loved ones in your house or you’re on a long car drive or something like that. Obviously, we can lapse into silence and that’s not a problem, but if we’re in the middle of a to-and-fro conversation, we’re generally not going to let that happen.
Beck: So I’m going to transcribe this Q&A later, and I’m going to edit all of those filler words like “um” and “uh” and “well” out of this interview, as I always do. But you write that these words are actually extremely important to conversation. What am I going to lose by cutting all of that out of this transcript?
Enfield: I think it’s the right thing to do, to edit it out when you write things down. You’re not going to lose anything too significant, and the reason is you’ve changed the context completely in which people are going to consume those words. At the moment, the words I’m producing are being interpreted by you in real time. Things never come out perfectly, and we have to edit on the fly. That’s what these words do. What they’re doing is telling you, “No, that word is not what I meant, I’ve doubled back and I’m now going to replace that word with this word.” Or, “Wait a second, I’m about to get the word I’m looking for.” But as soon as you transcribe those, people are not consuming the words at the same time and place as I’ve created them. Those “ums” and “uhs” just become superfluous.
Beck: So you don’t need the words that you use to edit yourself anymore because I’m literally editing you?
Enfield: Exactly. The thing about my book is that as a reader, you don’t know how many times I’ve rephrased a sentence. But you can’t hide that from someone in interaction because you’ve got the time pressure of turn taking. What we’re doing, it’s messy, there’s no getting around that. And that is completely hidden from view when you write something down and publish it because no one’s going to get access to all the drafts. But conversation is all draft.
Beck: Another thing you mentioned that I thought was super interesting was the way people use “um” as a way to claim more conversational space for themselves. Can you talk about that?
Enfield: In any form of interaction, we don’t have access to each other’s minds. It’s the classic problem of human life in a way. Things like “ums” and “uhs” signal there’s some delay in processing. But as a speaker, what I can do is exploit those kinds of signals. I can use them dishonestly. I can use something like “um” to give the overt signal that I’m having some sort of trouble with processing, but in reality, all I’m doing is trying to claim more ground and get you to keep waiting for me to finish.
All words can be used to lie. Whether they’re nouns and verbs, or whether they’re traffic signals, we can exploit them in dishonest ways. If you want to game the system, and all you want to do is hold the floor, then words like “um” can be exploited in that way. Obviously, there are limits to it. People are sensitive to these things, and after a while if you’re trying to dominate the floor, people will either wise up and grab it back or they will just get sick of you.
Beck: Another thing I do in interviews all the time, that I’m doing right now and I’m also going to cut out of the transcript, is I say “mm-hmm” a lot while the person is talking. It makes sense; it’s me just signaling that I’m still listening. But how important is that to our experience of conversation? If I wasn’t “mm-hmm”-ing, would that make a difference to how our conversation goes?
Enfield: Yeah, it would make a big difference. When you’re saying “Mm-hmm, uh-huh,” you’re really playing an important role in the smooth operation of this conversation machine. In the book, I talk about a study done by Janet Bavelas in Canada, with her colleagues. They brought people into the lab, they asked them to get into pairs, and they’d just randomly nominate one of them and say, “Think of a near-miss scenario you had and tell that to the other person.” The listener will look at them, they’ll nod, say “Uh-huh, mm-hmm,” and when the person gets to the punchline, they’ll say things like “Wow.”
Then they had a special condition where they tried to distract the listener. They said, “You have to press this button underneath the desk every time the person who’s telling their near-miss scenario uses a word that begins with the letter T.” It completely distracted the listeners from actually following along the content of the story. They produced many fewer of those “uh-huhs” and “mm-hmms.” It also meant the timing of them was kind of out of whack, and they didn’t really recognize when the speaker had reached the climax of the story—the moment when they’re supposed to say “Oh, wow.” They showed that when you distract the listener, then the storyteller tends to circle back and repeat themselves. They essentially become a less proficient and less fluent storyteller. It was a powerful demonstration of precisely the importance of those types of feedback markers for the performance of the person who is telling the story itself.
Beck: You talk a lot in the book about the “moral architecture” of conversation. Explain what that means in the context of these little traffic signals. What does using words like “um” have to do with morality?
Enfield: Morality’s a strong word. When you use that word, people think, “Oh you’re talking about is it okay to have sex with animals” or whatever. Thinking about grand moral questions. I’m talking about a much simpler code. In general, morals tell us how we should live. In the moral architecture of language, they tell us how we should talk. What the moral code does is it licenses us to hold other people accountable to that code. Like, “Hey I asked you a question,” that would be an example. I might not be saying it explicitly but I’m implying, “That’s bad. You shouldn’t be silent when I’m asking you a question, you should respond.”
When it comes to little words, I produce “um” and “uh” as a signal to you that I know I should be speaking right now. The right thing to do is to be speaking fluently, moving the conversation forward. The whole motivation for my producing those little traffic signals is to make clear that, despite current appearances, I am aware of and I’m following the basic stipulations of what it takes to produce an appropriate conversation. It’s that whole moral architecture that human beings have, it’s the root of so much of our cultural life and our social life: the defining of what’s appropriate, what’s inappropriate, and policing those things and judging others on the basis of those things. And in these extremely subtle ways it’s right there in every conversation that we have.



Olive oil is not what you think it is. According to Tom Mueller, the author of Extra Virginity: The Sublime and Scandalous World of Olive Oil, an olive is a stone fruit like a plum or cherry—meaning that the green-gold liquid we extract from it “is, quite literally, fruit juice.” And, while we’re blowing your minds, have you ever stopped to wonder what “extra virgin” means? “It’s like extra dead or semi-pregnant,” Mueller said. “I mean, it doesn’t make any sense at all.” This episode we visit two groves—one in the Old World, one in the New—to get to the bottom of olive oil’s many mysteries. Listen in this episode as we find out why the ancient Romans rubbed it all over their bodies, and whether the olive oil on our kitchen counters really is what it says on the label.
Olive oil’s original home lies along the shores of the Mediterranean, where its wild ancestor, the oleaster, can still be found today. Somehow, people realized that the bitter berry from these hardy trees tasted excellent when brined in salt and, even better, could be crushed to produce a liquid fat that was not only delicious but, Mueller says, burns as hot as benzene and has twice the heat-energy content of carbon. By the seventh century BCE, olive-oil production was taking place at industrial scale: Olive presses excavated at Ekron, in modern-day Israel, were capable of producing 500,000 liters of oil a year. The demand was equally enormous: Olive oil powered lamps and preserved and enhanced food, and it was used as an all-purpose medicine, a contraceptive—even an aphrodisiac. Olive oil was so critical to Greek and Roman culture that wars were fought over it and fortunes made, much like the petroleum sheikhs of today.
Today, olive oil is more popular in the kitchen than in a lamp, but it still enjoys a superior status to its fellow fats—particularly since the 1980s, when it began to be promoted by medical researchers as a key component of the heart-healthy Mediterranean diet. But all is not well in the olive groves. In Italy, millions of olives have already been killed by Xylella fastidiosa, an insect-borne pathogen that was detected in 2013. The disease can cause mature trees to die of thirst within two years. We speak to Rodrigo Almeida, a professor of environmental science at the University of California, Berkeley, to find out how worried we should be about the future of Italian olive oil. Meanwhile, olive-oil fraud is rampant in general, but especially in the United States, as profits from fraudulent oil can be more lucrative than dealing cocaine.
But fear not: We won’t leave you on this depressing note. Instead, olive-oil growers Anna Casadei of the Castello del Trebbio in Tuscany, and Kathryn Tomajan and Robin Sloan of Fat Gold in Sunol, California, lead us through harvesting, milling, and, most importantly, tasting, in order to equip us to buy, use, and love olive oil nearly as much as they do. Listen in now, and then enjoy a big glug of oil over your veggies—heck, we won’t judge if you want to rub it on your skin, too!


When Mike Brown first proposed that a hidden, massive planet lurks in the outer reaches of our solar system, he was confident someone would prove him wrong. “Planet Nine,” as the hypothetical world was nicknamed, was his explanation for the strange movements of half a dozen distant, icy planetoids that are farther away and smaller than Pluto: In theory, this huge, somehow-undiscovered planet could sway their orbits. But surely astronomers would be quick to find a more obvious explanation.
“Shockingly, in a year and a half, nobody has,” says Brown, an astronomer at the California Institute of Technology. “There have been so many claims of planets in the last 170 years, and they were always wrong. But I’m clearly a true believer at this point.”
Brown, the self-titled “Pluto Killer” who led the campaign that demoted the dwarf planet, and Konstantin Batygin, his coauthor at Caltech and a young star who plays in his own rock band, know how to spark debate. Since their proposal about Planet Nine, the lack of definitive evidence for or against its existence has divided the planetary community. Other astronomers have put forth alternative explanations, and some contend Brown and Batygin’s data are biased. Until someone clearly spots the new mystery planet in a telescope, they’ve come to an impasse.
The peculiar clustering of the six faraway objects that Brown and Batygin’s initial hypothesis highlighted is extremely unlikely to happen just by chance. According to the duo’s mathematical arguments, it would be naturally explained, though, by a planet about 10 times as big as Earth in the region known as the Kuiper Belt. Batygin has come up with more recent evidence, too: The orbits of other distant solar-system objects yoked to Neptune have gotten “detached” as well, and other objects’ orbits have gotten tilted to the side or reversed, so the solar system as a whole no longer resembles a thin record or CD with the sun at the center. Planet Nine, if it exists, could explain all of those phenomena.
“When all of these things come together, it’s becoming evident that without Planet Nine, the solar system has these weird puzzles and features that stand out,” Batygin says. His approach echoes that of astrophysicists when they inferred the existence of dark matter—which still can’t be seen—based on the rapid motions of stars in the outer realms of galaxies, and then clinched the idea with a more diverse array of evidence.
But other experts remain skeptical. Even the most basic facts are in dispute. Scientists with the Outer Solar System Origins Survey, or OSSOS, argue that Brown and Batygin’s data are actually biased by factors like bad weather and their telescope’s location, which could influence what’s seen and what’s missed, thereby introducing a spurious trend. If this is true, then there’s nothing weird about the little balls of rock and ice in the first place. The OSSOS researchers say these objects mostly seem randomly oriented and not aligned by some unseen force. If there’s no clustering, then Planet Nine’s linchpin disappears.
“While my research is skeptical of this planet, that’s not at all to say there isn’t a planet out there,” says Cory Shankman, an astronomer at the University of Victoria who led the research. He advocates for continuing the search for these hard-to-detect objects and understanding the biases while doing it. It’s slow, painstaking work.
Shankman’s survey only covered one-twentieth of the sky, however. Other astronomers, such as those affiliated with the Dark Energy Survey, question their findings, just like Shankman questions Brown and Batygin’s. “The more objects you can find that bear on the story, the easier it is to talk about them as a population rather than a small handful of curiosities” says David Gerdes, a University of Michigan astrophysicist. One way or another, he believes, the answer will be clear within the next year or two.
A theory’s more powerful if it doesn’t just explain what’s already known, but also makes successful predictions about things that haven’t been seen yet. If scientists find more objects throughout the Kuiper Belt and these objects are hardly clustered at all, it will deal a blow against Planet Nine. If the objects are similarly clustered as Brown and Batygin expect, it will strengthen their case.
And yet, another possibility remains, which is that the only solutions people find are the ones they have access to, like the proverbial story of the person who lost their keys at night and only looks for them under the streetlight. “Scientists are often good at doing contrastive assessments, like a Sherlock Holmes–style argument: Here’s my suspects, and here’s the one most likely to have done it,” says Christopher Smeenk, a philosopher of science at the University of Western Ontario. “But do you have the right list of suspects?”
There have been many planetary misses in history, Smeenk points out, such as 17th-century claims of a moon orbiting Venus, which better data demonstrated not to exist. Two centuries later, astronomers attributed Mercury’s slightly peculiar orbit to the gravitational forces from an unseen inner planet, dubbed Vulcan. But when Albert Einstein’s theory of general relativity came along, it explained the orbit, debunking Vulcan claims.
In the case of Planet Nine, Ann-Marie Madigan, an astrophysicist at the University of Colorado Boulder, believes everyone has missed a key suspect. The gravitational forces in the outer solar system could be more complicated, and the unlikely alignment of those icy bodies could all be a temporary coincidence.
She argues that there are millions, if not billions, of planetoids—more than previously thought—orbiting in that distant, dusty disk of material around our solar system. Most astronomers have assumed that the forces of these tiny objects are so small that they can be ignored in models, and it’s difficult to model their behavior. But Madigan includes them all in her models, and has found that, as they orbit over and over again, the nebulous pull of their gravity gently and gradually clumps some objects together over time. This “self-gravity” mechanism, as she calls it, could explain the other lines of evidence brought forth by Batygin as well.
“People think gravity’s dominated by Jupiter, Neptune, Uranus, and Saturn, and they’re not really thinking of the collective effects of all the smaller bodies,” Madigan says. “The main criticism I get from Planet Nine advocates is that there’s no evidence for such a mass of small bodies. But I don’t pay too much attention to that, because they haven’t seen Planet Nine yet either.”
Incidentally, both Batygin and Madigan invoke the principle of Occam’s razor, the notion that the simplest explanation is likely the correct one. But they come to completely different conclusions, highlighting that this seemingly straightforward principle is actually rather complicated, with no clear answer yet in sight.


On Wednesday, a team of scientists unveiled a newly discovered dinosaur that had the body and sickle-clawed feet of Velociraptor, the head and snout of a swan, and weird arms that were somewhere between grasping limbs and flattened flippers. This bizarre murder-swan, which the team christened Halszkaraptor, was so odd that when they first saw it, they suspected that it was a fake—a Frankensaur that had been assembled from parts of different dinosaurs. “All of us thought, when we first saw it: Oh come on now,” says Philip Currie from the University of Alberta.
But after using a particle accelerator to scan the animal, and the rock in which it is still encased, Currie and his colleagues are convinced that it’s the real deal.
Apparently This Is What a Swimming Dinosaur Looks Like
Not everyone is so sure, though. When Steve Brusatte from the University of Edinburgh first saw a picture of Halszkaraptor, his spidey sense also started tingling. Its posture, from the curve of its tail to the way its claws were almost perfectly fanned out, looked strange, as if it was ready for display. Its body looked like that of Velociraptor, but its skull looked like one from a different dinosaur group—the alvarezsaurs. And most worryingly of all, the specimen has a convoluted history. It was poached from Mongolia (as many dinosaurs are), and smuggled into Japan and Britain, before ending up in a private collection in France—a meandering route that offered few reassurances and many chances for tampering.
“This new discovery has thrown a lot of us for a loop,” he says. “It’s either really a new dinosaur, which would be awesome, or it’s been tampered with and I really hope that’s not the case. The authors worked really hard to demonstrate that it hasn’t, using the best tools at their disposal. But I wonder if we have the right tools, since the fakes are so sophisticated.”
No matter their views on Halszkaraptor, everyone I spoke to agreed that fossil forgery is a surprisingly serious problem. There’s a long history of hoax fossils, like the infamous Piltdown Man—a human skull that was glued to an orangutan’s jaw, and presented as a missing link in human evolution. That was in 1912, but a century on, fake or doctored specimens still abound in fossil shows, private collections, and even in small regional museums.  
These bogus specimens aren’t cries for attention. “In my experience, I’ve never seen a single specimen that was made by the scientific community to get more publicity,” says Mark Norell, a curator at the American Museum of Natural History. Instead, they’re almost always the work of people trying to earn a buck—often poor farmers in China.  
There is a lucrative market in fossils, which owes much of its existence to Sue, a beautifully preserved Tyrannosaurus that was unearthed in 1990 and auctioned for $7.6 million. After her sale, “a modern gold rush began, and it has not let up,” wrote Paige Williams in The New Yorker. Fossils became status symbols, attracting wealthy buyers and unscrupulous vendors. An extensive black market arose, connecting fossil-rich sites like China and Mongolia and anyone with money to burn and a penchant for trophies from another age.  
The fakes are sometimes created from whole cloth—or rather, from whole plaster. These are usually crude and easy to recognize. Currie remembers seeing photos of an alleged Archaeopteryx—a pivotal species which helped to show that birds evolved from other dinosaurs. It was in Beijing, but after Currie flew over, “within three seconds of seeing it, I knew it was 100 percent fake,” he says. The weight of it was wrong. The texture of the bone was wrong. No actual paleontologist would be fooled, but Currie found several such fakes in tourist shops, all with expensive price tags.
In many ways, the market in forged fossils is similar to the one in forged artworks, says Norell, who has a foot in both worlds. When he attends the gargantuan annual fossil show in Tuscon, he takes the same equipment that he uses to examine fine art, like a special ultraviolet light. “I have a carbide needle that I can heat up with a lighter and push onto something,” he says. “If it’s real, guys will let you do it. If it has epoxy, they won’t.”
Some phonies are harder to spot. Currie recalls seeing a Tarbosaurus—a Mongolian relative of Tyrannosaurus—that “looked pretty good and was sold for hundreds of thousands of dollars.” When his team analyzed it with a medical scanner, they showed that just a single bone in the jaw was real. “It was convincing enough that we couldn’t be sure how much was real and how much wasn’t.”
More often, actual fossils are touched up to make them more dramatic. Another Tarbosaurus, which was famously seized by the American government in 2012, was mostly real, but its teeth, fingers, and toes had all been reconstructed. Feathered dinosaurs are all the rage, so artisans will sometimes paint the outlines of feathers on their fossils. One dinosaur had a crest added to its skull by fossil-poachers, who wanted to make it look more dramatic and complete; scientists spent so much time removing the extra material that, in a bit of questionable victim-blaming, they named the beast Irritator. And by some estimates, around 80 percent of the marine reptile specimens that are on display in local and regional Chinese museums have been “altered or artificially combined to varying degrees.”
The vast majority of sham fossils are chimeras—two or more actual specimens that have been glued together to make them seem new. Sometimes, the work is ... not well-researched. Currie once saw a Confuciusornis—a crow-sized Cretaceous bird—that was clearly fake because its foot had been glued directly to its knee, with no shinbone in the middle. It was on display in a Chinese museum.
But these pseudosaurs can be far more convincing. In the Gobi desert of Mongolia, dinosaurs are often found in soft sandstone. Fakers have been known to crumble this rock and then re-harden it to glue separate specimens together. “It can be really hard to tell if that’s fake,” says Brusatte. “Is that original sandstone or sandstone that’s been put back together?”
Mostly, these counterfeits have little impact on what we know about dinosaurs. They don’t feature in high-profile papers, and they stay out of the limelight. But there are exceptions. In 1998, the owners of a small Utah museum acquired a specimen from China. The beast was never described in a scientific journal, but that didn’t stop National Geographic from hosting a press conference about it, or featuring it in the magazine as Archaeoraptor—a “true missing link in the complex chain that connects dinosaurs to birds.”
“The specimen was suspicious right from the beginning,” says Currie, who was one of the scientists called in to examine it. The tail didn’t seem to connect to the body. The feet were exactly symmetrical in a way that real limbs just aren’t, as if one had been made using the other as a template.
It turned out that Archaeoraptor was the dinosaur equivalent of a horse costume, with the front and back halves played by different actors. Someone had glued the head and upper body of Yanornis (a primitive bird) to the tail of Microraptor (a four-winged Velociraptor cousin) and the legs and feet of an unidentified animal. Some missing parts had been filled in with a paste that was made from ground-up bone. It looked like bone at first glance because it was, but a medical CT scanner revealed the trickery: The fakes lacked the complex internal structures of real bones.
“The Archaeoraptor fiasco really brought the issue home to dinosaur workers,” says Brusatte. “There hasn’t been a big forgery like that in dinosaurs in the last 15 years. But we’re due for something like this to come up again and I just hope that it’s not me who’s fooled by it.”
He has reason to worry because by his own admission, some of the fakes are “really, really good”—good enough to fool even the trained eye. And the wider problem is that no one really knows which techniques are good enough to distinguish fossils from faux-sils. CT scans helped to pierce the Archaeoraptor illusion, but “looking at a CT scan is oftentimes like looking at a Rorschach test, or at tea leaves,” says Brusatte. “You can see many things in there.” Would it be good enough to detect sandstone that had been joined together with reconsolidated sandstone? Who knows?
That’s why Currie and his colleagues scanned Halszkaraptor using a synchrotron, producing scans that are much higher in resolution. “You can look at the continuity of the bones and the rocks, and whether there’s any gluing or infilling,” he says. They could even show that the chemical composition of the rock is consistent across the fossil. To fake that, someone would have had to find two specimens from the same site, and join them together using rock that also had the same origins.
Currie is highly respected among his peers, and his name gives weight to the claims around Halszkaraptor, even among the skeptics. I asked him how certain he is that the specimen is real, on a scale of 1 to 10. “I’m at least a 9,” he tells me. “You can never say that you are 100 percent sure, because some of these people are really incredible artisans.”
“You’re running to catch up with the forgers,” says Brusatte. “I don’t know what the definitive evidence would be either way, or what sort of burden of proof would be enough. The only thing that can prove that it’s genuine is another team finding another skeleton and digging it up themselves. And I hope that happens.”


I never thought we were going to die. Even when the canyon air filled with smoke, when the flames came rushing up, when darkness fell and the sky glowed red both behind and ahead of us. So, okay, it was a little scary. But we were just a short drive from Portland, Oregon, on a well-traveled trail my family had hiked a dozen times in the last 10 years. No one dies in a forest fire when they’re that close to home. We weren’t outdoorsy enough to die in a forest anywhere. Or so it seemed to me.
On the West Coast, the 2017 onslaught of forest fires has been widespread and relentless—a char stretching from South Cariboo, British Columbia, last summer to the Caravaggio exhibit in the Getty Center just above west Los Angeles Thursday. Blazes are striking with growing regularity in the region, sparked in part by drought and record-breaking heat. Seven of California’s 10 largest modern wildfires have come in past 14 years.
The news coverage of these fires plays like a disaster movie. Forested hills wrapped in a devil’s fiery cloak. Well-tended homes reduced to scorched concrete and melted bikes. By this point, Americans are used to watching with a mix of horror and curiosity. But as the frequency of wildfires increases, it’s also more likely for people on the West Coast to find themselves in their paths—and not always because they’re away from home.
My experience began on a sunny Saturday afternoon in early September. Labor Day was the first time my wife and I had all three of our busy, nearly grown kids with us since Christmas. Figuring we’d spend the day together romping around Oregon’s natural playground, we drove 40 miles east to the Eagle Creek Trail, a path that follows a waterfall-clotted river on an uphill climb toward the richly forested Bull Run watershed.
When we arrived just after noon, the parking lot was so crowded that we had to double back and bootleg a spot on the side of the road. Setting out through a thicket of multigenerational tourist families, taut hikers, cooler-toting beer dudes, toddler-chasing couples, and dozens of other Oregon types, we continued for three breezy miles, had a shady lunch at the High Bridge, and after an hour or so headed to the Punch Bowl Falls swimming area for a cooling dip. We were back on the trail at 4 p.m. for the gentle two-mile stroll down to the car. A sweet end to a lovely afternoon, right until one of my sons, Teddy, came sprinting back from walking a few hundred feet ahead of us.
The trail was on fire, he shouted. In fact, the entire hillside was ablaze.
Thinking dad thoughts, I made a few steps forward to check it out, but Teddy put his hand up to my chest. You do not want to get closer.
Twenty minutes earlier, a few feckless teenagers had tossed a smoke bomb from the trail, and hell had been unleashed. Our cheerful family outing had sent us into the maw of a deadly siege. But that was absurd. So I sighed and girded myself for something more reasonable: an unexpected pain in the ass.
Back at Punch Bowl Falls, we spread the word among the 100-plus hikers, swimmers, and out-of-town visitors and joined them all in standing meekly on the riverbank watching the thin yellow haze of smoke grow into a seething black curtain. Forest Service choppers buzzed in to drop water on the blaze, made no impact, went off to reload. A smaller helicopter fluttered in to drop a note, but the instructions were vague: WE SEE YOU ... STAY PUT ... DANGER! Then that chopper was gone for good.
We stayed put on the riverside. Some teenagers tossed a Frisbee. Parents played with their kids. A few well-put-together women stood together and fretted. One asked me: Is this one of those fires that sucks the oxygen away and suffocates us right here?
I made a face. Of course not.
How do you know?
Because it just isn’t.
Denial is definitely not a smoke-wreathed river in Oregon, but I had to maintain some level of control of the situation. Not far away a ball of fire shot through the dense black smoke.
I kept peering past the tree line, waiting for some kind of cavalry to come galloping in. Instead, a 30-ish fellow with short dark curls and a beard climbed up on a boulder and called out for attention. He was Technical Sergeant Robert Dones of the U.S. Air Force, a veteran of two tours in Afghanistan. After scoping out the movement of the fire, he had some ideas. The flames were too close for us to stay on the riverbank. We needed to get out now, and no matter how fast, how slow, how young or old, we were all going together. We would stay together until we were all safe.
The cavalry, it turned out, had been among us the whole time.
Off we went, away from one fire and bound for another trail that the map said would lead to a road where rescuers would be waiting. There was one problem: The path would lead us to the hem of the Indian Creek Fire, a blaze that had been burning out of control since July. There was no way of knowing if the fire would consume the trail before we got there, but given no other options, we crossed our fingers and kept moving.
Through the gloaming and into the darkness, we trooped lit by cellphones across vertiginous shelves of cliffside scree. Five, eight, a dozen miles. Eventually, the canyon air thickened with smoke and the sky just above the western trees went red. The Indian Creek Fire was just over the hill, so close that its embers drifted around us—some tickling new flames from the underbrush. A Forest Service ranger hiked down to help us, and when she came near me, I pointed to a fresh blaze taking hold 100 feet from where we had stopped. “That doesn’t matter,” she replied in a taut monotone.
Two miles beyond the embers, we stopped again to rest for a few hours. My family had no clothes beyond our T-shirts and shorts, and only 10 carrot sticks left over from lunch. Hungry and exhausted, we huddled together in the dirt and tried to sleep. The plan was to stay until dawn, but the Eagle Creek fire picked up speed in the night, and the Indian Creek blaze was closing in, too. We were rousted an hour before dawn into a thick, scratchy fog of smoke. After an hour or two of hiking, we started to meet Forest Service firefighters carrying their axes and water jugs to the front of the Indian Creek fire.
Three hours later, we got to Wahtum Lake and trudged up the officially named Stairway to Heaven, a long, steep set of wood-and-dirt stairs, to find two dozen rescuers, a table full of military MREs, and school buses bound for safety.
Too exhausted to feel much of anything beyond relief, we located places on the bus and sat numbly through the hour-long ride back to where we’d left our cars less than 24 hours earlier. We were home 90 minutes after that, finding everything just as we had left the morning before. I took off my boots and hosed them down, trying to wash the smoke and ash and filth, and left them on the back porch to dry.
The next morning, I woke up to see smoke hanging above our yard. Taking my cup of coffee out to the back porch, I found my boots where I’d left them, just as ash-covered as they’d been before I’d hosed them out the day before. The winds had changed direction. We’d made it home, but the fire was still coming for us.


When Mark DePristo and Ryan Poplin began their work, Google’s artificial intelligence did not know anything about genetics. In fact, it was a neural network created for image recognition—as in the neural network that identifies cats and dogs in photos uploaded to Google. It had a lot to learn.
But just eight months later, the neural network received top marks at an FDA contest for accurately identifying mutations in DNA sequences. And in just a year, the AI was outperforming a standard human-coded algorithm called GATK. DePristo and Poplin would know; they were on the team that originally created GATK.
It had taken that team of 10 scientists five years to create GATK. It took Google’s AI just one to best it.
“It wasn’t even clear it was possible to do better,” says DePristo. They had thrown every possible idea at GATK. “We built tons of different models. Nothing really moved the needle at all,” he says. Then artificial intelligence came along.
This week, Google is releasing the latest version of the technology as DeepVariant. Outside researchers can use DeepVariant and even tinker with its code, which the company has published as open-source software.
DeepVariant, like GATK before it, solves a technical but important problem called “variant calling.” When modern sequencers analyze DNA, they don’t return one long strand. Rather, they return short snippets maybe 100 letters long that overlap with each other. These snippets are aligned and compared against a reference genome whose sequence is already known. Where the snippets differ with the reference genome, you probably have a real mutation. Where the snippets differ with the reference genome and with each other, you have a problem.
GATK tries to solve the problem with a lot of statistics. DNA-sequencing machines sometimes make mistakes, so the GATK team studied where the machines tend to made mistakes. (The letters GTG are particularly error-prone, to give just one example.) They thought long and hard about things like “the statistical models underlying the Hidden Markov model,” per DePristo. GATK then gives its best guess for the actual letter at a certain location in DNA.
DeepVariant, on the other hand, still does not know anything about DNA-sequencing machines. But it has digested a lot of data. Neural networks are often analogized as layers of “neurons” that deal in progressively more complex concepts—the first layer might respond to light, the second shapes, the third actual objects. As DeepVariant is trained with data, it learns which connections between “neurons” to strengthen and which to ignore. Eventually, it can sort the actual mutations from the errors.
To fit the DNA-sequencing data to an image-recognition AI, the Google team came up with a work-around: Just make it an image! When scientists want to investigate a mutation, they’ll often pull up the aligned snippets, like so:
“If humans are doing this as a visual task, why not present this as a visual task?” says Poplin. So they did. The letters—A, T, C, or G—got assigned a red value; the quality of the sequencing at that location a green value; and which strand of DNA’s two strands it is on a blue value. Together, they formed an RGB (red, green, blue) image.
And then it was simply a matter of feeding the neural network data. “It changes the problem enormously from thinking super hard about the data to looking for more data,” says DePristo.
Between publishing a preprint about DeepVariant last December and the release this week, the team continued improving the tool. Instead of three layers of data—represented by red, green, and blue—at any location in the genome, DeepVariant now considers seven. It would no longer make any sense as an image to the human eye. But to a machine, what’s just a few more layers of numbers?
To be clear, DeepVariant itself is unlikely to change genetics research. It is better than GATK, but only slightly so—and it is half as fast depending on the conditions. It does, however, lay the groundwork for AI’s influence in future genetics research.
“The test will really be how it can translate to other technologies,” says Manuel Rivas, a geneticist at Stanford. New sequencing technologies like Oxford Nanopore are becoming popular. If DeepVariant can quickly learn variant calling under these new conditions—remember the humans took five years with GATK—that could speed up the adoption of new sequencing technologies.
DePristo says that the idea of layering data on top of each location in the genome could easily be applied to other problems in genetics—the more important of which is predicting the effects of a mutation. You might imagine layering on, for example, data on when genes are active or not. DeepVariant started off with just three layers of data. Now it has seven. Eventually it might be dozens. It won’t make much sense to a human brain anymore, but to an AI, sure.


Massive wildfires are raging across Southern California, threatening thousands of homes and cultural landmarks like the Getty Museum in Los Angeles. Some of the largest fires were still barely contained by Wednesday afternoon.
It’s been an unusually bad year for the state—amid an unusually bad year for the West at large. Fires in California have destroyed more than 6,000 structures and incinerated hundreds of thousands of acres. Montana and British Columbia both also had some of their worst wildfire seasons ever.
Of course, most years are bad wildfire years now. Seven of California’s 10 largest modern wildfires have occurred in the last 14 years. (The state began keeping reliable records in 1932.) Given the scale of the blazes, and their increasing regularity, it makes sense to ask: Does global warming have anything to do with this?
The answer isn’t as clear-cut as it was this summer, when drought- and heat-stoked fires raged across the Rockies and Pacific Northwest. Instead, a mix of forces are driving the fires in Southern California, and only some of them have a clear connection to global warming.
“These fires are not immediately emblematic of climate change,” said John Abatzoglou, an associate professor of geography and climate at the University of Idaho, in an email. “Yes, California did have the warmest summer on record. But the big anomaly here is the delay in the onset of precipitation for the southland that has kept the vegetation dry and fire-prone.”
In other words, late-fall and winter rains would normally end California’s fire season in November. Because those rains haven’t yet arrived, the blazes continue.
“At least in Southern California right now, we are largely seeing textbook wildfires,” said Alexandra Syphard, a senior research scientist at the Conservation Biology Institute who studies fires. “Wind-driven fire events occur most typically in the fall, but can also occur like this, later in the year with fast-spreading, ember-driven fires under Santa Ana wind conditions.”
Here are some of the biggest factors that are shaping the wildfires in California—and how global warming is or isn’t changing them:
The Santa Ana winds
Blame for the wildfires in Ventura and Los Angeles counties lies first and foremost with the Santa Ana winds, famously hot and desiccating gusts that blow from the desert to the coast. The Santa Anas also set the stage for the massive wildfires in Napa and Sonoma earlier this year.
Fires depend on two variables—an ignition source and fuel to burn—and the Santa Ana winds increase the availability of both. First, they dry out vegetation, creating more fuel across the landscape. Second, they blow trees and other debris against power lines, providing the source of a spark.
When the Santa Anas blow this late in the year, they can start fires. In fact, writes Abatzoglou, “all December fires in the southland since 1948 have been associated with Santa Ana wind.”
But there are few signs—at least so far—that the Santa Ana winds are becoming more prevalent or that they’re systematically moving later in the year. The peak of Santa Ana season usually comes in September or October. There is no trend toward more or fewer Santa Ana fires—or Santa Ana winds generally—in the historical record, Abatzoglou told me.
A 2006 study from researchers at the University of California, Berkeley, and the Lawrence Berkeley National Laboratory suggested that by the end of the century, Santa Ana winds may become more common. They may also form later in the year, including in December.
La Niña
There’s currently a weak La Niña in the tropical Pacific, which means that global temperatures are cooler than they would be otherwise.
The same phenomenon is also keeping storms from making landfall in Southern California. Normally, California’s wet season would have started by this time of year. “Once [autumn] rains hit the region, fuel moistures recover and make the landscape fire-resistant, thus reducing the odds that a power-line failure or vehicle will start a fire,” said Abatzoglou.
But the rains haven’t yet appeared, he told me. “So far this autumn, much of the southern half of California is pitching a shutout in terms of rainfall to date. Some of this is characteristic of La Niña ... as the southern tier of the United States sees less precipitation during La Niña winters.”
It’s still unclear how climate change will affect the Pacific’s yearly dance between El Niño, La Niña, and a neutral state. A 2015 study in Nature Climate Change found that the Pacific Ocean may careen between extreme states—from an intense El Niño to a monster La Niña—by the end of the century, but more research on the question still needs to be done.
A very cold U.S. East Coast
Even as the West Coast remains warm and dry, the Eastern Seaboard is settling into some of its first cold weather of the season. This pattern—a warm West, a frigid East—is known as the North American winter dipole.
It’s caused when the jet stream—which both ferries storms into the continent and generally divides warm air from cold air—gets especially twisted across North America. It rises far into the Canadian Northwest, keeping most of the western United States warm and dry; then it cascades down across the middle of the country, bringing cold air well into the U.S. Southeast.
This phenomenon prolonged California’s drought during the first part of this decade, keeping any kind of storm system offshore. It also brought the infamous “polar vortex” down into the continental United States.
There are a number of theories about how this pattern comes to form, and most of them revolve around climate change, as Jason Samenow writes at The Washington Post.
Daniel Swain, a climate scientist at the University of California, Los Angeles, argues that the ridge forms in part because the West is warming up much faster than the East. If this is the case, then scientists might expect to see the phenomenon fade in decades to come, as the East Coast catches up to the West.
But a paper published this week in Nature Communications takes another view. It finds that the disappearance of sea ice over the Arctic Ocean could change the circulation of the Pacific Ocean, encouraging the jet stream to veer north. In other words, climate change will make something like the North American winter dipole keep reappearing.


Albert Einstein said that the “most incomprehensible thing about the universe is that it is comprehensible.” He was right to be astonished. Human brains evolved to be adaptable, but our underlying neural architecture has barely changed since our ancestors roamed the savannah and coped with the challenges that life on it presented. It’s surely remarkable that these brains have allowed us to make sense of the quantum and the cosmos, notions far removed from the “commonsense,” everyday world in which we evolved.
But I think science will hit the buffers at some point. There are two reasons why this might happen. The optimistic one is that we clean up and codify certain areas (such as atomic physics) to the point that there’s no more to say. A second, more worrying possibility is that we’ll reach the limits of what our brains can grasp. There might be concepts, crucial to a full understanding of physical reality, that we aren’t aware of, any more than a monkey comprehends Darwinism or meteorology. Some insights might have to await a post-human intelligence.
Scientific knowledge is actually surprisingly “patchy”—and the deepest mysteries often lie close by. Today, we can convincingly interpret measurements that reveal two black holes crashing together more than a billion light-years from Earth. Meanwhile, we’ve made little progress in treating the common cold, despite great leaps forward in epidemiology. The fact that we can be confident of arcane and remote cosmic phenomena, and flummoxed by everyday things, isn’t really as paradoxical as it looks. Astronomy is far simpler than the biological and human sciences. Black holes, although they seem exotic to us, are among the uncomplicated entities in nature. They can be described exactly by simple equations.
So how do we define complexity? The question of how far science can go partly depends on the answer. Something made of only a few atoms can’t be very complicated. Big things need not be complicated either. Despite its vastness, a star is fairly simple—its core is so hot that complex molecules get torn apart and no chemicals can exist, so what’s left is basically an amorphous gas of atomic nuclei and electrons. Alternatively, consider a salt crystal, made up of sodium and chlorine atoms, packed together over and over again to make a repeating cubical lattice. If you take a big crystal and chop it up, there’s little change in structure until it breaks down to the scale of single atoms. Even if it’s huge, a block of salt couldn’t be called complex.
Atoms and astronomical phenomena—the very small and the very large—can be quite basic. It’s everything in between that gets tricky. Most complex of all are living things. An animal has internal structure on every scale, from the proteins in single cells right up to limbs and major organs. It doesn’t exist if it is chopped up, the way a salt crystal continues to exist when it is sliced and diced. It dies.
Scientific understanding is sometimes envisaged as a hierarchy, ordered like the floors of a building. Those dealing with more complex systems are higher up, while the simpler ones go down below. Mathematics is in the basement, followed by particle physics, then the rest of physics, then chemistry, then biology, then botany and zoology, and finally the behavioral and social sciences (with the economists, no doubt, claiming the penthouse).
“Ordering” the sciences is uncontroversial, but it’s questionable whether the “ground-floor sciences”—particle physics, in particular—are really deeper or more all-embracing than the others. In one sense, they clearly are. As the physicist Steven Weinberg explains in Dreams of a Final Theory (1992), all the explanatory arrows point downward. If, like a stubborn toddler, you keep asking “Why, why, why?” you end up at the particle level. Scientists are nearly all reductionists in Weinberg’s sense. They feel confident that everything, however complex, is a solution to Schrödinger’s equation—the basic equation that governs how a system behaves, according to quantum theory.
But a reductionist explanation isn’t always the best or most useful one. “More is different,” as the physicist Philip Anderson said. Everything, no matter how intricate—tropical forests, hurricanes, human societies—is made of atoms, and obeys the laws of quantum physics. But even if those equations could be solved for immense aggregates of atoms, they wouldn’t offer the enlightenment that scientists seek.
Macroscopic systems that contain huge numbers of particles manifest “emergent” properties that are best understood in terms of new, irreducible concepts appropriate to the level of the system. Valency, gastrulation (when cells begin to differentiate in embryonic development), imprinting, and natural selection are all examples. Even a phenomenon as unmysterious as the flow of water in pipes or rivers is better understood in terms of viscosity and turbulence, rather than atom-by-atom interactions. Specialists in fluid mechanics don’t care that water is made up of H2O molecules; they can understand how waves break and what makes a stream turn choppy only because they envisage liquid as a continuum.
New concepts are particularly crucial to our understanding of really complicated things—for instance, migrating birds or human brains. The brain is an assemblage of cells; a painting is an assemblage of chemical pigment. But what’s important and interesting is how the pattern and structure appears as we go up the layers, what can be called emergent complexity.
So reductionism is true in a sense. But it’s seldom true in a useful sense. Only about 1 percent of scientists are particle physicists or cosmologists. The other 99 percent work on “higher” levels of the hierarchy. They’re held up by the complexity of their subject, not by any deficiencies in our understanding of subnuclear physics.
In reality, then, the analogy between science and a building is really quite a poor one. A building’s structure is imperiled by weak foundations. By contrast, the “higher-level” sciences dealing with complex systems aren’t vulnerable to an insecure base. Each layer of science has its own distinct explanations. Phenomena with different levels of complexity must be understood in terms of different, irreducible concepts.
We can expect huge advances on three frontiers: the very small, the very large, and the very complex. Nonetheless—and I’m sticking my neck out here—my hunch is there’s a limit to what we can understand. Efforts to understand very complex systems, such as our own brains, might well be the first to hit such limits. Perhaps complex aggregates of atoms, whether brains or electronic machines, can never know all there is to know about themselves. And we might encounter another barrier if we try to follow Weinberg’s arrows further down: if this leads to the kind of multidimensional geometry that string theorists envisage. Physicists might never understand the bedrock nature of space and time because the mathematics is just too hard.
My claim that there are limits to human understanding has been challenged by David Deutsch, a distinguished theoretical physicist who pioneered the concept of “quantum computing.” In his provocative and excellent book The Beginning of Infinity (2011), he says that any process is computable, in principle. That’s true. However, being able to compute something is not the same as having an insightful comprehension of it. The beautiful fractal pattern known as the Mandelbrot set is described by an algorithm that can be written in a few lines. Its shape can be plotted even by a modest-powered computer:
But no human who was just given the algorithm can visualize this immensely complicated pattern in the same sense that they can visualize a square or a circle.
The chess champion Garry Kasparov argues in Deep Thinking (2017) that “human plus machine” is more powerful than either alone. Perhaps it’s by exploiting the strengthening symbiosis between the two that new discoveries will be made. For example, it will become progressively more advantageous to use computer simulations rather than run experiments in drug development and materials science. Whether the machines will eventually surpass us to a qualitative degree—and even themselves become conscious—is a live controversy.
Abstract thinking by biological brains has underpinned the emergence of all culture and science. But this activity, spanning tens of millennia at most, will probably be a brief precursor to the more powerful intellects of the post-human era—evolved not by Darwinian selection but via “intelligent design.” Whether the long-range future lies with organic post-humans or with electronic superintelligent machines is a matter for debate. But we would be unduly anthropocentric to believe that a full understanding of physical reality is within humanity’s grasp, and that no enigmas will remain to challenge our remote descendants.
This post appears courtesy of Aeon.



At first, the fossil was smuggled out of Mongolia, as many dinosaurs are. It found its way to Japan, then Britain, then France. In 2015, the private collector who finally bought it contacted the paleontologist Pascal Godefroit to get his opinion on the specimen.
Godefroit’s opinion was: This is one weird dinosaur.
The creature was clearly a small predator, much like Velociraptor. Its feet even had the distinctive sickle-shaped claws that clinked across the kitchen floor in Jurassic Park. But its long neck and tapering snout resembled those of a swan. Its arms and hands also had unusual proportions—something halfway between the grasping limbs of other raptors and the flattened flippers of modern penguins. It looked like a Velociraptor that had adapted for life in the water—that is, if it was even an actual dinosaur.
“It was so strange that we suspected that it might have been a chimera—a mix of different skeletons glued together. It wouldn’t be the first time,” says Andrea Cau from the University of Bologna, who joined Godefroit’s investigation. “We had to be sure that it was a real dinosaur and not a fake.”
Since most of the animal was still encased in a 15-inch block of stone, the team took it to Grenoble, and scanned it using a particle accelerator. The scans showed that the block was a solid mass that hadn’t been assembled from separate pieces. And the parts of the skull still within it were identical to those on the outside, as were the hidden arm bones. This swan-necked, duck-snouted, almost-paddle-limbed, sickle-clawed creature was assuredly weird—but it was a real dinosaur. (Cau notes that the scans are all openly available in case any other paleontologists want to check them.)
The team called the creature Halszkaraptor escuilliei. The name’s first half, pronounced “hull-shka-raptor,” honors Halszka Osmólska, a Polish paleontologist who discovered more than a dozen Mongolian dinosaurs and has at least four species named after her. The second half honors François Escuillié, the French collector who bought the fossil, alerted Godefroit, and worked to return the poached specimen to its rightful home in Mongolia. It currently sits in Brussels and will remain there for a year or so while the team finishes studying it.
Halszkaraptor is one of the theropods—a group of mostly meat-eating dinosaurs that count Tyrannosaurus and Velociraptor among their ranks. But unlike its kin, Halszkaraptor’s odd features suggest it was a strong swimmer that perhaps chased fish underwater, much like modern cormorants do. Outside of birds, “this is the first time we see that in a dinosaur,” says Cau. (Other ancient reptiles like paddle-limbed plesiosaurs and the dolphin-esque ichthyosaurs are not actually dinosaurs.)
Like many other fish-eating specialists, Halszkaraptor had a lot of teeth in the front of its snout—more than twice the number of a typical dinosaur. “The first time I saw that, I asked my colleagues to repeat the analysis because I wasn’t convinced,” says Cau. The snout also contained branching bony chambers that would have once housed a large network of blood vessels and sensory nerves. Such features are common in modern crocodiles, giving them an exquisite sense of touch.
Halszkaraptor’s neck made up half its length from snout to hip, reminiscent of plesiosaurs, several groups of freshwater turtles, and birds like swans and herons—all of which use their long necks to catch fish. Halszkaraptor’s neck bones also had more side-to-side mobility than those of the average theropod. “That might be an adaptation to swimming, or it may indicate that the animal used rapid sweeping motions of the neck to capture some sort of small prey,” says Michael Habib, from the University of Southern California.
The arms “are the most problematic part,” says Cau, because they’re not quite like anything else. The long bones are flattened, and the fingers get progressively longer from the outside in—the opposite pattern to most theropods. They’re closest in proportion to the limbs of swimming birds like puffins, murres, and penguins. But they’re not flippers. “I prefer not to say if [Halszkaraptor] used its arms propulsively,” says Cau. “We don’t have information on the shoulder girdle, which would be important to determine if it swam like a penguin.”
Despite all these adaptations for swimming, Halszkaraptor’s back half is that of a typical landlubbing theropod. It had long legs, although it wasn’t well adapted for running. It had a longish tail, although one that was too thin to effectively counterbalance the exceptionally long neck. To compensate, Halszkaraptor probably stood upright, more so than other raptors, but not quite as erect as a penguin. Cau thinks it lived in an unstable environment, with cycles of freshwater and drought. Halszkaraptor evolved to cope with both worlds.
Halszkaraptor is the first amphibious dinosaur that we know of—and that’s strange. With mammals, you have digging moles, gliding squirrels, flying bats, swimming otters, oceangoing dolphins, running gazelles, climbing monkeys, and swinging gibbons. Where’s that diversity of lifestyles among the dinosaurs? Until recently, it seemed that aside from birds, “nearly all dinosaurs are considered to be typical ground-living animals,” says Xing Xu, a prolific dino discoverer from the Chinese Academy of Sciences. That’s surprising, given how diverse they were, and how thoroughly they dominated the planet for hundreds and millions of years.
But that view has changed in recent years, thanks to new discoveries. In 2014, scientists reimagined Spinosaurus, the sail-backed giant that menaced the cast of Jurassic Park 3, as a semiaquatic, crocodile-like fish hunter. They also revealed the complete skeleton of Deinocheirus, a “horse-headed, humpbacked dinosaur that looks like something out of a bad sci-fi movie.” A year later, Xu described Yi, a small predator which had both feathers and bat-like wings, and may have glided between trees. The year after that, another team described Limusaurus, a beaked plant eater that loses its teeth as it grows up. All of these species are theropods. And all of these differ wildly in their lifestyles, diets, physiques, and more. Halszkaraptor, the first amphibious theropod, is part of that revolution.
“It’s exhilarating and yet almost embarrassing thinking back on how typecast these beasties were just a few short decades ago,” says Lindsay Zanno, from the North Carolina Museum of Natural Sciences. In particular, “the theropod section of the dinosaur family tree continues to be in flux as specimens like this one keep popping up.”
“There’s great potential for future dinosaur-fossil hunting,” Xu adds.


Scientists searching for astronomical objects in the early universe, not long after the Big Bang, have made a record-breaking, two-for-one discovery.
Using ground-based telescopes, a team of astronomers have discovered the most distant supermassive black hole ever found. The black hole has a mass 800 million times greater than our sun, which earns it the “supermassive” classification reserved for giants like this. Astronomers can’t see the black hole, but they know it’s there because they can see something else: A flood of light around the black hole that can outshine an entire galaxy. This is called a quasar, and this particular quasar is the most distant one ever observed.
The light from the quasar took more than 13 billion years to reach Earth, showing us a picture of itself as it was when the universe was just 5 percent of its current age. Back then, the universe was “just” 690 million years old. The hot soup of particles that burst into existence during the Big Bang was cooling rapidly and expanding outward. The first stars were starting to turn on, and the first galaxies beginning to swirl into shape. Quasars from this time are incredibly faint compared to the nearest quasars, the light from some of which takes just 600 million light years to reach the Earth.
“It’s like finding the needle in a haystack,” said Eduardo Bañados, an astronomer at the Carnegie Institution for Science who led the international research team. Their double discovery is described in a study published Wednesday in Nature.
Black holes, mysterious as they are, are among the most recognizable astronomical phenomena in popular science. They’re pretty straightforward: Black holes are spots in space where the tug of gravity is so strong that not even light can escape. They gobble up gas and dust and anything that comes near, growing and growing in size. A supermassive black hole sits in the center of virtually all large galaxies, including the Milky Way. Astronomers can infer their existence by watching fast-moving stars hurtle around a seemingly empty, dark region.
Quasars, meanwhile, are a little trickier to understand, and you’d be forgiven for thinking they sound like something out of Star Trek. A quasar is, to put it simply, the product of a binge-eating black hole. A black hole consumes nearby gas and dust inside a galaxy with intense speed, and the violent feast generates a swirling disk of material around it as it feeds. The disk heats up to extreme temperatures on the order of 100,000 degrees Kelvin and glows brightly. The resulting light show is what we call a quasar, and what a light show it is.
“A quasar emits more light than an entire galaxy’s worth of stars, and it’s actually just a glowing disk of material that is the size of our solar system,” said Daniel Mortlock, an astrophysicist at Imperial College London and Stockholm University. In 2011, Mortlock and his colleagues reported their discovery of the most distant quasar found at the time.
The more material a black hole consumes, the bigger it becomes. Eventually, the black hole drains the surrounding area of material and has nothing to eat. The luminous disk around it shrinks and fades, and the quasar is extinguished. In this way, quasars—and the black holes that power them—are like volcanoes, erupting under one set of conditions and settling into dormancy under another.
Quasars were first detected in 1963 by the Dutch astronomer Maarten Schmidt with California’s Palomar Observatory. Astronomers thought these newly discovered points of light were stars because of their extreme brightness. But when they studied the spectrum of their light, they were stunned to find the “stars” were more than a billion light-years away. When light travels through space, it gets stretched thanks to the constant expansion of the universe. As it moves, it shifts toward redder, longer wavelengths. Astronomers can measure this “redshift” to figure out how long the light took to reach Earth, which indicates how far a certain object is. Schmidt and his fellow astronomers knew that for stars to appear so luminous to Earth from such great distances was impossible. They were dealing with completely new phenomena.
“They’re not something that anyone predicted at all,” Mortlock said. “Occasionally you get astronomical objects like [stars known as] brown dwarfs, where people had predicted that they would exist and waited for astronomy to find them. No one predicted anything like quasars. It’s one of those cases where our imaginations weren’t up to what nature turned out to provide.”
To find the latest record-breaking quasar, Bañados and his colleagues used computer algorithms to search through databases of large sky surveys. They selected points of light they suspected could turn out to be quasars and observed them with the telescopes at Las Campanas Observatory in Chile. One night in March of this year, they all gathered to look at the data, one quasar candidate at a time. Quasars, astronomers have found, are easily recognizable when raw data is plotted on a chart. The spectrum of a quasar—a plot of brightness against the wavelength of light—has a very distinctive shape. Features known as emission lines appear broad, rather than sharp, thanks to the Doppler effect, which means the object emitting the light it traveling at high speeds.
“These objects are so bright that basically in 10 minutes, I can know from the raw data if it’s a quasar or not,” Bañados said. They found a quasar in their search, and when they calculated its distance from Earth, they couldn’t believe what they’d found. The next day, Bañados started drafting proposals to get observation time on powerful telescopes around the world to further study this quasar.
From the data for the quasar, astronomers can infer the size of the black hole responsible for powering it. “To get a bright quasar like this, you have to build up a supermassive black hole,” Mortlock said.
Astronomers studied the galaxy where the black hole and its quasar reside using radio telescopes in the French Alps and New Mexico. They found that the galaxy, at a mere 690 million years, had “already formed an enormous amount of dust and heavy chemical elements. This means it must already have formed a large amount of stars.” Astronomers say they’ll need to rethink some existing models for the evolution of galaxies to explain how a young galaxy could accumulate so much matter so fast. The findings about the galaxy are published in a separate study in the Astrophysical Journal Letters.
Quasars are some of the best targets for studying the early universe. Like flashlights, they illuminate a cosmic time astronomers are still struggling to understand. The newly discovered quasar comes from a period in the universe’s history know as “the epoch of re-ionization,” when a mysterious source of radiation ionized hydrogen and transformed the gas in the universe from an indiscernible fog into something transparent. About this time, the first objects to radiate light also formed. The exact process, as well as which phenomenon happened first, remains poorly understood.
Mortlock said he feels some sense of ownership of the quasar he discovered, which is now the second-farthest ever spotted. To feel that way about an object billions of light-years away is “completely ridiculous,” he said with a laugh. “And it’s especially ridiculous because there was no way that the object we discovered was going to be the end of this process. As we get more data and observe larger areas of the sky and look more deeply, we’re always going to find more objects like this.”
Someday, Bañados’s discovery will be relegated to second place, too. “There must be more out there, especially fainter ones,” Bañados said. “I’m still searching for them.”


While dozens of people have lived there over the years—including six right now—the International Space Station is unlike any other home. Its residents sleep zipped into bags tethered to the wall so they don’t float away. They pee into a plastic hose that suctions urine into a processor and then turns it into drinking water. Their showers require squeezing globs of water out of pouches.
But just like in homes found on Earth, the residents of the International Space Station share their space with thousands of invisible roommates: bacteria.
A team of microbiologists at the University of California recently sought to examine the population of microbes on the station. They wanted to see how the ecosystem orbiting 200 miles above Earth would resemble that of homes—and the people who inhabit them—below. They collected swab samples from more than a dozen surfaces on the station and compared them to microbes that were taken from surfaces in terrestrial households and different spots on the human body.
The researchers say the ISS turned out more species-rich than they expected. They identified 12,554 species of microbes, most of them harmless. They found that the composition of microbes of the space staton and a typical household was significantly different, but the microbes on the station more closely resembled home surfaces than human ones.
These results are both surprising and unsurprising. The researchers had expected to find differences in the microbial ecosystems between the space station and terrestrial homes. “Unlike the ISS, homes on Earth are exposed to a variety of sources of microbes, including the outdoor air, tracked-in soil, plants, pets, and human inhabitants,” the study authors write. On the ISS, you can’t exactly crack open a window to let some fresh air in.
These conditions would seem to suggest that the microbes living on the surfaces of the station would have more in common with the ones residing on the humans inside it. And yet, the researches found the microbes to be “more similar to the surfaces of human homes on Earth than it is to human bodies.” So the microbes on the ISS don’t match up with the ones found in houses or on bodies, but they’re still more like ones living on your kitchen counter than on your skin.
The findings were published Tuesday in the journal PeerJ. In May 2014, Koichi Wakata, an astronaut from Japan and the crew commander at the time, swabbed 15 surfaces around the station, including telephones, laptop keyboards, handrails, and air vents. In the station’s microgravity environment, microbe-carrying dust tends to accumulate in air filters rather than surfaces.
The samples were stored in a lab freezer, sent back to Earth on a SpaceX spacecraft, and then shipped to a lab, where the researchers extracted DNA to build a census of the microscopic inhabitants. The researchers compared them to three databases: the Human Microbiome Project, which characterizes human microbes; the Wildlife of Our Homes project, which tracks microbe samples taken from household surfaces like kitchen counters, toilet seats, pillowcases, and door handles; and Project MERCCURI, a microbial experiment on the ISS.
The researchers tried to target surfaces on the ISS that may be similar to those used in the Wildlife of our Homes project, but they hit a few snags. “The kitchen surfaces aboard the ISS are in the Russian module, which we did not have permission to access,” they write. And “swabbing the toilet seat was deemed inappropriate due to biosafety concerns.”
Studying the specific microbial environment on the ISS is crucial for future space missions beyond low-Earth orbit. When humans leave the comfort of Earth’s protective bubble for more dangerous, extreme worlds, they won’t be going alone—they’ll take with them thousands, perhaps millions, of these microorganisms, tiny beings just as unprepared as they are to face environments they didn’t evolve in.
The microbes of the ISS recently popped into headlines in November when the Russian news agency TASS reported that cosmonauts had scraped living bacteria off the outside of the station during a spacewalk. The article quoted a cosmonaut saying the bacteria “have come from outer space and settled along the external surface,” prompting some on the internet to think it was of extraterrestrial origin. He didn’t add much else, but as science journalists have since pointed out, it’s more probable that the bacteria was delivered there by terrestrial activities, like repairs and maintenance by astronauts and cosmonauts during spacewalks. The likeliest culprits behind mystery bacteria in low-Earth orbit are always going to be humans. Beyond that, however, who knows?


When the botanist Richard Deakin examined Rome’s Colosseum in the 1850s, he found 420 species of plant growing among the ruins. There were plants common in Italy: cypresses and hollies, capers, knapweed and thistle, plants “of the leguminous pea tribe,” and 56 varieties of grass. But some of the rarer flowers growing there were a botanical mystery. They were found nowhere else in Europe.
To explain this, botanists came up with a seemingly unlikely explanation: These rare flowers had been brought as seeds on the fur and in the stomachs of animals like lions and giraffes. Romans shipped these creatures from Africa to perform and fight in the arena. Deakin takes care to mention in Flora of the Colosseum of Rome that the “noble and graceful animals from the wilds of Africa ... let loose in their wild and famished fury, to tear each other to pieces”—along with “numberless human beings.” As the animals fought and died in the arena, they left their botanical passengers behind to flourish and one day overtake the building itself.
This incredible piece of conjecture is hard to prove, but it shows just how much of the story of a ruined place can be found growing in the cracks between broken stones. Deakin, an Englishman from Royal Tunbridge Wells, opens his volume by calling the plants growing in the Colosseum “a link in the memory” that “flourish in triumph upon the ruins.” He lists their ancient medicinal properties, notes the species of fern that must have grown around the Fountain of Egeria, and pauses at one point over a particular species of grass that might have ringed the banks of Nero’s fish pond. The grasses that grow “in matted tufts,” he writes, “seem to be perennial weepers ... mourning over the vast destruction that reigns around them.”
Today, the Colosseum stands bald and bare. But for centuries, it was a wild and overgrown place, and its lost history as a primeval garden ruin has left traces in the art and poetry of countless generations that walked among its stands.
* * *
By the time the artists and painters of the Romantic Age began to turn their interest to the ruins of ancient Rome, the Colosseum had suffered greatly. Since the days of Rome’s flourishing, the great arena had been a cemetery, a quarry of quicklime, a rich family’s fortress, and a bullring. In the 16th century, it was even explored as a site for a wool factory staffed by former sex workers. It had been damaged by fire and struck by earthquakes five times, including one in 1349 that caused the entire outer south side to collapse.
Due to the belief that Christian martyrs had once been fed to the lions in the arena, the Colosseum was also a popular pilgrimage spot. Despite little evidence that Christians were ever actually killed in the arena, in 1749, Pope Benedict XIV endorsed the view that the Colosseum was a sacred site, outlawing the use of its stones in other buildings. By that point, the arena was a crumbling ruin. But it had become the home for a great variety of sumptuous greenery.
Plant life was so abundant in the ruined arena that at certain times in history, peasants had to pay for permission to collect the hay and herbs that grew there. It had become its own miniature landscape, and formed a perfect microclimate for biodiversity: dry and warm on its south side, cool and damp in the north. Pink dianthus grew down in the lower galleries, while white anemones dotted the stands during spring.
When the Italian scholar Poggio Bracciolini visited in 1430, he mourned over the site of the ruins. “This spectacle of the world, how it is fallen!” he exclaimed. “How changed! How defaced! The path of victory is obliterated by vines.” But others saw an alluring beauty in the arena’s greenery. Charles Dickens, in his 1846 Pictures From Italy, talks about his impressions on first seeing the Colosseum, and mentions the plant life there in particular detail, as a natural force reclaiming the site of past glory:
To see it crumbling there, an inch a year; its walls and arches overgrown with green; its corridors open to the day; the long grass growing in its porches; young trees of yesterday, springing up on its ragged parapets, and bearing fruit: chance produce of the seeds dropped there by the birds who build their nests within its chink and crannies; to see its Pit of Fight filled up with earth ... is the most impressive, the most stately, the most solemn, grand, majestic, mournful sight conceivable.
By the 19th century, countless painters and artists had visited the Colosseum and painted vistas of the arena’s overgrown stands and crumbling stones. The Scottish painter William Leighton Leitch (1804–1883) painted the Colosseum as an imposing empty space, a monumental hanging garden reminiscent of Babylon. Joseph Mallord William Turner’s “The Colosseum, Rome, by Moonlight” (1819) shows an almost tropical garden growing in the arena’s shadows:
For Ippolito Caffi, meanwhile, the ruins became an almost nightmarish vision, a primeval jungle ruin where strange lights dance between the stones wreathed in matted vines. The paintings that perhaps most effectively portray the strange, paradoxical atmosphere of the Colosseum in those days are those of Danish painter Christoffer Wilhelm Eckersberg (1783–1853), who captures the abundant greenery growing in the Colosseum’s stands as well as its lost history as a place of Christian worship, but does so with a faultlessly modern eye, never giving in to the Romantic excesses of some of his forebears. His paintings are by turns infused with a solemn quiet, and full of the life of common people.
Paintings like these inspired writers as well. In 1833, the then-unknown Edgar Allan Poe was able to publish a poem called “The Coliseum,” though he had never set foot in Italy. The poem mentions the flora of the arena particularly, with the lines, “Here, where the dames of Rome their yellow hair / Wav’d to the wind, now wave the reed and thistle.”
Earlier, in 1818, Percy Bysshe Shelley wrote lyrically of the way that the arena had itself become a natural entity, a landscape rather than a building.
It has been changed by time into the image of an amphitheater of rocky hills overgrown by the wild olive, the myrtle, and the fig tree, and threaded by little paths which wind among its ruined stairs and immeasurable galleries: The copsewood overshadows you as you wander through its labyrinths, and the wild weeds of this climate of flowers bloom under your feet.
Lord Byron, in the fourth canto of his “Childe Harold’s Pilgrimage,” also speaks of the “The garland-forest, which the gray walls wear, / Like laurels on the bald first Caesar’s head.” For him, the gaps and blanknesses in the ruins form portals to another time, bringing back the dead and unsettling the ruin-gazer.
But the abundant green life of the Colosseum was soon to come to an end. In 1870, Rome was captured by Italian nationalist forces, the final defeat of the Papal States in the war to unify the Italian peninsula. The Pope became an exile in his own palace, surrounded by armed soldiers who stopped him from appearing in public. A year later, Rome became the capital of the Kingdom of Italy, a new nation that was to be modern, democratic, and secular. As revenge for the Pope’s intransigence, Catholic convents, churches, and hospitals were seized for barracks. The Colosseum, too, was swept up in this upheaval.
As nation-states were being born around Europe and the rest of the world in the late 19th century, many of these new states reached back into their pasts to find a foundation on which to build their identities. The new Italian nation found this opportunity in the lost splendor of the Colosseum. The Italian government soon handed control of the arena over to archaeologists, who set about removing religious icons installed by the Christians and clearing it of its invasive greenery.
The work of these early archaeologists continued into the 20th century, and the Colosseum was finally totally denuded and its arena excavated under Mussolini’s fascist government, who sought even further to associate the modern Italian state with the monuments of ancient Rome.
* * *
Despite this loss, the Colosseum is today still a haven for plants. A study carried out between 1990 and 2000 found 243 distinct species still growing there, although this number is scarcely half what Deakin observed in the 19th century.
Plants growing today in the Colosseum include very rare species like Asphodelus fistulosus and Sedum dasyphyllum, which scientists believe can only survive when sheltered by the arena, a sanctuary from the urban environment outside. Due to increased pollution and the rising temperature of the city, the flora inside the ruined walls are beginning to change: Plants suited to a warmer and more arid climate are beginning to proliferate at the expense of those more used to cool and damp.
From the wild and magical ruins of the Romantic Age to the haunted and skeletal ruins of Dresden and Stalingrad, the place ruins hold in the collective imagination has always been as unstable as the crumbling structures themselves. No one sees the same ruin. Certain parts of a crumbling building, the ancient stones and arches, for instance, are deemed “proper.” Other parts, like the plant life and the later historical stages of the ruin, are deemed “improper” and are removed.
Increasing attention is being paid to these intangible elements of a building’s story, and archaeologists and conservationists alike are having lively debates about how the atmosphere of these sites—the inspiration for so many artists—can be preserved. Deakin believed that the wild spirit of a place can “teach us hopeful and soothing lessons, amid the sadness of a bygone age.” Today if you walk around the Colosseum, you might even see the occasional rebellious caper plant growing there, in defiance of security.


The headline on the big new gender survey from the Pew Research Center begins, “On Gender Differences, No Consensus”— and that could have been the report’s entire conclusion, too. The survey, released today, reveals deep divides in Americans’ perspectives on gender norms, including by political affiliation:
Of course, the recent wave of sexual-harassment allegations in politics have shown that people of all parties are capable of mistreating women. But this Pew survey and others reveal how, even though society’s ideas about gender are changing rapidly, Republicans are less likely to endorse those changes.
A poll conducted by PRRI and The Atlantic between October 5 and October 9 of last year found Trump supporters were more likely than Clinton supporters to feel that society punishes men just for acting like men. In that poll, Republicans, conservatives, and Trump supporters were also far more likely than liberals, Democrats, or Clinton supporters to think that society was becoming “soft.”
Social-science research suggests gender beliefs are an inherent part of what it means to be a liberal or conservative. As NPR’s Hidden Brain team reported, Republicans tend to have a “strict father” view of the world, in which strong figures decide what is best for the family (or the country). Democrats, meanwhile, tend to support the “nurturant parent” model, in which parents (and leaders) “feel their job is to empathize with their child, to know what their child needs, and to have open two-way discussions with their child,” the NPR reporters write. Those fundamental beliefs might later map onto more positive views of masculinity, in Republicans, and more free-flowing ideas about rules for men and women, in the case of Democrats. In academic studies, people are even more likely to describe the GOP in masculine terms, and the Democrats in feminine ones.
The 2016 election revealed a desire for better wages and conditions for working-class people of both parties. But on cultural issues—including gender—there still seems to be “no consensus.”


Once upon a time, the sun and moon argued about who would light up the sky. They fought, as anthropomorphic celestial bodies are meant to do, but after the moon proves to be as strong as the sun, they decide to take shifts. The sun would brighten the day, while the moon would illuminate the night.
This is one of several stories told by the Agta, a group of hunter-gatherers from the Philippines. They spend a lot of time spinning yarns to each other, and like their account of the sun and moon, many of these tales are infused with themes of cooperation and equality. That’s no coincidence, says Andrea Migliano, an anthropologist at University College London.
Storytelling is a universal human trait. It emerges spontaneously in childhood, and exists in all cultures thus far studied. It’s also ancient: Some specific stories have roots that stretch back for around 6,000 years. As I’ve written before, these tales aren’t quite as old as time, but perhaps as old as wheels and writing. Because of its antiquity and ubiquity, some scholars have portrayed storytelling as an important human adaptation—and that’s certainly how Migliano sees it. Among the Agta, her team found evidence that stories—and the very act of storytelling—arose partly as a way of cementing social bonds, and instilling an ethic of cooperation.
At first, Migliano wasn’t actually interested in storytelling. She wanted to know what qualities the Agta most value in their peers, given that they are nomadic and their camps continuously shift. So, her students asked 300 Agta to name the five people they’d most want to live with. They also asked the volunteers to nominate the strongest people they knew; the best hunters, fishers, and foragers; the ones whose opinions are most respected; and the ones with most medical knowledge. And finally, almost as an afterthought, they asked the volunteers to name the best storytellers. That, they assumed, was something relatively unimportant, and would make for an interesting contrast against the other more esteemed skills.
In fact, the Agta seemed to value storytelling above all else. Good storytellers were twice as likely to be named as ideal living companions as more pedestrian tale spinners, and storytelling acumen mattered far more all the other skills. “It was highly valued, twice as much as being a good hunter,” says Migliano. “We were puzzled.”
Fortunately, she had been working with Agta Aid, a nonprofit organization that had been trying to preserve the Agta’s oral stories in written forms. “We asked them if we could have a look at the stories they were collecting, and we realized that most of the content was about cooperation, egalitarianism, and gender equality.” The male sun and female moon divvy up the sky. A pig helps its injured friend—a sea cow—into the ocean so they can race side by side. A winged ant learns that she is not above her other wingless sisters.
These themes aren’t unique to the Agta. They’re also present in around 70 percent of the stories that Migliano compiled from work with other hunter-gatherer groups. “Hunter-gatherers move around a lot and no one has particular power,” she explains. “You need ways of ensuring cooperation in an egalitarian society, and we realized that you could use stories to broadcast the norms that are important to them.” People can use religion to achieve a similar end, enforcing good behavior through fear of a punitive deity. But Migliano points to research suggesting that high gods are a relatively recent invention, which emerged once human societies became large. Small communities like the Agta don’t have them. Instead, they use stories for the same purpose.
Migliano’s team asked Agta volunteers from various camps to play a simple game, in which they could share rice with their camp-mates. And they found that such sharing was more likely in camps with a higher proportion of good storytellers.
That’s just a correlation, though. It’s possible that the storytellers were actively fostering more generosity among their peers. Alternatively, Migliano says, “if you live in a more cooperative camp, perhaps you have more time and you just tell more fun stories.” But if that’s true, she adds, it wouldn’t explain why so many of the actual stories feature leitmotifs of cooperation, rather than other happy and positive themes. And it certainly doesn’t explain why storytelling skill is so beneficial for those who wield it.
Skilled Agta storytellers are more likely to receive gifts, and they’re not only more desirable as living companions—but also as mates. On average, they have 0.5 more children than their peers. That’s a crucial result. Stories might help to knit communities together, but evolution doesn’t operate for the good of the group. If storytelling is truly an adaptation, as Migliano suggests, it has to benefit individuals who are good at it—and it clearly does.
“It’s often said that telling stories, and other cultural practices such as singing and dancing, help group cooperation, but real-world tests of this idea are not common,” says Michael Chwe, a political scientist at the University of California, Los Angeles, who studies human cooperation. “The team’s attempt to do this is admirable.”
Still, it’s hard to know if it’s the specific act of storytelling that matters. As others have noted, “creativity comes with its own suite of personality traits, which may make [people] more attractive sexual partners,” says Lisa Zunshine, an English professor at the University of Kentucky.
And all of Migliano’s results hinge on the Agta accurately naming the best storytellers in their midst. Did they? Could they just have named people they were close to, or venerated celebrities who sprang readily to mind? Wouldn’t that explain both the fecundity and desirability of these supposed storytelling Jedi? Migliano thinks not. If the survey had been a mere popularity contest, the Agta should have also nominated the same people as exceptional hunters, gatherers, child minders, and so on. They didn’t. They singled out particular people for particular skills, including storytelling.
“It suggests that hunter-gatherers track this ability and perceive it as beneficial,” says Michelle Scalise Sugiyama from the University of Oregon, who has studied the origins of storytelling. Other societies, like the Tsimane of Bolivia, do the same, which “indicates that storytelling contributes something of adaptive value to human life.” That something might well be the reinforcement of norms and ethics. “As attested by the universality of the trickster figure, telling stories about rule breakers who get caught and punished is an effective means of persuading individuals to conform to group norms,” Scalise Sugiyama adds.
But “stories also contain valuable cultural knowledge, and accomplished storytellers are repositories of this knowledge,” she notes. Hunter-gatherers use their tales to pass down information about food, weather, and more—and often in ways that outsiders can miss.
For example, Andamanese people have a story about two quarreling weather gods, who eventually split up their wind-creating duties. You could see that as a story about avoiding conflict, or as a way of encoding information about the strong winds that buffet the Andaman Islands. A similar creation story talks about a monitor lizard that went into the jungle to hunt pigs, got stuck in a tree, and was helped down by a civet cat (whom it married). On the face of it, that’s a story of cooperation between the sexes. But Scalise Sugiyama notes that it also encodes information about the habitat, diet, and range overlaps between the local lizards, pigs, and civets. “These profiles may aid in the prediction of animal behavior, which is critical to locating, tracking, and stalking game,” she says.
This is not to say that people deliberately or consciously tell stories to pass down knowledge or to keep their communities together. “My guess is that they would say it’s fun,” says Migliano. That’s why individuals choose to tell stories on a moment-to-moment basis—it’s what biologists call the “proximate cause” of a behavior. But it’s the broader benefits—the “ultimate causes” like transmission of knowledge or inculcation of values—that might explain why storytelling arose in the first place.
The origin of storytelling doesn’t necessarily reflect its later uses, though. “Our very human love of stories has become adapted for different ends during later phases of human history,” says Sarah Blaffer Hrdy, an anthropologist from the University of California, Davis. “The Maya-speaking people I used to study in southern Mexico told tales about a winged, super-sexed demon with a six-meter-long, death-dealing penis, who reinforced proper sex roles for men and women, including proscriptions for postures during sex, menstrual taboos, freedom of movement. Rather than promoting sexual equality, these served to constrain women.”
“Alas, our wonderfully human universal of loving stories can also become an all-too-human vulnerability, fostering enmity as readily as amicable relations,” she adds.


They are known, in almost every local indigenous language, as “Bears Ears,” and when you look at photos, you can see why. The two buttes, ruddy and gentle-sloped, rise above the scrub and canyon land that surrounds them.
Thousands of years ago, now-forgotten indigenous peoples scribbled pictograms of men and animals on their rusty walls. A century and a half ago, Navajo leaders took refuge in those same caves and canyons as the U.S. government forcibly evicted them from their territory.
On Monday, their history gained another chapter. President Donald Trump has significantly shrunk the size of two national monuments in Utah, completing the largest rollback of public-land protections in U.S. history and opening a legal battle that could determine the fate of conservation in the United States.
One of these monuments includes the land surrounding Bears Ears, which President Barack Obama protected in a poetic and immediately controversial declaration during the final days of his administration. Obama extended federal protection to 1.3 million acres of land, roughly the area of Delaware; Trump has shrunk that to 220,000 acres, roughly the size of Dallas.
Grand Staircase–Escalante, another large monument created by President Bill Clinton in 1996, will be nearly halved in size.
“Some people think that the natural resources of Utah should be controlled by a small handful of very distant bureaucrats located in Washington. And guess what? They’re wrong,” said Trump, speaking in front of the Utah state capitol building.
Bears Ears was created after half a decade of lobbying by five indigenous tribes: the Hopi, the Navajo, the Ute, the Ute Mountain Tribe, and the Zuni. The nations had long sought special status for the land, which all five consider sacred. But when talks broke down with Utah lawmakers several years ago, they pressed Obama to protect the buttes through a national monument.
The five tribes have promised to sue the Trump administration, asserting that the president does not have the right to shrink national monuments. Environmental groups, united in anger at the president’s actions, have also said they will sue to preserve Grand Staircase–Escalante at its current size.
National monuments make for a strange kind of federal land designation. Locals can generally hike, camp, hunt, and fish on national monuments. They cannot mine coal or drill for oil. Grand Staircase–Escalante is known to contain coal reserves. Ryan Zinke, the secretary of the interior, said before the speech that that didn’t play into the president’s decision.
The Antiquities Act was passed in 1906 to enable presidents to protect indigenous archaeological or cultural sites. The law clearly grants the president the right to create new national monuments. But some conservative legal scholars argue that it also contains an implicit right to revoke or revise the size of national monuments.
Presidents have slashed the size of national monuments twice before. During World War I, Woodrow Wilson slashed land from Mount Olympus National Monument to secure lumber rights. The monument was later returned to its largest size and protected as a national park. Franklin Roosevelt also cut the size of Grand Canyon National Monument to open up more land for ranching.
But neither of those reductions were tested in court—and neither occurred during the modern era of environmental law.
Since its passage last century, presidents have activated the law 157 times to protect Native sites, natural wonders, historical landmarks, and vast stretches of seafloor. Congress later enshrined dozens of those monuments—including Grand Canyon, Bryce, Zion, Acadia, and Grand Teton—as national parks, which have an added level of protection.
“This law requires that only the smallest necessary area be set aside for special protection as national monuments,” said Trump, referring to the Antiquities Act.
“Unfortunately, previous administrations have ignored the standard and used the law to lock up hundreds of millions of acres of land under government control. It did so over the loud objections of the people of this state and their elected representatives,” he said.
Polling on the issue is conflicted. A slim majority of Utahans think Bears Ears is too big, according to a recent poll conducted by The Salt Lake Tribune and the University of Utah. But about the same number of Utahans also oppose changing the size of Grand Staircase–Escalante, which President Bill Clinton created in 1996.
Environmental groups fear that the president’s undoing of public-land protections would add public-land protection to the standard back-and-forth that accompanies every partisan changeover of the White House. If Trump wins in court, a future president could undo any swath of national monument, no matter how long ago it had been protected. A strong court precedent could strip the presidency of its powers to confidently and permanently protect public land.
“As a matter of public policy, it does not make sense for protective public-lands designations to be subject to partisan uncertainty every presidential election cycle. That is a poor management strategy,” said Sarah Krakoff, a professor of law at the University of Colorado Boulder, in an email.
“The president’s authority to act unilaterally is appropriately narrow—to act to increase federal lands protection. Otherwise, Congress retains the constitutional authority to make a broader range of decisions about the nation’s lands,” she added.
“When a monument is used to prevent, rather than protect, the president is right to take action,” Zinke told a reporter before the president’s speech. The state’s governor and its full Congressional delegation supported the size reduction, as did a public-land commissioner who represents some Navajo districts, Zinke said.
Indigenous nations had previously celebrated Bears Ears as a historic occasion because it represented a change in the history of the Antiquities Act. Where the act had once been written to paternalistically protect abandoned Native sites, it had now been extended at their behest.
“It actually brought tears to my face,” said Eric Descheenie, a Navajo leader who had fought for protection for Bears Ears and a Democratic member of the Arizona House of Representatives. “It’s so hard to even try to add up what this really means. At the end of the day, there’s only a certain place in this entire world, on earth, where we as indigenous peoples belong.”
“The regulators thousands and thousands of miles away—they don’t know your land and truly they don’t care about your land like you do,” said Trump. After he signed the two proclamations—above intermittent chants of “four more years!”—a Rolling Stones song that had often played at the president’s campaign rallies came on the AV. “You can’t always get what you want,” it said.


Even if countries take moderate action on climate change, by the end of this century, Phoenix is expected to have an extra month of days above 95 degrees Fahrenheit, while Washington, D.C., is expected to have another three weeks of these sweltering days, as the Climate Impact Lab and New York Times reported.
A new study suggests that even days that are an average of 90 degrees Fahrenheit, or 32 Celsius, might have long-term, negative impacts on developing fetuses. The stress of the hot weather might show up as reduced human capital once those fetuses reach adulthood.
Maya Rossin-Slater, a health-policy professor at Stanford University, said she and her team wanted to understand the long-term consequences of climate change on people. For the study, published today in the Proceedings of the National Academy of Sciences, she and other researchers looked at data on births, weather, and earnings in half the states in the United States. For a given county, on a given day, they measured how many days above 90 degrees a child born that day would have experienced during gestation and during their first year of life. They then compared that person’s salary as an adult to someone born in that same county on that same day in other years.
It turned out fetuses and infants exposed to a single extra 90-plus degree day made $30 less a year, on average, or $430 less over the course of their entire lifetimes. Right now, the average American only experiences one such day a year. (This study looked at the average temperature throughout the entire day, not the highest temperature that day.) By the end of the century, there will be about 43 such days a year.
In addition to birthday and county, the researchers also controlled for gender and race. Rossin-Slater said it is unlikely the difference in earnings could be explained by something other than heat.
“It’s really hard to figure out what else it could be. They set up a really good study design,” said Kathryn Grace, a professor of geography, environment, and society at the University of Minnesota, who was not involved in the study.
What’s more, the study used data from the 1970s, when more and more people were installing air conditioners in their houses. The researchers found the difference in earnings went away in areas where most people got air conditioners installed.
It’s not entirely clear how hot temperatures would be causing this dip in earnings. Fetuses and infants are especially sensitive to heat because they don’t yet have the ability to self-regulate their body temperatures. Rossin-Slater said there are three potential pathways by which being too hot could impact the fetus: The heat could overstress it, which could affect the child’s health. Heat could also affect how nutrients are delivered to the fetus, or harm its cognitive development, and thus potentially things like focus or self-control.
Nathaniel DeNicola, an obstetrician with George Washington University and an expert on environmental health, said it’s well established that extreme temperatures can affect fetuses, for example by restructuring proteins that are involved in organ formation, and extreme heat raises the risk of preterm birth and low birth weight. Those issues, in turn, can sometimes cause cognitive impairment.
Still, he said, the authors could be missing some other variables that could be contributing to adult salaries, and it’s not clear that adult salaries are a marker of good health. The overall message from the study, to him, should be that “there are clear health risks to extreme heat, and those risks are worse during the critical periods of development.” (In the study, there was no effect on earnings for children older than 12 months either way.)
Women shouldn’t get too worried if they are pregnant and living in a warm area, Rossin-Slater cautioned. The difference in earnings was small, after all, and nothing happened to the air-conditioned babies.
But that also doesn’t mean we should look to AC to solve all our climate-change woes. Air conditioning isn’t free, and people in the developing world—which is hotter—are more likely not to have access to it.
“In poor countries, we can sometimes be like, ‘Oh, it’s food insecurity, it’s drought, it’s not temperature,’” Grace said. “This is a place for us to start thinking about what physical environments do to our health.”


The viruses, Jeremy Barr realized, were in the wrong place.
Barr and his colleagues at San Diego State University had grown a layer of gut cells in a dish, much like those that line the surface of our own twisting intestines. The cells formed such tight connections with each other that bacteria couldn’t sneak past them. Even a dye couldn’t get through. The layer was meant to be impermeable, until the team infused the water on one side of it with viruses called phages.
After a few hours, they found a few of these phages on the other side. The cells had absorbed them at one end, and shoved them out the other. “It took us a while to realize what we were seeing, but when we did, it was really exciting,” Barr says.
Barr believes that the same process happens in our bodies, frequently and relentlessly. If he’s right, it means that our guts are absorbing billions of viruses every day, sending a steady stream of them into our bloodstream and the rest of our organs.
That’s not something to worry about. Phages don’t infect human cells and they don’t cause disease. Their full name, bacteriophages, means “eaters of bacteria,” and as that suggests, they infect and destroy bacteria. In doing so, Barr says, they could act as part of our immune system. They anchor themselves in the layer of mucus that lines our gut. By infecting the bacteria that also thrive there, they keep these microbial populations in check, and could determine which species get to live in our bodies.
This relationship is likely an ancient one. Mucus is universal to animals from corals to fish to humans, and phages are universal to mucus. Perhaps this was how the very first animals defended themselves against infections. They developed mucus to concentrate phages that were plentiful in their environment, and the viruses in turn helped their hosts to control the microbial multitudes around them. It was a mutually beneficial relationship between animal and virus, and one that continues today.
But the latest experiments from Barr’s team, many of which were done by his colleague Sophie Nguyen, suggest that this relationship between animals and phages is even more intimate. The phages aren’t just sitting atop human gut cells, acting as bouncers. They are actually being trafficked through the cells themselves. The team even used powerful microscopes to confirm the presence of phages within the cells. “A cell is enormous compared to a phage,” says Barr. “It’s like finding a cup of coffee by sectioning a skyscraper.”
In the experiment, just 0.1 percent of the total phages made it through. But based on their rate of travel, and the staggering number of them in the average human gut, the team estimated that our gut cells absorb around 31 billion phages every day. “The percentage feels like it can’t be that important but when you turn that percentage into absolute numbers, it feels biologically relevant,” says Corinne Maurice from McGill University, who also studies phages and was not involved in this study.
The team only did experiments using lab-grown cells, but Barr says there’s good reason to think that the same viral journeys take place in living bodies. For over 70 years, scientists have been “finding phages in parts of the body where they shouldn’t be,” he says, including supposedly sterile organs like the lungs. Microbiologist René Dubos found hints of this in 1943, by injecting phages into the guts of mice and finding those same viruses in the rodents’ brains.
“Phages can be detected outside the gastrointestinal tract, but there hasn’t been any real proof of how they get there,” says Lori Holtz from Washington University School of Medicine in St Louis. Many scientists believed that they were just leaking through gaps between the cells, but Nguyen’s work suggests that they are actually going through the cells themselves. In her experiments, the phages could traverse cells that line the kidneys, lungs, liver, and even the brain. “That’s absolutely astonishing in my view,” says Barr. The brain is separated from other organs by the blood-brain barrier—one of the most tightly controlled borders in the body. It’s incredibly hard for scientists to get small molecules through it. And yet, phages seem to do so.
This isn’t an infection in any meaningful way. The phages aren’t hijacking human cells to make more copies of themselves, as viruses like influenza, Zika, or Ebola might. Instead, Barr thinks that the cells are in control. They’re actively engulfing phages, and shuttling them from one end to the other. Why?
For a start, this would suffuse our bodies with a sparse but continuous stream of phages, which might then protect our organs against wayward, opportunistic bacteria. But Barr speculates there are more unexpected purposes at work. By sensing and studying the phages they absorb, cells could fine-tune the production of the mucus that houses these viruses, or the chemicals that feed the microbes that the phages then infect. If the cells break down some of the absorbed phages, they could access and use the viruses’ genetic material. All of this is possible, and none of it is certain. Scientists are only starting to eavesdrop on the three-way conversations between bacteria, phages, and our own cells. “It’s a big unknown,” says Barr.
Maurice also notes that phages aren’t just a homogenous group. They are incredibly diverse in their own right, and they exist in large communities. “Do they compete with each other?” she wonders. “Could some of them facilitate the entrance of others into human cells? I have no idea.”
These discoveries could have potential medical implications. For a century, scientists have looked to phages as a way of curing bacterial diseases, without having to resort to antibiotics. Although phage therapy fell out of favor in Western countries, research continued to blossom in Eastern Europe and Russia. And in recent years, there have been some spectacular successes, in which patients were pulled back from death’s door by infusions of these viruses
Martha Clokie from the University of Leicester notes that several infectious bacteria, including those that cause tuberculosis and Lyme disease, can enter and infect human cells. “If we want to treat these diseases, having phages that can cross into human cells would be very useful,” she says. “This is a neglected research field.”


Of the many catchy quotes attributed to Albert Einstein, this may produce perhaps the most anxiety among the scientists who have come after him: “A person who has not made his great contribution to science before the age of 30 will never do so.”
The exact origins of the oft-cited statement are murky, so it’s difficult to determine whether the great theoretical physicist said it in seriousness or jest. Whatever the intention, research on the connections between age and scientific output have frequently shown that Einstein’s claim was wrong—or at least, not exactly true for everyone. The study of these connections is far from new, and the results are usually tricky to extrapolate to larger populations. An effect found for top performers in one field may not necessarily apply for high achievers in another, for example. But the topic has long fascinated researchers and writers, including Helmut Abt, an astronomer and former longtime editor at The Astrophysical Journal.
Abt has been studying trends in the professional output of scientists since the early 1980s. He has found, as others have, that a specific category of researchers—the top performers, the Nobel Prize winners, the geniuses—seem to produce the most significant contributions to their fields during their 30s, with some exceptions (what is it about classical composers?). In his most recent study, published this fall in Publications of the Astronomical Society of the Pacific, Abt sought to explore the professional output of “average” individuals, specifically those in astronomy.
Abt examined the work of a small group of astronomers who died between last October and this June. He picked as his measure of productivity—a tremendously subjective thing—the number of times their research papers had been cited in other papers over the course of their lifetimes. “Of course we know that astronomers do many useful things, such as teaching, public education, service on committees both within their institutions and nationally, etc.,” Abt wrote in the paper. “This study concentrates only on research results that directly aided research by others.”
The sample included 22 men and three women, nearly all of whom began publishing papers in their 20s. Abt sifted through the citations for each astronomer, which can be found on a public database operated by the Smithsonian Astrophysical Observatory. He found that, by this measure, average astronomers peaked in their careers at the age of 45. His analysis also suggested they do half of their most important work after the age of 50.
The findings line up nicely with earlier research on the subject. In 1874, a study on age and scientific output found that peak performance in the science and creative arts usually occurred between the ages of 35 and 40. (That could be attributed, in some part, to the fact that these individuals, er, ran out of time to peak after that; the life expectancy for men—the people most likely to be carrying out research at the time—born in 1800 was about 44 years old at birth.)
An analysis in 1953 of the most successful performers in fields like art, science, music, literature, and others found the peak ranged from 34 years in math to 44 in astronomy. The psychology professor Dean Keith Simonton, well-known for his studies on creativity and age, concludes that creativity ramps up in the beginning of one’s career, peaks at about 40 or 45, and then gradually declines.
In 2011, researchers analyzed more than 400 Nobel Prizes in physics, chemistry, and medicine between 1900 and 2008 and found, with a few exceptions, most researchers were older than 30 when they produced their most important work. Nobel Prizes, however, have long been criticized as a flawed form of recognition of scientific contributions because they overlook many of the individuals who made them happen, as my colleague Ed Yong has written about here.
The reasons for why people seem to peak at these ages have remained just as fascinating and difficult to pin down as the effect itself. The researchers behind the 2011 analysis suggest a shift in the last century from theoretical work, where they say younger people tend to be more successful, toward experimental work, which usually requires a mature foundation of knowledge to carry out. Other factors include health and access to expensive equipment, which would be easier for older individuals to get. Abt speculates older astronomers may take high-level positions at universities and scientific institutions that remove them from the daily grind of research.
According to his own research, Abt has already peaked. He turned 92 this year. In 1952, when he was 27, Abt received the first doctoral degree in astrophysics the California Institute of Technology awarded, for his work on pulsating stars.
“I don’t think what I did in the next three years was necessarily some of my best research,” he said with a laugh. “So no, what Einstein said did not apply to me. But I am not one of the top prizewinners in astronomy. I’m just an average astronomer who just loves to do research.”


Consider two American children, one rich and one poor, both brilliant. The rich one is much more likely to become an inventor, creating products that help improve America’s quality of life. The poor child probably will not.
That’s the conclusion of a new study by the Equality of Opportunity project, a team of researchers led by the Stanford economist Raj Chetty. Chetty and his team look at who becomes inventors in the United States, a career path that can contribute to vast improvements in Americans’ standard of living. They find that children from families in the the top 1 percent of income distribution are 10 times as likely to have filed for a patent as those from below-median-income families, and that white children are three times as likely to have filed a patent as black children. This means, they say, that there could be millions of “lost Einsteins”—individuals who might have become inventors and changed the course of American life, had they grown up in different neighborhoods. “There are very large gaps in innovation by income, race, and gender,” Chetty told me. “These gaps don’t seem to be about differences in ability to innovate—they seem directly related to environment.”
The discrepancy in who gets patents is not the result of innate abilities, Chetty and his team, Alex Bell of Harvard, Xavier Jaravel of the London School of Economics, Neviana Petkova of the U.S. Treasury Department, and John Van Reenen of MIT, conclude. Children from many different backgrounds excel in math and science tests in third grade, for instance. But it’s the wealthy children who do well in math and science that end up getting patents. Why? Because they have more exposure to innovation in their childhood, the researchers say. This exposure comes mostly from interacting with people who are themselves inventors. If young kids know people who are inventors, or hear conversations at the dinner table about research and innovation, they’re more likely to become interested in pursuing careers in that field, Chetty told me. “Opportunity broadly, and exposure to innovation in particular, are really the keys to increasing innovation,” he said. Chetty, for instance, grew up in a family of academics, and overheard conversations about science and making discoveries, which, he says, influenced his decision to pursue a career in academia.
Inventors Are More Likely to Come From High-Income Families
Aaron Hertzmann, who is now a principal scientist at Adobe Systems, has eight patents. He grew up in Palo Alto, where he had a computer before he was 10 years old. He had a lot of exposure to inventors as a child—his father was a “tinkerer,” Hertzmann told me, and his stepfather was an academic in the field of computer science. “He exposed me to approaching math as something to explore, rather than it just being a homework assignment,” Hertzmann said, about his stepfather. Hertzmann and his mother would tag along to his stepfather’s conferences in places like Greece, meeting other academics and hearing them talk about their work. From that, the idea bloomed in Hertzmann’s mind that academia, and specifically math and science, were areas where he could have an impact. When he graduated from college, his stepfather guided him along the process of applying to study for a Ph.D. Hertzmann, now 43, has a Ph.D. in computer science, and develops new ideas and algorithms at the intersection of art and computer science.
Indeed, exposure to certain specific fields makes children more likely to pursue a career, and a patent, in those fields, the researchers found. This is how they know that exposure, in addition to neighborhoods, is important to innovation: It would be unlikely that growing up in a good neighborhood would inspire many children to patent in the same small field.  People who grew up in Minneapolis, where there are many medical-device manufacturers, were especially likely to get patents in medical devices, for instance. Among people living in Boston as adults, those who grew up in Silicon Valley were especially likely to patent in computers. Children whose parents have patents in a specific field—say, antennas—are also likely to patent in exactly the same field as their parents did.
The Place Where the Poor Once Thrived
Women who grew up in an area where women held a higher share of patents in a certain field were more likely to themselves get patents in that area when they grew up. Strikingly, it was especially important for children to see people who looked like them as innovators for them to pursue the same career path—girls in an area with a lot of male innovators wouldn’t necessarily envision themselves in the same career, while boys would. If girls were as exposed to female inventors as boys are to male inventors, the gender gap between male and female inventors would fall by half, the researchers estimate. (They find that 82 percent of 40-year-old inventors today are male.)
These findings have big implications for the state of the U.S. economy, which has seen innovation decline in recent decades. Innovation is often measured by what’s known as “total factor productivity,” which essentially tracks advances that have been made in using existing resources to increase output. Prior to 1973, total factor productivity increased at an annual rate of 1.9 percent—but since then, that growth rate has fallen to 0.7 percent, according to the Brookings Institution. Innovation is central to economic growth, Chetty says. About half of U.S. annual GDP growth is attributed to innovation. Innovation, says Chetty, is what allows people to live richer, healthier, and more productive lives.
Much of the past work out of the Equality of Opportunity project has been motivated by ideas about justice, and the idea that everyone, regardless of where they are raised, should have a fair shot at the American Dream, Chetty told me. But these results indicate that equality of opportunity is important for another reason too: It makes the economy stronger. “Opportunity might be vital for economic growth even if you don’t care about inequality or fairness concerns,” Chetty said. “If you give kids from lower-income families better training and better opportunities, maybe they would end up contributing more to the economy and that would help everyone essentially.” Chetty and his team estimate that if women, minorities, and children from low- and middle-income families invented at the same rate as white men from high-income families, there would be four times as many inventors in America as there are today.
Chetty and his team came up with these results by linking patent applications in the U.S. between 1996 and 2014 to federal income tax returns to create a dataset of 1.2 million inventors. (The study uses patents as a way to measure an individual’s contribution to innovation.) They tracked inventors’ lives from birth to adulthood to determine who becomes an inventor. They then linked data on math test scores from third to eighth grade from children who attended New York City public schools to see if the differences in who got a patent could be related to innate ability (they aren’t). They then showed that children who grew up in commuting zones with higher patent rates are “significantly” more likely to become inventors than children who did not. They also showed that children from both low-income and high-income families who attend universities like MIT go on to patent at relatively similar rates, suggesting that it’s factors in a child’s earlier life that determine whether they go on to patent.
This follows on earlier research from the Equality of Opportunity project that shows that growing up in an impoverished area can hurt a child’s chances of achieving many of the pieces of the American Dream. Living in certain neighborhoods makes it less likely that a child will attend college, that they’ll earn more than their parents did, and that they’ll postpone having children until they marry.
This paper’s results suggest that policies that increase exposure to innovation childhood could go a long way in stimulating economic growth. Internships or mentorship programs could link children interested in math and science with innovators, for example, which might make them more likely to pursue careers in that field. Integration could also help—if children have exposure to more types of people, the people they think of as their peer group changes, and they might be more likely to pursue a career that is dominated by people who don’t look like them. That will help them succeed individually, and it could have a positive effect on the economy as a whole.


At the time, news of the breakthrough on December 2, 1942, was conveyed  only in code: “The Italian navigator has landed in the New World.”
Our “Italian navigator” was Enrico Fermi, the physicist who had escaped fascist Italy for America. The “New World” was not a place but a time: the atomic age. On that day 75 years ago, Fermi’s team set off the first controlled and sustained nuclear chain reaction.
It all happened under the bleachers of University of Chicago’s Stagg Field. Fermi’s nuclear reactor was a pile of graphite, henceforth known as Chicago Pile-1. It produced all of a half-watt of power. But it proved that a neutron emitted by a splitting uranium atom could indeed split another uranium atom, which could split another and another, releasing energy with each reaction. With enough atoms, the chain reaction could unleash inconceivable amounts of energy. It proved, in other words, that an atomic bomb could exist.
The rest of the story is well-known: Bombs were made. Bombs were dropped. Hundreds of thousands of people died. A war was won.
As all of this receded into history, the anniversary of Fermi’s experiment has became a time to reflect on the legacy of nuclear science. “It’s always been a complicated story,” says Rachel Bronson, president of the Bulletin of the Atomic Scientists, the journal founded by former Manhattan Project scientists concerned about atomic weapons. Over the past 75 years, as the specter of nuclear annihilation has grown and waned and grown again, newspapers reporting on the anniversary have tried to grapple with that legacy.
* * *
The first time an anniversary of Chicago Pile-1 was commemorated publicly appears to be its fourth in 1946, and that was by proclamation of the War Department. In an October press release, Lieutenant General L.R. Groves, the commanding general of the Manhattan Project, suggested December 2 as the “birthday” of atomic energy.
The War Department helpfully released a packet of materials for journalists who were not present at the once secretive Chicago Pile experiment. Two public-information officers interviewed more than a dozen of the 50 scientists, and many of the small but colorful details that would be retold in later commemorations originated in their report.
Details like the bottle of Chianti wine, brought in secretly by the Hungarian-born theoretical physicist Eugene P. Wigner. When the experiment succeeded, Wigner opened the bottle. The participants drank out of paper cups and signed their names on the bottle’s straw wrapper.
And details like the graphite dust that blanketed everyone. (Graphite was used as a “moderator,” to slow down neutrons so they could split uranium atoms.) Albert Wattenberg, one of the young physicists that helped build the pile, told his interviewers:
“We found out how coal miners feel. After eight hours of machining graphite, we looked as if we were made up for a minstrel. One shower would remove only the surface graphite dust. About a half-hour after the first shower the dust in the pores of your skin would start oozing. Walking around the room where we cut graphite was like walking on a dance floor. Graphite is a dry lubricant, you know, and the cement floor covered with graphite dust was slippery.”
The Chicago Pile was a genuine scientific breakthrough, but other, more famous milestones like the Trinity test and the Hiroshima bombing have also been pegged as the beginning of the atomic age. Perhaps the War Department chose December 2, 1942, as the birthday of “atomic energy”—note: not “atomic bomb,” a phrase that never appears in the press release—because it represented a purer scientific achievement. Nuclear science had not yet been used for destruction; it could just as well power our homes and save lives through medicine.
When The New York Times covered the fourth anniversary in December, science writer William L. Laurence hinted only vaguely at “incalculable potentialities for good and for evil.” Laurence is credited with coining the term “atomic age” and he is a controversial figure in journalism. During the war, he worked for the Manhattan Project as its historian. Then he returned to the Times to continue reporting on the very project for which he worked, even winning a Pulitzer for his dispatches from Nagasaki. In 2004, journalists argued his Pulitzer Prize should be revoked because of his “uncritical parroting of propaganda.” He dismissed, for example, Japanese reports that people were dying from radiation days after the bombings.
“We will probably never know the true extent to which William Laurence was co-opted, compromised, or corrupted by his military and governmental connections and involvements. It appears that in many ways, he was never really certain himself,” Mark Wolverton recently wrote in Undark. But from the very beginning, the story of the birth of the atomic age was being written by the very people who ushered it in.
In 1952—now the 10th anniversary of the experiment—the Kentucky New Era quoted Arthur Compton, the physicist who oversaw Fermi’s work, speaking at a luncheon of the Chicago Association of Commerce and Industry. (Compton was the one who spoke the words: “The Italian navigator has landed in the New World.”) Compton defended the use of the bomb, but he was more eager to stress the civilian impacts of the experiment, emphasizing energy as the War Department’s press release did:
As a scientific tool, the importance of the nuclear reactor is comparable with that of the cyclotron. As a means of improving health, it may reasonably be compared with the betatron, a new type of supervoltage instrument for producing X-rays and beta rays ... As a means of defense, I would rate the atomic weapons as comparable in importance with the airplane. But the great significance of nuclear energy seems to be as a source of useful power.
When the 25th anniversary came around in 1967, World War II was receding from memory and the Cold War had come startlingly close to turning hot. It was atomic weapons that Americans were thinking about again. Volney Wilson, another physicist who worked on the Chicago Pile, speaking to the Schenectady Gazette, was far less optimistic: “It’s been a big disappointment to me ... I would have thought that the development of this horrible weapon would have been more of a force to bring the world together.” Wilson was a pacifist who was always ambivalent about building a bomb, but his words now had a note of bitterness.
The 50th anniversary came at a more optimistic time: 1992. The Soviet Union had dissolved. The United States was the world’s only superpower. The Soviet Union was not only dismantling its warheads, it was selling them to the United States for electricity. “Highly enriched uranium from former Soviet weapons once targeted on our cities will be used to light and heat those same cities as fuel in American nuclear power plants,” William S. Lee, president of Duke Power, said at a November 1992 meeting of the American Nuclear Society.
But, it was not lost on journalists that this was still the atomic age.  Articles written for the 50th anniversary note that Russia and the United States still had enough nuclear weapons to kill millions, and several other countries were pursuing their own. “Fifty years later, the legacy of the Chicago Pile remains mixed,” Earl Lane wrote in Newsday.
Which brings us to the75th anniversary of the Chicago Pile. Nuclear power is on the decline in the United States today. Nuclear weapons are ever present in the news again. Yet nuclear science has also produced real breakthroughs in science and medicine. The legacy of the Chicago Pile is mixed, and it probably always will be—until, and such is the nature of nuclear weapons, the day it is clearly not.


When Bernadette Demientieff was in high school, she gave up her heritage. Demientieff is a member of the Gwich’in, an indigenous tribe of roughly 9,000 people that spans north-central Alaska and northern Canada. “The ways of living in this world that are being pushed on our people” got to her, she told me. She moved south to Fairbanks, Alaska, and grew disconnected from her people and their land. She had kids. She grew up.
And then, one day in 2014, something called to her, she says. She was in Arctic Village, a small Gwich’in settlement at the edge of Alaska’s wilderness. She felt the urge to step out onto the tundra. She started walking, up and out of the center of town—and then she turned around and looked: In front of her stretched the Arctic National Wildlife Refuge, the largest area of untouched wilderness in the United States. The land, an open expanse of peaks and rivers, spanned hundreds of miles past the horizon to the unseen, icy flat of the Arctic Ocean.
“I started crying and crying,” she said. “And I asked the Creator for forgiveness.”
Now 42, Demientieff is the executive director of the Gwich’in Steering Committee. She has spent years trying to protect the Arctic National Wildlife Refuge, or ANWR (pronounced AN-wahr), from oil and gas exploration. That fight suffered a major loss Saturday, the result of lawmakers voting on an expansive and quickly written bill several thousand miles away.
The Tax Cuts and Jobs Act, which the Senate passed early Saturday morning, will change federal law on a matter that has little to do with the tax code. The bill authorizes the sale of oil and gas leases in a section of the ANWR on Alaska’s North Slope, the coastal plain that faces the Arctic Ocean. Soon, energy companies will be able to search for—and extract—oil and gas from the frozen tundra.
The Senate bill will now be reconciled with the House version in conference and go to President Donald Trump’s desk for his signature.
It brings a quiet end to the battle over whether to drill in the ANWR, one of the longest-running and most acrimonious battles in U.S. environmental history. The question has been embedded in federal law for 40 years, nearly as long as Alaska has been a state.
No one will be more affected by the opening of ANWR than Alaska’s indigenous people, who will live among—and work on—the rigs, drills, and pipelines that would follow the discovery of any oil or gas reserve. The discovery of oil or gas in the region could bring an economic windfall to the subsistence tribes that live on Alaska’s North Slope, the coastal plain that faces the Arctic Ocean. But if a major disaster—like an oil spill or gas leak—were to occur in the area, it would devastate their only homeland.
The issue still divides villages, counties, and Native nations in Alaska. It also sets tribes with differing claims to Alaska’s North Slope against each other. And both sides tend to assert that the overall public sides with them.
“The majority of Alaskans and majority of Alaksa Natives express their support for [drilling in ANWR]. It’s an issue of economic self-determination for our community,” said Richard Glenn, a member of the Inupiat tribe and the executive vice president of the Arctic Slope Regional Corporation, which owns nearly 5 million acres across Alaska’s northern coast. “This has been the unchanging position of the majority of the residents of our region for more 30 years.”
No recent polling data seems to be available on the issue. But even beyond public opinion, there’s a basic conflict.
The Inupiat live across the North Slope, including within the part of the ANWR that would soon be opened for drilling. Oil exploration already brings jobs and funds infrastructure in their communities. And the Arctic Slope Regional Corporation holds mineral rights to pockets of private land within the ANWR. If oil is discovered there, the corporation and its shareholders—roughly 13,00 members of the Inupiat tribe—could profit from the wealth.
The Gwich’in people, meanwhile, live hundreds of miles south in west-central Alaska. Their regional corporation does not own land on the North Slope. But the Gwich’in are spiritually connected to the porcupine caribou, a herd of more than 160,000 creatures who migrate annually across the U.S. and Canadian tundra. The herd’s calving grounds, the most sacred space to the Gwich’in, lies within the area which could soon be open to drilling. To many of them, drilling in the calving ground isn’t just an attack on the Gwich’in way of life. It’s an attack on the Gwich’in.
The immediate stakes of the fight go back to 1980, when Congress passed the Alaska National Interest Lands Conservation Act. The law protected more than 67,000 square miles of land (174,000 square km) across Alaska by establishing new national parks, national monuments, and wilderness areas. One part of the law, Section 1002, set aside 2,300 square miles of land (6,070 square km) on Alaska’s North Slope. Though this parcel of land—dubbed the “Section 1002 area”—was made part of the ANWR, Congress did not endow it with full wilderness protections and reserved the right to open it to gas exploration in the future.
Every decade or so, the question of whether to open the 1002 area has made it onto Congress’s agenda. In 2005, the Senate nearly opened ANWR to drilling before a Democratic filibuster turned the tide of public opinion. Ten years earlier, Bill Clinton vetoed a proposal to open ANWR. The closest Congress ever came to completing a deal was 1989, but the Exxon Valdez oil spill intervened, rendering the loosening of fossil-fuel rules in Alaska politically impossible.
While public opinion has always previously halted the opening of the ANWR, in today’s supersaturated news environment, Senate Republicans have slipped the drilling provision into the tax-reform bill without attracting the same outcry. Senator Lisa Murkowski, a Republican of Alaska, made drilling in ANWR a condition of her support for the tax bill, and it has been a de facto part of the legislative package since October.
It’s unclear how Americans feel about the ANWR proposal. A Morning Consult poll in 2014 found that only 50 percent of voters want to drill in the area, even though many more—61 percent—support increasing oil extraction in the United States. Neither a majority of Democrats nor Independents supported drilling.*
The modern history of the Gwich’in people is inseparable from drilling in the ANWR question.
In 1988, Congress began exploring how to open the refuge to drilling. The Gwich’in could not abide the thought. They held their first conference in 150 years, an unprecedented gathering of their people from across the United States and Canada. Demientieff remembers those early meetings—sitting on the floor as a little girl, hearing the fear and anguish of the adults. The elders gave her little tasks to keep her involved, like passing out crackers and dried fish to the members, as angry, passionate arguments raged around them.
No one ever sat her down and told her about the preciousness of ANWR. But just by growing up among the Gwich’in, she learned the importance of her people’s generations-old circuit through the land, in pursuit of the porcupine-caribou herd. Every year, more than 160,000 porcupine caribou move across the high Arctic tundra. Their journey starts in the meadows around the Porcupine River, which flows through modern-day Yukon and Northwest Territories in Canada. Then the caribou come north, through the vast, peak-rimmed plains of the ANWR, until they arrive at their calving ground on Alaska’s coastal plane, at the foot of the Arctic Ocean—and in the Section 1002 area.
The Gwich’in have followed the caribou across much of this odyssey for tens of thousands of years. But they knew not to enter the calving ground, which is called Iizhik Gwats’an Gwnadaii Goodlit, “the sacred place where life begins.” Even in the famines which followed first contact with the West, the Gwich’in did not trespass on the calving ground, Demientieff told me. “The porcupine caribou herd and the Gwich’in people are one,” she said. “I’m not just making up numbers when I say that we migrated with them for 20,000 years. These caribou have been here for 2 million years.”
Caribou aren’t the only animals that live in the ANWR. Polar bears, brown bears, and black bears all trundle through its streams and meadows. Lynx, moose, Arctic fox, walrus, and ringed seal lounge on the Arctic coast. And migratory birds—including merlins, sandpipers, and peregrine falcons—summer in the reserve before returning to the continental United States for the winter.
Even though the bill passed, Demientieff and a group of Gwich’in and Inupiat people who oppose the drilling plan to visit Washington, D.C., later this month, on the 57th anniversary of the creation of ANWR. They will drum and dance and sing and visit with members of Congress.
But they will shun Lisa Murkowski, their senator. The Murkowski family has pushed to open the ANWR as long as they’ve worked in politics. Frank Murkowski served in the U.S. Senate for more than 20 years. He led successive efforts to open ANWR to oil drilling, all of which failed. When he was elected governor in 2002, he resigned from the Senate and named the majority leader of the Alaska House of Representatives—his daughter, Lisa—to his old seat. She now chairs the Energy and Natural Resources Committee.
The ANWR provisions were widely seen as the chit Murkowski needed to support the tax bill. “For Murkowski to just turn as soon as they put that in there—it’s like they’re doing that just for her vote, and she’s falling for it,” Demientieff argues.
Protest isn’t the only way she is marking the anniversary. Demientieff is expecting a fourth grandchild due December 6, the anniversary of ANWR’s creation. She has five children, all Gwich’in or Gwich’in-Yupik.** Her 9-year-old daughter, Lexine, is part of Our Children’s Trust, the group of kids suing the federal government for its lack of a climate policy. Demientieff calls her “my little Gwich’in warrior.”
I asked Demientieff what it would feel like to know drilling would go through at ANWR. “Just when you said that right now, I got a big lump in my throat,” she told me. “We shouldn’t have to be fighting for our human rights. We’re not asking for anything, we’re not asking for money, we just want to continue our identity as Gwich’in. And that identity—a big, huge part of it is the porcupine-caribou herd.”
The history of drilling in ANWR arises from the history of the state of Alaska. When Alaska was a territory, the federal government owned virtually all of its land. Upon granting it statehood in 1959, the government transferred about a quarter of Alaska’s total area to the state government. The next year, President Dwight Eisenhower also established federal protections on the area that would become ANWR.
Eleven years later, the U.S. government transferred more federal land—about a third of the state’s area—to Native tribes in Alaska. But it didn’t transfer the land to them directly. Instead, it established regional corporations to hold the land for tribal shareholders.
Glenn manages land and natural resources for one of those companies, the Arctic Slope Regional Corporation, which owns acreage across the northern coast. The corporation has fought for the right to drill for years, and he marveled at how speedily it had been approved now. “It’s a faster-moving issue than it has been in the past 30 years,” he said.
The Republican tax bill assumes that drilling in ANWR will generate $1 billion in federal revenue over the next 10 years. During its last survey of the region, the U.S. Geological Survey said that 12 billion barrels of recoverable crude oil may lie beneath the reserve.
Glenn brushed away the estimates. “The USGS has published potential reserves, and I’m sure the industry explorers have their own number,” he told me. “I’m a geologist by training, and the only thing that proves a reserve is a drill bit.”
What was important, he said, was that the process of looking for oil was allowed to continue. It was this search for oil that drove the Inupiat communities on the North Slope. “Our region basically depends on continued safe, responsible exploration and development,” he told me. “It’s improved the quality of life in our community only because we’ve been able to tax the presence of the oil industry in our region.”
He pointed to Point Hope, a town of roughly 630 people on the western tip of Alaska’s Lisburne peninsula. “The runway there, the roads, the schools, and the ability to flush a toilet are only because of the presence of taxable revenue in our region. We’re all native folks, and we depend on our environment for our sustenance. But we also depend on having communities to come home to,” he said.
Glenn grew up in the small city of Utqiaġvik, commonly known as Barrow, on Alaska’s North Slope. The 54-year-old now lives in Anchorage, but remains active in Inupiat life on the North Slope: He is a subsistence hunter and fisher, and a whaling captain.
He described how he’ll often encounter human skeletons on his hunt—a relic of the time when aging members of a community might wander away from a village or tent in the winter, because they had become too much of a burden to the community. Seeing the skeletons, he said, “means our whole region is sacred land.” But he takes a decidedly pragmatic approach to the holiness of the land. “The environment is just as sacred to us as it is to everyone else, but you know what? We need an airstrip,” he said. “Even the place where we built a sewage lagoon for our village is sacred land—but we needed a sewage lagoon.”
These pieces of infrastructure were not just niceties, but the basis of their community, he told me. “When you get dropped off on the tundra in our region in winter or summer, you’ll never be so happy to find a little bit of infrastructure.”
Glenn believes the region can endure industry in part because it’s done so before. From the 1950s to the 1980s, a chain of U.S. Air Force bases lined the North Slope of Alaska. Together they formed the Distant Early Warning system, a Christmas-light string of installations that linked Alaska to central Greenland, scanning the sky for missiles. Each DEW base brought “power plants and landing strips and people,” says Glenn, but the region persevered. “The animals were there; the animals survived. The people were there; the national psyche survived,” he said. “There’s been industry and Natives all across the coastal plain.” (Glenn is, in part, a product of the DEW line: His dad, a white American, met his Inupiat-Eskimo mom while stationed in the high Arctic.)
I asked Glenn how he, as a geologist who has studied Arctic sea ice, thinks about the climate consequences of extracting oil from the North Slope. “I think about it all the time,” he said. “But the reality is that our region depends on oil and gas development. If we stop exploration, our communities dry up. And [by stopping exploration] we don’t change the climate one bit—it just means someone’s gonna open up the valve somewhere else.”
Not all Inupiat approve of drilling in ANWR, however.
“For me, it’s disappointing. We should have an area that we’re able to have added protections, where we don’t change what’s natural to our environment,” said Rosemary Ahtuangaruak, an Inupiat woman and a public-health advocate who lives in Nuiqsut, Alaska.
Nuiqsut, a village of 500, sits on the North Slope, to the west of Prudhoe Bay. It is surrounded by the oil and gas industry. Ahtuangaruak told me that some nights the particulate-matter pollution from the natutal-gas flares gets so bad that she has to stay up all night, tending to people in respiratory distress.
She also mourned what the drilling would do to the caribou. “There are four major herds in our community. Three of the herds are in severe declines. The only herd that isn’t is the porcupine-caribou herds,” she said.
Ahtuangaruak doubted that ASRC always acted in the best interests of the tribe. They were a corporation, she said, and not a government. “Their priority is profitability at all costs. But when we’re tribal people, our priority is our way of life and who we are in the future.”
Sharon Lord also opposes the ANWR plan. She operates a bed-and-breakfast in Kaktovik, Alaska, a village of several hundred people that is the sole settlement within the Section 1002 area. Lord’s father, Robert Thompson, is a famous anti-drilling activist. “This land is beautiful,” she says. “And I like our lifestyle the way it is. I don’t want it to change. If the oil company comes in here, they’ll turn it into an industrial area. ”
She adds: “There’s always a potential for an oil spill. There’s absolutely no way an oil spill can be cleaned up here. It would create an environment of irreversible damage.”
Most of the community, she said, opposed the drilling.
If that’s true, then it hasn’t reached Matthew Rexford, the Inupiat tribal administrator of Kaktovik. In November, he endorsed the drilling plan in testimony to Congress. “The Arctic Inupiat will not become conservation refugees,” he said at the time. “We do not approve of efforts to turn our homeland into one giant national park, which literally guarantees us a fate with no economy, no jobs, reduced subsistence and no hope for the future of our people.”
He delivered those remarks sitting next to Richard Glenn. Weeks later, back in Anchorage, Glenn encouraged me to “find something to help you sort the wheat from the chaff” on the emotion behind the ANWR drilling plan.
“The habit of journalists tends to be to find Native folks on both sides of the issue, and leave it to the reader to decide,” he said. “It would be nice if we could rise above that and say, what do those folks closest to the issue think?”
* This article previously misstated the proportion of Democrats and Independents that opposed drilling.
** This article previously misstated the tribal identity of Demientieff’s children. We regret the errors.


Earlier this May, Gregory Holt had just finished doing the morning rounds at Miami’s Jackson Memorial Hospital, when he got a call about a new patient in the emergency room. He went down with seven colleagues to find an unconscious 70-year-old man with breathing problems and signs of septic shock. He was alone and had no identification. His blood was full of alcohol, and its pressure was dropping. And when the doctors peeled back his shirt, they found a tattoo, running along his collarbones.
It said: DO NOT RESUSCITATE.
The NOT was underlined. There was a signature under the final word.
Holt was shocked. “We’ve always joked about this, but holy crap, this man actually did it,” he says. “You look at it, laugh a little, and then go: Oh no, I actually have to deal with this.”
By default, doctors would treat patients in this man’s condition as if they were “full code”—that is, they’d want everything possible done to prolong their life. “When faced with uncertainty, you pick the choice that’s not irreversible,” Holt explains. “So let’s do everything we can and when the dust settles, we can determine what the patient wanted if it wasn’t clear from the beginning. The tattoo threw a monkey wrench into the decision.”
In Florida, patients can ask not to be resuscitated by filling in an official form and printing it on yellow paper. (Yes, it has to be yellow.) Only then is it legally valid. Clearly, a tattoo doesn’t count. And yet, the patient had clearly gone through unusual effort to make his wishes known. The team members debated what to do, and while opinions differed, “we were all unanimous in our confusion,” says Holt.
They decided to temporarily ignore the tattoo, at least until they could get advice. In the meantime, they gave the man basic treatments—antibiotics, an IV drip, an oxygen mask to help him to breathe, and adrenaline for his plummeting blood pressure. But they avoided putting a tube down his throat and hooking him up to a ventilator. “It would have hurt to see a man with a DNR tattoo having a tracheal tube hanging out of him,” Holt says.
All of this bought them enough time to get a hold of Ken Goodman, the codirector of the University of Miami’s ethics programs. “My view was that someone does not go to the trouble of getting such a tattoo without forethought and mindfulness,” Goodman says. “As unorthodox as it is, you do get a dramatic view of what this patient would want.”
But tattoos are permanent and desires are fleeting, so the team pondered whether the words actually represented the man’s desires. And there’s good reason to be cautious. Back in 2012, Lori Cooper at the California Pacific Medical Center was caring for a (conscious) patient who was going to have a leg amputated, when she noticed a “DNR” tattoo on his chest. The man revealed that he got the tattoo after losing a poker bet many years ago, and actually, he would very much like to be resuscitated if the need arose. “It was suggested that he consider tattoo removal to circumvent future confusion about his code status,” Cooper wrote. “He stated he did not think anyone would take his tattoo seriously and declined tattoo removal.”
Holt’s unconscious patient couldn’t weigh in, but social workers used his fingerprints to track down his identity. He had come from a nursing facility, and to everyone’s immense relief, they had an official DNR form for him, printed on the requisite yellow paper. The man’s condition deteriorated, and he passed away in the night.
The team did the right thing, says Nancy Berlinger from the Hastings Center. They provided basic care to buy time, called for an ethics consult, and got social workers involved. “Even if the records weren’t there, it was right to honor the patient’s preferences,” she says. “Paper gets lost, and some people do not trust paper. This man may have been trying to safeguard against that, and [the tattoo] might have been the most reliable way to make his voice heard. It was right to take it seriously.”
But Lauris Kaldjian, an ethicist at the University of Iowa, says he wouldn’t have honored the tattoo without finding the official form. A DNR order isn’t an end in itself, he says. It’s a reflection of a patient’s goals—how they want their life to end. Patients are meant to discuss those goals with a physician so they can hear all the options available to them, and make an informed decision; the physician must then sign the order. “That’s not meant to be a paternalistic move,” Kaldjian says. “It’s meant to give evidence that a rational discussion was had, and I don’t think tattoo parlors are a place to have to have a code-status discussion.”
It’s the discussion that matters, not the words on the form (or the tattoo), says Joan Teno from the University of Washington, who studies end-of-life wishes. And in many cases, those discussions don’t happen, or aren’t respected. In a study of bereaved family members, she found that one in 10 say that something was done in the last month of a patient’s life that went against their wishes. “The fact that someone has to resort to a tattoo to have their wishes honored is a sad indictment of our medical system,” Teno says. “We need to create systems of care where patients have the trust and confidence that their wishes will be honored. That’s the important message from this case.”


By our third day at sea, we’d found it: a dozen bare and jagged piles of rock surrounded by ocean the color of Windex. It was smaller than I had imagined, all told about twice the size of a soccer field. There was no white sand, no volcanic peaks, no palm trees, none of the trappings of other tropical island chains at this latitude, just razor-sharp umber peaks iced in a thousand years of bird shit—the whole of it resembling a kind of sinister Gilligan’s Island.
But we didn’t motor from the coast of Brazil more than a thousand kilometers (620 miles) across the Atlantic on a beach holiday or three-hour tour. We came to explore the waters deep below the sunlit surface. We came to collect clues from this place, known as the Saint Peter and Saint Paul Archipelago, about how life on Earth first began—and how alien life might evolve on other planets in the solar system.
These are big, serious questions, and we’ve brought a big, serious team to investigate them, including a crew of more than 40 geologists, microbiologists, geophysicists, biologists, engineers, deep-ocean divers, and deckhands from a dozen nations. The team will spend the next two weeks aboard the MV Alucia—a 56-meter (184-foot) research vessel operated through the Dalio Ocean Initiative—scanning the ocean floor, sampling rocks, analyzing water samples, and diving research submarines a thousand meters beneath the surface.
Nobody has ever explored these deep waters, and no one on the team knows what we’ll find.
“It’s a unique area, and so it might host some unique life systems,” says Frieder Klein, a marine geologist, who’s leading the scientific team from Woods Hole Oceanographic Institution. Klein is standing barefoot on Alucia’s top deck in cargo shorts and a faded MC5 T-shirt, squinting in the noonday sun. A few hundred meters to the north, waves crash and fizz on the shores of the 15 bite-sized rock islets.
Klein tells me that below us, millions of years ago, the tectonic plates of the Mid-Atlantic Ridge began splitting apart. This gap has widened by about a finger’s width every year since, which is why Europe and North America are now separated by nearly 7,000 kilometers (4,350 miles) of ocean. Over the course of this very slow process, mantle rock, which usually lies hidden 6 kilometers (3.7 miles) below the crust, has been forced to the surface.
Mantle rock isn’t particularly rare; it covers wide swaths of the deep seafloor around the globe. Here, though, it’s much shallower, much more accessible—and it’s continuing to evolve as it interacts with seawater. “There is really no place like this in the world,” Klein says, wiping his forehead of sweat.
The rock here could also be harboring entirely new forms of life. Klein explains that a chemical reaction between seawater and the iron in mantle minerals creates hydrogen molecules. Microbes, single-celled or multicelled microorganisms, feed off this hydrogen. These organisms are similar to those that existed on Earth billions of years ago and may be closely related to our planet’s earliest life forms. Klein and his team will seek out microbes in the deep and analyze the chemical processes within the mantle rocks as they occur. In doing so, the scientists hope to catch a glimpse of early life systems—a sort of window back in time to our most primitive selves, and perhaps to our alien counterparts.
“Icy moons of Saturn and Jupiter, Europa and Enceladus, have water below their surfaces; we know that,” says Klein. “And these moons contain the same rocks that are on these islands.” If distant moons in our solar system have the same rock, and the same water, they could have the chemical processes that feed the same basic forms of life here on Earth.
Klein and I peer over the handrail and stare down into the ocean depths that plummet 4,000 meters (13,100 feet), the equivalent of 10 Empire State Buildings stacked on top of one another. Suddenly, it feels less like we’re on a boat looking into the surface and more like we’re on a spacecraft hovering over some alien world.
“We’re going where nobody has gone before,” says Klein. The captain cuts the motor and the Alucia gently drifts toward the southernmost islet. Klein gathers his phone and a water bottle, excuses himself, and hustles downstairs. After a year and a half of planning, it’s time to go deep.
* * *
It’s a hard thing to fathom, the concept that you, me, the birds, and the bees—all life that is and has ever been—came from a few chemical reactions on some ugly rocks a few billion years ago. Proposing such a theory in the 1500s would have likely gotten you beheaded for heresy. Even 50 years ago, it might have gotten you canned from a tenure-track teaching job, or at least ostracized by the scientific community.
That all changed in 1977 when an Oregon State University marine geologist named Jack Corliss chartered a research vessel off the coast of Ecuador and steamed out some 320 kilometers (200 miles) to the Galapagos Trench. Corliss suspected that a volcano, what marine scientists call a hydrothermal vent, was erupting on the deep seafloor in the area. Corliss and his crew deployed a remotely operated vehicle rigged with a camera named ANGUS to investigate. In one particular spot, at a depth of around 2,500 meters (8,200 feet), ANGUS’s temperature gauge registered a significant spike. After several hours, the team lugged ANGUS back on deck, cracked open the camera, and developed the film.
The 13 grainy photographs that ANGUS had captured when the temperature spike occurred revealed something extraordinary. There was life down there—crabs, mussels, lobsters, worms—all flourishing in complete darkness around a toxic plume of seawater hot enough at its source to melt lead. The incredible pressure, 250 times that on the surface, kept the water from turning to steam. Corliss had found a pressure cooker of life. And not only were all of the animals in this pressure cooker new to science; but, even stranger, they survived in an entirely different biological system.
Unlike surface life, which requires the sun’s light to survive, these life forms lived off the chemical energy in these superhot toxic plumes—a process called chemosynthesis. Corliss called the place the “Garden of Eden.”
In the years that followed, researchers would find more chemosynthetic communities on seafloors around the world. The deep ocean, it appeared, was not a wasteland but a sort of galaxy composed of independent biospheres, each orbiting around its own life-giving chemical “sun.” And the animals and microbes there had been flourishing for billions of years, perhaps longer than life in the terrestrial world.
Hydrothermal vent discoveries spurred geologists and microbiologists to dig even deeper, into even more extreme environments, on a quest to find the absolute limit of deep life. They drilled 3,600 meters (nearly 12,000 feet) through Antarctic ice and discovered an underground lake twice the size of Delaware that has likely been sealed off from the surface for 15 million years. In a single half-liter of water, they discovered thousands of bacteria that could survive in nearly every imaginable environment: extreme heat up to 122 degrees Celsius (252 Fahrenheit), extreme cold to -20 degrees Celsius (-4 Fahrenheit), acidic, alkaline, aerobic, anaerobic, and everything in between.
Then researchers plumbed beneath the seafloor of the world’s deepest ocean—nearly 11,000 meters beneath the surface to one of the most inhospitable environments on the planet. They found twice the amount of microbial life that had been discovered at milder, shallower depths.
They dug into the terrestrial surface as well, more than 4 kilometers (2.4 miles) through the crust, to find life forms steaming in water and sulfur that fed not off of the sun, or chemicals, but on radiation from the surrounding rocks. And this stuff had been living there for millions of years.
These discoveries suggest that there is virtually no limit to life. Even at the Earth’s most vicious extremes—from the edges of volcanic calderas to the black water pressurized to 15,000 pounds per square inch to radioactive waste sites—life finds a way. Life persists.
It turns out that in many ways, rocks at the bottom of the deep sea, buried under a mile of Earth’s crust, or covered in bird crap on Saint Peter and Saint Paul Archipelago aren’t inanimate objects at all. They are undulating, “breathing” systems crammed with organisms so tiny and metabolizing so slowly that nobody ever noticed. Until a few hardy scientists started looking.
Most researchers never bothered. Seeking out extreme life requires traveling to some of Earth’s most far-flung and miserable environments. Only a handful of microbiologists and geologists have had the will, fortitude, and resources to endure weeks in the triple-digit heat within African mines, or months in the frozen expanses of Antarctica, or years sifting through the polluted oil fields of Dagestan to find answers.
Which makes it all so much more guilt-inducing for our team to be lounging aboard the Alucia. While the dangers and discomforts of bobbing around in the middle of the Atlantic 600 miles from the nearest hospital are real and many, we’re at least comforted by the borderline luxury of our living and work quarters. Here every square inch of interior space is acclimatized to a refreshing and humidity-free 72 degrees Fahrenheit—so brisk that some of us futz around wearing sweaters and socks while the outside temperature climbs toward 100.
Tonight’s dinner, served in the mess hall, includes quinoa, steak, chicken, sautéed green beans, roasted potatoes, farmer’s salad, and homemade crème brûlée for dessert.
I grab a plate and scoot next to the two other lead researchers on the scientific team. Diva Amon is a deep-sea biologist from the Natural History Museum of London. She grew up swimming off the shores of Trinidad and Tobago and at an early age was fascinated with the diversity of sea life, especially the animals that dwelled beneath the curtain of permanently black waters. She’s come on the expedition with the hope of finding large-scale chemosynthetic life, such as crabs, tube worms, shrimp, or whatever else may lie beneath.
“We really have not even a basic understanding of many animals in the ocean, especially chemosynthetic life—how they live, where they live, and why,” she says.
While the deep ocean below 200 meters (650 feet) represents 70 percent of the habitable space of the planet, Amon tells me that less than 1 percent of it has been explored. The largest animal communities and the majority of biomass on the planet live there. And the threats they face are many. Pollution, trawl fishing, mining, and climate change all put this environment and the estimated 750,000 undiscovered species down there at risk.
“We could be destroying the deep-ocean habitat, and its inhabitants, before we even know what’s there,” says Amon. “I think it’s imperative to document it all while we still can.”
Sitting beside Amon is Florence “Flo” Schubotz, a geochemist from MARUM Center for Marine Environmental Sciences at the University of Bremen, Germany. She’s come here for the same reasons as Amon, but her interests are smaller: microscopic life living within the mantle rock.
“You think about it, the organisms that live at hydrothermal vents could be some of the earliest [forms of] life on the planet—around way before life on land,” says Schubotz, who is wearing a T-shirt from a past expedition with the Japan Agency for Marine-Earth Science and Technology. “These are ancient systems.”
Schubotz explains that 3.8 billion years ago, there was scant oxygen in the atmosphere. Life relied on other chemicals to survive, including hydrogen, carbon dioxide, and methane. As these primitive organisms flourished, some of them—the cyanobacteria—evolved a type of metabolism that produced oxygen as a waste product. Around 2.4 billion years ago there was enough “waste gas” oxygen to support new forms of aerobic “oxygen-consuming” life. These oxygen-fueled life forms grew more complex, eventually evolving into plants and animals, which eventually became us.
To see these microbes in action, Schubotz hopes to collect samples of deepwater mantle rock and feed it different chemical “foods” such as hydrogen, carbon dioxide, and methane to try to spark to life dormant organisms that those samples might contain.
In a sense, she’s hoping to create a few test-tube Jurassic Parks, but instead of a human-eating T. rex, she’ll be reanimating ancient microbes.
* * *
The following day, Schubotz, Amon, and Klein are standing in Alucia’s mission control, a dimly lit room plastered in wall-to-wall flashing monitors. Everyone is staring wide-eyed at a gargantuan video display of what looks like a rainbow pie with a few pieces missing.
With every passing second, a few more pixelated lines appear on the image and the pie is a little more complete. Klein is entranced, subconsciously oohing and aahing like a stockbroker watching the ticker of an initial public offering.
What the scientists are looking at is a high-resolution bathymetric map (the underwater equivalent of a topographic map) of the seafloor below us—a rendering of data gathered by the Multibeam Echo Sounder System, a sophisticated sonar device, mounted to the bottom of the Alucia. For the next two days the ship will circle the archipelago, moving farther away with every round, like a needle on a vinyl record playing in reverse. As we pass over the ocean floor the Multibeam will scan every nook and cranny down to a resolution of about 3 meters (9.8 feet) and down to a depth of 1,200 meters (4,000 feet).
“Nobody has ever seen any of this before,” says Klein. “It’s all very exciting.” He is looking for anachronisms on the otherwise rather featureless underwater cliff. If active hydrothermal vents are here, they’ll likely be marked by telltale spires of carbonate.
Carbonate is a common substance that can be forged from many different processes. Calcium carbonate, which makes up the shells of marine organisms, covers more than half of the ocean floor. The remains of these dead organisms makes up the white stuff in the toothpaste you brushed your teeth with this morning and are in the concrete of the sidewalk you walked on outside your front door.
But the carbonate Klein is looking for is likely not created by biological activities but from minerals coming out of solution when scalding hydrothermal fluids meet cold seawater.
“There’s something promising here,” says Klein. He points to a peculiar outcropping on the western slope of the island chain. He says it seems unlikely that a rock just tumbled down from above and landed in this spot. It looks as if the structure emerged from the rocks below. “At minimum,” says Klein, “this area is worth exploring.”
* * *
We spend the next few days battling high winds and strong currents. Still, the geochemists are able to take a half-dozen water samples from around the outcropping Klein identified on the map, and deeper on the ocean floor. The water in the area has elevated levels of methane, well above what is considered normal. It’s a promising sign.
Some of our planet’s oldest life forms survived on methane and are still found around hydrothermal vents. While some organisms fed off hydrogen and carbon dioxide and expelled methane as a waste product, others fed off methane and expelled carbon dioxide. The combinations and uses may vary, but what we do know is that the presence of carbon dioxide, hydrogen, and methane usually signifies an environment that can support primitive life forms.
Which is what made it so exciting to microbiologists when, in April 2015, NASA’s recently retired Cassini spacecraft flew by Saturn’s icy moon, Enceladus, and discovered huge amounts of hydrogen spewing from its surface. Not only that, but the plumes also contained carbon dioxide and other organics, and enough energy to support huge colonies of microbial life—what one geochemist described as “the caloric equivalent of 300 pizzas per hour.”
The chemicals on Enceladus are believed to be produced continuously by vent systems similar to those on our planet, and possibly right below us at Saint Peter and Saint Paul Archipelago.
By midmorning, Klein and Amon are hoping to find out. On the aft deck the Alucia crew rolls out Nadir, a three-person submarine rigged with half a dozen cameras and lights. We watch from the outside of the sub’s acrylic pressure sphere as Amon, the submarine pilot, and a videographer squirm in their seats, unpack water bottles, and ready themselves for takeoff.
Behind them, Klein is seated in Deep Rover, a smaller, more agile two-person submarine. The plan is for Klein to grab as many samples as he can with Deep Rover’s mechanical arms while Amon makes observations of the ecosystem and any chemosynthetic animals that might live there.
Carefully, slowly, a crane lifts Nadir, then Deep Rover, over the deck and plops them into the water. A plume of bubbles, a few goodbye waves, and the subs dip below the surface, growing smaller and fuzzier until they disappear.
For the next six hours we’ll sit around, staring at the sonar readouts, waiting, watching, and listening for signs of life.
* * *
By evening, the Alucia is once again a flurry of activity. The crew is hosing off the subs, Klein is bustling around buckets of rock samples, and geochemists Sean Sylva and Jeff Seewald are cooking up water samples that Deep Rover sucked up from around the seafloor.
On deck, Sylva places a water sample in a gas chromatograph, what looks like a steampunk version of a mid-1980s microwave. Sprouting from the chromatograph’s sides is a rat’s nest of wires, tubes, and knobs all held together by wood clamps. The tubes and wires, of course, have a purpose. As the water heats up in the oven, the compounds in the water will travel through the tubes at different speeds, depending on their size. A computer rigged to the device will analyze the speed at which the compounds move, allowing the team to gauge the proportion of methane and other chemicals in the water from that site.
Meanwhile, in an adjoining makeshift laboratory, Klein and Schubotz are inspecting rock samples that Deep Rover grabbed from a depth of more than 500 meters (1,640 feet). “I got three ugly rocks, and one big freaking rock,” says Klein, wiping his hands on an old Amoeba Records T-shirt.
Klein puts a rock in my hand and points out a web of tiny veins. He explains that when olivine, a common mineral made up of magnesium, iron, silicon, and oxygen, comes into contact with seawater, it destabilizes, allowing the water to penetrate more deeply into the rock. These little veins act as rivers for life forms within the rock, delivering energy and nutrients and removing waste. Over time, the olivine slowly dissolves and other minerals form within the veins. This process creates a marbleized rock that the ancient Romans called verde antico, or what geologists like Klein call serpentinite.
“What this rock says is, yes, the process of serpentinization has occurred on Saint Peter and Saint Paul Archipelago,” says Klein. “But are we just looking here at an archive of some past process? That’s what we need to figure out.”
As Schubotz and Klein slice and pulverize the rock samples and Sylva and Seewald vaporize seawater, I step outside to the roof deck to get a breath of fresh air. It’s twilight and the night sky is already so wet with stars it looks as if there’s more light than black.
I read a scientific study hours earlier in which researchers described collecting and comparing microbes from some of the most disparate and remote areas of the planet. Nineteen of those microbes were genetically identical, regardless of where they were collected.
Microbes can’t just get up and walk, or fly, or swim from one location to another. And even though some of these identical 19 microbes were separated by more than 16,000 kilometers (nearly 10,000 miles), they metabolized food in the same way, replicated in the same way, and shared the exact same DNA. How did all these identical life forms find themselves in these far-flung locations? It would be a bit like finding members of the Osmond family on every planet in our solar system, and beyond.
Standing beneath the canopy of stars and moons and planets I can’t help but wonder: Since we all start with the same basic building blocks, might all life follow the same path? Billions of alien habitats above and below where I stand are made of the same rocks, the same water, and susceptible to the same chemical reactions that first sparked life here on Earth—and eventually evolved into the hands that are scribbling down these words and the eyes that are staring at those stars.
How many other eyes made from the same stuff, the same reactions may be staring back at us now?
This thought is quaint, for sure, reminiscent of late-night, freshman-year philosophy rants, and most likely fueled by the three cans of cheap Brazilian beer I guzzled at dinner. I get that. But later that night, as I’m lying in my bunk, staring out a port window into a sky dusted with a billion distant stars, I can’t seem to shake it.
* * *
It’s our 13th day at the edge of Saint Peter and Saint Paul Archipelago, hovering over the Mid-Atlantic Ridge. This is the morning I’ve been both anxiously anticipating and subconsciously dreading since I first signed up for this assignment months ago.
We’ve lost a week or so due to strong currents that have kept the subs onboard the Alucia, but today, the skies are clear, the sun is shining, and the ocean is glass. But I’m also starving and my throat is parched. I haven’t had a sip of water in the past 14 hours and probably won’t eat or drink until later this afternoon. Amon suggested that this all-out fast is the best way to ensure my well-being. “The last thing you want is to, you know,” she paused and shot a knowing a nod. “You just don’t [want to] feel uncomfortable down there.”
By “down there” Amon is referring to the hundreds of meters below the surface I’ll be exploring aboard Nadir over the next several hours. For any adventurer or scientist, or reasonable citizen of the world, this excursion would be an absolute dream. But, sadly, all I’ve been able to think about is what will happen if I suddenly need to relieve myself, or feel claustrophobic, or suddenly feel the urge to stretch my legs, or arms, or back. There are no windows to open a thousand meters below the surface, no bathrooms, no pulling off to the side of the road. I’ll be stuck in a child-sized seat with my legs tucked to my chest for the time it takes to watch The Godfather, Part I. Twice. Including credits.
“You should lighten up,” says Colin Wollermann, a crop-haired American who will be piloting Deep Rover. He’s sitting at a mess-hall table across from me, shoveling bacon, buttered bread, and eggs into his mouth, washing it all down with liberal gulps of coffee. “My personal approach is just to get as gassy as possible,” he laughs, and takes another bite.
Alan Scott, the pilot for Nadir and Alucia’s submarine team leader, is beside Wollermann, packing handfuls of candy bars and potato chips into a backpack should we get hungry along the way. “It’s easy, mate,” he says in a thick Scottish brogue. “It’ll go by so fast you won’t even know what happened.”
Strangely, one thing that hasn’t crossed my mind this morning are the dangers involved in cruising around along an unexplored seafloor of the Atlantic in a pressurized bubble, 1,000 meters (3,280 feet) below the ocean surface and a thousand kilometers from the nearest hospital, or airport, or doctor. When I asked Klein if he ever worried about doing this kind of research, he demurred. “The only thing dangerous about it is to have spent a year and a half planning this trip and come home empty-handed,” he chuckled. “The rest of it, riding in subs, sailing out here? That’s the fun part.”
Amon was a tad less sanguine. Days earlier she had told me a story about the Johnson Sea Link, a four-person submarine. In the summer of 1973, the same year the sub was first launched, a team of two pilots, an ichthyologist, and dive master headed out on what was considered a routine dive 24 kilometers (15 miles) off the coast of Key West, Florida. The mission was to recover a fish trap from a sunken destroyer 100 meters (330 feet) below the surface.
While attempting to ascend, the Sea Link got caught on a cable extending from the sunken ship. The passengers sat back, relaxed as best they could, and waited for help. With the emergency oxygen reserves onboard, the pilots estimated they had about 42 hours before they suffocated.
Hours passed. The temperature plummeted to 42 degrees Fahrenheit. Soon the passengers were suffering from hyperthermia. Worse, their calculations for fresh air were far too optimistic. The concentration of carbon dioxide in the air began to rise to dangerous levels.
Eight hours after the pilots called for help a Navy support ship arrived and made several attempts to disentangle the sub. Nothing worked. The passengers began to lose consciousness.
The Sea Link was finally freed 32 hours after it had been launched. Two members of the sub team were dead of carbon-dioxide poisoning; the other two were treated immediately and would live.
While the Johnson Sea Link was an extremely rare disaster and happened more than 40 years ago, it’s impossible to overlook the fact that diving hundreds of meters deep in a transparent acrylic sphere the size of a phone booth has implicit risks. Motors can fail, electronics can short, lost fishing nets can entangle. Fortunately, though, the new generation of submarines are built with so many levels of redundancy and fail-safes, the chances of anything bad happening are remote. Of the hundreds of dives Deep Rover and Nadir have made, the crew members here have never experienced a problem.
“There are risks with any research, for sure,” Amon says. “But to me, the rewards so far outweigh any of that. It’s incredible to be out here doing this kind of field research.”
A half-hour later, I’m about to experience those rewards for myself. At 10 a.m., I’m standing in socks at the foot of a steel staircase. Below me is Nadir’s open top hatch. Scott is sitting inside the sub guiding me in. “Okay, now, go slow,” he says. With a few torso twists and some sloppy footwork, I manage to wriggle into the passenger seat. Following me is Susan Humphris, a geologist who will be surveying the underwater terrain and biology during the dive.
Scott seals shut Nadir’s hatch, gives the deckhands a thumbs-up, and we slowly creep out along the ship’s aft deck toward open water. Another thumbs-up and a crane lifts us from the deck until we’re swinging a dozen feet in the air like the pendulum of an old clock. I peer between my feet and see the Alucia crowded with crew and researchers. Between them, Klein and Wollermann roll out in Deep Rover. In front of us, there is nothing but horizonless blue ocean.
We lower down into the water, splash at the surface, detach from the guide rope, and float away from the ship. “Okay, all clear,” says Scott into the sonar radio. With a gurgle of bubbles we sink below until there’s nothing but gradients of blue water all around. It’s stunning.
These bands of color aren’t a distortion from the acrylic sphere and we’re not imagining them. What we’re seeing is the spectrum of sunlight being absorbed by water molecules. Long wavelengths of light—reds, oranges, and yellows—are absorbed first, so they disappear near the surface. As we sink lower, past 15 meters (50 feet), I notice that my beige pants, my shirt, my skin, and notepad have all faded into the same bluish-gray metallic tone.
Deeper still, we sink until there is no blue, no gray, no purple—no light at all. Nothing but black. Scott flicks on Nadir’s lights. We have reached 500 meters. At this depth, photosynthesis is no longer possible. The ocean world around us now is almost entirely animal and mineral.
“Affirmative, Deep Rover, I see you,” says Scott. In the distance, two pinprick white lights emerge from the blackness. It’s Deep Rover. Even though we are only a hundred or so meters away, Wollermann and Klein will need to wait about four seconds before they receive our transmission. Radio waves can’t transmit through water, so the submarines must communicate through sound waves via a sonar system. Each audio transmission we send travels through the water column all the way up to the Alucia where it is then sent back down to Deep Rover.
About 10 seconds after our transmission, we hear an echoey squawk layered in reverb and noise come through Nadir’s speaker. Scott tells me that understanding sonar transmission takes a trained ear and time, the same kind of listening skills dentists use to translate the open-mouth garble of patients.
The sub pilots exchange a few more commands, then we turn to face Deep Rover head-on. While moving a machine around in the terrestrial world may take a few seconds, down here in the deep ocean even simple maneuvers can take minutes due to the water’s resistance and the limited power of the subs, which crawl at a maximum of about four knots (4.6 miles per hour).
The syrupy slowness of our movements combined with the rising humidity inside the pressure hull gives the whole scene a dreamlike, meditative quality. After a while, it feels as though our bodies are slowing down too—seconds turn to minutes, minutes turn to hours.
“Carbonate, very interesting,” says Humphris. She’s been recording our every move like a stats fan at a baseball game ever since we hit the seafloor. The presence of carbonate rocks, Humphris says, is an indication that there was likely, or still is, hydrothermal activity in the area. “Promising, for sure,” she says, penciling another acronym onto the sheet.
Meanwhile, in front of us, Klein has extended a mechanical arm from Deep Rover and is attempting to grab some of what looks like carbonate rocks on the seafloor. Operating a mechanical arm is difficult, and the work is slow going.
Scott takes the opportunity to hand out our lunch boxes. We munch on chips and cheer when Klein manages to coerce a rock into the sample bucket; and we boo when samples slip through the mechanical fingers and are swallowed by the blackness below.
This goes on for an hour, or two. It occurs to me how bizarre the pursuit of deep life has become. Here we are in a hollowed-out plastic marble, sitting in perfect comfort 500 meters below the surface of the Atlantic Ocean, nibbling on Flamin’ Hot Cheetos and dark-chocolate Kit Kats, watching the little steel fingers of a robotic arm probe holes into million-year-old microscopic bones. If we told our ancestors a hundred years ago we’d be doing this, nobody would have ever believed us. As I sit here, experiencing this, I’m not even sure I believe it myself.
Scott takes another handful of Cheetos, grabs the control stick, and leans back. The oxygen gauge in the sub is reading about 20 percent. Although we have dozens of hours of reserves, it’s always best to play it safe.
“Okay, that’s it,” says Scott into the receiver. “Heading up.” He flicks a switch, the electric motors hum, and we begin to rise upward. Inky blackness bleeds to deep purple which bleeds to neon blue and finally, at the surface, blinding, glorious yellow sunlight.
“Easy, huh?” says Scott, squinting in the magnified sunlight. I look at my phone. Five hours have passed since we left the surface. I nod to Scott, “Easy.” My only regret is it went by so fast I didn’t even know what happened.
* * *
That evening Schubotz is in a makeshift laboratory jostling between cases of sparkling water, condiments, and cases of Brazilian beer. She is organizing test tubes on a cutting board covered with what looks like black dust. Over the past few days, Schubotz has been soaking the samples in hydrogen, carbon dioxide, and methane, hoping to spark some kind of reaction. She’s also been trying to “feed” them heavy carbon. If there are microbes on the rocks, they’ll likely consume the carbon and become measurably heavier.
Cells in the human body, such as those in the small intestine, can replicate, or “turn over,” in just a matter of days. The turnover of some deep microbes, however, can take weeks to months to years or even decades. “It’s a lot of detective work, which makes it so fascinating,” she says. “You’re dealing with just such a crazy dimension.”
Schubotz and the rest of the scientific team are certain that hydrothermal activity occurred here at Saint Peter and Saint Paul Archipelago, but suspect that the activity occurring now, if it’s occurring, is likely lower-temperature, subtler, and slower than at most other vent systems.
Over the next several months, Schubotz will take the samples back to the laboratory in Bremen, Germany, and try to determine if the heavy carbon has been consumed, which will prove that microorganisms are active—that the rocks here are harboring hydrothermal life. “Only time will tell,” she says, shooting a smile.
* * *
On our last day here, we finally have the opportunity to set foot on dry land. Not as if there’s much of it, and not as if there’s much to see. Nothing really grows on Saint Peter and Saint Paul Archipelago; there is no sand, no shade. The only people who live here are a crew of mostly shirtless Brazilian Navy sailors who rotate every fortnight. As we approach in a smaller tender boat, the sailors wave us in. We tie up, climb a rusty ladder over a sheer wall of rock, and gather on an elevated wooden boardwalk.
Charles Darwin came to these little islets in 1832 while on a five-year cruise around the world aboard the HMS Beagle. Upon landing, he described being surrounded by two species of pelicans and gulls, so “gentle and stupid” they remained perfectly calm, and perfectly still in his presence, surely because they had never seen humans before.
Those days are, sadly, gone. As the crew and I scurry past, the few hundred brown boobies (Sula leucogaster) that call the archipelago their home lunge at our ankles, shins, and knees. We manage to escape them and enter the porch of the plywood Brazilian Navy bunkhouse. We exchange a few bom dias, sip some water, and take a seat. The tour of Saint Peter and Saint Paul Archipelago is over.
While the rest of our group heads out for a swim, I excuse myself, hop off the walkway to explore the unpaved crevices, and discover a little secluded cove frothed in spindrift. From this vantage there are no crap-covered satellite disks, broken outhouses, bunkhouses, or plastic bottles. No sign of human presence, just bare mantle rock surrounded by blue ocean that goes on forever.
This is how these rocks looked when they were first pushed up from the seafloor, when they cracked and fissured and coupled with the seawater to give birth to primitive life.
And here we are, a few billions of years later, back together again, the descendants of that rock and water, staring at each other, still piecing together our family tree, trying to find a way back home.
This article appears courtesy of BioGraphic.
Portions of the scientific history and some facts in this article also appeared in Nestor's book Deep.


Sometime in the mid-2020s, the United States plans to launch a new member of its fleet of space observatories, one with a field of view 100 times more powerful than the Hubble Space Telescope. The Wide-Field Infrared Survey Telescope, or WFIRST, will spend six years scanning the universe. It will scour the Milky Way in search of hundreds of more exoplanets, and it will soak up the light from distant stars in the hope of understanding, even just a little, the great mystery that is dark energy.
But first, NASA has to figure out how to foot the bill.
The estimated cost of WFIRST has ballooned steadily since astronomers proposed their preliminary designs for the mission in 2011, from less than $2 billion to more than $3 billion. NASA officially began working on WFIRST in February 2016, and staff has spent the months since looking for ways to reduce its price tag. Their efforts went into overdrive in November, thanks to a troubling report from an independent committee that NASA established to help the space agency manage the mission.
The report, provided to WFIRST in October but released publicly for the first time last week, had some sobering words for NASA: The mission, as is, is “not executable” unless they find even more funding. NASA headquarters, in response to the warning, sent a memo to WFIRST folks in October instructing them to find a few hundred millions of dollars in their budget and cut it.
“It’s not fun for anybody,” said Jeffrey Kruk, the project scientist for WFIRST. “It’s very stressful. We’re trying to come up with the right answer, the best answer we can.”
The new report, as well as other official warnings about WFIRST’s rising costs, have provided a rare glimpse into the complicated, nitty-gritty operation behind funding some of NASA’s biggest missions, a process that usually doesn’t spill over into public view. Talk of saving money isn’t unusual in the earliest stages of any mission, but the level of scrutiny the WFIRST project has received so far is.
WFIRST will be the next great space telescope after the James Webb, which is scheduled to launch in spring 2019. Webb, itself the scientific successor to Hubble, will be the most powerful space telescope ever built, charged with spotting the faint light of the earliest stars and galaxies. It’s also been really, really expensive to build. In 2010, NASA estimated the Webb would cost about $5 billion. A year later, the space agency said it would take more than $8 billion. NASA has repeatedly asked Congress for money to cover cost overruns over the years, and the mission has eaten away at resources for other science programs. Officials don’t want a repeat with WFIRST.
“WFIRST, as the next in line, will be subject to scrutiny that 20 years ago it may not have gotten,” said Alan Boss, an astronomer at the Carnegie Institution for Science and one of the many members of the committee that first brought WFIRST to life.
The mission emerged from a decadal survey by the National Research Council, which outlines priorities in astronomy and astrophysics for the United States. WFIRST “presents relatively low technical and cost risk, making its completion feasible within the decade, even in a constrained budgetary environment,” the report said in 2010. This year, the cost reached $3.6 billion, and NASA headquarters is now trying to get it down to about $3.2 billion. The independent review board, however, says that figure is “not realistic for the scope, complexity, and expectations” of the mission, and suspects it’s going to cost more like $3.9 billion.
The particulars of WFIRST’s budget are not publicly available, so it’s not known where exactly all the money is going. But the report attributes the growing cost to several decisions made by NASA, which have created a mission “more complex than probably anticipated.”
Among them is the use of a 2.4-meter telescope, which the National Reconnaissance Office, an agency within the U.S. Department of Defense, donated to NASA for free. WFIRST will use the telescope to feed its main science instruments. One would think a giant gift like this would help considerably, since the WFIRST team wouldn’t have to build their own from scratch. But the telescope is bigger and heavier than the one the team initially envisioned, so it requires more heater power, a bigger spacecraft to hold it, and then a bigger launch vehicle to send it into space. Engineers also have to tweak it to suit their needs for this particular mission. “Having to work with an existing design forces a variety of small engineering choices in ways that can’t be optimized the way one would if starting from a blank sheet of paper,” Kruk said.
Another decision involves an instrument called a coronagraph, which would directly image and study the chemical compositions of exoplanets. At first, the WFIRST team planned to treat the coronagraph as a technology demonstration. This meant that it would do enough science to show its usefulness for future generations of space observatories, but not enough to classify as a full-fledged science instrument. Somewhere along the way, the scientists decided to push the boundaries a little bit—imagine the potential findings of such advanced technology!—and added to its capabilities. That move set the mission up for more planning and testing and eventually got too expensive.
And so the WFIRST team has followed the report’s recommendation to reduce the scope of the coronagraph’s capabilities. “When the gun’s at your head, you realize maybe we could do this a simpler way,” said David Spergel, a Princeton University astrophysicist and cochair of the WFRIST science team. “This pressure, while painful, is actually good.”
Removing the coronagraph altogether would chop $400 million from the total mission cost, but NASA headquarters is intent on keeping it. WFIRST’s main science instrument, a wide-field imager that will investigate dark energy, can’t do detailed analysis of the exoplanets it finds. “The nice thing about the coronagraph is it makes the entire package much more palatable to the public and to congresspeople that are interested in exoplanets,” Boss said. “Getting an image of a nearby planet, to many people, is a lot more interesting than seeing some strange number about dark energy.”
Kruk said most of the cost-cutting this month has come from constant reshuffling of the design, development, and testing schedules. The more things they can do at the same time, the less money they spend. The project is also nearing agreements with other nations, like Canada and Japan, to participate and contribute hardware—$50 million here, another $50 million there, Spergel said.
The WFIRST mission has a busy few months ahead. Next week the House Committee on Science, Space, and Technology will hold a hearing on NASA’s next generation of large telescopes, including Webb and WFIRST, and the recent report on the latter will surely come up. In 2011, Congress placed an $8 billion cap on development costs for Webb. The mission has faced frequent delays, and its launch date has slipped steadily, from the proposed 2014 to 2019. Jeremy Kasdin, a Princeton professor who is leading the coronagraph’s development on WFIRST, is wary of the public making comparisons between the financial histories of the telescopes. “It’s important that people understand it’s not a runaway cost problem like the [Webb telescope] was, he said.”
In February, WFIRST scientists will present to headquarters their latest designs and science objectives, and, if approved, will move the project onto the next phase of development. By that time, they need to hit the $3.2 billion target, too. So far, the scientists I spoke with seem confident about cutting enough costs. They just know it’s going to hurt.
“The problem is astronomers always want to make the best possible instrument they can and engineers are happy to oblige them because engineers get their jollies making something work really well in space,” Boss said. “But unfortunately, they also charge you for it.”


In 2019, if everything goes according to plan, the much-delayed James Webb Space Telescope will finally launch into orbit. Once assembled, it will use an array of 18 hexagonal mirrors to collect and focus the light from distant galaxies. This segmented-mirror design was developed in the 1980s, and it has been so successful that it will feature in almost all the large telescopes to be built in the near future.
But as always, nature got there first. For millions of years, scallops have been gazing at the world using dozens of eyes, each of which has a segmented mirror that’s uncannily similar to those in our grandest telescopes. And scientists have just gotten a good look at one for the first time.
Yes, those scallops—the pan-seared pucks of white flesh that grace our dinner plates. Those pucks are just the muscles that the animals use to close their beautiful shells. Look at a full, living scallop, and you’ll see a very different animal. And that animal will be looking right back at you, using dozens of eyes that line the fleshy mantle on the inner edges of its shell. Some species have up to 200 eyes. Others have electric-blue ones.
Inside the eyes, the weirdness deepens. When light enters a human eye, it passes through a lens, which focuses it onto the retina—a layer of light-sensitive cells. When light enters a scallop eye, it passes through a lenslike structure, which ... doesn’t seem to do anything. It then passes through two retinas, layered on top of each other. Finally, it hits a curved mirror at the back of the eye, which reflects it back onto the retinas. It’s this mirror, and not the lens, which focuses the incoming light, in much the same way that those in segmented telescopes do.
Michael Land from the University of Sussex discovered much of this in the 1960s, by carefully eyeballing the eyes under a microscope, and tracing the path that light must take within them. He identified the mirror, he showed that it consists of layered crystals, and he suggested that the crystals are made of guanine—one of the building blocks of DNA. “It’s very impressive how Land was right about pretty much everything from some pretty simple approaches,” says Daniel Speiser from the University of South Carolina, who also studies scallop eyes. “But no one has gotten a good look at an intact mirror before.”
The problem is that powerful microscopes tend to dehydrate samples in the process of analyzing them, and that would ruin the placement of the mirror’s crystals. Now, Lia Addadi from the Weizmann Institute of Science has found a way around this problem. Her team, including Benjamin Palmer and Gavin Taylor, used a microscope that rapidly freezes samples, so everything within stays in the right place. They’ve finally reconstructed the structure of the mirror in glorious detail, confirming many of Land’s ideas, and fleshing others out.
The mirror consists of flat, square guanine crystals, each a millionth of a meter wide. They tessellate together into a chessboard-like grid. Between 20 and 30 of these grids then stack on top of each other, with a liquid-filled gap between them. And the layers are arranged so that the squares in each one lie directly beneath the squares in the one above. The crystals and the gaps between them are respectively 74 and 86 billionths of a meter thick, and these exacting distances mean that the mirror as a whole is great at reflecting blue-green light—the color that dominates the scallop’s underwater habitat.
The whole structure is a master class in precision engineering. “When there is an elegant physical solution, the evolutionary process is very good at finding it,” says Alison Sweeney, a physicist at the University of Pennsylvania who studies animal vision.
This precision is all the more remarkable because guanine crystals don’t naturally form into thin squares. If you grow them in the lab, you get a chunky prism. Clearly, the scallop actively controls the growth of these crystals, shaping them as they form. Guanine crystals grow in layers, and Addadi thinks that the scallop somehow shifts the orientation of each layer by 90 degrees relative to the ones above and below it. As the layers grow outward, they do so in only four directions, creating a square. How it does that is a mystery, as is everything else about the way the mirrors form.
Also, the mirror is not an inanimate structure within the eye. It’s a living thing. The square crystals grow inside the cells of the scallop’s eye, filling them up. It’s the cells that then tessellate together to form the layers. “The cells can’t be dead,” Addadi says, “or the whole thing would break apart.” So not only must the cells control the growth of the crystals inside them, but they also have to communicate with each other to arrange themselves just so. “How do they do that? I really don’t know,” she adds.
Whatever their trick, it clearly produces results. Scallop vision isn’t going to rival ours anytime soon, but it’s far sharper than you might expect for an animal that’s basically a fancy clam. Speiser demonstrated this a decade ago by putting scallops in little seats and playing movies of drifting food particles. Even when the particles were just 1.5 millimeters wide, the scallops would open their shells, ready to feed. “The idea that these animals are forming really nice images with their eyes feels very solid to me,” Speiser says.
Addadi’s team also noticed that the scallop’s mirror is slightly tilted relative to its retinas. As a result, the mirror focuses light from the center of the animal’s visual field onto the upper retina, and light from the periphery onto the lower one. Perhaps that’s why the creature has two retinas: They allow it to focus on different parts of its surroundings at the same time.

“It’s a really amazing study,” says Jeanne Serb from Iowa State University, who has also studied scallop eyes. It helps to solve the mystery of the double retinas—something that scientists have long tried to address, with no success.
But Speiser isn’t completely convinced. He says that the eyes get easily deformed when they’re dissected, and even a gentle squish could change the orientation of the mirror and retinas. Still, he doesn’t have a better explanation, despite testing several possible ideas over the last 12 years. “Nothing checked out, and this is as good a hypothesis as any,” he says.
The next big goal for scallop aficionados, he adds, is to work out why scallops have quite so many eyes. They probably allow it to scan a wide area, but does it consider the information from each eye separately, or combine them all into a single image? After centuries of study, scientists finally know how each individual eye sees. But “we still have no idea what the animal as a whole is perceiving,” he says.


Millennials don’t always buy cars. But when they do, they apparently buy SUVs.
“The floor at this year’s Los Angeles Auto Show will look a lot like America's roads: full of SUVs,” the AP wrote this week. Car sales overall are actually slightly down this year, but SUV sales are up 6 percent.
The AP speculated that the boom in Wranglers and Explorers is the result of “a combination of low gas prices, growing millennial families, and a host of new models.”
But according to the authors of The Spirit Level, a book about the human costs of social inequality by epidemiologists Kate Pickett and Richard Wilkinson, skyrocketing SUV sales could also be tied to declining levels of social trust in U.S. society.
Fewer Americans now agree with the statement “most people can be trusted” than at any point in the past 40 years, and plummeting social trust tends to motivate individuals to make decisions that will protect their own family, class, or tribe.
The rise of SUVs in the 1980s and ’90s, Wilkinson and Pickett write, coincided with other markers of suspicion, like a growing number of gated communities and the increasing sales of home alarm systems. They point out how some of these cars’ names—Outlander, Pathfinder, and Crossfire—seem geared toward tough, suburban loners. With their rugged boxiness, SUVs are much manlier than small, socialist-approved minivans, which outsell SUVs in gentle, trusting Canada by two to one, they write. (The ratio is reversed in ballsy, do-it-yourself America.)
The main evidence for this theory is a 2005 paper by media-studies professor Josh Lauer. Lauer points out that SUVs are neither more spacious nor safer than minivans and station wagons. Instead, he argues, SUVs reflect Americans’ growing fear of others and our desire to sequester ourselves and our families from them. In other words, he writes, the “space” people actually seek in SUVs is personal space, and the “safety” is not road safety but personal safety.
Crime did not rise in the United States during the 1980s and ’90s, but fear of crime persisted, and so did sales of giant cars. Around that time, Americans also began buying mace and pepper spray, reflecting the “individualization of social risk”—or the idea that it’s on every individual to protect themselves from harm, Lauer writes.
He quotes Keith Bradsher as writing, in The New York Times in 2000, that:
The United States is in some ways becoming a medieval society, in which people live and work in the modern equivalent of castles—gated communities, apartment buildings with doormen, and office buildings with guards—and try to shield themselves while traveling between them. They do this by riding in sport-utility vehicles, which look armored, and by trying to appear as intimidating as possible to potential attackers.
Bradsher later wrote a book about SUV ownership, and, relying on market research, noted that their owners do, in fact, tend to be “insecure, vain, self-centered, insecure, and ... frequently nervous about their marriages.”
SUV ads hint at weekends full of mud-splattered adventures—the iconic image is of a Jeep Grand Cherokee perched on a desolate cliff—while dog-whistling to the buyer that the car is actually for protection back home. Lauer quotes one Toyota 4Runner ad, which showed the car parked in front of a city townhouse, as saying “It’s the only four-wheel drive to have in this neck of the woods” and that it’s “the ideal way to make tracks in the urban jungle.”
“The image of circled Conestoga wagons comes to mind,” Lauer writes.
Other ads suggest SUVs are dramatically roomier than other cars: “Imagine taking your favorite room wherever you go,” one read.
“These are not ads targeted at working-class outdoorsmen or seasoned campers,” Lauer concludes, “but insecure cosmopolitan drivers less concerned with actually roughing it than with being roughed up.”
In The Spirit Level, Wilkinson and Pickett contend that, by making people more anxious about their place in the world, inequality contributes to all manner of social ills, including violence, poor health, and, yes, reduced social trust.
SUVs, if they are an emblem of this problem, are probably a lesser one. (More significant data points might include that CEOs make 271 times the salary of the typical worker, or that people without college degrees are dying of despair.) Aggressive-looking cars are just the canaries in the coal mine—or, shall we say, the Sorento on the cliff.


In astronomy, an observation method called spectroscopy extends humanity’s reach into the cosmos. Through spectroscopy, astronomers can study different wavelengths of light coming from very distant objects in the universe, from single stars to massive galaxies, and determine their chemical composition. The technology may, one day, uncover life-giving molecules in the atmosphere of an exoplanet.
It’s a very cool thing, so it’s unfortunate that spectroscopy’s name makes it sound like an uncomfortable medical procedure. Here’s another way to think of the method: It’s the Leonardo DiCaprio of astronomy instruments, the Inception version, who insists, again and again, that we go deeper into the unknown.
Recently, one international team of astronomers spent two years heeding Leo’s call. The team, led by Roland Bacon of the Centre de Recherche Astrophysique de Lyon in France, used a spectroscopy instrument called MUSE, installed on European Southern Observatory’s Very Large Telescope in Chile. They pointed the instrument at a small patch of sky known as the Hubble Ultra-Deep Field. The field is our deepest view of the cosmos, a photograph of the universe as it was 13 billion years ago. The Hubble Space Telescope captured the view in 2004 after spending several months absorbing the light from the earliest galaxies.
The survey ended up measuring the properties of 1,600 faint galaxies, which the astronomers say is 10 times as many as have been recorded using other ground-based telescopes over the years. The survey includes 72 galaxies that have never been detected before, not even by Hubble. Altogether, the data have produced the deepest spectroscopic observations ever made, according to the team. Their findings are described in 10 papers published Wednesday in Astronomy & Astrophysics.
“When we started the project, I did not expect that we would be so successful to detect so many galaxies,” Bacon said in an email.
The 72 galaxies were hidden from earlier spectroscopy studies because they shine in only one color of light called Lyman-alpha. These galaxies, known as Lyman-alpha emitters, are young, star-producing factories. The dynamics of Lyman-alpha emitters are still poorly understood, and Bacon said studying them “must tell us something about the star formation in the early universe, a key ingredient for galaxy formation.”
The survey also found that the presence of halos of hydrogen gas around galaxies is a pretty common phenomenon in the early universe. Observing these halos is key to understanding the fundamentals of galaxy formation, Bacon said.
ESO has produced a visual tour of this deeper view of the cosmos, constructed using MUSE measurements of the distances of the galaxies from Earth. The video captures the data better than a single composite image could. Traveling through the field of view feels like staring up at falling snow:

The new survey demonstrates how spectroscopy can spice up the usual ways astronomers study distant galaxies, said Massimo Stiavelli, an astronomer at the Space-Telescope Science Institute in Baltimore. Instruments like MUSE have the capacity to provide information about the chemical compositions of every galaxy in their path, whether they can be detected in visible wavelengths of light or not.
“We tend to take an image, identify objects that we think are promising, and then take spectra of these objects,” Stiavelli said. “By being prejudiced by images, we would be missing some objects.”
The MUSE survey provides a tiny hint of what’s to come, when NASA launches its next space telescope, the James Webb, in 2019, kicking off a veritable spectroscopy party. The Webb, an infrared-light observatory, will be capable of measuring the spectra of some of the most distant exoplanets, stars, and galaxies in the universe.
Stiavelli said he has no doubt Webb will find the 72 galaxies the MUSE team discovered, and see them in even greater clarity. When that happens, Webb will unseat MUSE—as well as pretty much every other similar tool on or around the planet—as the Leonardo DiCaprio of astronomy instruments.
“This is a very meaningful appetizer of things we should be able to do with James Webb,” Stiavelli said.


New York City is a place where rats climb out of toilets, bite babies in their cribs, crawl on sleeping commuters, take over a Taco Bell restaurant, and drag an entire slice of pizza down the subway stairs. So as Matthew Combs puts it, “Rats in New York, where is there a better place to study them?”
Combs is a graduate student at Fordham University and, like many young people, he came to New York to follow his dreams. His dreams just happened to be studying urban rats. For the past two years, Combs and his colleagues have been trapping and sequencing the DNA of brown rats in Manhattan, producing the most comprehensive genetic portrait yet of the city’s most dominant rodent population.
As a whole, Manhattan’s rats are genetically most similar to those from Western Europe, especially Great Britain and France. They most likely came on ships in the mid-18th century, when New York was still a British colony. Combs was surprised to find Manhattan’s rats so homogenous in origin. New York has been the center of so much trade and immigration, yet the descendants of these Western European rats have held on.
When Combs looked closer, distinct rat subpopulations emerged. Manhattan has two genetically distinguishable groups of rats: the uptown rats and the downtown rats, separated by the geographic barrier that is midtown. It’s not that midtown is rat-free—such a notion is inconceivable—but the commercial district lacks the household trash (aka food) and backyards (aka shelter) that rats like. Since rats tend to move only a few blocks in their lifetimes, the uptown rats and downtown rats don’t mix much.
When the researchers drilled down even deeper, they found that different neighborhoods have their own distinct rats. “If you gave us a rat, we could tell whether it came from the West Village or the East Village,” says Combs. “They’re actually unique little rat neighborhoods.” And the boundaries of rat neighborhoods can fit surprisingly well with human ones.
Combs and a team of undergraduate students spent their summers trapping rats—beginning in Inwood at the north tip of Manhattan and working their way south. They got permission from the New York City Department of Parks and Recreation, which gave them access to big green spaces like Central Park as well as medians and triangles and little gardens that dot the city. And they asked local residents. “More often than not, they were very, very happy to show us exactly where they had rats.” says Combs. A crowdsourced map of rat sightings also proved very helpful.
Rats, although abundant, are not easily fooled into traps. They’re wary of new objects. To entice them, the bait was a potent combination of peanut butter, bacon, and oats. And the team placed their traps near places where rats had clearly crawled. They looked for rat holes, droppings, chew marks on trash cans, and sebum marks—aka the grease tracks rats leave when they traverse the same path to the garbage over and over again.
For the DNA analysis, Combs cut off an inch or so of the rats’ tails. (Over 200 of these tails are still saved in vials in a lab freezer.) The team also took tissue samples for other researchers interested in studying how rats spread diseases through the urban environment. And some of the rats they skinned and stuffed for the collections of the Yale University Peabody Museum of Natural History, where they will join stuffed rats from 100 years ago.
Combs is now writing his dissertation on the ecology of New York’s rats. He’s looking at how a number of characteristics—natural features like parks, social factors like poverty, physical infrastructure like the subway system—account for the spatial distributions of rats in Manhattan.
How Portland Lives With, Not Against, Its Rats
The point of all this, ultimately, is to help New York manage its rat problem, which is annoying as well as a genuine public-health hazard due to rat-borne diseases. In July, New York Mayor Bill de Blasio announced a $32 million war plan against the varmints. The New York Times noted wryly that when it came to rats, “There have been 109 mayors of New York and, it seems, nearly as many mayoral plans to snuff out the scourge. Their collective record is approximately 0-108.”
After two years of trapping rats, Combs has come to respect the enemy. At the end of our conversation, he launched into an appreciation of rats—their ability to thrive on nearly anything, their prodigious reproduction, and their complex social structure, in which female rats will give birth all at the same time and raise their offspring in one nest. “They are, quote-unquote, vermin, and definitely pests we need to get rid of,” he says, “but they are extraordinary in their own ways.”


When Margaret Rubega first read about how hummingbirds drink, she thought to herself: That can’t possibly be right.
Hummingbirds drink nectar using tongues that are so long that, when retracted, they coil up inside the birds’ heads, around their skulls and eyes. At its tip, the tongue divides in two and its outer edges curve inward, creating two tubes running side by side. The tubes don’t close up, so the birds can’t suck on them as if they were straws. Instead, scientists believed that the tubes are narrow enough to passively draw liquid into themselves. That process is called capillary action. It’s why water soaks into a paper towel, why tears emerge from your eyes, and ink runs into the nibs of fountain pens.
This explanation, first proposed in 1833, was treated as fact for more than a century. But it made no sense to Rubega when she heard about it as a graduate student in the 1980s. Capillary action is a slow process, she realized, but a drinking hummingbird can flick its tongue into a flower up to 18 times a second. Capillary action also is aided by gravity, so birds should find it easier to drink from downward-pointing flowers—and they don’t. And capillary action is even slower for thicker liquids, so hummingbirds should avoid supersweet nectar that’s too syrupy—and they don’t.
“I was in this very odd position,” says Rubega. “I was only a graduate student and all these really well-known people had done all this math. How could they be wrong?”
Even while she turned her attention to other birds, the hummingbird dilemma continued to gnaw at her. And decades later, as a professor at the University of Connecticut, she hired a student named Alejandro Rico-Guevara who would help her solve the mystery.
Born in Colombia, Rico-Guevara remembers spotting a hermit hummingbird on a fateful field trip in the Amazon. In the jungle, most animals are heard rather than seen, but the hermit flew right up and hovered in front of his face. “It was just there for a split second but it was clear that it had a completely different personality than other birds in the forest.” He fell in love, and started studying the birds. And when he read the capillary-action papers, he felt the same pang of disbelief that Rubega did. “We decided to go after it,” says Rubega. “Is it capillary action? And if not, what’s going on? We just wanted to know.”
Rico-Guevara handcrafted artificial flowers with flat glass sides, so he could film the birds’ flickering tongues with high-speed cameras. It took months to build the fake blooms, to perfect the lighting, and to train the birds to visit these strange objects. But eventually, he got what he wanted: perfectly focused footage of a hummingbird tongue, dipping into nectar. At 1,200 frames per second, “you can’t see what’s happening until you check frame by frame,” he says. But at that moment, “I knew that on my movie card was the answer. It was this amazing feeling. I had something that could potentially change what we knew, between my fingers.”
Here’s what they saw when they checked the footage.
As the bird sticks its tongue out, it uses its beak to compress the two tubes at the tip, squeezing them flat. They momentarily stay compressed because the residual nectar inside them glues them in place. But when the tongue hits nectar, the liquid around it overwhelms whatever’s already inside. The tubes spring back to their original shape and nectar rushes into them.
The two tubes also separate from each other, giving the tongue a forked, snakelike appearance. And they unfurl, exposing a row of flaps along their long edges. It’s as if the entire tongue blooms open, like the very flowers from which it drinks.
When the bird retracts its tongue, all of these changes reverse. The tubes roll back up as their flaps curl inward, trapping nectar in the process. And because the flaps at the very tip are shorter than those further back, they curl into a shape that’s similar to an ice-cream cone; this seals the nectar in. The tongue is what Rubega calls a nectar trap. It opens up as it immerses, and closes on its way out, physically grabbing a mouthful in the process.
“This has been going on literally under our noses for the entire history of our association with hummingbirds and there it was,” says Rubega. “We were the first to see it.”
This same technique is also how the hummingbird swallows. Every time it extends its tongue, it presses down with its beak, squeezing the trapped nectar out. And since there’s limited space inside the beak, and the tongue is moving forward, there’s nowhere for that liberated nectar to go but backward. In this way, the tongue acts like a piston pump. As it pulls in, it brings nectar into the beak. As it shoots out, it pushes that same nectar toward the throat. The tongue even has flaps at its base, which fold out of the way as it moves forward but expand as it moves backwards, sweeping the nectar even further back.
The thing that really astonishes Rico-Guevara about all of this is that it is passive. The bird isn’t forcing its tongue open—that happens automatically when the tip enters liquid, because of the changing surface tension around it. Rico-Guevera proved that by sticking the tongue of a dead hummingbird into nectar—sure enough, it bloomed on its own. Likewise, the tongue closes automatically. It releases nectar automatically. It pushes that nectar backward automatically. The bird flicks its tongue in and out, and all else follows.
In hindsight, the surprising reality of the hummingbird tongue should have been entirely unsurprising. Almost everything about these animals is counterintuitive. Hummingbirds are the bane of easy answers. They’re where intuition goes to die.
Consider their origins. Today, hummingbirds are only found in the Americas, but fossils suggest that they originated in Eurasia, splitting off from their closest relatives—the scythe-winged swifts—around 42 million years ago. These ancestral hummingbirds likely flew over the land bridge that connected Russia and North America at the time. They fared well in the north, but they only thrived when they got to South America. In just 22 million years, those southern pioneers had diversified into hundreds of species, at least 338 of which are still alive today. And around 40 percent of those live in the Andes.
As evolutionary biologist Jim McGuire once told me, “the Andes are kind of the worst place to be a hummingbird.” Tall mountains mean thin air, which makes it harder to hover, and to get enough oxygen to fuel a gas-guzzling metabolism. And yet, the birds flourished. Their success shows no sign of stopping, either. By comparing the rates at which new species have emerged and old species go extinct, McGuire estimated that the number of hummingbird species will probably double in the next few million years.
As they evolved, they developed one of the most unusual flying styles of any bird—one that’s closer to insects. The wings of medium-sized species beat around 80 times a second, but probably not in the way you think. When I ask people to mimic a hummingbird’s wingbeats, they typically stick their hands out to the side and flap them up and down as fast as they can. That’s not how it works. Try this, instead. Press your elbows into your sides. Keep your forearms parallel to the ground and swing them in and out. Now, rotate your wrists in figure eights as you do it. Congratulations, you look ridiculous, but you’re also doing a decent impression of hummingbird flight.
That unusual wingbeat allows them to hover, but it also allows for more acrobatic maneuvers. Hummingbirds use that aerial agility to supplement their nectar diet with insects, which they snatch from the air. While many birds can do that, they typically have short beaks and wide gapes. Hummingbirds, by contrast, have long flower-probing bills and narrow gapes. “It’s like flying around with a pair of chopsticks on your face, trying to catch a moving rice grain,” says Rubega.
But once again, she has shown that there’s more to these birds than meets the eye. Another of her students, Gregor Yanega, found that as the birds open their mouths, they can actively bend the lower half of their beaks, giving it a pronounced kink and getting it out of the way. Then, the hummingbirds essentially ram insects with their open mouths.
High-speed cameras again revealed their trick. “The moment Gregor first saw a bird fly into frame and open its beak, he stopped, and said: Hey, can you look at this?” says Rubega. She walked in and he played the footage. She asked him to play it again, and he did. Just one more time, she said. He played it again.
“That is wild, and you should know that nobody has ever seen that before you,” she told him.


In the fall of 2013, Charlotte Lindqvist got a call from a film company making an Animal Planet documentary about the yeti, the mythical apelike creature that roams the Himalayas. So, not the kind of thing scientists usually like to mess with. “Friends or colleagues were saying, ‘Oh, watch out. Don’t get into this whole area,’” she recalls with a laugh. But she said yes.
Lindqvist said yes because she is a geneticist who studies bears, and the rare Himalayan brown bear is one possible origin of the yeti legend. The team from Icon Films wanted to use science to investigate whether the yeti is real; Lindqvist wanted to investigate the enigmatic bears of the Himalayas.
Wild bear DNA is not easy to come by. Over the years, Lindqvist, a professor at the University at Buffalo, has built up a network of wildlife-biologist contacts in Alaska, who send her samples that have helped illuminate the evolution of polar bears. Scientists know much less about bears that live around the Himalayas. But if a film-production company was going to pay a crew to travel around the mountain range collecting possible samples of fur and bone, then she just might get a scientific project out of it, too.
The results of that unusual collaboration were published Tuesday in the Proceedings of the Royal Society B. Lindqvist and her colleagues used DNA to identify nine “yeti” samples.
These include: a thigh bone found by a spiritual healer in a cave that turned out to be from a Tibetan brown bear; hair from a mummified animal in a monastery that turned out to be from a Himalayan brown bear; a tooth from a stuffed animal collected by Nazis in the 1930s that turned out to be from a dog. The rest of the samples turned up five more Tibetan brown bears and an Asian black bear. For comparison with verified bear samples, Lindqvist also reached out to her network of research contacts in museums, zoos, and Pakistan’s Khunjerab National Park, who provided her with bear hair, bone, and scat to sequence.
Altogether, this search for the yeti yielded a surprising portrait of bears living around the Himalayas. The Tibetan brown bear and Himalayan brown bear, long considered to be subspecies, are quite distinct genetically. The latter diverged from all other brown bears about 650,000 years ago, when the formation of glaciers may have isolated a population that became the first Himalayan brown bears. Today, this ancient lineage of bears is critically endangered.
Lindqvist focused her analysis on DNA in the mitochondria—structures in the cell that have their own small pieces of DNA separate from the DNA in chromosomes. Mitochondria DNA is only passed down the maternal line, but when it comes to sequencing, it has the advantage of being more abundant in cells. This is especially important when working with degraded and decades-old samples. Her team eventually sequenced, for the first time, the entire mitochondrial genome of the rare Himalayan brown bear.
Other scientists have sequenced supposed yeti samples before—notably Bryan Sykes, a geneticist at Oxford who actually appeared in a previous yeti film by the same documentary team that aired on the United Kingdom’s Channel 4 in 2013. (Interest in the yeti never dies, apparently.) In it, Sykes says the hair matched no modern bears but an ancient 40,000-year-old polar bear, suggesting the yeti is actually an unknown, perhaps hybrid bear. Sykes later published the results in a scientific journal, but other scientists criticized him for extrapolating too far from a fragment of a single mitochondrial gene.
Lindqvist thinks she has resequenced one of the same samples, and based on the whole mitochondrial genome, the purposed yeti hair indeed came from a Himalayan brown bear. Ross Barnett, a paleogeneticist at the University of Durham, praised the methods in the new study. It’s the first time, he says, that he knows of a study using whole mitochondrial genomes to place bears in their evolutionary and geographic context.
The Animal Planet film eventually aired in May 2016 as Yeti or Not? Near the end, Lindqvist appears to reveal the last of the DNA-sequencing results. The show has been building up to this moment, hinting at possibilities like a new hybrid bear or maybe even an undiscovered hominid. “When I had to reveal to them that okay, these are bears, I was excited about that because it was my initial motive to get into this,” says Lindqvist. “They obviously were a little disappointed.”


Many people visit the fossil hall at Chicago’s Field Museum for the dinosaurs; but a certain kind of art lover goes for the murals. Originally painted by the famed wildlife artist Charles R. Knight in the late 1920s, each of the hall’s 28 murals presents an elegantly composed moment in time: armored squid tossed onto a desolate Ordovician beach, a duel between Tyrannosaurus and Triceratops, saber-toothed cats snarling at flocks of giant vulture-like Teratornis. There’s a dreamy quality to the images, impressionistic landscapes blending with vibrant animal figures. It doesn’t quite matter that the renderings are now scientifically out of date; they’re convincingly alive.
Such works of paleoart—a genre that uses fossil evidence to reconstruct vanished worlds—directly shape the way humans imagine the distant past. It’s an easy form to define but a tricky one to work in. Paleontological accuracy is a moving target, with the posture and life appearance of fossil species constantly reshuffled by new discoveries and scientific arguments. Old ideas can linger long after researchers have moved on, while some artists’ wild speculations are proved correct decades after the fact. Depictions of extinct animals exist in the gap between the knowable and the unknowable, and two recent books, Paleoart: Visions of the Prehistoric Past and Dinosaur Art II: The Cutting Edge of Paleoart, probe the different ways creators have tried to bridge that divide.
The Artists Who Paint Dinosaurs
As The Atlantic’s Ross Andersen wrote in a piece about paleoart in 2015, “To contemplate a dinosaur is to slip from the present, to travel in time, deep into the past, to see the Earth as it was tens, if not hundreds, of millions of years ago.” Paleoart, published by Taschen this fall, is primarily focused on how this past appeared to artists starting in the 19th century, when the genre first took root. A lavishly reproduced gallery of 160 years of prehistory-themed art, the book includes a series of short contextual essays from its author, the journalist Zoë Lescaze. Many of the animals presented in Paleoart may look odd to the modern eye: bloated, skeletal, or dragging their tails in the scientific fashion of the time. Lescaze doesn’t spend much time reflecting on the changing paleontological ideas that informed the drawings and paintings, though. “I came at the artwork through a more cultural lens,” Lescaze told me. “How they might reflect the political events of that period, or events in that artist’s own personal biography, and other techniques that any art historian would bring to a work of fine art.”
The oldest entries in the genre, in particular, illuminate how paleoart can reflect both political and aesthetic movements, Lescaze said. The first formal reconstructions of extinct animals appeared in the 1800s, around the time the first Mesozoic fossils came under scientific study. Europe was in tumult, with empires wrangling over colonial territory, and discoveries around biodiversity, extinction, and evolution were coming at a blinding pace. As such, reconstructions often took on an allegorical cast. The French artist Édouard Riou depicted marine reptiles such as Plesiosaurus and Ichthyosaurus squaring off like warships on the high seas, perhaps reacting to the naval battles of the Napoleonic wars, according to Lescaze. In the apocalyptic watercolors of John Martin, nightmarish beasts writhed and flailed in the antediluvian ooze. The artist Benjamin Waterhouse Hawkins thrilled Victorian Britain with paintings and sculptures of dinosaurs presiding as regal monarchs over tropical kingdoms full of lesser reptiles.
But paleoart didn’t really come into its own until the arrival of Knight. An American painter who began his career in the late 19th century and reached his peak in the early 20th, Knight worked closely with scientists such as Henry Fairfield Osborn and Barnum Brown to portray his subjects as accurately as possible, given the assumptions at the time. (In keeping with Osborn’s ideas, Knight gave his dinosaurs reedy, lizardy limbs, rather than the beefy, bird-like legs the fossils actually suggested.) Nearly blind by the time he was in his 30s, Knight opted for a naturalistic style full of heft and movement, with complementary colors, soft palettes, and expansive scenery. By Knight’s death in 1953, Lescaze said, his creations had directly influenced films like King Kong and Fantasia, writers such as Ray Bradbury, and a plethora of young paleontologists and artists.
During Knight’s life—and for some time afterward—paleoart remained a fairly loose field. Painters came from an assortment of backgrounds; some were trained illustrators, and others were enthusiastic amateurs. While they adhered to the larger paleontological views of the time, not everyone was necessarily concerned with anatomical rigor. In the 1930s and ’40s, European artists like Mathurin Méheut sought romance in prehistory with Art Nouveau designs and evocative watercolors, setting his bat-winged pterodactyls and drooping long-necked dinosaurs among asymmetrical arabesques. The Soviet paleontologist Konstantin Konstantinovich Flyorov (a great fan of Knight’s, Lescaze said) escaped the enforced artistic realism of the USSR by depicting the ancient world as a series of off-kilter fairy tales filled with dragon-like dinosaurs.
Toward the end of the 20th century, however, overt metaphor and experimentation were largely replaced by rehashes of Knight’s style, and artists drifted further away from the genre’s scientific underpinnings. The majority of those illustrating extinct animals were commercial artists without much knowledge of paleontology. A lack of accurate references encouraged large amounts of plagiarism; any one artist’s whim—a pose, a speculative anatomical detail—often became the de rigueur way of picturing an animal for decades afterward. (Knight’s dinosaurs, for example, have had a long and productive career in books, in movies, and on lunch boxes since his death.) There were exceptions, Lescaze said, such as the moody forests and skeletal dinosaurs that Ely Kish began painting in the 1970s. Paleoart ends its survey with her work. In doing so, it misses out on one of the most transformative periods in the genre’s history.
* * *
A major reassessment of dinosaurs that began in the 1960s, and finally took hold in the 1980s, positioned them not as dull evolutionary failures but as active, warm-blooded animals. Researcher-illustrators like Gregory Paul and painters like Mark Hallett began developing a rigorous anatomical style in accordance with new findings, slimming their animals down to lean creations of muscle and bone. In 1993, Jurassic Park tapped into this momentum, setting a new baseline for what dinosaurs should look like and sparking a popular craze that never quite faded.
The internet had a fundamental effect on paleoart, too. It became easier to find technical information on prehistoric animals’ anatomy, or the latest theories about their behavior. Image-hosting sites like DeviantArt, curated websites like The Dinosauricon, and dedicated blogs served as hubs for a growing paleoart community. Email listservs and the rise of social media meant researchers, professional artists, and amateurs could collaborate with each other on a wider scale. The field, in the 2010s, has become more accessible, accurate, and forward-looking than ever before—as well as more stylistically constrained.
Dinosaur Art II: The Cutting Edge of Paleoart is a dispatch from this internet age of paleontology, and is in some ways a revealing companion to Taschen’s Paleoart. Published in October by Titan Books, it compiles in-depth interviews and curated work from modern paleoartists across the globe, as collected by Steve White, a U.K. comics artist. (The book is a sequel to 2012’s Dinosaur Art: The World’s Greatest Paleoart.) Some of the featured illustrators, like Brazil’s Julio Lacerda, create digital images that look like photographic collages, while the artist Andrey Atuchin works in a clean, detailed style akin to that of classic National Geographic drawings. All the animals in Dinosaur Art II conform closely to modern scientific convention; most of the profiled artists work in the hyper-realistic mode that has come to define the genre. Compared to the breadth of approaches contained within Lescaze’s book, the results can look a little standardized and tame.
Today, the field is seeing a growing tension between a more cautious approach to paleoart and an urge for experimentation. In an attempt to make paleoart more academically credible, artists of the last few decades have often emphasized skeletal fidelity over all else. This proved to be a bit of an overcorrection: Compare a cat skull and a living cat, and it’s easy to see that skeletons aren’t always a good reflection of an animal’s flesh-and-blood appearance. Dinosaurs and prehistoric reptiles illustrated in the modern era have a tendency to look like skin shrink-wrapped over bone. A certain amount of cultural inertia and cliché also lingers, even in more carefully reconstructed art. Predatory dinosaurs in particular are still often depicted in relentless battle, mouths open in frozen roars.
In the 2010s, paleontologists and artists have been pushing for more radically imaginative approaches to soft-tissue anatomy and behavior, and less reliance on standard tropes. The “All Yesterdays” campaign—named after a provocative paleoart book published in 2012—challenged artists to think more broadly about prehistoric animals as living creatures, with sleep habits, social interactions, and foraging behaviors. All Yesterdays–style dinosaurs might have humps, or extravagant inflatable sacs, or unsuspected feathers. “There’s a nihilistic aspect to [the movement],” Mark Witton, a British paleontologist and one of the artists in Dinosaur Art II, told me. “We don’t really know what’s right or wrong about our [soft-tissue] reconstructions, so we might as well be as bold with them as our science will allow. … It’s more just about being honest, and exploring many possible truths rather than one tried-and-tested take on a subject species.”
Only traces of this new approach appear in Dinosaur Art II. Artists like Brian Engh, David Orr, and Rebecca Groom are exploring a wider range of styles, including conscious homage, fine art, and Pixar-inflected designs. As long as the art is grounded by a scientific understanding of the animal in question, Witton said, there’s still a lot of room for inventiveness. “Certain styles distort reality by necessity, so if we simplify the form of our subjects into basic geometries … or apply surreal color palettes, are we still making paleoart?” Witton asked. “We’re still scratching the surface of paleoart’s potential diversity.”
* * *
While paleoart is a form of scientific art, its value doesn’t always lie in its level of accuracy. According to Lescaze, while researching Paleoart, she met a Smithsonian paleontologist who showed her an original Knight dinosaur painting he had in his office. He’d fished it out of a dumpster after a new director disposed of outdated art to make space in the collections. “They’re complex artifacts, and vulnerable in a way that other works of natural history illustration aren’t,” Lescaze said of vintage pieces of paleoart. “Nobody’s going to throw out the John James Audubon, but works of paleoart that are rendered obsolete regularly get discarded. … It’s really important to look back at some of these and say, yeah, they’re not scientifically accurate anymore, but who cares? What else can they teach us?”
Whatever the influences or techniques, paleoart is fundamentally an attempt to glimpse something that can never be fully seen. Anybody who tries to reconstruct prehistory fills in the gaps with their own preoccupations, turning real animals into symbols of obsolescence, savagery, or martial power. Many modern artists are trying to strip these projections out of their art, but changing cultural ideas and paleontological consensus can make doing so difficult. “Evolution is a brush, not a ladder,” the artist Emily Willoughby notes in Dinosaur Art II: not a direct route going anywhere, but, rather, a messy bundle of approaches. It’s only fitting that the art depicting its sweep should be similarly difficult to pin down.



From The Flintstones to Focus on the Family, the stereotype has long been that men hunt and provide, while women just stir the pot. Thankfully, today many women—and men—reject both that biological essentialism and the resulting division of labor. But what can science tell us about the role our earliest female ancestors played in providing food for themselves and their communities? Meanwhile, given the fact that women have been confined to the kitchen for much of recent Western history, how have they used food as a tool of power and protest, escape, and resistance? Just in time for the holiday season, this episode we dive into two books that take on the science and history of women’s relationship with food. First, science journalist Angela Saini helps us upend conventional wisdom on “women’s work” and biological differences between the sexes; then food historian Laura Shapiro reveals an entirely new side to six well-known women through their culinary biographies. Join us this episode as we hunt, gather, and cook with women throughout history, from feral pigs to shrimp wiggle.
The idea that men and women are fundamentally different—that women are physically weaker, less rational, and equipped with smaller brains—was accepted as fact by scientists and enforced by culture for centuries. Charles Darwin himself claimed that women were at a lower stage of evolution. One widely believed argument for male superiority was that early men were the hunters—and that, through hunting, they not only provided food for their families, they also invented the first tools and the earliest forms of language, and thus, by extrapolation, everything that made humans distinctively human. In her new book, Inferior: How Science Got Women Wrong—and the New Research That’s Rewriting the Story, Saini exposes the flaws in that hypothesis to offer an inspiring revision of gender equality among early humans.
For the heroines of the culinary historian Shapiro’s newest book, What She Ate: Six Remarkable Women and the Food That Tells Their Stories, food has also proved itself an instrument of power, if not exactly equality. In this episode, Shapiro helps us explore how the disgusting food served in the Roosevelt White House—widely regarded as the worst in American history—offers a new perspective on the tensions that underlay Eleanor Roosevelt’s marriage. And she tells us how the delightful novels of British author Barbara Pym use food to explore gender, class, and character—but also help rehabilitate the appalling reputation of English cuisine. Together, these books reinforce the power of food to reshape the stories we tell about ourselves. Listen in this holiday season, then read—and then eat!
This post appears courtesy of Gastropod, a podcast cohosted by Cynthia Graber and Nicola Twilley that looks at food through the lens of science and history.


In the summer of 1922, as Simeon Burt Wolbach and Marshall Hertig slid their scalpels into 13 common house mosquitoes, they had no idea that they were about to stumble across one of the most successful microbes on the planet, nor that a century later, their discovery could potentially save millions of lives.
In those dissected mosquitoes, the duo found a new bacterium, and 14 years later, Hertig christened it Wolbachia after his colleague. Having named it, they ignored it. But in the subsequent decades, long after Wolbach’s death in 1954, scientists started to realize that his bacterial namesake is omnipresent, devious, and powerful.
It infects around four in 10 species of insects and other arthropods, which are themselves the most diverse and numerous animals in the world. It manipulates the sex lives of its hosts, changing some from males to females and allowing others to clone themselves, all so that it can spread quickly into the next generation. It even contains multitudes of its own: Nestled within its genome is a virus, and hidden within that virus are genes that originally came from spiders.
Wolbachia also makes some of its hosts resistant to viruses—a quality that a team of Australian scientists, led by Scott O’Neill, have spent decades trying to harness. When they implant the bacterium into the mosquito Aedes aegypti, the insect can no longer transmit the viruses behind dengue fever, yellow fever, Chikungunya, and Zika. And best of all, the bacterium is so good at manipulating its hosts that it can rapidly spread through a wild population. Release Wolbachia-infected mosquitoes into the wild, and within months, all the local bloodsuckers change from carriers of disease into culs-de-sac.
This approach, which I’ve written about before, is now being tested in 10 tropical countries around the world. It’s testament to the value of research for the sake of it: There was no way Wolbach and Hertig could have predicted where their arcane explorations of mosquito carcasses could have led.
The project also shows just how long it takes to translate basic discoveries into life-changing tools: It took decades for O’Neill’s team to successfully inject Wolbachia into mosquito embryos, to show exactly how the bacterium acts against viruses, to simulate its spread among wild insects to show that their approach could work in the field, and to get all-important community support for their work. You can watch their quest in the video below—the fourth in a series of online films produced by HHMI Tangled Bank Studios, which adapt the stories in my book, I Contain Multitudes.


In caves and rock walls of the southern Utah desert, pictographs have been painted, added to the backs of clamshell-shaped sandstone enclosures. Many are noted to have acoustic properties, meaning these ancient, indigenous images seem to be correlated with the way sound reflects around them. I’ve spoken in a normal voice back and forth from one sheltered rock-art panel to another an eighth of a mile down the canyon. The way sound spreads and is refocused, we could hear each other’s every word.
James Farmer, from the Utah Rock-Art Research Association, wrote that panels from the ghostly and enigmatic Barrier Creek tradition in Utah contain what he sees as thunderstorm motifs. At one of these Barrier Creek panels, he witnessed a cloudburst with thunder, waterfalls, and falling boulders. He wrote about the intensification of sound from the storm around the rock art, “it seems inconceivable to me that any ancient archaic hunter-gatherers witnessing a similar event would not have been just as astonished as me, and would have naturally invested the location with divine, supernatural powers.”
The nascent field of “archaeoacoustics” studies the way sound and archaeological sites interact. I look at this as not just an ancient feature, but one that we walk through every day. Cathedrals and capital domes have been noted for the way they capture and amplify sound. By happenstance or not, resonance is part of the way we relate to architecture, whether human-made or carved by nature.
Have you ever walked through an airport or the lobby of a building and noticed a sudden change in the acoustics? Even in a crowd, you hear your own footsteps as if you’d walked across a microphone. Like the acoustics of Barrier Creek panels, this is something I’ve explored in modern human environments. A friend who goes looking for them with me calls them “vocalizers.”
Once you start looking and listening, you find them all over, outdoor gardens, entrances to skyscrapers. One of the best I’ve found is a dome of focused sound created by the ceiling of Terminal C of the George Bush Intercontinental Airport, near the turn to gates 24–27. It’s like stepping through an invisible veil into a secret space. These are architectural simplicities, a circle or cupola pleasing to the eye, maybe with benches or planters or sculptures. The center is often marked with some small feature, a compass star, an intersection of lines, a mosaic of a circle or globe, or simply a drainpipe if not too fancy.
If you happen to pass over that center, or pause in conversation, the effect is immediate. You’ll hear your own voice reflected back on you with startling and encompassing clarity, louder than any other sound in the vicinity.
A related phenomenon is the “whispering wall” or “whispering gallery.” This is where a curved surface carries the slightest vocalization to another, distant location. Grand Central Station in New York has a famous one. On the lower concourse, just outside of the Oyster Bar, the voice of a person standing in one corner will travel up and over the crowd and land on the ears of a person standing in the opposite corner 30 feet away.
The “vocalizer” is slightly different, perhaps more ubiquitous. These are places that broadcast yourself back to you. I’ve taken to calling them “whispering wells.”
Circular is best, though a semicircle will do. Step into the center and say something or make a hiss. Whatever sound you create, even a clearing of your throat if you don’t want to attract attention, will come back from surrounding walls at exactly the same instant, none of the messy humdrum of baffles and angles, no single echo point any closer or father away than the others.
Tell somebody about it, a stranger walking by. I’ve done this; it generally works once they get past their understandable suspicion. Lead them to the center (many of these are in very public and nonthreatening locations) and when they speak to ask what they should do, they shut their mouth instantly, then say something else like hello or echo and look back at you amazed. Some people become giddy with excitement. Try the front of the Transamerica Pyramid in San Francisco—not the finest example, but an easy one to find. It just takes a few seconds. Step into the middle of one of its outdoor circles. An acoustic bubble envelopes you. You can hear the traffic and noise around you, but if you start singing quietly, which generally draws only mild attention, you are singing back to yourself, audience of one.
I move through the city the same way I move through the desert, looking for shaded alcoves that might hold rock art, hissing or clucking my tongue to hear the sound bounce back. Buildings become canyons, and the rounded architecture of lobbies, capital domes, or outdoor sitting areas where professionals eat their lunches become natural shelters, sites of acoustic reflections. And there I stand humming out loud, apparently imbued with supernatural powers.
This post appears courtesy of The Last Word on Nothing.


In the winner-takes-all game of fertilization, millions of sperm race toward the egg that’s waiting at the finish line. Plenty of sperm don’t even make it off the starting line, thanks to missing or deformed tails and other defects. Still others lack the energy to finish the long journey through the female reproductive tract, or they get snared in sticky fluid meant to impede all but the strongest swimmers. For the subset of a subset of spermatozoa that reach their trophy, the final winner would be determined by one last sprint to the end. The exact identity of the sperm was random, and the egg waited passively until the Michael Phelps of gametes finally arrived. Or so scientists have thought.
Joe Nadeau, principal scientist at the Pacific Northwest Research Institute, is challenging this dogma. Random fertilization should lead to specific ratios of gene combinations in offspring, but Nadeau has found two examples just from his own lab that indicate fertilization can be far from random: Certain pairings of gamete genes are much more likely than others. After ruling out obvious alternative explanations, he could only conclude that fertilization wasn’t random at all.
“It’s the gamete equivalent of choosing a partner,” Nadeau said.
His hypothesis—that the egg could woo sperm with specific genes and vice versa—is part of a growing realization in biology that the egg is not the submissive, docile cell that scientists long thought it was. Instead, researchers now see the egg as an equal and active player in reproduction, adding layers of evolutionary control and selection to one of the most important processes in life.
“Female reproductive anatomy is more cryptic and difficult to study, but there’s a growing recognition of the female role in fertilization,” said Mollie Manier, an evolutionary biologist at George Washington University.
* * *
The idea of sexual selection is as old as Charles Darwin himself. In On the Origin of Species, he wrote of the peacock’s showy tail and the elk’s giant antlers as examples of traits that evolved to help males show off their appeal as mates to females. For the next century, biologists focused on all the aspects of sexual selection that operated in the events leading up to copulation. After mating, the female had made her choice, and the only competition was among the sperm swimming to the egg.
This male-oriented view of female reproductive biology as largely acquiescent was pervasive, argued Emily Martin, an anthropologist at New York University, in a 1991 paper. “The egg is seen as large and passive. It does not move or journey but passively ‘is transported’ ... along the fallopian tube. In utter contrast, sperm are small, ‘streamlined,’ and invariably active,” she wrote.
Beginning in the 1970s, however, the science began to undermine that stereotype. William Eberhard, now a behavioral ecologist at the Smithsonian Tropical Research Institute, documented all the ways that females can affect which males fertilize their eggs even after mating. It’s a long list, and scientists still can’t say for sure whether they’ve documented everything. The belatedness of these discoveries wasn’t all due to sexism. Two walruses dueling with their tusks is easy to observe; games of hide-and-seek with sperm inside the female reproductive tract are much less so.
“As soon as you have eggs and sperm, you have sexual selection. There are incredible things that eggs and seminal fluid can do,” explained Andrea Pilastro, an evolutionary biologist at the University of Padova in Italy.
In those species in which fertilization happens outside the body, the females often coat their eggs with a thick, protein-rich ovarian fluid. Experiments in 2013 by Matthew Gage of the University of East Anglia in England showed that this fluid contains chemical signals to help attract the correct species of sperm. When they exposed eggs from salmon and trout to mixtures of sperm from both species, the eggs’ own species successfully fertilized 70 percent of the time, significantly more than to be expected by chance.
“The sperm behaved differently in different ovarian fluids. They actually swam straighter in their own fluid,” Gage said.
Internal fertilizers have their own methods of what Eberhard dubbed “cryptic female choice.” Some female reproductive tracts are labyrinthine, complete with false starts and dead ends that can stymie all but the strongest sperm. Some females, including many species of reptiles, fish, birds, and amphibians, that copulate with more than one male (which biologists estimate are a vast majority of species) can store sperm for months, even years, altering the storage environment to stack the odds to favor one male over another. Many female birds, including domestic chickens, can eject sperm after mating, which lets them bias fertilization in favor of the best male.
All these strategies, however, provide females with opportunities only to select the sperm of different males. Within an ejaculate, which sperm fertilized the egg still seemed to be left to chance.
In fact, the randomness of fertilization is implicit in the principle of segregation—the first law of genetics going back to Gregor Mendel. Parents carry two copies of each gene, which are divided randomly into gametes that carry only one copy. It’s what gives rise to many of the probabilities students learn in high-school biology. If both parents are heterozygotes—meaning they carry two alternate versions of the same gene—then half their offspring would also be heterozygotes. A quarter of the offspring would be homozygotes carrying two copies of one version, and the remaining quarter would be homozygotes with the other version.
“It’s one of the most broadly applicable rules in biology,” Nadeau said.
Yet these probabilities work out only if fertilization is random. If the egg or the sperm can somehow influence the identity of the other gamete involved in fertilization, then those ratios could be very different. This striking difference was what caught Nadeau’s attention back in 2005. When he started looking at the inheritance of two particular genes in mice, the probabilities were all off. In his Seattle lab, he began to wonder: Could Mendel have been wrong?
* * *
Nadeau hadn’t set out to question Mendel. Instead, he wanted to know how interactions between two genes (Apobec1 and Dnd1) affected risks for testicular cancer, one of the most heritable forms of cancer. When Nadeau and his doctoral student Jennifer Zechel bred female mice carrying one normal and one mutant copy of Dnd1 with heterozygote Apobec1 males, everything appeared to follow Mendel’s rules. So far, so good. But when they reversed the breeding (a female Apobec1 heterozygote mated with a male Dnd1 heterozygote), things got weird: They found that only 27 percent of the expected offspring carried copies of mutant Apobec1, mutant Dnd1, or both, compared with the 75 percent they expected to see.
As a researcher who had spent several decades studying heredity, Nadeau was aware of myriad factors that could affect Mendel’s ratios. If a fertilized egg ended up with two mutated copies of a recessive gene, the resulting embryo might die early in development. Such embryonic lethal mutations would alter the ratio of homozygotes to heterozygotes, but it would also reduce the average number of mouse pups in each litter. Yet all of Zechel and Nadeau’s mice had standard litter sizes, and they found no evidence that embryos were dying early after fertilization.
Perhaps, Nadeau reasoned, the problem lay in the sperm, not the egg. He therefore bred male mice with and without the mutation to healthy mutation-free females and found no differences in the males’ fertility—something that would have become obvious if the mutation were affecting sperm formation. Step by step, Nadeau and his team eliminated every possible cause of these wonky ratios of offspring genotypes ... except one: that during fertilization, the egg and sperm were genetically biased against the mutant genotype.
Surely, someone else must have already seen this, Nadeau reasoned, so he searched the scientific literature. Although he could find plenty of examples of unexplained offspring ratios, no one had seriously pursued genetically biased fertilization as an answer.
“Everyone just interpreted it as embryonic lethality because we see what we look for and we explain it using what we know,” Nadeau said.
One of those examples Nadeau found was from the lab of the cancer researcher Roseline Godbout at the University of Alberta. Godbout studied the role of a protein called DDX1 in the development of retinoblastoma, a highly heritable childhood cancer. Mice that were missing one functional copy of the DDX1 gene (but with another, fully functional gene as backup) seemed normal and healthy. When Godbout and Devon Germain, now a postdoctoral fellow at the Max F. Perutz Laboratories in Vienna, bred such heterozygote males and females, they found that none of the offspring lacked both copies of DDX1, even though simple Mendelian math would suggest 25 percent of them should. Given the gene’s importance to DNA replication, however, this wasn’t surprising: The homozygotes without DDX1 presumably died after conception. Godbout and Germain also found lower-than-expected numbers of homozygote offspring with two copies of DDX1. A complicated series of mating experiments led the scientists to propose that their results came from a rare mutation that had occurred in the DDX1 gene during their experiments.
Nadeau wasn’t convinced. He wrote to Godbout to ask how her lab had verified that the “knockout” homozygotes without DDX1 genes had died as embryos. They hadn’t. He also asked whether they had considered genetically biased fertilization, wherein the egg preferred to fuse with a sperm of the opposite DDX1 genotype.
“We really thought it was just a weird pattern of inheritance,” Germain recalled. “We hadn’t thought about nonrandom fertilization.”
Later, on a whim, Germain decided to review all the raw data from his experiments. As he looked over the results, he remembered Godbout’s questions that had been prompted by Nadeau’s email. The more he looked at the data, the more that genetically biased fertilization looked like “the most plausible explanation,” he said.
Frustrated at how few scientists had seriously considered genetically biased fertilization as an explanation for their results, Nadeau wrote up his hypothesis in “Can Gametes Woo?,” an article published in October in Genetics. His goal, he said, was to spur more research into this area and determine if and how egg-and-sperm interactions can alter fertilization.
“We’ve been blinded by our preconceptions. It’s a different way to think about fertilization with very different implications about the process of fertilization,” Nadeau says.
Other scientists, such as Manier at George Washington University, say that Nadeau’s hypothesis is intriguing and even plausible, but they point out that no one has any evidence about how it could happen. Nadeau agrees and points to two possibilities.
The first involves the metabolism of B vitamins such as folic acid, which form important signaling molecules on sperm and eggs. Research in Nadeau’s lab has shown that these molecules play an outsize role in fertilization, and he believes abnormalities in certain signaling genes may alter how much sperm and egg attract each other.
A competing hypothesis builds on the fact that sperm are often present in the female reproductive tract before the final set of cell divisions that produce the egg. Signals from the sperm could influence these cell divisions and bias the identity of the cell that becomes the egg.
Whatever the mechanism might be, this work challenges the standard view of female physiology as passive during fertilization. “Females were seen as passive objects with no choice, but females are going to have a vested interest in the outcome of fertilization,” said Renee Firman, an evolutionary biologist at the University of Western Australia. “We still have a long way to go to understand this process, but I don’t think we still really appreciate how common this is and how often it happens.”
Finding data to support or refute this hypothesis could be challenging, Manier said. It will depend on showing that genes within the sperm affect their surface molecules, and that the egg can sense these differences. Such results will require detailed biochemical studies of individual sperm cells and sequencing information about their genome.
Nadeau is prepared for skeptics—he’s encountered many at conferences when he presents the results of his mouse studies and his hypothesis for what’s going on. Critics often approach him after the talk and begin asking him questions. Whether they walk away convinced is unclear, but Nadeau feels they are much less certain that biased fertilization doesn’t happen. To Harmit Malik, a geneticist and virologist at the Fred Hutchinson Cancer Research Center, the situation is the ultimate Sherlock Holmesian solution.
“If you’ve eliminated the impossible, then what remains, however unlikely, must be the truth,” he quipped.


Dolly the sheep was the first animal to be cloned from an adult cell, and like many firsts, she came to stand in for all of her kind.
So when scientists suspected she had short telomeres—stretches of DNA that normally shorten with age—people wondered if it was because she was cloned from an adult cell. When she started to limp at age five, headlines said that her arthritis “dents faith in cloning.” And when she died at age six—as the result of a common lung virus that also killed other sheep in her barn—her short life again became a parable about cloning. A certain narrative took hold.
Then last year, Kevin Sinclair, a developmental biologist at the University of Nottingham, published a paper about several clones including Dolly’s four “sisters,” who were created from the same cell line as Dolly and lived to the old age of eight (about 70 in human years). They were quite healthy for their age. So of course, he kept getting questions, like if these animals are so healthy, then why was Dolly so unhealthy? It was Dolly that everyone cared about.
Sinclair would point out that Dolly was not so unhealthy. But the questions inspired his team to go looking for Dolly’s health records from the early 2000s. The records, however, were lost. “Everything has moved on. People had moved away, and people are doing other things,” says Sinclair. But after her death in 2003, Dolly’s bones were turned over to the National Museum of Scotland. Sinclair’s team got permission to study them—along with the bones of Megan and Morag, two sheep cloned from non-adult cells who were prototypes for Dolly, and Dolly’s naturally conceived daughter Bonnie.
A team of veterinarians scored X-rays of the bones for signs of arthritis. Megan and Bonnie, who had died at the ripe old ages of 13 and nine, respectively, did indeed have signs of arthritis, which was normal for their age. Megan, who had died at age four in an earlier outbreak of same lung virus that killed Dolly, did not. Even Dolly’s knee did not show signs of arthritis.
Arthritis also affects the soft tissue in a joint, so Sinclair notes the bones alone do not point to a conclusive diagnosis or lack thereof. (And Dolly really did limp.) But the overall set of data from Megan, Morag, and Bonnie as well as Dolly’s elderly sister clones suggest arthritis is no more common among clones than ordinary sheep. Fears about prematurely aging clones may be greatly exaggerated. “We felt the record needed to be set straight,” says Sinclair.
Even the fears about Dolly’s too-short telomeres haven’t quite borne out. Telomeres are repetitive DNA sequences at the ends of chromosomes, and they are shortened every time a cell divides. In 1999, scientists published data suggesting Dolly’s telomeres were too short for her age. Since then, scientists have cloned a whole menagerie of animals: mice, horses, cattle, pigs, dogs, and so on. Studies of their telomere lengths have turned up every possible result: Clones have shorter telomeres, clones have longer telomeres, and clones have normal telomeres—depending on the species or cloning technique.
Clones do have unique health problems, just not the ones that dominated headlines about Dolly. Clones are less likely to make it to term in pregnancy, and when they are born, they are more likely to be a little maladjusted. “You have to baby them—give them oxygen, give them glucose until they normalize,” says George Seidel, who studies animal-reproduction technologies at Colorado State University. The clones that make it to adulthood are generally pretty normal. But that makes it extremely expensive to clone livestock, on the order of $20,000 each. Despite much ado about cloned meat, cloning is not an economical way to produce cattle for meat or milk.
It does have niche applications in the livestock industry, though, like recreating million-dollar bulls with extremely valuable genes. Seidel, who also has an cattle ranch, buys semen from Final Answer 2, the clone of the famed sire Final Answer who died in 2014. Semen from the right bull is also incredibly valuable to the dairy industry. “You’ve probably had some cheese from the offspring of a clone,” says Seidel. But actual clones are not sold for food in the United States.
Of Clones and Clowns
For all the attention Dolly once attracted, cloning itself has not radically altered the world. The research that created her did teach scientists a lot about how embryos can be manipulated, which has inspired new lines of research into stem cells.
Dolly’s four sister clones—Daisy, Debbie, Denise, and Dianna—were actually euthanized last year. Sinclair’s group is still studying their preserved cells and tissues for answers to question like the unresolved debate over telomere length. But it’s the end of a certain era. “You get to know them very well,” says Sinclair about Daisy, Debbie, Denise, and Dianna. “Even though they’re clones they have their own personalities, and this is an important point to make.” They may have started with the same DNA as Dolly, but they were not the same as her.


In September, Bill Snape and his family took the 90-minute drive from their home near Washington, D.C., to the grounds of Shenandoah National Park. It was a trip they had made many times before, but this time Snape was taking it to check out a rumor. He had heard that something unusual was on sale at Skyland Lodge, a hotel on the park’s premises.
At the gift shop of the lodge, Snape found what he was looking for: multiple cases of Trump wine, produced at the Trump family’s nearby winery.
“At first, it just annoyed me. And then I thought, what is the law?” he told me on Wednesday. Snape is a senior counsel at the Center for Biological Diversity, an environmental advocacy organization.
Snape worries that the sale poses a major conflict of interest and may even violate the Constitution’s emoluments clause. “Emoluments means advantage. You cannot use your public office for your personal advantage,” he said.
The company behind Skyland Lodge has now confirmed that it sold Trump wine at Shenandoah National Park this year. This week, Snape and the Center for Biological Diversity filed a request under the Freedom of Information Act to learn how Trump wine came to be sold at Shenandoah in the first place.
The sale gets at a wrinkle in how the federal parks system operates: While Trump wine was on sale at Shenandoah National Park, the National Park Service wasn’t actually the organization that was selling it. Delaware North, a company based in Buffalo, New York, operates the hotels and gift shops on the grounds of Shenandoah, including Skyland Lodge. It also manages attractions in Yosemite National Park and Grand Canyon National Park.
“At Shenandoah National Park we offer wines from several different Virginia vintners,” said Glen White, a spokesman for Delaware North.
Up until September, he continued, the company had been offering wines from Trump Winery, as well as Kluge Estate (the name of the vineyard before Trump bought it) “because they are locally produced.” (Snape said Trump wine was on sale until at least October.)
“This was only at Shenandoah, where it is a local product, and not at any other parks,” White said. “The National Park Service did not request or require us to carry it.”
Jeremy Barnum, a spokesman for the National Park Service, also told me that Delaware North had been selling Kluge Estate wine for years. He said the park service “does not specify what brands of these products should be sold” by concessioners like Delaware North.
Delaware North is one of more than 500 companies with a contract to sell goods or services within a national park. The National Park Service says that concessioners across the country employ more than 25,000 people, who are not federal employees, and generate more than $1 billion in receipts.
But the wine was definitely still on sale at Shenandoah at the time that Donald Trump promoted it during a presidential press conference. After his statement on the white-supremacist march in Charlottesville, Virginia, in August, Trump told reporters: “I own actually one of the largest wineries in the United States—it’s in Charlottesville.”
The winery itself says this is not true: Donald Trump turned ownership of the property over to his son, Eric, in 2011. Town & Country magazine also disputes that Trump’s winery is one of the largest in the country.
Snape says that even if a concessioner sold the wine—and not the park service—it still doesn’t pass muster.
“It’s like a Rubik’s Cube, it looks bad from every angle,” he told me. “Why is the concessioner behaving that way—is the concessioner looking for special favors? And why is the park service allowing it to be sold?”
“Whatever you want to say about George Bush or Jimmy Carter or Bill Clinton, we weren’t eating Jimmy Carter peanuts, or buying George Bush oil, or using Bill Clinton condoms,” he said. “Where is the line between the public duty and his personal profit seeking?”
Trump family investments and the National Park Service do not only overlap at Shenandoah National Park. The observation deck of Trump’s new hotel in Washington—the Trump International, on Pennsylvania Avenue, just a few blocks from the White House—is operated and overseen by park-service rangers.


Left undisturbed by brushing and flossing, the bacteria in your mouth will form a sticky film called plaque. Left further undisturbed, plaque will turn hard and yellow, calcifying on teeth as dental calculus, also known as tartar.
At this point, the tartar is very durable. Just ask these Neanderthals, whose 40,000-year-old tartar scientists recently analyzed to figure out the real paleo diet. Tartar grows in layers—almost like tree rings—entombing DNA from tiny bits of food as well as bacteria in the mouth. Forty thousand years later, scientists can analyze that DNA to reconstruct what was going on in the mouths of long-dead Neanderthals.
Neanderthal Dental Plaque Shows What a Paleo Diet Really Looks Like
Having traveled so far back in time using ancient tartar, some of the same scientists have embarked on a more ambitious project: using the DNA from the bacteria in tartar to figure out how humans settled the 10 million square miles of Polynesia.
Polynesia has confounded the traditional ways of tracing human migration—archaeology, linguistic analyses, even human DNA—because large parts of it were settled so fast. Humans first reached the Society Islands, in the center of the Polynesian Triangle, perhaps around 1,000 AD. Then in the span of just a couple hundred years, they took canoes across vast tracts of open ocean to find specks of inhabitable rock as far-flung as Hawaii, New Zealand, and Easter Island. How Polynesians navigated these waters in the 11th century is a subject of considerable fascination. But even more basically, archaeologists are not sure exactly when the islands were settled and in what order. That’s where the tartar comes in.
Because bacteria are constantly dividing, they accumulate frequent mutations in their DNA. By comparing the mutations in tartar found in one island and the next, the researchers hope to figure out whether early settlers systematically hopscotched across Polynesia or skipped certain islands. “The traditional means of looking at human migrations might be too coarse. Hopefully, the rapid rate of evolution in that bacteria will allow us to answer some of the questions,” says Raphael Eisenhofer, a Ph.D. candidate at the University of Adelaide, and coauthor of a recent paper proposing the use of tartar to track human migrations.
He and a group of DNA researchers, as well archaeologists, have since begun sampling teeth from Polynesia. Removing ancient tartar is not that different from modern dentistry, says Laura Weyrich, an ancient-DNA researcher at the University of Adelaide and Eisenhofer’s adviser. She pops it off with a dental pick. The group has collected hundreds of samples from collections of the Natural History Museum in London and other museums.
It’s much easier, Weyrich says, to convince museums to give up some tartar than to sacrifice a bone for human DNA analysis. In fact, museums traditionally cleaned the gunk from teeth—to better analyze their shape and to make them look better on display. “From my perspective, it’s incredibly frustrating. It’s like, ‘Noooo, what did they do?’” says Weyrich. More often than not, the most famous specimens have been cleaned. “We’re looking for the skulls that were left in the closet and still dusty and nobody cleaned,” she says.
Back in Adelaide, Eisenhofer is sequencing the samples in a lab built specifically for ancient DNA. Working with ancient DNA is tricky because samples are usually small and degraded, easily swamped by the modern DNA shed by bacteria living in and on our bodies. To prevent contamination, he wears a suit, a mask, and three pairs of gloves.
The team has picked out dozens of strains of common mouth bacteria that bind to the tooth surface and are commonly found in plaque. And they hope to sequence specific genes and track mutations in them over time. Scientist have analyzed specific pathogens—such as Helicobacter pylori in the gut—as proxies for human migration before, but Weyrich and Eisenhofer hope to extract more information by studying the overall community of mouth bacteria. There could be a lot of information hidden in the gunk that museums once threw away.
When Weyrich samples tartar from teeth, she’s careful to pick jaws that have more than one tooth and to always leave some tartar behind. You don’t want to destroy it all, she explains, because you never know what techniques might come along in the future.


When she was in high school, Madhavi Colton was known as Miss Enthusiasm. “I’ve always been a die-hard optimist,” she says. “I tend to be perky. In my family, I was always the one who thought that everything was going to be fine, that we can do this.”
Recent years have tested her optimism. Colton is now a director at Coral Reef Alliance, a nonprofit dedicated to protecting coral reefs. And corals need all the help they can get. A third of reef-building corals are in danger of extinction, and their growth rates have plummeted by 40 percent since the 1970s. They have been pummeled by hurricanes, disease, and pollution. Acidifying water makes it harder for them to create their rocky reefs. Rampant overfishing kills off the grazing fish that keep competitors like seaweed and algae in line. Rising temperatures force them to expel the symbiotic algae in their tissues, which normally provide them with both food and vivid colors. Without these partners, the corals starve and whiten. Once-lush ecosystems full of kaleidoscopic fish become spectral wastelands, where scuzzy green algae grows over the bleached white skeletons of dead and dying corals.
The continuing desecration has taken an immense toll on the mental health of people like Colton who have devoted their lives to studying and saving these ecosystems. How do you get up and go to work every day when every day brings fresh news of loss? When everything you are working to save is collapsing, how do you stop yourself from collapsing, too? Maybe everything isn’t going to be fine, after all. Maybe we can’t do this. “Are we going to lose an entire ecosystem on my watch? It’s demoralizing. It’s been really hard to find the optimism,” she says. “I think Miss Enthusiasm has gone away.”
There was a time, just a few decades ago, when this crisis seemed unimaginable, when reefs seemed invincible. Phil Dustan, from the College of Charleston, similarly remembers being fresh out of grad school and telling the famed explorer Jacques Cousteau that “reefs are so big that humans couldn’t hurt them.” Those words seem hopelessly naïve now. Dustan recently dove at Dancing Lady Reef in Jamaica—a place that he had studied as a graduate student in the 1970s, and where scientists “first became intimate with the science of reefs,” he says. “I dropped into the water and I just choked. It was like someone going through their home after a forest fire has gone through, picking through the ashes.” Elsewhere in the Caribbean, he took his son snorkeling at Carysfort Reef, another site of once-legendary beauty. “He stayed real close to me and he wouldn’t range around because he was fearful,” Dustan says. “Finally, he said: Dad, we have to leave this place. It creeps me out. It’s all dead.”
This catastrophe has unfolded slowly. Nancy Knowlton, from the Smithsonian Institution, says that when it comes to corals, the bad news is usually incremental, and only obvious to those who work in the affected places. “But what happened in the Great Barrier Reef was so spectacularly bad that you didn’t need to work there to know it was bad,” she says.
What happened was this: In 2015, the world experienced a mass-bleaching event, where heat waves started killing corals in all three oceans where they thrive—the Pacific, Atlantic, and Indian. Two such global crises have happened before, in 1997 and 2010, but the 2015 one was unprecedented in its severity, and in its implications. Just 9 percent of the individual reefs that make up the 1,400-mile-long Great Barrier Reef, off the coast of northeastern Australia, were unscathed. All told, more than a quarter of the corals have died there, with a much higher proportion in the northern sections. No one had seen anything like it before. When the coral researcher Terry Hughes revealed the scale of the devastation to his students, they all reportedly wept.
“The news has been especially upsetting, because of the scale of the event, the iconic nature of the Great Barrier Reef, and the fact that there were gifted filmmakers on site to document it,” says Knowlton. “It was a perfect storm of attention.”
For many coral aficionados, it was also a tipping point for despair. One year of bleaching would be bad enough, but the Great Barrier Reef experienced a second in 2016 and there are signs that a third wave might hit by the end of this year. Corals bounce back, but consecutive blows could take even these resilient animals down for the count. “Our models said that wouldn’t happen for a long time, and I’m worried that we’ve underestimated the pace of change,” says Colton. “Things are even worse than we thought, and that’s been hard to cope with.” For her, that gnawing unease has led to sleepless nights, and curtailed her ability to deal with other worrying world events. “My barrel is full,” she tells me. “One more drop and it spills out. My resilience is gone.”
The feeling is ironic, because that’s exactly the problem that corals are facing. “I’m a coral reef,” Colton says. “I’m also failing to cope.”
But she also recognizes that she and other scientists are privileged. They care about reefs, but they’re not like the 450 million people around the world who rely on reefs for tourism revenue, food from fish, and protection against storms. For them, the losses are a daily reality. The last time Bette Willis, from James Cook University, went out to the Great Barrier Reef, the woman who ran her boat “would alternate between rants and depression,” she recalls. “She’s out there several times a week. She knows each coral. She could see her whole livelihood go down the drain.”
Everyone I spoke to talked about becoming very good at compartmentalizing—at acknowledging the scale of the tragedy, but also putting it aside to focus on their work. “I don’t find it productive to be angry or depressed all the time. It’s corrosive, and it isn’t going to solve the problem,” says Knowlton. She is perhaps the poster child for ocean optimism, having created a movement called ... Ocean Optimism. “I started that because there was no way to get people engaged in doing something about these problems, if they didn’t realize that there was something they could do.”
There is reason to hope, she says. Large protected areas have been established around coral reefs, which will protect them from overfishing and pollution, and make them more resilient. That won’t ward off bleaching events, which will continue as long as the climate keeps changing. But Knowlton thinks that the rise of electric cars and the increasingly competitive costs of renewable energy will reduce the pace of global warming, even if the current administration refuses to enact policies to mitigate it.
Willis also notes that corals have been around for millions of years, and are less fragile and more adaptive than people give them credit for. They might be able to swap the algae in their tissues for more heat-resistant varieties, or alter the composition of microbes on their surface to improve their health. Willis’s coping mechanism is to believe in the corals’ coping mechanisms. “It’s almost a faith,” she says.
Ruth Gates, from the Hawaii Institute of Marine Biology, finds solace in action. She remembers what her therapist told her several years ago, for reasons unrelated to corals: You can’t really control what happens around you; you can only control your response. “That was a profound statement,” she tells me. “My response to the gloom and doom is to ask what we can do about it.” She is now trying to breed “super-corals” that can better withstand a warming world, either because they’ve partnered with heat-tolerant algae, or been conditioned from an early age to take the heat.
Whatever the eventual strategy, Gates at least wants to try something—and she fears that the cautious inertia of academic science will stop her peers from pursuing interventionist approaches. “In the last five years, I’ve come to terms with the fact that we’ll have to do something to help reefs get through 2050,” she says. “I find it worrying that people think we have time to plan, and the research is telling us that we have to act.”
Phil Dustan agrees. When he started studying corals as a young scientist, he was interested in questions of basic ecology, like how the reef community responds to light. But now, “we really don’t have the time to be interested in how light structures the community because it’s going to be gone,” he says. Instead, he has shifted his attention to working with communities in Bali, teaching them to care for their reefs and helping them to set up their own protected areas.
This sense of shared purpose is perhaps the greatest vaccine against looming despair. Every four years, coral-reef researchers gather for the International Coral-Reef Symposium. The latest meeting took place last June. It was a rough five days, full of talk of decline and death. But “it was the most level playing field we’ve ever had,” says Gates. “There was a greater sense of community than I’ve ever seen—a sense that we’re going to have to bring our skill sets together to solve the problem.”
She was also relieved to see that younger scientists were not, as she once feared, dissuaded from studying ecosystems that could conceivably die out within their lifetimes. “It’s a very weighty thing to take on at the start of one’s career, but the young people coming into the field are extraordinarily driven to solve problems,” she says. “That’s very heartening.”


Nobody saw it coming.
The rocky object showed up in telescope images the night of October 19. The Pan-STARRS1 telescope, from its perch atop a Hawaiian volcano, photographed it during its nightly search for near-Earth objects, like comets and asteroids. Rob Weryk, a postdoctoral researcher at the University of Hawaii Institute for Astronomy, was the first to lay eyes on it, as he sorted through the telescope’s latest haul. The object was moving “rapidly” across the night sky. Weryk thought it was probably a typical asteroid, drifting along in the sun’s orbit.
“It was only when I went back and found it [in the data from] the night before that it became obvious it was something else,” he said. “I’d never expected to find something like this.”
Weryk and his colleagues scrambled to secure more telescope time to study this mysterious, fast-moving object. They called in reinforcements in the astronomy community. Initial observations suggested the space rock was a comet. When new data showed the object lacked some important properties of comets, they decided it did in fact have to be an asteroid. But it wasn’t acting like any asteroid they’d ever seen.
When astronomers examined and measured the object’s movements, they were stunned. The object didn’t originate in our solar system. It had come from somewhere else, and had traveled through interstellar space for who knows how long to get here.
Astronomers announced the discovery of the object October 26, calling it A/2017 U1. The University of Hawaii team eventually gave it a permanent name of Hawaiian origin, ‘Oumuamua, “a messenger from afar arriving first.” After weeks of follow-up observations, they have released more information about the finding in a new paper, published Monday in Nature, that confirms ‘Oumuamua is the first known interstellar object in our solar system.
‘Oumuamua is a cigar-shaped, 400-meter-long asteroid, red in color, with a surface similar to comets and organic-rich asteroids found elsewhere in our solar system, according to the astronomers. Little is known about its composition. But its existence is, for now, exciting enough.
Astronomers have long predicted this event could happen. Our solar system, in its adolescence, was a turbulent place. As the planets swirled into shape, some of the bigger ones jostled nearby material, sending some of it flying toward the edge of the solar system and beyond. Some of the rejected material could even make its way to another star. Since planet formation is quite uniform across the universe, astronomers believe ‘Oumuamua is one of these outcasts, tossed out of its home system. By this logic, there are likely pieces of our own solar system coasting somewhere in interstellar space or past another star.
Astronomers only had about two weeks after the discovery to observe ‘Oumuamua before it disappeared from the view of optical telescopes. “Because the object is moving fast, and the light we get from it is reflected sunlight, the faster it moves away from both the sun and the Earth, the faster it fades in brightness,” said Karen Meech, an astronomer at the University of Hawaii Institute for Astronomy and the lead author of the paper. She and her colleagues condensed weeks or months of work into days and raced to apply for observation time at the world’s most powerful telescopes, which is competitive and tightly scheduled. Observatories squeezed them in and other colleagues donated time out of their own projects.
Astronomers found that the properties of ‘Oumuamua are unlike any of the approximately 750,000 asteroids or comets known to humanity. “In our simulations, you can see that this could not have been from our solar system—it’s simply going too fast,” said Davide Farnocchia, a navigation engineer at NASA’s Jet Propulsion Laboratory who was responsible for figuring out ‘Oumuamua’s trajectory.
Its orbit was completely different, too, Meech said. Scientists can figure out the shape of the orbit of objects that move around our sun, a measurement known as eccentricity. The eccentricity of all objects bound to the gravity of the sun falls between 0 and 1. The highest known eccentricity, 1.058, belongs to a comet that was discovered in 1980, but astronomers interpret this, along with other measurements that stray from the norm, as the result of objects getting jostled as they moved past giant planets like Jupiter. The eccentricity of the interstellar visitor is nearly 1.2. The difference looks small on paper, but it’s big enough to confirm that ‘Oumuamua doesn’t play by our rules.
Naming the space rock posed an interesting challenge. Comets are usually named after their discoverers, while asteroids are named only after their orbits have been accurately computed and established. The International Astronomical Union, the organization in charge of naming these objects, didn’t have guidelines for christening an interstellar rock; in its designations, the IAU uses the letter C for comet and A for asteroid, and this thing wasn’t either, not really. “This object is only going by once,” said Paul Chodas, the manager of NASA’s Center For Near-Earth-Object Studies. The IAU eventually came up with a new designation: I, for interstellar.
Ground-based telescopes in Chile and Hawaii have already lost sight of ‘Oumuamua. The Hubble and Spitzer space telescope are observing the rock this week, and may be able to track it until December. ‘Oumuamua is now outside the orbit of Mars. It will pass the orbit of Jupiter next May, then Neptune in 2022, and Pluto in 2024. By 2025, it will coast beyond the outer edge of the Kuiper Belt, a field of icy and rocky objects. It will take many more years for the object to reach the Oort cloud, another region of floating objects, at the edge of the solar system.
The arrival of ‘Oumuamua had ignited the astronomical community, particularly asteroid researchers like Andy Rivkin, a planetary astronomer at Johns Hopkins University. Its departure feels just as abrupt. Rivkin put his own twist on an old refrain to describe how he felt about ‘Oumuamua fading from view. “Don’t be sad that it’s over. Be happy that you saw it,” he said. “Because it is really amazing that we saw it.”


Fox News anchors sometimes remind viewers that: We report. You decide.
The company took an especially broad outlook on what kind of information merited a decision on Monday, when it appeared to question the reality of the Apollo missions and the moon landing.
“You be the judge: Skeptics say picture debunks moon landing,” the network tweeted. The accompanying story cited an anonymous YouTube conspiracy theorist’s video that claimed to show discrepancies in a photo from the Apollo 17 mission. The story does not have a byline. A spokeswoman for the network did not respond to a request for comment about whether it considers the moon landing up for debate.
By the afternoon, the story had also made it on the front page of Google News. Newsweek and The Daily Mail also amplified the conspiracy theory.
The story may seem like a giant leap for Fox in terms of what sources it deems newsworthy. The network—whose reporters have questioned the existence of climate change and advanced sketchily sourced conspiracy theories—has given airtime to many viewpoints that challenge the established evidence in the past. But even so, the fact that NASA went to the moon is a particularly strange story to pick up. The Apollo landing sites are not only a product of American scientific and technological leadership; they’re also an easily confirmable fact. Probes from Japan, China, and India have all seen the landing sites, as have later NASA missions. A preponderance of evidence from the period also testifies to the reality of the landing.
But questioning the veracity of the moon landing in particular is not actually a new exercise for television journalists. In 2001, an hour-long special aired on Fox that questioned whether the moon landings were fake. Fox News has hosted a moon-landing conspiracist at least twice since that year. Not to be outdone, CNN covered Bart Sibrel, the same conspiracy theorist who produced the Fox documentary, in an online story in 2009, as well as NASA’s response to the Fox documentary in 2001.
And The New York Times covered Sibrel as well as other conspiracy theorists in 2009—though it also added that polls suggested only 6 percent of Americans believed the rumors.
One of those Americans—as it happens—is Roger Stone, a confidante of President Trump’s who once insisted (or perhaps joked) that the moon landing was fake.
Likely the Fox News tweet achieved such stardom because it darkened journalists’ timelines on the waning hours of a preholiday Monday. It seemed to confirm that Fox occasionally caters to the fringe. But the same network also amplified a poorly sourced conspiracy theory about Seth Rich, a murdered Democratic staffer, for weeks, before suddenly abandoning the story. Other journalists from the network have misreported stories about climate change and asked how global warming can be real if it also sometimes snows. Which coverage is more harmful? We report, you decide.


After a nearly decade-long fight over its construction—which grew to include three states, two provinces, several indigenous tribes, tens of thousands of activists, and two U.S. presidents—the Keystone XL pipeline seemed set to clear its final major hurdle on Monday morning.
By a vote of 3-2, the Nebraska Public Service Commission voted to allow the pipeline to pass through the state. The commission’s vote was the last significant regulatory approval that Keystone XL required before construction could begin. Montana, South Dakota, and the U.S. federal government already okayed the 1,100-mile-long project this year.
But—in a twist—the Nebraska commission’s vote might have merely opened a new chapter in the saga. While the commissioners approved Keystone XL, they also ordered that the pipeline take an alternate route through the state. The new route—which adds a 63-mile detour and parallels a preexisting pipeline—increases the cost and legal difficulties of an already expensive and delayed project.
The commissioners said they were forcing the new route because Keystone XL must “take advantage of any opportunity” to run along the preexisting pipeline corridor.
TransCanada, the pipeline’s developer, did not seem to celebrate the approval. Russ Girling, its president and chief executive officer, said in a statement that the company was “assessing how the decision would impact the cost and schedule of the project.” He also assured investors that the company was pursuing other improvements.
And as the day went on, environmentalists and indigenous groups opposed to Keystone XL sounded upbeat about the possibility of postponing its construction further.
“It’s complicated—indeed, wack—but the lawyers are sounding increasingly cheerful,” tweeted Bill McKibben, a writer and activist who first galvanized opposition to the pipeline. “Lots of room to fight.”
“By pushing Keystone XL onto a new route, the commission all but guaranteed more delays and hurdles for TransCanada to work through. We’ll be there with our allies pushing back on them every step of the way,” said May Boeve, the director of 350.org, a climate-activism organization. She also said that activists should prepare to peacefully assemble in the pipeline’s path.
Sarah Krakoff, a professor of natural-resources law at the University of Colorado, agreed that pipeline’s new route would probably complicate its completion. “I think at a minimum that this slows things down a bit compared to a straight-up approval. Given the recent massive spill in South Dakota, this might buy time to galvanize opposition,” she told me in an email.
Monday wasn’t the first time TransCanada made headlines this month. The Keystone pipeline—the longer and older sister project of the Keystone XL proposal—leaked more than 210,000 gallons of crude oil in South Dakota last week, leaving a spindly black splotch on the farmland and setting off a large cleanup effort. TransCanada said it caught the leak within about 15 minutes. (The Nebraska Public Service Commission is prohibited by law from considering the risk of oil leaks when approving pipelines.)
The pipeline project commands so much attention largely because it has grown into a symbol of the climate fight.
In 2011, McKibben seized on the pipeline, arguing that taking global warming seriously would mean no longer issuing permits for any fossil-fuel infrastructure. A year later, James Hansen, then the director of the NASA Goddard Institute for Space Studies, wrote in The New York Times that fully exploiting oil beneath the Canadian tar sands—as Keystone XL is meant to do—would spell “game over for the climate.”
President Barack Obama first blocked the pipeline in early 2012, saying it posed too much of a risk to the Sand Hills of Nebraska. Analysts said that he would likely approve the pipeline after his reelection. But his State Department slow-balled the approval process, and—in late 2015, a month before the UN negotiations on climate change in Paris—Obama formally blocked its approval.
“America is now a global leader when it comes to taking serious action to fight climate change,” he said at the time. “And frankly, approving this project would have undercut that global leadership.”
President Donald Trump reversed this decision in the first week of his administration, ordering the federal government to reexamine Keystone XL. “The regulatory processes in this country have become a tangled-up mess, and very unfair,” he told members of the press from behind the Resolute Desk. The government approved the pipeline’s passage within months.
Yet much has changed about the energy industry since the Keystone XL was first proposed in the late 2000s. Cheap natural gas has flooded the market, knocking down the price of oil and turning the United States into a net energy exporter. Canada has also improved the energy-export infrastructure around the Tar Sands, reducing the need for a new massive pipeline. While some analysts have wondered whether the Keystone XL might no longer make economic sense, TransCanada says that it has secured enough early reservations for the project to move forward.
The new route could change that. The Omaha World-Herald reported that some landowners along the route may not even know that Keystone XL now affects them. TransCanada will have to notify them and seek to persuade them to give up chunks of their land. And the company still must secure a permit to cross federal waterways in the United States—the same regulatory hurdle that nearly broke the Dakota Access pipeline last year.
And the climate fight itself has altered the energy market, too. Fossil-fuel infrastructure projects like Keystone XL can now unpredictably transform into national news stories: a new challenge for their investors and backers. By 3 p.m., McKibben was feeling triumphant enough to tweet a story from the National Journal.
“Insiders: Obama Will Approve Keystone XL Pipeline This Year,” it crowed, citing a poll of energy experts. The year it was published? 2011.


No matter how many words you can define, your vocabulary isn’t like a dictionary. Your mind stores language not as a list of words, but as a network of categories, properties, and meanings, with stronger connections between related words, like newspaper and magazine, than unrelated ones, like wallet and avalanche.
At six months old, a baby probably doesn’t know what wallet or avalanche means—but even at such a young age, months before children start talking, they do understand some basic nouns, like ball and dog. And a new study suggests that the few words infants know are structured in their minds the same way as an adult’s vocabulary, in a complex web of related concepts. The evidence: When words have similar meanings, babies can get confused. That confusion hints that babies know more about language, at a younger age, than scientists have found before.
In the study, which was published Monday in the Proceedings of the National Academy of Sciences, infants were shown images of two different common objects at a time: a blanket and a dog, a book and a diaper, a stroller and a car, and so on. The babies’ parents would name one of the images, and researchers would then track where the babies looked by reflecting infrared light off their eyes. When the images depicted related words, like nose and mouth, the infants spent more time looking at the wrong picture than when the images depicted unrelated words, like nose and bottle.
The fact that the children were more confused by related images reflects that they somehow understand that the concepts are related, said Elika Bergelson, an assistant professor of psychology and neuroscience at Duke University and the study’s lead author. The same is true of adults, who have shown such a lag on a smaller scale when they do similar experimental tasks. If you were supposed to look at the nose in Bergelson’s experiment, it would take your brain a few milliseconds to make sure that the mouth isn’t what you’re supposed to be looking at, because the entries for nose and mouth are more closely connected in your mental vocabulary. It would take less time to rule out the bottle.
Researchers already knew that toddlers and older children showed the same pattern as adults, but this is the first study to find the effect in six-month-olds. It’s part of a growing body of research on very young infants that suggests they have a deeper understanding of language than previously thought.
Janet Werker, a professor of psychology at the University of British Columbia who was not involved in the study, called the results “just amazing.” If further research shows that babies have a deep understanding of the similarity between the meanings of nose and mouth, and aren’t just used to seeing mouths and noses in the same places, it would reflect that they are driven by a “search for meaning,” she said. That would mean “our whole approach to infant cognition is going to be really turned on its head.”
Outside of the lab, the study’s authors analyzed the infants’ exposure to common objects in their own homes. They found that certain circumstances, like objects being present when parents talk about them with their children, appeared to help the infants look more at the correct objects during the in-lab task. But Bergelson emphasized that parents shouldn’t think her study is a how-to guide for improving their kids’ vocabularies. What they should take away is that “babies are listening, and you should treat them as conversational partners,” she said.
Both Werker and Bergelson acknowledge the study’s shortcomings, especially with respect to the group of babies that participated. For one, it’s relatively small—only 51 children—because recruiting families for both in-lab and in-home studies is, as Bergelson put it, “a huge pain in the butt.” Bergelson was also careful to point out that the babies came mostly from white, middle-class, well-educated families, which means more work needs to be done to figure out how generalizable her results are. Werker would like to see bilingual babies included in future studies.
The more researchers know about the typical development of young infants’ language skills, the earlier they can identify when something might be wrong. Bergelson noted that most parents and doctors can’t say whether a child has a language delay until they start talking, at around one-and-a-half or two years. But “if we can know enough about what a six-month-old’s vocabulary ‘should’ be like,” she said, language delays can be identified earlier. What doctors and families do from there is hard to say, since the origins of language delay can range from poverty to being on the autism spectrum. But no matter the cause, Bergelson said, “the earlier you intervene, the better the outcomes are.”


If you were an elephant, you might be puzzling over human behavior this week. On Monday, the animal-rights attorney Steven Wise filed a writ of habeas corpus on behalf of three privately owned Asian elephants, arguing that the animals are “legal persons” who have a right to bodily liberty and should be free to live in a sanctuary. Then, on Thursday, the U.S. Fish and Wildlife Service announced that the remains of elephants legally hunted in Zimbabwe and Zambia could now be legally imported to the United States as trophies.
This new policy overturned a ban put in place by the Obama administration in 2014. African elephants are considered “threatened” under the U.S. Endangered Species Act, a step below being endangered. The animals’ numbers have plunged from around 10 million 100 years ago to around 400,000 today, largely because of poaching and habitat loss. The Fish and Wildlife Service has not changed the elephants’ status; instead, it now argues that supporting “legal, well-managed hunting programs” will help provide “much-needed conservation dollars to preserve habitats and protect wild herds” in Zimbabwe and Zambia, the agency’s principal deputy director, Greg Sheehan, said in a news release.
But then, to further complicate matters, President Donald Trump tweeted Friday evening that nothing would actually change until he “reviews all conservation facts.”
The idea that killing more elephants will help save the species is counterintuitive, and its line of reasoning is difficult for many conservation organizations to support: Let rich hunters pay hefty sums to shoot elephants, and use the money to help conservation efforts and local communities. Supposedly, the villagers won’t then need to poach elephants to feed their families and pay their kids’ school fees. Still, the International Union for Conservation of Nature, or IUCN, a respected organization that sets the conservation status for all species, supports the notion.
But the evidence that “hunting elephants saves them” is thin. The hunting-safari business employs few people, and the money from fees that trickles down to the villagers is insignificant. A 2009 report from the IUCN revealed that sport hunting in West Africa does not provide significant benefits to the surrounding communities. A more recent report by an Australian economic-analysis firm for Humane Society International found that trophy hunting amounts to less than 2 percent of tourism revenue in eight African countries that permit it.*
And then, there is a larger moral question: How does hunting affect male elephants, especially the “big tuskers” that hunters want, and the overall population?
If elephants are recognized as legal persons, a term the U.S. courts have granted corporations and a New Zealand court gave to a river (elsewhere the term has been extended to chimpanzees, a bear, and the environment), it would be more difficult to hunt them at all—let alone import their body parts. Wise’s lawsuit cites extensive scientific studies that have established elephants’ cognitive abilities, emotional and empathetic natures, complex social lives, lifelong learning, and memory skills. “Taken together, the research makes it clear elephants are autonomous beings who have the capacity to choose how to live their lives as elephants,” he tells me.
One thing elephants would not choose, Wise and elephant researchers agree, is to be hunted. “It doesn’t matter to elephants if they are killed by poachers or trophy hunters,” says Joyce Poole, who has studied African elephants in the wild in Kenya and Mozambique for more than 40 years and is the codirector of ElephantVoices, a conservation organization. “Either way, you’re a killer. And if elephants understand that about you, they change their behavior.”
Elephants aren’t considered game animals in most African countries with substantial populations of these animals. But trophy hunters after large male elephants can seek their prey in South Africa, Namibia, Cameroon, Zambia, Zimbabwe, Tanzania, Gabon, and Mozambique. Kenya banned the sport in 1973, while Tanzania continued to permit legal hunting. That caused problems for the elephants of Kenya’s Amboseli National Park, says Poole, who was studying the large males in the park at the time. The park borders Tanzania, and after the Tanzanian government opened a hunting block on the opposite side, the Amboseli male elephants who wandered across became prized targets.
“It was an awful time,” Poole recalled, “because on one side, the elephants learned to trust tourists—generally white people—in cars. From our studies, we know they can smell the difference between whites and local people. They also distinguish us by our languages. They know people who speak Maa, the language of the local Maasai people, may throw spears at them; those who speak English don’t.” However, the tables were turned on the Tanzanian side of the border. There, white people in cars who drove up close to see an elephant might lean out with a camera—or a rifle.
“The elephants didn’t run because they didn’t expect to be shot,” Poole said. Two of the large males she was studying were lost this way to trophy hunters. She and others protested to the Tanzanian government, and these particular hunting blocks were eventually closed.
Poole does not know how the loss of these big males, who’d fathered many calves, affected the other elephants. Female elephants, though, do mourn family members who die, and are especially troubled when the matriarch, their leader, passes. In 2003, for instance, researchers in Kenya’s Samburu National Reserve watched as Eleanor, an elephant family’s matriarch, died from natural causes. When Eleanor fell heavily to the ground, Grace, a matriarch from another family, used her tusks to lift her friend and helped her to her feet. Despite Grace’s efforts, Eleanor died that night. She had a tiny, six-month-old calf who never left her side. In a photograph, the calf stands like a small sentinel beside her mother’s body, while the rest of the family bunches together, grieving.
Researchers have rarely seen similar moments among male elephants, who as adults, live away from the female herds they grew up in, and return only to mate. That behavior led to a “myth that males are far less social than females,” said George Wittemyer, a conservation biologist at Colorado State University in Fort Collins who has studied elephants in Kenya for more than 20 years. His new research contradicts this notion. “Actually, the males are always in groups and have preferences for certain companions. They’re not the loners they’ve been made out to be,” he said.
“The death of a bull will cause less disruption than the death of a family member,” said Iain Douglas-Hamilton, a zoologist who founded the organization Save the Elephants. “If a bull is shot while associating with a family the others will normally run away.” But he noted: “Bulls will defend or help each other sometimes, when one is down.”
From a population standpoint, “older male elephants are very important to the health and genetic vitality of a population,” said Cynthia Moss, who has led the Amboseli Elephant Research Project in Kenya since 1972. While hunters in the past have used the belief that older males are reproductively senile as an argument for killing them for their ivory, research has revealed that they are in fact an elephant population’s primary breeders. “By living to an older age, [older males show that] they have the traits for longevity and good health to pass on to their offspring,” Moss said. “Killing these males compromises the next generation of the population.”
It’s not clear if the Fish and Wildlife Service will consider how trophy hunting affects individual elephants or their families. The agency didn’t comment on Trump’s tweet when contacted, but later issued a public statement confirming that permits would be put on hold. “President Trump and I have talked and both believe that conservation and healthy herds are critical,” Interior Secretary Ryan Zinke said in the statement.
Wise believes that the emotional and psychological suffering the elephants endure from this sport is obvious. “One day it will be seen for the moral outrage that it is,” he said.
Before Trump’s tweet, the Fish and Wildlife Service had intended to begin issuing permits for importing elephant trophies on Friday. The new policy would apply to elephants hunted in Zimbabwe between January 21, 2016, and December 31, 2018, as well as elephants hunted in Zambia from 2016 to 2018. Regardless of how hunting affects elephants, if the policy goes through, some hunters will have trophies waiting for them in those countries.
* This article originally misstated the portion of tourism revenue contributed by trophy hunting. We regret the error.


In the early 1880s, the French astronomer Étienne Léopold Trouvelot published a dreamy illustration of Jupiter based on his telescope observations. Back then, the gas giant looked, through telescopes, like a fuzzy, gray marble, a dust particle hanging in the night sky. Trouvelot, who in his life created 7,000 astronomical drawings, sought to add a little more detail to the picture to enhance the planet’s features.
“My intent is ... to represent the celestial phenomena as they appear to the trained eye and to an experienced draughtsman through the great modern telescopes provided with the most delicate appliances,” he wrote at the time. He drew, with distinct lines and colors, Jupiter’s bands of swirling clouds and its trademark blotch, the Great Red Spot. “My aim is to combine ... accuracy in details ... with the natural elegance and delicate outlines peculiar to the objects depicted,” he explained.
More than a century later, the desire to share Jupiter with the public in this way lives on. Since July of last year, NASA’s Juno spacecraft orbiting Jupiter has returned batch after batch of grainy, raw images. When a new set arrives, the images are quickly uploaded to a public website, where a band of space enthusiasts, sprinkled around the world, grab them and get to work. They stitch the images together, make a few color corrections, and start sprucing. Some adjustments to contrast here, a little brightening there. They try to show Jupiter as Trouvelot did—the planet as it is, yes, but also as it is if humanity could get even closer, peer a little deeper, and see the wondrous details of a neighbor in the solar system.
The process has led to a trove of stunning photos of Jupiter, unlike anything other space missions have ever produced. The detail in stunning. Zoomed in, Jupiter’s clouds look like cream swirling in coffee, or like the textured brushstrokes of a Van Gogh. They look like art.
Some of the amateur processors behind these pictures hang out on Unmanned Spaceflight, a no-frills internet forum where users can share and discuss spacecraft imagery. “It’s time to start a new topic for Juno’s Perijove-09,” wrote Gerald Eichstädt in a post on the forum last week. Eichstädt is a mathematician who works in software and lives in Stuttgart, Germany. Juno had recently completed its ninth flyby of Jupiter—a close approach known as a perijove—and Eichstädt was waiting for the pictures to show up online. Juno returns photos about every 53 days, thanks to an elongated orbit that brings the spacecraft toward the planet for a few hours before flinging it back out.
“More power to you Gerald,” wrote back Seán Doran, a designer in London. “Next week is going to be busy and fun!”
Doran wished his fellow image processor luck because translating data from Juno’s camera into something usable is no easy feat. JunoCam, as it’s called, doesn’t take pictures like the camera on a smartphone, or even like the camera on other spacecraft. It photographs in hundreds and hundreds of narrow strips through red, green, and blue filters—all while spinning around about every 30 seconds. These strips, called framelets, have to be arranged and stitched together to create coherent, composite photographs.
While Juno cruised to Jupiter, Eichstädt developed a computer program that automates this assembly, and he’s still tweaking it today. When a fresh batch of pictures becomes available, Eichstädt dumps the raw images into the program and lets it run. When that’s done, he shares the composite images with his fellow processors, and everyone jumps over to Photoshop to tinker with the images, produce their own takes, and create animations.
Their final photos, shared widely on social media, transform Jupiter from an abstract, distant planet into a dynamic world swirling with stormy weather. “You can see the clouds—that’s something we can wrap our minds around,” Doran said. “All of a sudden, the planet becomes real.”
The adjustments make the colors of Jupiter—all kinds of shades of orange and red and blue—pop. Without tinkering, Jupiter would look muted. For example, here’s a true-color version, from Björn Jónsson, a software engineer in Iceland who has also processed images from the Cassini, Galileo, and Voyager missions.
Most processed photos are usually not “true color” images of Jupiter—but that’s not a bad thing.
“I’m not looking for ‘true/natural color’ in my images, because enhanced color can show us a little more,” explains Roman Tkachenko, an amateur astronomer and music producer in Kursk, Russia. Tkachenko assembles the many raw images manually before taking a Photoshop brush to them. “I just want to show people something more than they can see in unprocessed images,” he said.
Doran agrees. “It’s like peeling back a curtain,” he said. “You just want to reveal what’s there. That’s what motivates me—and I try not to upset the scientists too much.”
The people behind the Juno mission see it that way, too. Candy Hansen, a senior scientist at the Planetary Science Institute who leads the JunoCam team, said using artificial hues makes features in Jupiter’s atmosphere stand out. “We don’t turn up our noses at artificial color,” Hansen told me for an earlier story about JunoCam’s photos. “We love artificial color.”
The Juno mission doesn’t have a dedicated staff to process raw images from JunoCam, and usually relies on the group of processors to produce photos—for free—to share with the public and include in press materials. JunoCam wasn’t designed for scientific purposes—its sole mission is, quite literally, to take pretty pictures—but the Juno team has used the images to better understand the meteorology of gas giants, in our solar system and beyond. “Jupiter can be considered as representing a population of gas giants, likely a widespread population of celestial bodies in the observable universe,” Eichstädt said. “Understanding Jupiter means understanding non-negligible portions of our universe.”
The first rule of Juno club is that image processors be up-front about the changes they make, said Emily Lakdawalla, an editor and scientist at The Planetary Society and one of the administrators of Unmanned Spaceflight. “You need to be very clear about what you’ve done in order to create the image so you aren’t saying it’s a true representation.”
The Juno processors seem like a supportive and collaborative bunch, even though they’ve never met. There’s a hint of competition, but that’s to be expected in a community of artists, said Jason Major, a graphic designer in Rhode Island. “It’s a very friendly competition—who finds what interesting feature, who can work the quickest, who can get the best result.”
Major said they’ve been at this for enough time that it’s possible to guess which processor produced a particular Juno picture. “Everybody has their own eye, their own aesthetic sense as far as what looks good, and that’s what makes it interesting,” he said. “If you were to send 10 photographers out into the world and have them come back a day later and show you their results, you’re going to be seeing very, very different things.”
Take, for example, processed photos of the Great Red Spot, a storm that has been raging perhaps for centuries. Juno came within 5,600 miles of the spot in July, closer than any spacecraft before it. The raw images from that flyby were some of the most exciting shots the Juno processors had seen, and they raced to process and post them online. The depictions, each of them certainly dazzling, varied. The color of Major’s Great Red Spot turned out a soft orange, while Eichstädt’s was a fierce red.
The image processors have a few more weeks until the next perijove in December. The flyby will produce, again, hundreds and hundreds of strips ready for stitching, but they won’t look the same as last time. In fact, no photo of Jupiter, raw or processed, can ever look the same. The planet’s storms are always brewing, always shifting. A gorgeous filigree of clouds captured during one flyby may be gone in the next.
“Jupiter's clouds are constantly changing,” Jonsson said. “Each day it looks in some way at least slightly different from what it has ever done before.”


One afternoon in May 2008, a graduate student named Pat Kramer was in northwestern Pennsylvania catching purple martins. The bird, a large swallow that nests in artificial birdhouses across North America, is a well-studied species. But one particular purple martin Kramer and some fellow researchers from York University caught was about to revolutionize ornithology.
Kramer let an exclamation mark creep into his otherwise staid field notebook when he found it: “At 2:45p.m., Evelina captured Yellow 2551 in WH-43!”
Yellow 2551, the identification code assigned to this martin, was wearing a geolocator, a small device that uses a light sensor to calculate latitude and longitude and track a bird’s movement over time. The geolocator had traced this female martin’s migratory journey to Brazil and back via the Yucatan Peninsula. In doing so, it provided the first data on what had been a massive blind spot in the scientific understanding of the otherwise familiar purple martin: Where, specifically, does the bird go during migration? And what route does it take?
In the decade since Yellow 2551’s pioneering journey, scientists have used geolocators and a variety of newer technologies to gain an increasingly sophisticated understanding of how migratory songbirds move across the globe. As a result, a much more nuanced picture has emerged of how conditions on wintering grounds and along migration corridors affect birds’ survival. And very soon, with the deployment of some cutting-edge gadgetry on the International Space Station, ornithologists will finally be able to delve into the most disturbing mystery of all: why half of the migratory songbird species in North America are disappearing at alarming rates.
* * *
As so often happens, all this began with a coincidence. At a conference in Mexico, Bridget Stutchbury, an ornithologist at York University, stumbled across a poster on geolocators, which the British Antarctic Survey first developed to study seabirds. She realized that at 1.5 grams, the geolocators were small enough to put on very large songbirds like the purple martin. The following year, Stutchbury and her students deployed 20 of them, strapped to the birds with a backpack-style harness. Though they recovered just two from this initial batch—geolocators are “archival,” meaning they don’t transmit data remotely—they revealed an immediate surprise: Yellow 2551 had flown north in the spring much quicker than expected, covering the thousands of miles from Brazil to Pennsylvania in just two weeks.
“It was the first time anyone had been able to track songbirds from start to finish in their annual migration,” recalls Stutchbury. “We know conceptually that they do it, but to see it on the map ... it’s like, ‘Yes, the bird did this. Here’s the proof.’”
The research made a splash after it came out in Science in 2009, and soon the journals were regularly publishing geolocator studies of migrating songbirds. Through September 2017, a total of 121 such papers have been published, according to a forthcoming review study by Emily McKinnon, a postdoctoral fellow at the University of Windsor. Many of them tell remarkable stories. After recovering geolocators from two Connecticut warblers in Manitoba in 2016, for instance, McKinnon was astonished to discover that the tiny birds’ journey south to Bolivia the previous fall had included a nonstop, two-day haul over the Atlantic Ocean.
“One of the things that really continues to be amazing is that these small songbirds are doing things that we did not think that they are capable of,” she says.
This geolocator science is filling in fundamental gaps in the natural histories of entire species. Stutchbury eventually tracked about 400 purple martins with geolocators, and showed that the species’ core wintering range is in the Brazilian Amazon, rather than more densely populated areas of the country much further south and east, as scientists had previously thought.
In 2017, understanding of purple-martin migration took another leap forward thanks to archival GPS tags, which log birds’ locations down to just 10 meters, as opposed to geolocators’ several-hundred-kilometer margin of error. The research, led by Kevin Fraser at the University of Manitoba, showed that purple martins spend significant amounts of time on low-lying river sandbars in the Amazon. At this level of precision, specific conservation implications begin to emerge. Protecting this sandbar habitat, for example, could be key to maximizing purple martins’ winter survival.
Another technology out there is called Motus, which uses radio telemetry and is non-archival: Special towers simply record the presence of Motus-tagged birds that pass within range (though that range is small and there aren’t many towers out there). There are now satellite tracking tags small enough to use on large songbirds, although weight still prevents them from being widely applied in songbird research. And laboratory techniques like stable-isotope analysis of bird feathers also make valuable contributions by showing, for example, where birds pause during the year to molt new feathers.
The Space Station Is Becoming a Spy Satellite For Wildlife
Together, it all furthers the increasingly nuanced understanding of songbirds’ full annual cycles from breeding grounds to winter hideouts and back again, and the intricate cause-and-effect relationships between them that are only just now becoming clear. “We’re basically rewriting the textbooks,” says Pete Marra, head of the Smithsonian Institute’s Migratory Bird Center.
* * *
One final frontier remains, however: Where and how do birds die? Recovering data from archival tracking devices depends on birds surviving migration—and researchers catching them again afterward. That means that the many, many migratory songbirds that don’t survive their journeys simply disappear into oblivion, telling no tales about the circumstances of their demise. “If we can’t figure out where they die, then we can’t figure out why they’re dying, and we can’t then implement conservation strategies to stop those declines,” says Marra.
That’s what could make the ICARUS initiative—short for “International Cooperation for Animal Research Using Space”—the next ornithological game changer. Scheduled to come online next year, after Russian cosmonauts install a new antenna on the International Space Station, ICARUS will remotely track tagged birds’ movements with such precision that it will be able to tell when they stop breathing, says Martin Wikelski, the project leader at the Max Planck Institute for Ornithology.
When ICARUS launches, the lightest tags will weigh about 3.5 grams—still too heavy for most songbirds—and they’ll only communicate with the orbiting antenna once a day. But Wikelski predicts a one-gram tag will be available in two to three years, and plans are in the works to put more antennae in orbit, improving ICARUS’s coverage and allowing more frequent data transmissions. The idea is to allow scientists to follow individual birds across the globe while keeping tabs on an enormous amount of data—speed, altitude, temperature, heading, acceleration, and so forth—much like airplane passengers now track their flights on the seat-back display. As tags get smaller and satellite coverage improves, the birds they track can be smaller and the data they receive gets closer and closer to real time.
This means that any interested party should soon be able to follow a dot on a computer screen representing an individual purple martin en route from Pennsylvania to some tiny speck of a sandbar in the Amazon. It is, obviously, an exciting prospect for the ornithologists who study this stuff, as well as for the wider bird-watching public. Wikelski emphasizes the educational angle: Imagine an elementary-school science unit built around a single purple martin’s journey to Brazil and back again.
Perhaps that purple martin makes that journey just fine. But then again, maybe the bird fatigues and plunges, Icarus-like, into the sea. Or maybe it goes winging gracefully into an office tower’s plate-glass windows. Maybe some other fate befalls it. That might upset those elementary students, but, with ICARUS beaming back all the gruesome details, this dead bird would no longer be another scientifically useless tick mark in the mortality column. Instead, it would do its part in revolutionizing scientists’ understanding of songbird migration yet again, by showing exactly what happens to the many migratory songbirds who don’t make it back home—and by pointing to ways to keep them migrating in the future.


MIAMI—When Hurricane Irma sprinted toward Miami–Dade County, Jeff Ransom couldn’t sleep. He wasn’t just worried about gusts shattering windows, or sheets of rain drowning highways—that’s far from unusual near his home in Broward County, where extreme weather verges on routine, and patches of US-1 are regularly submerged.
Ransom, the county archaeologist, was preoccupied with an oak tree and its 350-year-old roots. If the tree capsized with enough intensity, he worried, the flailing roots could dislodge human remains.
On a blazing blue morning in early November, weeks after the storm, we trek to the site of the Tequesta Native American burial mound that kept Ransom awake.
“All night long, I was just thinking about that oak tree flipping over,” he says. “The big roots are growing right into the burial mound. That would’ve just blown human bone everywhere.”
Irma’s winds shaved canopies off the trees at the Deering Estate, a historic homestead that contains the burial mound and other fossil sites and is managed by the Miami-Dade County Department of Parks, Recreation and Open Space. Under those bald branches, growth was rapid as vines and chutes—nourished by seaweed deposits—scrambled for sunlight. The result has been a second spring: bright, young leaves, greedy for purchase among the gumbo-limbo and strangler figs. Ransom knocks a path for us with a machete, which he carries slung in a holster. Two thwacks splinter the Brazilian pepper branches—but that’s only because the machete is dull, he tells me. Usually, a single smack is enough to slice straight through, like butter.
Ransom is 52, with a GI Joe jawbone and black aviator sunglasses. At one point, these vanish into the carpet of leaf litter, gone shaggier since the storm, and Ransom spends a few minutes poking around for them beneath the slashed fronds before remembering that he has a nearly identical backup pair.
The burial ground was—is—fine. The oak’s trunk is sturdy and thick; the roots are sunk deep into the soil. We sit for a moment on benches nearby, guzzling water in the shade while Ransom uses his machete’s blunted edge to scrape burrs off his pants and shoes.
The storm didn’t bear down on the city with all its might: In general, southeast Florida was spared the breadth of damage that forecasters had conjured. A half-mile of mangroves buffered the Cutler Midden, another archaeological site on the Deering Estate, against damage wrought by crashing waves. Ancient shell tools and pottery fragments survived intact.
Irma could have bitten harder. But in isolated pockets, the storm was ravenous. We pass fragments of a historic boardwalk, which the archaeologists had laboriously documented and annotated. The structure “had been chunked up” in the storm, explains Mallory Fenn, the public-archaeology coordinator at the southeast/southwest Florida branch of the Florida Public-Archaeology Network (FPAN). The network is a project of the University of West Florida; the southeast/southwest division operates out of Florida Atlantic University.
Fenn’s earrings are made from gator teeth, and the boardwalk looks masticated and spit out, its component parts hardly visible. An orange-and-white barrier marches across the crumpled walkway, as if it wasn’t patently clear that there’s trouble ahead.
* * *
Before I fly down to Miami to trail her and Ransom through the swamp, Sara Ayers-Rigsby sends me a packing list. Ayers-Rigsby is the southeast/southwest regional director of FPAN, and the trunk of her car is stocked with supplies, from bug netting to single-serving bags of pretzels. She’ll have ample bug spray and sunscreen to share, she writes, but I’ll want to wear long sleeves, long pants, and the most waterproof boots I’ve got. We’ll be wading into the height of the king tides; the water might rise up to our knees. Heat and mugginess can have a scrambling effect. Ayers-Rigsby later describes it as “brain-meltingly hot.”
“The weather in South Florida is inhospitable,” she warns.
Writ broadly, that’s precisely the problem. Numerous projections forecast a future of extreme weather and persistent flooding that is incompatible with many elements of life as it’s known on the peninsula. Of all of the U.S. states, Florida is the most vulnerable to sea-level rise, and Miami-Dade is at particular risk.
As the plane drifts toward descent, water is everywhere: in green-blue pools that reach for the horizon, in mud-colored eddies, in staid intercoastals studded with white yachts. From the air, many of these basins look overfull, ready to spill with the slightest top-off.
Sooner or later, the water will swallow the shoreline. When it comes to the magnitude, severity, and timetable, there are shades and gradations of apocalyptic hues. In 2015, a working group comprised of officials from across southeast Florida set out to get on the same page about the threats and to strategize about mitigation efforts. Their projection draws from local tide measurements and is aligned with estimates from the U.S. Army Corps of Engineers and the National Oceanic and Atmospheric Administration. By 2030, they anticipate a sea-level rise of 6 to 10 inches from a 1992 baseline; they predict a rise of up to 26 inches by 2060, and 61 inches by 2100.
Even if the water doesn’t crawl quite that high, damage could still be widespread and devastating. Twenty-five percent of land in Miami-Dade County sits less than three feet above current sea level, according to the World Resources Institute. Ten percent is nearly flush with the sea.
And if water does splash to the maximum level, the results could be cataclysmic. In a recent report, the real-estate company Zillow estimated that, if the sea level were to rise by six feet, 24 percent of Miami’s housing stock would be drenched.
Troublingly for Ransom and Ayers-Rigsby, a sea-level rise of just half that height could destroy as many as 16,095 archaeological sites across the state. As the terrain goes soggier or washes away, how do you protect objects embedded in it?
“You can’t wrap an archaeological site in bubble wrap and put it on a high shelf,” Ayers-Rigsby told me via phone soon before Irma swept past. Some sites can be stabilized or buffered with mangroves or oyster beds, but when it comes to safeguarding them from the pummeling rain or surging waves of a hurricane-strength storm, options are limited. “Other than building a massive construction around it,” Ayers-Rigsby said, “there’s not that much you can do.”
Among officials in Miami-Dade, “there’s no sugarcoating or backtracking” about the threat of climate change, Ransom tells me. Its consequences play out in real time, in flooded streets and waterlogged basements, and voters throw their weight behind mitigation efforts at the polls. After his landslide victory in this month’s elections, the incoming City of Miami Mayor Francis Suarez told the local ABC affiliate that “Miami should be and must be the most resilient city in the world.” That same day, voters approved a bond measure that directed $192 million to pumps, walls, drains, and other projects to keep the city drier. Meanwhile, Ransom, Ayers-Rigsby, and their colleagues work to keep thousands of years of history from being lost to the sea.
* * *
If you wonder what archaeology Florida can boast of, you’d hardly be the first. In a carpool from the airport, I told two Australian businessmen what had brought me to the city. They cocked their heads. Miami, to them, evoked beaches, surgically altered bodies, and hefty Cuban sandwiches. What else was there?
I recount this to Ayers-Rigsby while we sit on a choked concrete freeway, inching from Fort Lauderdale to Biscayne Bay. She groans and slumps her head toward the steering wheel. Ayers-Rigsby, 34, relocated to Florida from the mid-Atlantic, and is now somewhat evangelical about the region’s overlooked merits. Around her neck, she wears a pendant with the state’s silhouette.
For as long as people and creatures have inhabited present-day Florida, they’ve been shedding traces of their lives. Fenn says the flitting snowbirds and rotating crop of transplants can be afflicted with a virulent case of historical amnesia. But the scattered sites testify to millennia before the shores were dotted with high-rises fashioned from glass and steel.
The Cutler Fossil Site is a watering hole into which all manner of Pleistocene beasts toppled. Sandwiched between the limestone layers of the sinkhole, some 16 feet above the current sea level of nearby Biscayne Bay, were bones of dire wolves, mastodons, camels, llamas, saber-toothed tigers, and the American lion. Though the site is protected, the city has sprawled around it in the intervening 10,000 years. Looking down into the ancient pit from the ridge, you can hear the rumble of nearby cars. But the site is hidden and sheltered from the road and the water, protected by its isolation and its elevation.
Other sites sit more uneasily with the present. In the late 1990s, archaeologists discovered a circle of postholes cut into the limestone bedrock at the mouth of the Miami River. Carbon dating of wood fragments helped identify the site as the home of a structure built nearly 2,000 years ago by the Tequesta Indians. “People have been partying in Miami for thousands of years,” Fenn jokes, as she shows me around the site. Archaeologists, Native activists, and a galvanized public sparred with a developer, who had purchased the property as the future site of luxury condos. (A flurry of controversy swirled at the time, when some scholars wondered whether the pattern was, more simply, the drain site for a septic system. Archaeology magazine solicited input from other archaeologists, scholars, and a master septic-tank contractor, the latter of whom summarily dismissed the possibility.)
The Miami Circle was designated a National Historic Landmark in 2009. Today, the site is a grassy expanse shaded by towering condos and hotels that have sprung up around it, overlooking cruise ships and cargo freight lumbering in the distance. It’s a rare green space in a vertiginous corner of the city—and that means it sometimes becomes a place for dogs to lift their legs. A fluffy white dog squats nearby as Fenn describes working on an archaeological site just across the narrow river, where archaeologists unearthed additional Tequesta artifacts in 2014 in the prospective footprint of a massive mixed-use development. These excavations are a trippy mash-up of the ancient and the dizzyingly modern. “When you look down, you think it’s the 1850s, with a sifter and a trowel,” she says. “Then you look up and see skyscrapers, and the Metromover going by.”
During Irma, water breached the walls just below the Miami Circle site. It rushed onto the grass, carrying palm fronds washed in from the river. Fenn, who lives nearby, “ran out pretty much the second we were allowed to be outside” to check in on it. The water soon receded, leaving no apparent damage. This particular spot, loaded with infill, has been shored up to withstand exactly this type of barrage.
Other sites, which lack these preventive measures, are more vulnerable. But studying them can reveal important data about the rising sea—and how long scholars have to hatch a plan.
* * *
Ransom and Ayers-Rigsby pick through a dense thicket and a floor carpeted with spiky bromeliads. They know what they’re looking for—orange-capped rebar that they sank into the bank of the Oleta River—but Irma blew down the trees onto which they’d tied yellow ribbon to help them identify the sites at a distance. Those orange markers have been coated with dirt.
This squishy portion of the riverbank is the site of a prehistoric midden containing traces of shell tools, pottery, and other daily items that would have been used by Native American tribes who lived on the shore.
“If any site is going to erode, it’s going to be this one,” Ransom says, sloshing through the muck.
The midden, or ancient trash heap, is nearly flush with the water level, which makes this site an ideal candidate for tracking inundation and water rise before and after storm events and king tides. By obtaining a baseline measurement and a set of comparisons, the archaeologists can document both accumulation and erosion—noting which events seem to pile more sediment on the top of the site, and which strip it, ultimately threatening to haul the artifacts out to sea.
The notion of using this area as a proxy for fluctuations in the water level dates back decades. In the late 1970s and early ’80s, when he was working as the country archaeologist, Robert Carr found evidence of ancient charcoal buried about two feet below the surface. Since a fire needs to be dry, Carr reasoned that that portion of the site was once above water. At the time, climate change “certainly wasn’t on anybody’s radar” in the archaeology community, he tells me via phone. There was “no particular movement or focus going on.” Carr advocated for using soil inundation, radiocarbon dating, and water levels as firm evidence for past and future variations. His work laid the foundation for what Ransom and Ayers-Rigby are doing.
On a recent afternoon, the mangrove roots are flecked with odd pieces of very modern garbage: foggy glass bottles, a boogie board speckled with barnacles, a black DVD case, a wrinkled bag of Ruffles chips. These aren’t the signs of someone sneaking in to use the forest as a dump, Ayers-Rigsby says—the refuse has been carried in on waves.
She and Ransom slog through the sucking mud, brushing biting ants from their backs and shoulders, to measure the distance from the rebar to the waterline. They jot down the measurements in a yellow notebook, its pages warped by wetness. In some spots, the sediment is piled higher than it was the last time they measured, before Irma blew in. That accumulation suggests that the water level breached a good chunk of the shoreline during the storm, Ransom says.
Carr explains that’s not unequivocally dangerous—there’s not yet sufficient clarity about whether inundation is an impediment to preserving sites in the same way that erosion is. Conceivably, he says, a site “could be better preserved underwater than it is above ground, if sea-level rise is gradual, not a result of pounding waves hitting shoreline and tearing up and removing soils.”
Through her work at FPAN, Ayers-Rigsby has also helped recruit a team of citizen scientists to fan out across the state and conduct regular monitoring of at-risk sites. Inspired by a U.K. program, Scotland’s Coastal Heritage at Risk, the Heritage Monitoring Scouts, a brigade more than 200 people strong, survey publicly accessible sites—not the more sensitive ones, like unmarked burial grounds—and upload their impressions onto a website form. They look out for signs of flooding, erosion, or wave action, or any artifacts that may have been dredged to the surface, and flag any places that need urgent attention.
Sixty-two-year-old volunteer Paula Streeter surveys the shell midden on Calusa Island, a dot of land off the state’s southwest coast once inhabited by Calusa Indians. Streeter has a wide-ranging background—her resume includes “a zillion, million, trillion things,” she tells me via phone. Since retiring from the city clerk’s office, she’s begun assisting archaeologists. “I only started this,” she says via phone. “It was the most amazing thing in my life, and it only happened two years ago.”
Already, the Calusa shoreline is being eaten by waves and wind action, Streeter says. Artifacts are surfacing in the midden, relics of the tribe’s use of shells for tools and weapons—but the average beachgoer might not notice them. “If you’ve been trained, you know that’s an ancient form of a hammer made from a whelk shell or a horse conch,” Streeter says.
The Calusa Island site is only accessible via boat or kayak—“you can’t just zip out there,” Streeter says. Before the recent hurricanes and king tides, the team intended to survey once a month. (The site is also monitored by researchers from the University of Florida.) When toppled trees exposed these artifacts, the team upped the frequency to once per week—and instead of leaving all of the artifacts in situ, the volunteers diagram the original locations and bag some of them, so they’re not tugged out to sea. Heritage Monitoring Scouts use rebar installations to measure the distance from the midden edge to the beach. Even without their precise computations, it’s easy to see the effect of the waves and wind in exposed roots and a dramatically angled ledge of sand.
Some of these sites contain clues to enriching or correcting the historical record. One example is the dwindling island of Egmont Key, off of the Tampa coast.
A few years ago, the U.S. Army Corps of Engineers reached out to the Seminole to ask about the dwindling island. It was eroding heavily—shrunk to 280 acres, half of its size—and they were wondering whether to replenish it with sand. Was the tribe interested in preserving it?
The imminent threat to the landmass was the impetus to uncover the site’s history. With his colleagues, Paul Backhouse, the director of the Ah-Tah-Thi-Ki Museum and tribal historic-preservation officer for the Seminole Tribe of Florida, pursued some research and learned that, during skirmishes with the U.S. Army in the mid-1800s, the island functioned as a detainment site for Seminoles who were caught evading the ships deployed to remove them out west. Judging by contemporary accounts, conditions were grim: There were no sources of fresh water, and the captives were trapped.
The island sits no more than six feet above sea level. Did the tribe want to keep it above the waves? Among the Seminole community, “the overwhelming response was yes,” Backhouse says via phone. Archaeologically, there was much to learn from the site and the 19th-century artifacts that accumulated there—but it could also function as a place of catharsis and education. “Youth can come and remember the struggle their ancestors went through to remain in Florida,” Backhouse says. “This history is a hidden history—it’s not one that’s in any of the textbooks, because it’s an embarrassment to normal American history.”
Egmont Key is on the front lines. With enough elevation or distance from foot traffic, many other sites will be safe for a relatively long time, by virtue of staying dry or hidden. But as the sea creeps higher, choices will have to be made.
* * *
This fall has been an expensive one at the Deering Estate. Hurricane Irma and the October king tides packed a double punch, explains Jennifer Tisthammer, the estate’s director.
During that first king tide, storm surge swamped the service road with ankle-deep water and flooded the back lawn, where many of the estate’s special events take place. Irma’s gales ripped off 80 percent of the tree canopy; 6,000 cubic yards of seaweed washed ashore. Tisthammer’s long-term vision is to raise the back lawn—but in the meantime, the staff looked for prophylactic measures to mitigate the aesthetics and promote drainage. Sod is best, Tisthammer says, but white rock looks better than soggy, brown grass. When the staff spread out truckloads of drain rock and sand, the puddles that had been taking weeks to drain were siphoned off within a few days.
Even if the fully underwater future is far off on the horizon, the king tides offer a regular reminder—and a kind of trial run. On a page devoted to king tides and climate change, the Environmental Protection Agency notes, “Sea-level rise will make today’s king tides become the future’s everyday tides.”
Places like the Deering Estate are already factoring preventative and adaptive strategies into line items on the budget. “You’re gonna have some loss,” Tisthammer says. “Do you put $3 million into something you know will eventually go under, or allocate it differently?”
The kind of data that Ayers-Rigsby and Ransom are collecting can be used to inform broader city planning and budgeting—and this December, Miami-Dade and three surrounding counties are taking archaeological sites into account, adding provisions to the updated action plan from the Southeast Florida Regional Climate-Change Compact. The document isn’t binding, but it encourages local officials to work with historic-preservation specialists to map and rank at-risk sites; to appeal to FEMA, local emergency-management offices, and other agencies for financial resources; and to implement sustainable preservation tactics such as planting mangroves and cordgrass, or “hard armoring” sites with rocks or concrete. These strategies aren’t without drawbacks. “Hard methods may negatively impact sites by the weight and shifting of large rocks, not to mention the cost of acquiring and moving these to remote places,” Ransom says.
The solution is also not as simple as plucking artifacts from the ground and shuttling them to museum collections, where they might be preserved behind plexiglass vitrines. For the Seminole tribe, as for many other indigenous groups, Backhouse says the prevailing philosophy is that items discarded over the centuries should be left in place. He acknowledges that this mantra of noting objects, “working around them, planning around them, and not thinking of those objects as just research vehicles” might “go completely against the grain of what most people think archaeology is.” But Ayers-Rigsby and Ransom likewise consider excavation to be something of a last resort.
In the Seminole culture, Backhouse says, there’s a difference between something being upturned by an earthquake, versus pulled to the surface by human hands. The underlying philosophy is seeking harmony and balance with nature, he says—and “indigenous cultures don’t have an idea that nature’s always nice.”
Last spring, my colleague Linda Poon reported that the vast majority of states lacked any mention of historic resources in their disaster-management plans. Up until this point, that’s been the case in Miami-Dade, says Ayers-Rigsby. “One of the reasons I was so happy we had some language put into the draft of the Climate Action Fund was just to get it on people’s radar,” she adds. “Before, it was not even included at all at any level.” There’s momentum in this direction: Earlier this fall, the city of Annapolis, Maryland, hosted a conference called Keeping History Above Water, dedicated to solutions for historic preservation and cultural resources. In August, Backhouse and the Seminole tribe participated in the Tidally United Summit, cosponsored with FPAN and the Florida International University Global Indigenous Forum, which focused on the relationship between climate science and historic resources.
Meanwhile, Ayers-Rigsby is sensitive to the emergent, unfolding toll that storms and flooding can wreak on people and property. “You have to put the human aspect in the present first,” she says. “You have to prioritize people’s safety and people’s livelihoods. Archaeology and historic resources are obviously necessarily secondary to that, but they should still be discussed.”
It’s painful enough to put a price tag on property—homes, cars, neighborhoods—that we will lose in the reckoning with the waves. And it can be an uphill battle to nudge residents and officials toward the level of abstraction required to dwell in the realm of forecasts and best guesses. “A risk in the future feels a lot less scary than a risk that’s presented right now,” the risk-perception expert David Ropeik told my colleague Laura Bliss in 2015. Even in Florida, where volatile weather is undeniable, it requires a few mental acrobatics to tumble toward an understanding of the sites that are at stake—sometimes literally below the surface.
But if the goal of archaeology is to preserve and interpret the past for the future, there’s plenty of work to be done—careful and quick, down in the muck and in legislative offices—before traces of that past slip away. In those strata are testaments to lives lived, forgotten, and remembered over the course of millennia: a record of what it has meant to be human.
No matter what they do, Ayers-Rigsby says, the time capsule will be incomplete. “Some things will be lost forever.”
This post appears courtesy of CityLab.


The Keystone pipeline was temporarily shut down on Thursday, after leaking about 210,000 gallons of oil into Marshall County, South Dakota*, during an early-morning spill.
TransCanada, the company which operates the pipeline, said it noticed a loss of pressure in Keystone at about 5:45 a.m. According to a company statement, workers had “completely isolated” the section and “activated emergency procedures” within 15 minutes.
Brian Walsh, a state environmental scientist, told the local station KSFY that TransCanada informed the South Dakota Department of Environment and Natural Resources about the spill by 10:30 a.m.
TransCanada estimates that the pipeline leaked about 5,000 barrels of oil at the site, Walsh said. A barrel holds 42 U.S. gallons of crude oil.
The Keystone pipeline system is nearly 3,000 miles long and links oil fields in Alberta, Canada, to the large crude-trading hubs in Patoka, Illinois, and Cushing, Oklahoma. It was completed in 2011. The entirety of its northern span—which travels through North Dakota, South Dakota, Nebraska, Kansas, Oklahoma, Missouri, and Illinois—would stay closed until the leak was fixed, the company said.
TransCanada said it was still operating the pipeline’s southern span, which connects Oklahoma to export terminals along the Gulf Coast.
The pipeline’s better-known sister project—the Keystone XL pipeline—was proposed in 2008 as a shortcut and enlargement of the Keystone pipeline.
In 2011, climate activists seized upon the Keystone XL pipeline, warning that its completion would allow the exploitation of much of Alberta’s tar sands and lock in too much future carbon pollution. James Hansen, then the director of the NASA Goddard Institute for Space Studies, warned in The New York Times that exporting oil from the Albertan tar sands would mean “game over for the climate.”
In 2015, President Barack Obama blocked the pipeline as part of his administration’s preparation for the UN climate-change talks in Paris. But less than a week after his inauguration, President Donald Trump ordered that decision reversed.
TransCanada faces one final approval—from the Nebraska Public Service Commission—before it can finish building Keystone XL. In a fluke of timing, that all-important meeting will take place this Monday, November 20.
TransCanada has already completed two spans of new pipeline around the trading hub in Oklahoma. Those projects make up about 40 percent of the Keystone XL project, and oil is already flowing through them. But even if the company wins approval next week, some analysts argue that the falling price of oil has made Keystone XL uneconomical.
The long-term environmental costs of an oil spill can vary greatly by location and the length of time the spill goes unnoticed. In 2006, BP spilled 267,000 gallons of crude oil in Prudhoe Bay, on Alaska’s northern coast. Though that is about the same amount that leaked from Keystone on Thursday, it flowed from the BP pipeline over at least five days. BP ultimately paid more than $100 million to the federal government, the state of Alaska, and Alaskans for damages related to the incident.

* This article originally misstated that the spill occurred in North Dakota. We regret the error.


On September 1, 1914, an old, trembling passenger pigeon named Martha died at Cincinnati Zoo. With her demise, her entire species slid into extinction. But in many ways, the species was already gone, for a solitary passenger pigeon is almost not a passenger pigeon at all. This is an animal that existed in gestalt. Its essence was in the flock.
Passenger pigeons were once the most abundant bird in North America, and quite possibly the world. At their peak, there were a few billion of them, traversing the continent in gargantuan, nomadic flocks that would blacken the sky for hours as they passed overhead. Simon Pokagon, a Potawatomi author and leader, described them as “the grandest waterfall of America” and their sound as that of “distant thunder” or “an army of horses laden with sleigh bells.”
And then, people started shooting them. They poisoned them, netted them, gassed them, hit them with sticks. In a matter of decades, the continent’s most common bird has been completely wiped out, down to the last individual. “It’s always astounded me how something could have that large a population and entirely disappear,” says Beth Shapiro from the University of California, Santa Cruz. “Why didn’t tiny populations survive somewhere in refugia? I mean, we are pretty good at murdering things, but how did we kill every one of them?”
These questions have been debated for decades. But Shapiro and her colleagues Gemma Murray and André Soares have found some new twists to the old answers by collecting bits of skin from around 200 passenger pigeons, whose century-old, taxidermied bodies sit in museums around the world. Using these samples, they sequenced the full genomes of four individuals, and compared them to the genome of the band-tailed pigeon—a close relative that still exists but lives in considerably smaller flocks.
At first, nothing jumped out. On average, the passenger pigeon’s genome looked to be extremely diverse—two to three times more so than that of any other bird that had been sequenced thus far. That made sense, given how many of them there were.
But averages are deceptive. DNA is packaged in chromosomes, and the team found that the genetic diversity at the ends of these chromosomes was exceptionally high, while the diversity in the middle was exceptionally low. The band-tailed pigeon doesn’t share the same pattern; its genome has roughly the same level of diversity throughout. Indeed, Shapiro had never seen anything like this before. This pattern—and the evolutionary forces that produced it—have important implications for understanding both why the passenger pigeon died out, and whether it'll be possible to bring it back.
Here’s why the pattern exists. When animals reproduce, their chromosomes mix and mingle, shuffling their genes into new groups. This process, known as recombination, breaks up blocks of genes, allowing natural selection to weed out the worst mutations and keep only the best ones. But in birds, recombination happens more often at the ends of chromosomes than in the middle.
Imagine that you’re going through your wardrobe trying to chuck out any clothes you hate, while keeping only the ones you love. Unfortunately, you find that some miscreant has stitched all the shirts, skirts, and pants together. If you want to keep a particular shirt, you’re forced to keep everything else that goes with it. That’s what happens in the middle of the pigeon’s chromosomes. Recombination is low, so genes stick together in large blocks, making it hard to select for one without getting all the hangers-on. But in your wardrobe, the hats, scarves, socks, and shoes are still free and loose, allowing you to consider each item individually and choose the best ones for your ensemble. Same goes for the ends of the passenger pigeon’s chromosomes: That’s why they’re more diverse than the middle.
It’s commonly assumed that animals with massive populations should be genetically diverse. But “the passenger pigeon’s genome is that of both a low-diversity species and a high-diversity one,” says Ben Novak, who worked on the study. “In either case, it was well-adapted for its preferred lifestyle.”
Indeed, that’s why its genome is so weird. Genomes can evolve either through drift, in which DNA changes randomly, or through natural selection, in which genes become more or less common because they affect their owner’s ability to survive and reproduce. Typically, both forces are important. But the passenger pigeon was so absurdly abundant that natural selection dominated. “There was almost no portion of the passenger pigeon’s genome that was evolving neutrally,” says Shapiro.
This might all sound pretty wonky, but it matters when thinking about why passenger pigeons died.
In 2014, Chih-Ming Hung and colleagues from National Taiwan Normal University used the genomes of passenger pigeons to reconstruct their historical population size. They concluded that the pigeon was a boom-and-bust bird, whose numbers cycled dramatically between incredible highs and stark lows. That created a natural vulnerability, which humans inadvertently exploited by persecuting the pigeons during a bust phase.
But Shapiro says that Hung’s team made a mistake. They used a technique which assumes that genomes are evolving neutrally—and the passenger pigeon’s largely isn’t. (Hung stands by the conclusions of his study.) Instead, it had been crafted by natural selection to an extent that most other species are not. “Passenger pigeons were not a naturally vulnerable species,” as has been repeatedly suggested over the last five decades, says Novak. “It was a superspecies in its natural element.”
So, why did this superspecies die out? Shapiro thinks it’s because the bird specifically evolved to live in mega-flocks, and developed adaptations that became costly when their numbers suddenly shrank at human hands. “Maybe they were simply not adapted to being in a small population, and didn’t have time to recover,” she says. Maybe they hit a tipping point when there were just too few of them to survive, regardless of whether they were being hunted.
That’s a bit of a leap from the data, but it’s an idea that’s worth entertaining, says Erich Jarvis from Rockefeller University, who studies bird genomes. “It makes one think: Just because humans leave a small population behind without killing off the rest of the species, it does not mean that the species will survive. It would be good to see if there are other species like this.” Shapiro agrees, and wants to see if the same patterns exist in the genomes of other animals that live in massive groups—perhaps herring, bristlemouth fish, or red-billed queleas.
Meanwhile, Novak cautions that we don’t know if the pigeon actually did suffer at low densities, or the minimum number that would have been necessary to prevent extinction. Indeed, he says that there’s evidence the passenger pigeon would have fared reasonably well at low numbers, if we had just left them alone. Records show that they were breeding just as efficiently when there were only a few hundred left as when there were billions.
The fact is that “human persecution was relentless right up until the very end,” he says. “The rarer the birds became, harvesting efforts only grew more intense. Whatever maladaptive trade-offs may have existed for the passenger pigeon, their decline was simply too rapid for these trade-offs to show symptoms.”
Novak is part of a group called Revive and Restore, which wants to use modern genetic tools to resurrect the passenger pigeon. These birds were crucial parts of America’s eastern forests; their huge numbers and voracious appetites created a patchwork of destruction and regeneration that many animals and plants actually depend on. Novak’s team wants to bring those cycles back, and they need passenger pigeons to do so. But if the bird can’t survive at low numbers, their project seems doomed to failure unless it simultaneously resurrected a flock of millions.
Not so, says Novak. His team isn’t trying to raise passenger pigeons, Lazarus-style. Instead, they are trying to turn the band-tailed pigeon into something passenger-ish, tweaking its genes so it becomes far more social. If their flocks get big enough, they can step into the role that the passengers once played. And over time, their genomes should evolve into the same pattern—diverse ends, uniform middles. “This isn’t something we will engineer, it is something that will happen due to selection and recombination in nature, just as it happened to passenger pigeons originally,” Novak says.
It’ll be a long process, and he won’t be around to see its conclusion. But he wants to see its beginning.


The first thing that hit me about Zealandia was the noise.
I was a 15-minute drive from the center of Wellington, New Zealand’s capital city, but instead of the honks of horns or the bustle of passersby, all I could hear was birdsong. It came in every flavor—resonant coos, high-pitched cheeps, and alien notes that seemed to come from otherworldly instruments.
Much of New Zealand, including national parks that supposedly epitomize the concept of wilderness, has been so denuded of birds that their melodies feel like a rare gift—a fleeting thing to make note of before it disappears. But Zealandia is a unique 225-hectare urban sanctuary into which many of the nation’s most critically endangered species have been relocated. There, they are thriving—and singing. There, their tunes are not a scarce treasure, but part of the world’s background hum. There, I realized how the nation must have sounded before it was invaded by mammals.
Until the 13th century, the only land mammals in New Zealand were bats. In this furless world, local birds evolved a docile temperament. Many of them, like the iconic kiwi and the giant kakapo parrot, lost their powers of flight. Gentle and grounded, they were easy prey for the rats, dogs, cats, stoats, weasels, and possums that were later introduced by humans. Between them, these predators devour more than 26 million chicks and eggs every year. They have already driven a quarter of the nation’s unique birds to extinction.
Many species now persist only in offshore islands where rats and their ilk have been successfully eradicated, or in small mainland sites like Zealandia where they are encircled by predator-proof fences. The songs in those sanctuaries are echoes of the New Zealand that was.
But perhaps, they also represent the New Zealand that could be.
In recent years, many of the country’s conservationists and residents have rallied behind Predator-Free 2050, an extraordinarily ambitious plan to save the country’s birds by eradicating its invasive predators. Native birds of prey will be unharmed, but Predator-Free 2050’s research strategy, which is released today, spells doom for rats, possums, and stoats (a large weasel). They are to die, every last one of them. No country, anywhere in the world, has managed such a task in an area that big. The largest island ever cleared of rats, Australia’s Macquarie Island, is just 50 square miles in size. New Zealand is 2,000 times bigger. But, the country has committed to fulfilling its ecological moonshot within three decades.
Beginning as a grassroots movement, Predator-Free 2050 has picked up huge public support and official government backing. Former Minister for Conservation Maggie Barry once described the initiative as “the most important conservation project in the history of our country.” If it works, Zealandia’s fence would be irrelevant; the entire nation would be a song-filled sanctuary where kiwis trundle unthreatened and kakapos once again boom through the night.
By coincidence, the rise of the Predator-Free 2050 conceit took place alongside the birth of a tool that could help make it a reality—CRISPR, the revolutionary technique that allows scientists to edit genes with precision and ease. In its raw power, some conservationists see a way of achieving impossible-sounding feats like exterminating an island’s rats by spreading genes through the wild population that make it difficult for the animals to reproduce. Think Children of Men, but for rats. Other scientists, including at least one gene-editing pioneer, see the potential for ecological catastrophe, beginning in an island nation with good intentions but eventually enveloping the globe.
In 2007, a retiree named Les Kelly returned to New Zealand after 25 years of working in Australia, and marked his homecoming with a four-month walking tour. And during that time, he realized that something had gone horribly wrong. The birds he remembered from his youth were gone. Learning that introduced pests were responsible, he conceived a bold plan to purge them and championed it through a self-created lobby group called Predator-Free New Zealand. Word got around, and in 2011, a regionally famous physicist named Paul Callaghan mentioned the idea in a rousing speech at Zealandia. “It can be done,” he said. “It’s crazy but it’s ambitious, and I think it might be worth a shot. I think it’s our great challenge.”
Callaghan died a few months later, but those words, delivered by a well-liked celebrity, kept gathering momentum. They certainly lit a fire in James Russell, a young ecologist who was born and raised in New Zealand. “I grew up in suburban Auckland with kakariki—these really rare parakeets that my mother raised,” he tells me. “Now, rats kill most of them, and it breaks my heart.” In 2015, he and three colleagues wrote a paper in which they laid out the benefits of eradicating pests nationwide, and estimated that a 50-year scheme would cost 9 billion NZD ($6 billion).
From there, the idea became a movement. “It stopped being aspirational,” Russell says. The government got on board, setting up a limited company to administer an initial $28 million NZD worth of funds. The public embraced the idea, too. People who had been individually trying to control invasive predators on their own land found common cause behind a unifying theme.
There are, of course, naysayers. Some accuse the initiative of ecological xenophobia, unfairly persecuting creatures that didn’t hail from New Zealand but sure as hell are part of it now. But Russell notes that these displaced predators are still wreaking havoc. “Something is going to die,” he says. “Either a bird is going to be killed by a rat that we brought here, or we’re going to kill the rat. And I would rather humanely kill the rat than have the rat inhumanely kill a bird.”
Other skeptics say that the task is simply too huge. So far, conservationists have successfully eradicated mammals from 100 small islands, but these represent just 10 percent of the offshore area, and just 0.2 percent of the far larger mainland. It’s one thing to cull pests on small, waterlocked pimples of land whose forests are almost entirely owned by the government. It’s quite another to repeat the feat in continuous stretches of land, dotted by cities and private homes.
But Russell, ever the optimist, notes that the daunting ascent ahead shouldn’t distract people from the path already climbed. In 1963, after decades of unsuccessfully trying to save birds from invasive predators, the legendary conservationist Don Merton finally divested a tiny island of its rats, by poisoning them by hand. In later decades, when the Department of Conservation started dropping poisoned bait by helicopter, larger islands became rat-free. Heavily visited islands just off the coast of Auckland were cleared. The mainland is a much bigger challenge but one that could be tackled gradually, by creating large sanctuaries like Zealandia and slowly expanding them. “This is a 2050 aspiration,” says Russell. “It’s not going to be solved in 3 to 5 years.”
“It has become less about technical feasibility but about cost,” he adds. “We could just use the tech today but it would be infinitely expensive. We need new control techniques that would be orders of magnitude cheaper. And that’s when we get into questions about CRISPR.”
In 2014, Kevin Esvelt, a biologist at MIT, drew a Venn diagram that troubles him to this day. In it, he and his colleagues laid out several possible uses for gene drives—a nascent technology for spreading designer genes through groups of wild animals. Typically, a given gene has a 50-50 chance of being passed to the next generation. But gene drives turn that coin toss into a guarantee, allowing traits to zoom through populations in just a few generations. There are a few natural examples, but with CRISPR, scientists can deliberately engineer such drives.
Suppose you have a population of rats, roughly half of which are brown, and the other half white. Now, imagine there is a gene that affects each rat's color. It comes in two forms, one leading to brown fur, and the other leading to white fur. A male with two brown copies mates with a female with two white copies, and all their offspring inherit one of each. Those offspring breed themselves, and the brown and white genes continue cascading through the generations in a 50-50 split. This is the usual story of inheritance. But you can subvert it with CRISPR, by programming the brown gene to cut its counterpart and replace it with another copy of itself. Now, the rats’ children are all brown-furred, as are their grandchildren, and soon the whole population is brown.
Forget fur. The same technique could spread an antimalarial gene through a mosquito population, or drought-resistance through crop plants. The applications are vast, but so are the risks. In theory, gene drives spread so quickly and relentlessly that they could rewrite an entire wild population, and once released, they would be hard to contain. If the concept of modifying the genes of organisms is already distasteful to some, gene drives magnify that distaste across national, continental, and perhaps even global scales.
Esvelt understood that from the beginning. In an early paper discussing gene drives, he and his colleagues discussed the risks, and suggested several safeguards. But they also included a pretty Venn diagram that outlined several possible applications, including using gene drives to control invasive species—like rats. That was exactly the kind of innovation that New Zealand was after. You could spread a gene that messes with the rodent’s fertility, or that biases them toward one sex or the other. Without need for poisons or traps, their population would eventually crash.
Please don’t do it, says Esvelt. “It was profoundly wrong of me to even suggest it, because I badly misled many conservationists who are desperately in need of hope. It was an embarrassing mistake.”
Through mathematical simulations conducted with colleagues at Harvard, he has now shown that gene drives are even more invasive than he expected. Even the weakest CRISPR-based gene drives would thoroughly invade wild populations, if just a few carriers were released. They’re so powerful that Esvelt says they shouldn’t be tested on a small scale. If conservationists tried to eliminate rats on a remote island using gene drives, it would only take a few strongly swimming rodents to spread the drive to the mainland—and beyond. “You cannot simply sequester them and wall them off from the wider world,” Esvelt says. They’ll eventually spread throughout the full range of the species they target. And if that species is the brown rat, you’re talking about the entire planet.
Together with Neil Gemmell from the University of Otago, who is advising Predator-Free 2050, Esvelt has written an opinion piece explicitly asking conservationists to steer clear of standard gene drives. “We want to really drive home—ha ha—that this is a technology that isn’t suitable for the vast majority of potential applications that people imagine for it,” he says. (The only possible exceptions, he says, are eliminating certain diseases like malaria and schistosomiasis, which affect hundreds of millions of lives and have proven hard to control.)
It’s not ready yet, either. Even if gene drives were given a green light today, Gemmell says it would take at least 2 to 3 years to develop carrier animals, another 2 years to test those individuals in a lab, and several years more to set up a small field trial. And these technical hurdles pale in comparison to the political ones. Rats are vermin to many cultures, but they’re also holy to some, and they’re likely to be crucial parts of many ecosystems around the world. Eradicating them is not something that any single nation could do unilaterally. It would have to be a global decision—and that’s unlikely. Consider how much effort it has taken to reach international agreements about climate change—another crisis in which the actions of certain nations have disproportionately reshaped the ecosystems of the entire world. Genetic tools have now become so powerful that they could trigger similar changes, but faster and perhaps more irreversibly.
“In a global society, we can’t act in isolation,” says Gemmell. “Some of these tools we’re thinking about developing will cross international borders. New Zealand is an island nation relatively isolated from everyone else, but what if this was a conversation happening in the United States about eradicating rodents? What if Canadians and Mexicans had a different view? This is something that should be addressed.”
Russell agrees with a precautionary approach but he isn’t ready to dismiss gene drives yet. For a start, he feels that Esvelt’s simulations overestimate the risk that such drives would establish themselves in the wild. Yes, rats are very good at traveling and colonizing new lands, but they’re surprisingly bad at invading places where other rats already exist. “Rats have a strong incumbent advantage,” he says. “You really have to introduce a lot of individuals” for them to successfully invade an already-established population.
Esvelt thinks that people would do exactly that. Gene-drive rats may not be able to swim or stow away in sufficient numbers to occupy new lands, but people could carry them. There is precedent for this: In 1997, farmers illegally smuggled a hemorrhagic virus into New Zealand to control rabbit pests. They could just as easily smuggle gene-drive rats in the other direction, to control the rodents in their own particular corners of the world. “New Zealand has very good biosecurity but it’s mostly focused on stopping things from getting in,” says Gemmell. “I’m not sure we’re that good at stopping things from getting out.”
If gene drives are deployed, it’s not unreasonable to imagine a black market in genetic rodenticide, which is exactly the kind of deliberate malfeasance that Esvelt says scientists rarely anticipate. “We don’t consider everything that will happen when technology gets in touch with reality,” he says.
All of this assumes that genes drives would be used to spread genes that kill or suppress pests outright. Instead, conservationists could use them to spread genes that are tied to particular ecosystems. “Imagine giving all rats in New Zealand a peanut butter allergy, and then we feed them all peanut butter,” Russell says. “Well sure,” Esvelt counters, “but then you’ve just converted all the rats in the world into GMOs without asking other countries.” The same problem remains: How do you keep the modification from spreading beyond New Zealand?
Esvelt is working on a couple of tricks for corralling the awesome power of gene drives. In a basic gene drive, a chosen gene has all the components it needs to spread itself. But you could split those components between several genes that are connected in a daisy chain, so that gene C is driven by gene B, B is driven by A, and A is driven by nothing. If rats with these genes were released into the wild, the A-carriers would initially spread the B and C genes, but would eventually disappear themselves. After they go, B would follow. Ultimately, so would C. These “daisy drives,” as Esvelt calls them, are self-exhausting. They’re designed to run out of steam. If they work, they are tools that countries could justifiably use without involving the entire world.
To be clear, despite the buzz around gene drives in New Zealand’s conservation circles, there are no concrete plans to actually use them. “There is currently no research being conducted in New Zealand to develop gene drives for Predator-Free targets, nor are there any plans for such research in the near future,” says Andrea Byrom, director of New Zealand’s Biological-Heritage National Science Challenge. Indeed, Predator-Free 2050’s research strategy mentions only the most exploratory of steps, such as sequencing the genomes of local rats, talking to international experts like Esvelt, and running mathematical simulations. Genuine research into the drives themselves wouldn’t begin any earlier than 2020, and would depend on “technological hurdles being surmounted, supportive policy, and New Zealand/international appetite to proceed.”
The group has also funded social research looking into how New Zealanders feel about using genetic technologies to control pests. That’s the right order, Byrom says: Work out what people want, and act accordingly. The first results, published this week, showed that 32 percent of the 8,000 people surveyed were comfortable with technologies like gene drives, 18 percent felt that they should never be used, and 50 percent were undecided or wanted strong controls.
Much of this work has been done in consultation with Māori scientists and tribal leaders. But “the conversation happens in pockets, around networks that scientists have,” says Maui Hudson from the University of Waikato, who studies Māori research ethics. That’s good for working out the Māori perspective on gene drives, but not for actually engaging those communities in the debate about the risks. Aroha Te Pareake Mead, a political scientist who has studied indigenous perspectives on biotechnology, agrees that there hasn’t yet been a robust and far-ranging discussion with Māori groups (iwi). “The idea of a predator-free New Zealand is widely endorsed throughout Māoridom,” she says. It fits with the concept of kaitiaki or guardianship—the imperative to protect one’s biological heritage. But the means of achieving that goal are more contentious.
“We’ve had many initiatives over the years that have sought to address environmental concerns, with unintended detrimental consequences,” Mead says. “Māori tend to have a precautionary approach because we’ve already had many cases of wrongdoing for the right reasons. Generally speaking, we are suspicious of any kind of genetic modification.”
Despite those reservations, she enjoyed meeting Esvelt two months ago, when he spoke about daisy drives at a community meeting. “I found him to be refreshing as a scientist,” she says. “He wasn’t defensive and he thought that questioning the risks was essential. That gave the Māori who were present a lot of comfort because we’re used to a very different type of geneticist who comes in, says this is the best thing since sliced bread, and if you question it, you’re ignorant and you don’t know the science. We want to be given a range of tools and to make an informed decision about the best one for the purpose.”
Gene drives are not the only game in town. The people behind Predator-Free 2050 are also working on ways of upgrading tried-and-tested technology. The most commonly used traps, for example, are simple one-use devices that must be manually checked and reset. But some companies have made self-resetting traps that can repeatedly kill dozens of rats with a gas-powered piston to the head, or traps that can spray 100 stoats with toxins before needing to be reset. Others are developing sensors that will tell trappers when their snares have snagged an animal, so they don’t have to laboriously check every one.
These traps are typically baited with food, but food goes off in the field and must be frequently restocked. Ironically, it also becomes less effective in well-protected areas where actual prey are common. But stoats, it turns out, are far more attracted to the scent of ferrets—a fellow species of weasel—than they are to food smells. Scientists are now trying to isolate the chemicals that make Eau de Ferret so enticing, to turn them into a super-lure.
Aerial drops of 1080 poison, which have freed so many islands from predators, will almost certainly be part of any mainland campaign. Its use is controversial: It can harm the playful kea parrot, and the occasional unwary pet dog. But conservationists could deploy poisons more effectively if they had better ways of detecting pests, like footpad sensors that could track a rodent’s footfalls, or cameras whose images are automatically analyzed by artificial intelligence. One team is also trying to develop more specific toxins, by analyzing the genome of possums to find chemicals that will affect them alone.
And Russell believes that for Predator-Free 2050 to succeed, it has to marshal the most effective tool around: human enthusiasm. Thousands of volunteer groups already exist around the country, monitoring for invasive species and setting traps. That kind of fervor has to spread, especially if mammals are to be exiled from cities. Any pockets of resistance or apathy would create strongholds where pests could thrive. “Conservation must be something that happens not just in national parks and the backcountry, but in people’s backyards,” Russell says. “They not only allow it but participate in it.”
Regardless of the technology that Predator-Free 2050 eventually settles on, there’s no question that such measures are needed. Consider the kakapo—New Zealand’s endearing, bumbling, giant, flightless parrot. In the 1960s, people thought it was extinct. Now, after the discovery of a surviving population and three decades of intense work, the population stands at 153.
The adults have been relocated to predator-free islands, but “in terms of large sites that would hold a decent population, we’ve saturated the market,” says Deidre Vercoe, a manager at the Kakapo Recovery program. Her team will have to start releasing the birds into places where stoats and rats are still a threat. If Predator-Free 2050 achieved its goal, they could do so with relaxed smiles rather than gritted teeth. Even if Stewart Island, New Zealand’s third-largest island, could be stripped of predators, “it would be an answer for kakapo for many, many years,” she says.
New Zealand is far from the only country grappling with these issues. Over the last seven centuries, 60 percent of the vertebrates that have disappeared from the planet have disappeared from islands—and in half of those cases, invasive species are the culprits. If Predator-Free 2050 makes the right choices, it can indeed change the world—but not with an unstoppable wave of gene-drive rodents. Instead, it’ll show other nations that islands can be protected, that invasive pests can be eradicated, that vanishing wildlife can be saved—even at scales once thought impossible.
“Even if we don’t get to the finish line, the fact that we ran most of the marathon will be pretty damn impressive,” says Russell.


Updated on November 15 at 12:37 p.m ET
On Monday, 44-year-old Brian Madeux spent three hours hooked up to an IV and made scientific history.
The clear liquid that dripped into his arm set off a chain of events that is supposed to end with the precise insertion of a gene that Madeux has lacked since birth into the DNA of his liver cells. With that, he might be cured of Hunter syndrome, a genetic disorder that causes a range of symptoms including joint stiffness,  breathing problems, and developmental delay. Madeux has had 26 surgeries to deal with it all.
If it works, the change will be permanent. If the gene gets inserted in the wrong place, that will likely be permanent too. Doctors may not know for several months.
The Associated Press broke the news of Madeux’s infusion Wednesday. He is the first patient treated in a clinical trial from Sangamo Therapeutics, which is using microscopic gene-editing tools called zinc-finger nucleases to alter DNA inside the bodies of patients. So invested is Sangamo in this technology that the company is concurrently recruiting patients with hemophilia B and Hurler syndrome for two other clinical trials using zinc-finger nucleases.
Gene therapy, in which a missing gene is inserted into a patient’s DNA, has been around since 1989. What makes Sangamo’s treatment different is precision. Past gene therapies have made use of certain viruses, which insert genes in somewhat random places in the genome—sometimes in an appropriate place, but sometimes in places that can cause cancer, which makes the treatment  risky.
Sangamo’s zinc-finger nucleases are engineered to find a specific stretch of DNA, where a new gene can be safely inserted. “You know exactly where you’re going in the genome. It’s not like using a shotgun hoping you’re hitting a bird. It’s like using a rifle,” says Chester Whitley, a principal investigator on Sangamo’s clinical trial and a pediatrician at the University of Minnesota. Madeux’s infusion contained billions of copies of the gene he is missing as well as zinc-finger nucleases.
Zinc-finger nucleases are also distinct from CRISPR, an even newer gene-editing tool that can also find precise stretches of DNA. CRISPR for gene editing in the body has not reached clinical trials in the U.S. yet.
Sangamo had previously used zinc-finger nucleases to edit immune cells taken from HIV patients. Once those gene-edited cells were injected back into the patient, they were resistant to HIV infection. But now, Sangamo is inserting zinc-finger nucleases directly into the body to do their editing. It’s a new level of risk but also a new level of possible reward.
A Dying Boy Gets a New, Gene-Corrected Skin
Madeux’s treatment will not be able to reverse all of the damage in his body. But it could halt the progression of the disease. It could also eliminate the need for expensive and time-consuming enzyme-replacement therapies, which many Hunter-syndrome patients currently use to manage their disorder.
But what Sangamo really hopes to do in the future is to cure kids. When I spoke to Sangamo’s president Sandy Macrae earlier this year, he told me the first trials have to be in adults because that’s the prudent thing to do. “Once we’re sure the medicine is safe and effective, our intention is to get it as quickly as possible into children,” he said.
So if it is safe and effective—still big ifs—then Madeux’s infusion could kick off a new era for genetic disorders, one where kids never have to suffer their effects in the first place.


There’s a wrinkle in how the United States talks about climate change in 2017, a tension fundamental to the issue’s politics but widely ignored.
On the one hand, Democrats are the party of climate change. Since the 1990s, as public belief in global warming has become strongly polarized, the Democratic Party has emerged as the advocate of more aggressive climate action. The most recent Democratic president made climate policy a centerpiece of his second term, and the party’s national politicians now lament and oppose the undoing of his work. Concern for the climate isn’t just an elite issue, either: Rank-and-file Democrats are more likely to worry about global warming than the median voter.
On the other hand, the Democratic Party does not have a plan to address climate change. This is true at almost every level of the policy-making process: It does not have a consensus bill on the issue waiting in the wings; it does not have a shared vision for what that bill could look like; and it does not have a guiding slogan—like “Medicare for all”—to express how it wants to stop global warming.
Many people in the party know that they want to do something about climate change, but there’s no agreement about what that something may be.
This is not for lack of trying. Democrats have struggled to formulate a post-Obama climate policy because substantive political obstacles stand in their way. They have not yet identified a mechanism that will make a dent in Earth’s costly, irreversible warming while uniting the many factions of their coalition. These problems could keep the party scrambling to face the climate crisis for years to come.
* * *
The roots of this crisis go back to 2009, when Democrats held unified control of the White House and Congress. The end of the last decade was a unique moment in climate politics: Thanks to a string of intense hurricane years, and the unexpected success of Al Gore’s documentary An Inconvenient Truth, public support for addressing climate change through legislation was higher than it had ever been.
Democrats responded with the American Clean Energy and Security Act, widely known as “Waxman-Markey,” after its two sponsors, Congressmen Henry Waxman of California and Edward Markey of Massachusetts. The bill proposed creating a carbon-emissions trading market across the United States. Under its terms, the government would have distributed a number of “right to emit carbon” credits to companies, which they could then have bought and sold to each other. As the years passed, the government would allot fewer credits, forcing the price of emitting carbon to increase, which would—in theory—ultimately decrease the amount of carbon dioxide released into the atmosphere.
Though more than a little technical, a pollution market was a proven idea in U.S. environmental law: George H.W. Bush established a similar “cap-and-trade” system during his presidency to reduce the pollutants that create acid rain.
In June 2009, Waxman-Markey passed the House. But as that summer wore on, the bill’s prospects floundered. By August, the Tea Party rose to command more media attention, and public opinion turned against Democrats. Senate Majority Leader Harry Reid—focused on passing what would become the Affordable Care Act—declined to take the climate bill to the Senate floor. By the middle of the next summer, Waxman-Markey was effectively dead. Only a few years after it opened, the window to pass climate legislation had already shut.
Even in defeat, Waxman-Markey cost the party dearly. More than two dozen congressional Democrats who had supported the cap-and-trade bill lost in the 2010 midterm election. The casualties included Rick Boucher, a 14-term veteran of Congress whose district included much of southwest Virginia’s coal country. Boucher had negotiated concessions for local coal companies into Waxman-Markey, but this could not save his seat. Ten House Democrats, including Boucher, voted for Waxman-Markey and against the Affordable Care Act. Six of them lost their seats in 2010.
Indeed, Democrats seemed to prevail only when they ran against the climate bill. Joe Manchin, then the Democratic governor of West Virginia, won a special election that year to serve in the Senate, but only after he ran an ad that showed him shooting a pile of paper with a rifle. “I sued EPA, and I’ll take dead aim at the cap-and-trade bill,” he said in the commercial, which received wide media coverage.
So Waxman-Markey failed. And then, freed from a unified campaign, American progressives took climate policy in two different directions.
First, the Obama administration pressed ahead with its plans to use the Clean Air Act to limit carbon dioxide. (This power predated his presidency: In 2007, the Supreme Court told the EPA it must consider regulating greenhouse gases under that law.) This push eventually produced the Clean Power Plan in 2015, a set of rules meant to reduce greenhouse-gas emissions from the power sector by 30 percent from their historic peak.
Though it dominated press coverage of Obama’s climate policy, the Clean Power Plan never actually took effect. In February 2016, the Supreme Court blocked it from gaining the force of law. But the administration got many other rules aimed at drawing down carbon pollution on the books by the end of Obama’s term.
Second, a swath of environmentalists abandoned the hope of passing climate legislation and returned to a more grassroots, project-by-project approach. Instead of reducing greenhouse-gas emissions through law, these activists hoped to deprive oil companies of their social license to operate altogether.
Chief among these new groups was 350.org, led by the author Bill McKibben. 350 opposed the Keystone XL pipeline, a mega-infrastructure project that linked the Canadian tar sands to U.S. export terminals, warning its completion would spell “game over for the climate.”  In 2010, the Sierra Club also began to use environmental litigation and public pressure to shut down hundreds of coal-burning power plants as part of its Beyond Coal campaign.
This climate antagonism was one aspect of the left’s general resurgence during this period. Inspired in part by Naomi Klein’s 2014 book This Changes Everything, many activists came to see climate change as one more symptom of a dehumanized and extractive capitalism.
These two sides even had something of a proxy battle during the 2016 Democratic primary. During that race, Hillary Clinton opposed a carbon tax and endorsed the Obama administration’s ongoing regulatory efforts. Sanders endorsed a carbon tax and called for more aggressive investment in climate mitigation. Unlike Clinton’s policies, Sanders would surely have required a Democratic Congress to enshrine his policies.
In some ways, though, the 2016 primary was an imperfect battleground for climate policy. Sanders did not seek an energy economist’s ideal climate policy. He wedded climate-hawk positions—like his opposition to natural-gas fracking—to the 1970s’ classically green opposition to nuclear power. (Even though his proposed carbon tax would have been the greatest boon to the nuclear-power industry in decades, as nuclear plants emit no greenhouse gases.)
And there likely would have been little substantive policy difference between either candidate’s presidency, at least at first. Had Clinton or Sanders won the election, their EPA would have dutifully defended the Clean Power Plan in court. And their administration would have benefited from one more liberal seat on the Supreme Court to help enshrine more aggressive climate protections into law.
But Trump won. And that brings us to the present.
* * *
On October 10, Scott Pruitt formally repealed the Clean Power Plan, the first phase of a planned disembowelment of Obama’s environmental legacy.
Pruitt is not assured to succeed in this effort. Many environmental lawyers argue that his legal arguments are not particularly well-supported. To push his repeal through, Pruitt had to muck with the EPA’s internal cost-benefit calculations. His EPA changed how it estimates the social harms of carbon dioxide, calculating a number more than 50 times smaller than what the Obama administration used; it also changed how it values the threat that air pollution poses to the American public.
It may all be for naught. Some environmentalists argue that the Clean Power Plan already accomplished its main goal, which was sending an anti-coal price signal to utility managers. Michael Bloomberg, a UN special climate envoy and the former New York mayor, has argued that the falling cost of renewable energy and the availability of cheap, bountiful natural gas will help the United States meet its carbon-reduction goals even without the federal policy.
But Pruitt does not need to win a total victory to succeed. The Clean Power Plan repeal will now be litigated in court, with the EPA lobbying on the side of deregulation. The fight will last years. It may even wind up in front of a Supreme Court more conservative than it is now.
No one knows the future, but it’s not hard to sketch possibilities. Pruitt may very well secure a ruling that effectively keeps the Clean Air Act from ever regulating greenhouse gases. Even if he fails, a future Trump EPA may succeed in permanently limiting the law’s regulatory power.
And when a climate-concerned Democrat next enters the White House, whether in 2020, 2024, or 2028, he or she may find that the Obama administration’s main policy implement for fighting carbon emissions has been dulled into uselessness. Attorneys at some future EPA will, at that point, probably improvise some new way to address climate change in regulation. They will have no other choice. But there will likely be a need, too, for new legislation. What will Democrats do?
* * *
Democrats face at least three major problems in trying to formulate a climate policy.
First, the relationship between environmentalists and labor groups has disintegrated since 2009. Once upon a time, unions widely supported Waxman-Markey. The bill funded assistance for workers put out of work during the transition away from fossil fuels and launched “green-job” retraining programs. It also provided tens of billions in funding for “carbon capture and sequestration,” an experimental technology that would possibly have allowed coal plants to keep running. But when it failed, the pan-Democratic consensus fell apart with it.
Neither of the two climate strategies that have succeeded Waxman-Markey offer particular appeal to organized labor. The Clean Power Plan essentially made nonprofits and already-strapped state budgets (in GOP-governed states) responsible for softening the economic blow of carbon regulation. In large part, they declined.
The antagonistic approach to pipelines and other fossil-fuel projects has also alienated labor. Construction-union workers still spend much more time building pipelines than installing renewables. Divorced from a unified legislative campaign—and the promise of federal funding—the new environmental antagonism can seem to run directly counter to worker interests.
This isn’t necessarily the fault of the antagonists alone: Unionization in the renewable industry is also lagging. Renewable-energy companies are often as anti-union as any other California-based tech firm. In October, for instance, Tesla—which swears it is trying to hire more employees as fast as it can—fired 400 to 700 workers in what many analysts understood as a union-busting move. United Auto Workers has filed a complaint with the National Labor-Relations Board.
Green-energy companies don’t only harbor an anti-regulatory, Silicon Valley–style dislike for organized labor. They also see themselves as fighting a desperate battle on price with oil and gas companies. Every cent the union might add to production cost, they argue, is a cent that disadvantages them against fossil fuels.
The green-labor breakdown is the party’s biggest political obstacle. But its second problem is that Democratic voters still don’t care about climate change very much. Like other Americans, most of the party’s electorate experience it as a “low-intensity” issue. Though a majority of Americans in every state believe in climate change, very few people use climate policy to decide whom to vote for. Even Democrats say that a candidate’s proposed climate policy matters less when making a voting decision than his or her proposed policies about jobs, health care, the economy, education, income inequality, and terrorism.
As it happens, Trump is helping solve this problem: Due to his steady siege on Obama-era climate policy, he seems to be motivating rank-and-file Democrats to address the issue. His withdrawal from the Paris Agreement was much more unpopular than the treaty itself. But it remains unclear whether the passage of climate policy motivates its supporters more than it aggravates and alienates the issue’s “anti-constituency” (such as coal and oil workers).
Finally, dealing with climate change through any policy is just hard. Most of the good news in climate change lately has come from the power sector, where emissions have dropped by 18 percent over the last five years. But electricity production only makes up about 29 percent of total U.S. greenhouse-gas emissions. For the United States to decarbonize—which even the newest and most optimistic projections say must proceed at a historically unprecedented pace—it must also tackle carbon pollution from other sectors.
Those other sectors will be tougher to crack. Most of the emissions from the transportation sector (which itself makes up more than a quarter of U.S. greenhouse-gas pollution) come from cars and trucks. In order to draw those emissions down by 2050, consumers will have to opt in to electric vehicles en masse and service stations will have to erect electric chargers across the country. Analysts say that’s unlikely to occur without large public investment.
And there are few ideas about how to tackle the 21 percent of U.S. emissions that come from the industrial and manufacturing sector.
* * *
These questions are more pressing than they seem. Eighteen months from now, Democratic policy advisers will meet to develop their chosen candidate’s primary policies. If Democrats win unified control of Congress and the White House in, say, 2020, history suggests they will get a sliver of time to commit any kind of new policy to statute before public opinion turns against them. During that window, dozens of issues will compete for law makers’ attention.
In that light, consider how Democrats have treated health care over the past 10 months. While there’s no party-wide consensus, small groups of voters around the country have organized around the issue, calling their senators every day for months. More than a third of Democratic senators have supported a bill to introduce a single national health insurer. And Bernie Sanders, the party’s 2020 frontrunner, has debated Trumpcare (in its many forms) on CNN several times.
In sum, many Democrats have coalesced around a single phrase (if not quite a policy to accompany it) and promised to deliver it the next time they’re in power. And even if some Democrats see “Medicare for All” as a base play—as a slogan more likely to be deployed in Vermont than Colorado—it remains a policy-focused promise about future governance. The party has similar promises for other issues, too: On immigration, it can promise the DREAM Act; on LGBT rights, the Equality Act.
I suspect that many voters (including most rank-and-file Democrats) believe that there’s a similar strategy on climate change. They think there’s some bill waiting in the wings that would address the issue. They trust that Democrats have a legislative plan to resolve climate crisis, and that the party only needs to be granted control of Congress to pass it.
But nothing of a similar scale exists, and some of the Senate’s most vocal Democrats on the issue resist formulating one. Sheldon Whitehouse, who delivers a weekly climate-themed haranguing on the Senate floor, recently told Vox’s Jeff Stein that way to politically solve global warming is to convert Republicans to the cause.
There are only two bills that come close to serving as a flagship bill. The first is the 100 by ’50 Act, released in April by Senators Jeff Merkley of Oregon and Bernie Sanders of Vermont. “100 by ’50” is an ambitious economic-planning package that would require 100 percent of American electricity to come from clean or renewable energy by 2050.
The bill’s release was timed to the People’s Climate March in Washington, D.C., and McKibben attended its unveiling. It represents the triumph of the 350.org wing of the environmental movement, blocking future fossil-fuel investment and directing plenty of funding to help historically at-risk and marginalized communities. But the 100 by ’50 Act debuted to a fizzle and Sanders, its more prominent cosponsor, spends little time discussing it publicly.
The only other bill is Merkley’s the Keep It in the Ground Act, which would prohibit new oil and gas leases on federal lands. Though cosponsored by Kirsten Gillibrand and Elizabeth Warren, it receives little attention beyond Merkley’s own press statements.
Labor sources have also told me that both of these bills would face problems if Democrats tried to run with them. By design, the “100 by ’50” Act includes no economy-wide mechanism to phase down carbon emissions, like a carbon tax or cap-and-trade market. (Many of the more leftist environmentalist leaders reject these arrangement as failed technocratic policies.)
Indeed, most Democrats told me that climate change could only make it to the legislative docket through some other kind of bill. The days of a Waxman-Markey climate-only bill are seen to have passed, but it’s possible that climate could be addressed in a tax or jobs bill.
If Republicans succeed in overhauling the U.S. tax code, then Democrats could slide a carbon tax into a progressive rewrite of the system. Aging Reaganite Republicans proposed just such a carbon tax-and-rebate scheme earlier this year. Such a policy would bump up the cost of gas by several dozen cents per gallon, discouraging carbon pollution (in theory)—but it would also send every American family a check for $1,500 four times a year. In the eyes of its supporters, this would reduce carbon emissions while also testing an anti-poverty universal basic income.
Democrats could also institute a carbon tax that funds renewable-energy development, as Vox’s Dave Roberts has proposed and as some national polling supports. Some economists worry that such a mechanism would be less politically durable at the national level.
Or Democrats could choose another route and pass a “green jobs and infrastructure” law that subsidizes renewable-energy construction across the country. They could underwrite the electrification of steel production and impose an import tariff on Chinese steel. They could also compel renewable companies to respect unionization drives—a feasible goal, since the short-term success of the solar and wind industry depends in large part on Democratic victories.
The party could do any of these things. But a glance around the infrastructure of the Democratic establishment reveals that little of this planning work is actually getting done. There is no consensus about whether a carbon tax is a good idea. There is no ideal policy embraced by Democrats in lieu of a carbon price. There is, as far as I could find, no think tank putting a bill together or thinking through legislative language. I could barely find professional Democrats planning how a future offensive on the issue would look.
Meanwhile, as Kate Aronoff writes at The Intercept, climate change has come to dominate headlines in the past few months like never before. Three historic hurricanes have wreaked havoc across the United States, leaving the worst blackout in American history (which, at writing, is ongoing). Wildfires have destroyed neighborhoods across the West. And 2017 is almost certain to be the second-hottest year ever measured.
Against this background, the Trump administration has waged an assault on environmental policy and science with little precedent in U.S. history. In response to this, the Democrats have admonished Trump, have lamented the downfall of Obama-era policy, and have sworn their allegiance to the Paris Agreement’s goals—but have promised no substantive alternative. Waxman-Markey is dead, and the Clean Power Plan is writhing on the floor. What’s next, Democrats?


Discovering an exoplanet isn’t what it used to be. Since the first detection of a planet around another star in 1995, astronomers have found thousands more, thanks in large part to the Kepler Space Telescope, which—truly an overachiever—has discovered nearly 5,000 potential worlds and verified about half of them. Even the discovery of the most exciting kind of exoplanet—an Earth-sized world orbiting inside a star’s habitable zone—has become routine: Kepler has confirmed the existence of more than 30 of them.
But that doesn’t mean the discoveries are boring.
A rocky planet similar in size and temperature to Earth is orbiting a nearby star called Ross 128, astronomers announced Wednesday. The star is a red dwarf, a type of star that is smaller and cooler than stars like our sun, located just 11 light-years from our solar system.
This makes the planet, which has been named Ross 128 b, the second-closest temperate, Earth-sized planet to our little corner of the universe. The closest is Proxima b, which resides four light-years away in the habitable zone of Proxima Centauri, our nearest star, and was discovered last year.
Ross 128 b was found by ESO’s High-Accuracy Radial-Velocity Planet Searcher (HARPS), an instrument located at the La Silla Observatory in Chile, and the discovery is described in a paper in Astronomy and Astrophysics. Astronomers at HARPS spent 10 years gathering and refining data, which included more than 160 measurements of Ross 128, before they found the signal that turned out to be an exoplanet, said Xavier Bonfils, the paper’s lead author, an astronomer at Institut de Planétologie et d’Astrophysique de Grenoble in France. “To detect such a small signal requires a lot of data,” Bonfils said.
Stars like Ross 128 move ever so slightly because of the gravitational tug of nearby objects. These tiny movements can be seen as changes in a star’s light spectrum; when the star moves toward an observer—in this case, the HARPS instrument—its light looks slightly bluer, and when it moves away, it turns redder. If these shifts occur in regular patterns, that means there’s an exoplanet lurking. Astronomers can analyze this information to determine some of the planet’s properties. Nicola Astudillo-Defru, an astronomer at the University of Geneva and Bonfils’s coauthor, said he reacted with “a big wow” when the data showed a rocky and temperate world. “I feel so excited that I mailed Xavier minutes later with a clear URGENT in the subject,” he said in an email.
Astronomers say Ross 128 b completes an orbit around its star every 9.9 days. They estimate the exoplanet has an equilibrium temperature somewhere between -60 degrees and 20 degrees Celsius (-76 degrees to 68 degrees Fahrenheit).

Astronomers don’t know whether Ross 128 b resides in its star system’s habitable zone, that sweet spot where temperatures are just right for liquid water to pool on the surface of a rocky world. But they’re still hopeful that it might be able to support life. Red dwarfs like Ross 128 are dimmer than sun-like stars, emitting infrared instead of visible light. So while Ross 128 orbits 20 times closer to its star than the Earth does to the sun, the exoplanet receives only 1.38 times more stellar radiation than Earth. Bonfils said Ross 128 is “quieter” than other red dwarfs, like Proxima Centauri, which can unleash flares that bathe orbiting planets in ultraviolet and X-ray radiation.
“Some even think that the atmosphere can erode, can evaporate, due to these activities,” Bonfils said. The quiet nature of Ross 128 may have created a comfortable cosmic environment for life on the exoplanet to take hold, despite its planet’s close proximity to the star.
Ross 128 popped into headlines in July of this year, when astronomers at the Arecibo Observatory in Puerto Rico said they had detected a mysterious radio transmission coming from the star for about 10 minutes in mid-May. Stars can emit electromagnetic radiation in the form of radio waves, but this signal was recorded at a frequency scientists hadn’t seen before in red dwarfs. Arecibo and other telescopes quickly trained their eyes on Ross 128 for more observations, but the transmission never appeared again. The astronomers entertained several possible explanations, including—much to the internet’s delight—extraterrestrials. They eventually said the signal probably came from one or more geostationary satellites. But the discovery of an Earth-sized exoplanet around the star could restart some of that conversation.
“We are considering additional follow-up in light of the new discovery at radio and optical wavelengths,” said Andrew Siemion, the director of the Berkeley SETI Research Center who runs the Breakthrough Listen Initiative, aimed at finding evidence of extraterrestrial civilizations. The Breakthrough Listen team helped Arecibo astronomers observe Ross 128 this spring. “Nearby exoplanets are particularly exciting from a SETI perspective as they permit us to search for and potentially detect much weaker signals than from more distant targets.”
While Ross 128 is, in cosmic terms, just down the street from us, it would take a spaceship traveling at the speed of light 11 years to reach it. Humans, at least the kind Earth has today, will never reach their stellar neighbor. But they can get a closer look. In the next decade, powerful telescopes like ESO’s Extremely Large Telescope will be able to sniff out the atmospheres of distant exoplanets like Ross 128 b for hints of biomarkers like oxygen. This technology will usher in a new era in the search for exoplanets, in which astronomers learn not only that more are out there—and certainly, more are—but discover what they’re truly made of. The detection of life-giving molecules in the atmosphere of a distant world will be a momentous occasion. Perhaps thousands of those discoveries, on many worlds, will follow. Perhaps someday, that will start to become routine, too.


In February 1977, the Alvin—a submersible big enough for three people, provided none of them stretched out their arms—dove to the bottom of the ocean, just north of the Galapagos Islands. The scientists who had crammed themselves into the sub were expecting to find hydrothermal vents—hypothetical sites where superheated water belched forth from the Earth’s interior. And they succeeded. But to their shock, they also discovered life, in extreme and implausible abundance.
At a depth of 2,400 meters, in pitch-black waters that were disconnected from the sun’s energy and compressed by the full pressure of the overlying ocean, the Alvin crew found what they described as a “Garden of Eden.” Clams and shellfish clung to the belching chimneys of rock. Crabs clambered over them and fish swam past. And strangest of all, giant worms encrusted the vents in stony tubes of their own making. Their scarlet gills protruded from these cylinders like tubes of lipstick—beautiful, if eerily so.
That life existed here at all, let alone in such wealth, was extraordinary. The crew, all of whom were geologists, were so unprepared for such a find that they hadn’t brought any biologists with them. And when they returned to the surface, the only substance they had for preserving the specimens they had collected was vodka.
Those vents, and others like it, have changed our understanding of the extreme conditions in which life could thrive. And the giant tube worms led to the discovery of an entirely new way of life—one that relies entirely on microbes, and that allows creatures to eat even though they’re mouthless and gutless. It’s called chemosynthesis, and its story is told in the video below—the third in a series of online films produced by HHMI Tangled Bank Studios, which adapt the stories in my book, I Contain Multitudes.



It’s nearly Thanksgiving, which, for most Americans, marks the one time a year their dinner table is adorned with jewellike cranberries, simmered into a delicious sauce. But hundreds of years ago, cranberry sauce was a mainstay of daily meals all around the United States. How did this acidic, tannic berry, so hard to love in its raw form, become one of the most popular fruits in America, and how did it fall so deeply out of fashion? Meanwhile, as cranberry sauce was relegated to Thanksgiving, cranberry juice became a popular drink—and mixer. But why is the juice so widely believed to combat urinary-tract infections, and does science support that claim? Join us this episode for all that, plus a tour of the cranberry bog of the future.
When the European colonists arrived in North America, they discovered that Native American tribes enjoyed a tart, bright-red berry growing wild in sandy bogs around New England. In fact, tribes across the continent’s north harvested cranberries and ate them in combination with fats, meats, corn, and other berries, in addition to using them for medicine and dye. But the colonists didn’t copy the local Native tradition of pounding cranberries with meat to create a protein-rich power bar called pemmican, says Robert Cox, the author of Massachusetts Cranberry Culture: A History from Bog To Table—in part because the cranberry fit perfectly into their own tradition of cooking tart berries into sauce to accompany meats. Used like this, as a substitute for the British gooseberry or redcurrant, cranberry sauce became so popular that, Cox told Gastropod, “People would joke that if you visited a New England home in the 18th or early 19th century, the tablecloth on the table was held down at each and every corner by big pots of cranberry sauce that were served with anything for breakfast, anything for lunch, anything for dinner.”
Today, however, cranberry sauce has almost entirely disappeared from our culinary vocabulary, aside from Thanksgiving and, in the United Kingdom, Christmas dinner. Instead, most people now consume cranberries in their dried and juiced forms. In part, cranberry juice’s popularity has thrived due to its supposed ability to help either treat a urinary-tract infection (UTI), or prevent future ones from occurring. This belief stretches back to the Native Americans. But while the cranberry is generally thought to be a healthful fruit snack overall, the science on its UTI-fighting powers has been decidedly mixed. Frustrated, Manisha Juthani-Mehta, an associate professor of medicine at Yale University, conducted her own rigorous, standardized, controlled scientific study to determine whether the juice can actually help, and she reveals the results on Gastropod.
As cranberry juice and Craisins have grown in popularity, cranberry growers’ modern bogs have kept pace with the demand; today’s bogs are far more productive than the wild cranberry bogs of the past, as we discovered when we visited A.D. Makepeace, the largest cranberry grower in the world, to check out the future of cranberry growing. New varieties, new harvesting techniques, and new farming technologies mean we’re awash in cranberries, presenting cranberry growers with a new challenge: how to break cranberries out of their monogamous relationship with turkey, and convince us to eat more of this most American berry all year round. Listen in!
This post appears courtesy of Gastropod, a podcast cohosted by Cynthia Graber and Nicola Twilley that looks at food through the lens of science and history.


To find the world’s most sinister examples of mind control, don’t look to science fiction. Instead, go to a tropical country like Brazil, and venture deep into the jungle. Find a leaf that’s hanging almost exactly 25 centimeters above the forest floor, no more and no less. Now look underneath it. If you’re in luck, you might find an ant clinging to the leaf’s central vein, jaws clamped tight for dear life. But this ant’s life is already over. And its body belongs to Ophiocordyceps unilateralis, the zombie-ant fungus.
When the fungus infects a carpenter ant, it grows through the insect’s body, draining it of nutrients and hijacking its mind. Over the course of a week, it compels the ant to leave the safety of its nest and ascend a nearby plant stem. It stops the ant at a height of 25 centimeters—a zone with precisely the right temperature and humidity for the fungus to grow. It forces the ant to permanently lock its mandibles around a leaf. Eventually, it sends a long stalk through the ant’s head, growing into a bulbous capsule full of spores. And because the ant typically climbs a leaf that overhangs its colony’s foraging trails, the fungal spores rain down onto its sisters below, zombifying them in turn.
The fungus’s skill at colonizing ants is surpassed only by its skill at colonizing popular culture. It’s the organism behind the monsters of the video game “The Last of Us” and the zombies of the book The Girl With All the Gifts. It’s also an obsession of one David Hughes, an entomologist at Pennsylvania State University, who has been studying it for years. He wants to know exactly how this puppet master controls its puppets—and his latest experiments suggest that it’s even more ghoulish than it first appears.
Hughes’s student Maridel Fredericksen used a special microscope to julienne infected ants into slices that were just 50 nanometers thick—a thousandth of the width of a human hair. She scanned each slice, compiled the images into a three-dimensional model, and painstakingly annotated which bits were ant and which bits were fungus. It took three months to mark up just one muscle. To speed things up, Hughes teamed up with computer scientist Danny Chen, who trained an artificial intelligence to distinguish ant from fungus.
When the fungus first enters its host, it exists as single cells that float around the ant’s bloodstream, budding off new copies of themselves. But at some point, as Fredericksen’s images show, these single cells start working together. They connect to each other by building short tubes, of a kind that have only ever been seen before in fungi that infects plants. Hooked up in this way, they can communicate and exchange nutrients.
They can also start invading the ant’s muscles, either by penetrating the muscle cells themselves or growing into the spaces between them. The result is what you can see in this video: a red muscle fiber, encircled and drained by a network of interconnected yellow fungal cells. This is something unique to Ophiocordyceps. Hughes’s team found that another parasitic fungus, which fatally infects ants but doesn’t manipulate their minds, also spreads into muscles but doesn’t form tubes between individual cells, and doesn’t wire itself into large networks.
Whenever Hughes or anyone else discusses the zombie-ant fungus, they always talk about it as a single entity, which corrupts and subverts a host. But you could also think of the fungus as a colony, much like the ants it targets. Individual microscopic cells begin life alone but eventually come to cooperate, fusing into a superorganism. Together, these brainless cells can commandeer the brain of a much larger creature.
But surprisingly, they can do that without ever physically touching the brain itself. Hughes’s team found that fungal cells infiltrate the ant’s entire body, including its head, but they leave its brain untouched. There are other parasites that manipulate their hosts without destroying their brains, says Kelly Weinersmith from Rice University. For example, one flatworm forms a carpet-like layer over the brain of the California killifish, leaving the brain intact while forcing the fish to behave erratically and draw the attention of birds—the flatworm’s next host. “But manipulation of ants by Ophiocordyceps is so exquisitely precise that it is perhaps surprising that the fungus doesn't invade the brain of its host,” Weinersmith says.
In retrospect, that makes sense. “If such parasites were merely invading and destroying neuronal tissue, I don’t think the manipulated behaviors that we observe would be as compelling as they are,” says Charissa de Bekker from the University of Central Florida. “Something much more intricate must be going on.” She notes that the fungus secretes a wide range of chemicals that could influence the brain from afar.
So what we have here is a hostile takeover of a uniquely malevolent kind. Enemy forces invading a host’s body and using that body like a walkie-talkie to communicate with each other and influence the brain from afar. Hughes thinks the fungus might also exert more direct control over the ant’s muscles, literally controlling them “as a puppeteer controls as a marionette doll.” Once an infection is underway, he says, the neurons in the ant’s body—the ones that give its brain control over its muscles—start to die. Hughes suspects that the fungus takes over. It effectively cuts the ant’s limbs off from its brain and inserts itself in place, releasing chemicals that force the muscles there to contract. If this is right, then the ant ends its life as a prisoner in its own body. Its brain is still in the driver’s seat, but the fungus has the wheel.


Think of the Earth’s climate system as a pair of dice. You never know exactly how a roll will end. But some outcomes, like rolling a seven, are much more likely than others, like snake eyes. But when we warm the globe, we essentially load the dice to favor extreme outcomes, including some of the most unpleasant weather possible in the United States.
A new study, rapidly conducted in September and published Monday in the Proceedings of the National Academy of Sciences, finds that the dice are increasingly likely to roll some very unpleasant weather indeed. Global warming may not have caused Hurricane Harvey, which ravaged Houston over the course of a week earlier this year, but it made it much more likely.
The study argues that storms that dump more than 20 inches of rain on Texas are about six times more likely now than they were at the end of the 20th century. Hurricane Harvey dropped 20 inches of rain across a swath of the state, and it deluged some parts of Houston with a record-smashing 60 inches of rain.
Harvey-like hurricanes will only get more likely as the century wears on. Between 1981 and 2000, there was about a one-in-100 chance that a hurricane would dump 20 inches of rain on Texas. By the end of this century—from 2081 to 2100, as children today enter old age—there will be a roughly one-in-5.5 chance that such a flooding hurricane would form if global warming continues apace.
“When you’re planning for the future of cities like Houston, it would be unwise to assume that the climate of the future is pretty much similar to what it’s been for the last 100 years or so,” said Kerry Emanuel, the author of the paper and a professor of atmospheric science at the Massachusetts Institute of Technology. “Even if the climate wasn’t changing, records of rainfall are too short, and the quality of them is too low, to really get a handle on flooding risks.”
Global warming will worsen hurricanes for two reasons. First, otherwise identical storms of all types—not just hurricanes—will retain more moisture in a warmer climate. “It’s one of the risks that we’re most confident of,” he told me. “The physics are so elegantly simple: Warm the air and it can hold more water.”
Second, global warming will make storms move slower. What made Hurricane Harvey so damaging was not its intensity but its longevity: The cyclone moved ashore, parked over Houston, and then rotated in place for three days. It became an atmospheric conveyor belt, picking up water from the Gulf of Mexico and raining it out over the Houston metropolitan area. These days of rain caused the city’s devastating inland floods.
In Emanuel’s study, global warming helped slow similar hurricanes down. Though tropical cyclones unleash some of the fiercest winds on Earth at their center, their movement across the surface of the planet—and thus their storm track—is determined by the slower high-altitude winds summoned by distant high- and low-pressure systems. This planetary engine is slowing down as global warming pushes land and ocean temperatures closer together.
“If [general circulation] slows down, then places near the coast will get more rain,” Emanuel said. “But the main reason our technique shows increasing rainfall is that there’s more water in the air.”
Emanuel conducted the study through the use of a computational tool he pioneered: embedding a hurricane and ocean model in a coarse, planetary-scale climate model. To determine the current climate’s propensity for a Harvey-like storm, he used three meteorological models (with the embedded hurricane tool) to simulate thousands of years of late-20th-century weather. Then, he used six different global climate models (each with the same hurricane tool) to simulate thousands more years of late-21st-century weather. By examining the models’ output of rainfall data and storm tracks, he could arrive at more accurate probability risks.
“This analysis provides strong evidence for why climate-change information should be incorporated into any plans for building Houston back stronger,” said Heidi Cullen, the chief scientist and director of the World Weather Attribution program at Climate Central, a nonprofit based in Princeton, New Jersey, in an email.
She said the study was a “very nicely constructed analysis” that “makes it clear that we must act quickly to both adapt to the changes that are already in the pipeline while also reducing our dependence on fossil fuels.”
But it did not fully capture the future risk of flooding rains in the Houston area or Texas, she added. “About one-third of extreme precipitation events on the Gulf Coast are not associated with a hurricane,” she told me, citing a forthcoming Climate Central analysis of Hurricane Harvey. “When you are looking at extremes, it’s good to look at all rain-producing events.”
While this paper was prepared for publication quickly—Emanuel said he rushed it toward completion so it could inform Houston’s rebuilding efforts—it uses some of the most cutting-edge techniques in climate science. Embedding a cyclone model inside a climate model helped a team of researchers conclude last month that New York City will soon see major, hurricane-induced flooding every five years.
And Emanuel’s scholarship helped inform the Climate Science Special Report, released this month by 13 U.S. federal agencies, which predicted that global warming will increase the strength and destructiveness of hurricanes by the end of 21st century.
The paper also examined how likely it is that a Harvey-like storm would hit the Houston metro area specifically. While major flooding cyclones will be once-in-100-year events by the end of this century, they were as rare as once-in-2,000-year events at the end of the last. “By the standards of the average climate during 1981–2000,” the paper says, “Harvey’s rainfall in Houston was ‘biblical’ in the sense that it likely occurred around once since the Old Testament was written.”


Over the last 24 hours, fans of the talk-show host Sean Hannity have taken to destroying Keurig coffee-pod machines.
In what’s being called the “Keurig Smash Challenge,” many people have posted videos online of aggressive destruction. At least one person did so with a golf club.
The challenge is metaphorical, as the coffee makers are small and fragile. And the performances are rather a protest of a protest, unfolding in the wake of Keurig pulling its advertising from Hannity’s show as a result of his tepid coverage of the child-molestation allegations surrounding Alabama judge and senate candidate Roy Moore. Hannity applied a criminal precedent to a political race, arguing that “every single person in this country deserves the presumption of innocence.”
In response to consumer complaints, Keurig wrote Saturday, “We worked with our media partner and Fox News to stop our ad from airing during The Sean Hannity Show,” in a tweet that has since been deleted. (Keurig CEO Bob Gamgort today clarified that “the decision to communicate our short-term media actions on Twitter was done outside of company protocols.”)
The ideological basis of the Keurig Smash Challenge is murky and filled with directionless energy, not unlike coffee. The protesting of Keurig is a small step removed from overtly supporting the reported actions of Moore. What’s more, destroying Keurig machines also clearly aligns these people with a global environmentalist movement. In 2015, the “Kill the K-Cup” campaign took hold among those concerned about the net waste of so many pods. A Canadian advocate encouraged people to publicly abandon the machines.
As I reported at the time, even John Sylvan, the coinventor of the Keurig machine, has regrets about inventing the system because of its wastefulness. He told me then, “I feel bad sometimes that I ever did it.”
Within days of that story’s publication, a wave took hold. Keurig announced a forthcoming machine that would allow for recyclable pods. Several other green-oriented companies entered the pod-coffee market with compostable pods or reusable devices that can be filled with coffee grounds. Still the majority remain in dumps and landfills, and vocal progressive advocates have encouraged people to get rid of their pod-based machines.
A Brewing Problem
The environmental campaign may actually go against Moore’s political agenda. Moore’s campaign site does not give a clear position on energy policy, but he is not evidently aligned with anti-Keurig-level environmentalism. He wrote in a 2009 op-ed, “Not only is there no constitutional authority for Congress to regulate carbon emissions, but the premise of ‘global warming’ and ‘climate change’ upon which such environmental theories are based does not have the support of a scientific consensus.” This was false even at the time.
Now America finds itself in the midst of hordes of angry people with clubs who will soon be going through caffeine withdrawal. They could go to Starbucks, though the chain has also been condemned and boycotted by some isolationist conservatives, since earlier this year it promised to hire 10,000 refugees in response to President Trump’s executive order barring them from the country.
Sometimes it’s difficult to avoid supporting a social cause.
Hannity himself is now well aligned with the “Kill the K-Cup” environmentalist movement, as well as unearned coffee handouts. He tweeted Sunday, reassuring his potentially lethargic followers, “Deplorable friends, I am buying 500 coffee makers tomorrow to give away!!”


In June, several dozen scientists flew to Big Sky, Montana, to discuss the latest in CRISPR research. They had a lot to talk about, given that CRISPR—a tool that allows scientists to cut DNA to disable genes or insert new ones—is currently the hottest topic in biology, mentioned in the same breath as pronouncements like “changing the world” and “curing humanity of disease.”
On the second day in Big Sky, a Japanese researcher named Osamu Nureki got up to play a short movie clip. “I was sitting in the front, and I just heard this gasp from everyone behind me,” says Sam Sternberg, who worked in the CRISPR pioneer Jennifer Doudna’s lab at the University of California, Berkeley. It was, he says, the biggest reaction to data he’s ever seen at a conference.
Nureki’s paper was published in Nature Communications Friday, and by early morning, the video that astonished the room in Big Sky was making the rounds on science Twitter, too. I watched it, still bleary-eyed from sleep, and I jolted awake immediately.
Single-molecule movie of DNA search and cleavage by CRISPR-Cas9. pic.twitter.com/3NQxmbvzJF
The video is grainy, blobular, and dark, but for a molecule-scale movie, it is remarkably clear. You can see CRISPR, in real time, cleaving a strand of DNA in two. There is nothing that surprising in the clip given what scientists already knew, but that is exactly what makes it so astonishing: Scientists had figured out so much about CRISPR without ever seeing it.
It had the satisfying snap of things falling into place—like the first time a telescope sighted Neptune years after a French astronomer had predicted its existence from perturbations in the orbit of a neighboring planet.
CRISPR obviously exists, and it obviously works. Scientists have used the technique to do everything from ridding mice of HIV to beefing up dogs. But the evidence for how it works had always been indirect. Modern biochemistry research is a series of elaborate workarounds to infer the behavior of molecules until now too small to see.
Take, for example, X-ray crystallography, which Nureki had previously used to study the structure of CRISPR-Cas9. (“CRISPR” is the popular shorthand, but Cas9 is the name of the actual enzyme that cuts DNA.) Scientists spend months perfecting the art of coaxing Cas9 to grow into crystals. Then they take these precious crystals to a particle accelerator to shoot X-rays through them, producing a pattern unintelligible to the average person but which experts can measure to infer the structure of Cas9. The end result is a computer model of Cas9, resembling a clump of curly ribbons. Repeat all of this using Cas9 frozen at different stages of editing DNA, and you can after many months or even years get a handful of static, computer-generated snapshots.
Scientists Can Use CRISPR to Store Images and Movies in Bacteria
Compare that to the fluidity and directness of the real-time video. His team used a technique called high-speed atomic-force microscopy, in which a tiny needle moves back and forth probing the shape of Cas9. The needle moves so fast that it produces a movie. “The result is fairly easy to understand,” says Hiroshi Nishimasu, one of Nureki’s collaborators on the paper. “People say, ‘Wow!’ It’s very simple.”
Nishimasu posted the video to Twitter, and that dark, grainy clip has since gotten more than 2,500 likes. The response has been far and beyond that to any other paper he’s published, including ones in the most prestigious journals like Nature and Cell.
In fact, he says, the team did submit the videos to top-tier journals, who rejected it citing a lack of novelty—which, perhaps, is true. Scientists are supposed to be dispassionate creatures of data. But they are also only human, and humans believe their eyes. “I realized that seeing is believing,” says Nishimasu.


Mark Harris says funeral directors talk about it all the time. More and more people are growing tired of traditional funeral services and opting for something a little more creative. “It’s getting more difficult to offer the cookie-cutter send-off,” explains Harris, the author of Grave Matters, which examines how people have started to think, er, outside the box about death.
And so, Harris wasn’t surprised to hear that a new British company is offering to send cremated remains to the stratosphere. High-altitude latex balloons will float to 100,000 feet above the surface of the Earth, where the curvature of the planet appears against the darkness of space, and then release the ashes into the cold, creating a glittering display. “Scatter your loved one’s ashes in space,” Ascension Flights says on its website. “We are all made of stardust.” The stratosphere is not technically space, but for their purposes, it’s close enough.
Ascension Flights, run by funeral directors and a near-space launch firm, will soon offer its high-altitude funerals, with the cheapest package starting at £795, or about $1,040. For more money, customers can choose the launch site and have the scattering photographed and filmed.
Where the Dead Become Diamonds
The near-space funeral is, at first glance, a contrast to “green” burials, which return remains to the soil in biodegradable coffins or urns. In this way, the deceased can meet “the green reaper,” as a Guardian article in 2014 colorfully put it, and contribute to the physical processes of the Earth. Blasting ashes into the stratosphere sure sounds like the opposite of that, but Ascension Flights promises some kind of return to the planet. “As the particles eventually return to Earth, precipitation will form around them, creating raindrops and snowflakes,” its website explains. “Small amounts of nutritious chemicals will stimulate plant growth wherever it lands.”
Harris, who favors going the natural route, said this promise seems considerably less certain than that of green burials, where at least “I wouldn’t have to worry about having my loved one’s ashes raining down from space on some random location like a landfill or a Superfund site or a nuclear power plant,” he said.
Both kinds of memorials are part of the same growing trend in end-of-life affairs, Harris said. People are becoming increasingly interested in how their physical remains, and the remains of their loved ones, will be handled. They want something more personal and more personalized.
These days, people can forgo metal caskets and be buried in bamboo or recycled cardboard instead, or have their remains wrapped in banana leaf, cotton, or wool. A company called Eternal Reefs will fashion an environmentally friendly artificial reef out of cremains—cremated remains—and drop it into the ocean for nearby marine life to populate. Cremains can be pressed into diamonds, incorporated into paint, and ejected as fireworks. The variety of options for the dead reflects the consumer culture of the living, says Phil Olson, a Virginia Tech professor who studies funeral practices, like the home-burial movement. “There are at least seven kinds of Coke, 500 kinds of cigarettes—options, options, options,” Olson said. Consumers want just as many choices in death as in life.
The option to send a loved one’s ashes to actual space has existed for several years already, for a steeper price than Ascension Flights charges. Since 1997, the company Celestis has flown missions into space delivering the cremains of dozens of people, including Star Trek creator Gene Roddenberry. The payload is launched inside a capsule to more than 300,000 feet, beyond the boundary of space, and eventually falls back to Earth.
While the concept of commemorating life’s final frontier in the final frontier may seem incredibly high-tech, the emotion behind it is no different than run-of-the-mill funerals on Earth. Funeral services can be, in the end, more for the benefit of those who are left behind than those who’ve passed away. They are about processing grief, and grief is personal. For some, the thought of sending their loved one’s ashes into the stratosphere is, simply, very fitting, and it’s difficult to pin down the exact reasons why.
Olson points to alkaline hydrolysis as an example of the funeral industry misunderstanding its customers. Providers of alkaline hydrolysis, which reduces bodies to skeletons in a liquid solution, believed the appeal of the process came from its eco-friendliness. They later found that the primary reason people gave for choosing hydrolysis was that they perceived it to be gentler than cremation. “For some reason, people see being dissolved in caustic alkaline as being gentler than being incinerated,” Olson said.
Perhaps having more options to memorialize the dead may ease the grieving process in some way, he said, even if it’s not clear exactly how.
“We can speculate all we want for people’s motivations for doing this, but we could be dead wrong,” Olson said of the high-altitude memorial and, when I laughed in response, quickly realized his choice of words. “Sorry, pardon the pun. I didn’t even notice that.”


The day is finally here. From the northern reaches of New England to the southernmost stretch of the Chesapeake Bay, one of autumn’s most famous performances will take place this weekend.
On Friday or Saturday night, the first hard frost will likely sweep down the coast. Ginkgo trees—known for their fractured, twisted branches and broad, fan-shaped leaves—will react like a surprised burglar and drop all their leaves to the ground at the same time. On city streets lined with ginkgo trees—like my old block in northwest Washington, D.C.— a soft yellow padding will cover everything, erasing the distinction between sidewalk and street. Only the occasional fire hydrant hints at the manmade understory.
Early on Thursday morning, the ginkgo tree in front of James Hall at the University of New Hampshire got an early start on the act, sloughing its leaves to form a small circle of gold. The event is affectionately anticipated by the school’s department of natural resources, which is housed inside the red brick hall. Since 1977, students and faculty in the department have played a guessing game, trying to anticipate when the tree’s leaves will dump. A box in the building’s foyer entreats students to place their guesses.
“I’ve been in the department for 15 years, and I knew this was going on but didn’t pay too much attention,” says Serita Frey, a soil microbiologist and a professor at the university.“It didn’t have anything to do with climate change at first. Everyone just knew that the ginkgo tree dumps its leaves in one day.”
Why the single-day drop? In the autumn, deciduous trees form a scar between their leaves and stems to protect themselves from diseases and winter’s coming chill. Most flowering trees, like oaks and maples, form the scar at different rates, in different parts of the tree, over the course of weeks. Their leaves then fall off individually. But ginkgoes form the scar across all their stems at once. The first hard frost finishes severing every leaf, and they rain to the ground in unison.
A few years ago, Frey became curious about whether there was data documenting the ginkgo-dump day over the years. According to the National Climatic Data Center, fall temperatures in New Hampshire are now more than 2 degrees Fahrenheit warmer than they were in the late 1970s. Did that affect the flagship ginkgo tree? Had someone even kept records about the contest that far back?
For the first few days of her search, she couldn’t locate the record. “And then I found a piece of paper with handwriting on it that some secretary back in 1977 started, and that someone from the department had been adding to every year,” she said. “I put all that information in a spreadsheet, and I’ve been updating that graph every year since.”
The graph revealed that the ginkgo-dump day had been sliding forward over the ensuing decades. Every decade, the ginkgo tree loses its leaves an average of three days later than it had 10 years prior. When the James Hall ginkgo dumped its leaves this Thursday, November 9, it was the second-latest that the tree had ever hit the autumn milestone.
When Did the James Hall Ginkgo Lose Its Leaves?
“It’s our poster tree—our local example of climate change,” she told me. Frey uses the slide in her classes about global warming.
The James Hall ginkgo is not, of course, the only organism subject to the creeping relocation of the seasons. In 2016, the U.S. National Park Service examined when the first leaf or first bloom of spring arrived in 276 of its parks. In three-quarters of parks, spring was arriving earlier than it had in the past; in half of the parks, spring now arrives earlier than it did in 95 percent of the years since 1901.
Nor is the story of the James Hall ginkgo data trove unique. About two decades ago, climate scientists realized that springtime bloom records were some of the longest-running observations of the climate system available. By examining and digitizing old records, they have found:
The most impressive of these seasonal records exists in Japan, where an environmental scientist has compiled the date of the first cherry-blossom bloom in the city of Kyoto, going back to 800 CE. That study found that cherry blossoms in that city now flower earlier than they have in almost 1,000 years.
But all these measurements examine the first (and often advancing) appearance of spring. Frey’s ginkgo-tree record looks at the often-regressing signature of autumn. The ginkgo-leaf rain is also a seasonal symbol that appears across the East Coast: I have seen great-leafed ginkgoes sprout on small farms, on suburban streets, and in tiny downtown Manhattan parks. Urban arborists often plant ginkgoes because they are more resistant to pollutants and pesticides than other trees. And why not? Ginkgoes have persevered in something close to their current form since dinosaurs walked the Earth.
Not that everyone loves the tree’s annual autumnal transformation. Along with their leaves, ginkgoes also dump their big pungent berries, which split and rot on the pavement. A frustrated New Yorker once complained that they reeked of “boiled egg farts.”


Nearly two months after Hurricane Maria struck Puerto Rico, many residents are still without power and struggling to get access to water, food, and basic services. This week, a failed transmission line knocked out what little of the island’s electrical grid had been restored, temporarily leaving thousands of people in the dark once again.
The recovery from the devastating Category 4 hurricane is expected to be long and costly for all of Puerto Rico, including at the Arecibo Observatory, the world’s second-largest radio telescope, which sits in a mountain range in the island’s northwest.
As the storm approached Puerto Rico in September, several researchers remained at Arecibo to keep watch over the 305-meter-wide dish, taking shelter in concrete buildings with food, water, and fuel for generators. The staff made it through unharmed, and when the worst of Hurricane Maria passed, they found the facility had survived, with some damage. The main dish, while intact, lost some of its reflective panels, and most of a 29-meter-long antenna was destroyed. But it wasn’t as bad as they expected, considering Hurricane Maria’s 155-mile-per-hour winds. “It’s a thing to be thankful for,” Joan Schmelz, the observatory’s deputy director, told Science magazine back then.
The observatory was back up and running on generators a week after the hurricane hit. But as of this week, Arecibo still hasn’t resumed normal operations. The observatory is still running on generator power, according to Suraiya Farukhi, a spokesperson for the Universities Space-Research Association (USRA), one of Arecibo’s operators.
Right now, the radio telescope is locked in a single direction, a configuration meant to save energy, Science reports. Maneuvers to point the telescope in different spots would double fuel consumption, and fuel is in high demand on the island.
Arecibo has observed the skies since the 1960s, searching for stars, exoplanets, asteroids, and potential signals from extraterrestrial civilizations. The iconic observatory has served as a research base for, among others, Jill Tarter, the cofounder of the ET-searching SETI Institute; Frank Drake, the creator of the famous equation for estimating the number of potential alien civilizations; and the late Carl Sagan, perhaps the best-known advocate for space exploration. In 1993, astronomers won a Nobel Prize in physics for discovering of a new kind of pulsar—a fast-rotating star that emits beams of light—using the Arecibo dish. The observatory employed the fictional scientist Ellie Arroway in 1997’s Contact, a character inspired by Tarter.
Some worry the expensive repairs necessary to return Arecibo to normal may influence the National Science Foundation’s thinking on the observatory’s future. NSF, which owns Arecibo, has long considered reducing its annual contributions to the facility and even passing off management to another party. The NSF spends $8.3 million on operations each year, with NASA providing $3.6 million, according to Science. The National Science Board, which governs the NSF, is expected to publicly announce plans for Arecibo’s future next week.
One of the research facilities at Arecibo, the Planetary Habitability Laboratory, which is managed by the University of Puerto Rico, said the lab sustained “extensive damage to its walls, floor, computers, materials, and furniture” and is asking for help in rebuilding. “The lab is in the process now of a complete rebuild that might take a few weeks or even months,” a post on the laboratory’s website said Thursday. “Most of the structural damage to the lab should be covered by FEMA, the insurance, and the university. However, about $14,000 of damage to our furniture, computers, and scientific materials might not be covered.”
The scientists who work at Arecibo face their own personal recoveries. USRA said this week that it had shipped from Maryland to Puerto Rico 20 portable generators and donated them to USRA employees who badly need electricity in their homes.
Several employees who hunkered down at Arecibo during the hurricane, tweeting various updates—from the state of the observatory to the kinds of snacks they’d brought with them—continue to share news on social media. Their messages highlight the incredibly slow pace of recovery. “And, lo! Our water supply was restored!” Robert Minchin, head of radio astronomy at Arecibo, tweeted Wednesday, seven weeks after the hurricane arrived. “This morning I have water, power, and broadband at home!” he wrote a day later.


The sound of death can take many forms: the retort of a gun, the screech of tires, the hack of a cough. But for many moths, death sounds like a series of high-pitched squeaks.
Moths are hunted by bats, which track them down by releasing high-frequency calls and analyzing the rebounding echoes. This skill, known as echolocation, allows them to view their world—and their prey—even in total darkness. Bats evolved the ability to echolocate tens of millions of years ago, and in the intervening time, moths have developed their own countermeasures. Some evolved ears, which allow them to eavesdrop on the calls of hunting bats and take evasive action. Others play the bats at their own game, releasing their own ultrasonic clicks to jam the radar of their predators, or to feign the echoes of distant objects.
Bats, in turn, have evolved their own tricks for circumventing the moths’ defenses. Some, for example, use stealth.
Bat calls are too high-pitched for us to hear, and we should all be grateful for that because they’re also some of the loudest sounds produced by any animal on land. If we could hear them, it would be like listening to a passing ambulance, a jackhammer, or a rock concert. But Townsend’s big-eared bat—a North American species with a foot-long wingspan—is an exception. Aaron Corcoran and William Conner from Wake Forest University have found that when it hunts, it does so at a whisper, with very quiet calls that moths can’t hear. It has evolved into a winged ninja—silent and undetectable, until it’s too late.
This discovery helps to settle a question that’s been bugging Corcoran for a while. He and other evolutionary biologists often talk about evolutionary arms races, in which predators and prey evolve ever-more sophisticated measures and countermeasures to outwit each other. But while scientists have documented myriad examples of prey adapting around their predators, there are surprisingly few strong examples of predators doing the reverse. (These examples include the shocking powers of the electric eel, the snakes that have evolved to resist the poisons of newts, and the mouse that turns scorpion venom into a painkiller.)
Partly, that’s because it’s hard to show that predators are adapting to their prey specifically. For example, for decades, scientists have suggested that bats could shift their calls to higher or lower frequencies that moths can’t hear—and indeed, there are bats that do this. But higher-pitched calls give them a sharper view of their surroundings, and lower-pitched ones travel further and give a wider view of the world. It’s possible that such calls evolved to help bats navigate, and were only incidentally useful for subverting the ears of prey.
In 2010, Hannah ter Hofstede from the University of Bristol found a more unambiguous countermeasure. She showed that the barbastelle, a small European bat, is also a whisperer, with echolocation calls that are 10 to 100 times quieter than those of other moth-hunting bats. This seemed like a clear-cut case of an anti-prey adaptation, since there’s really only one advantage to quieter sonar: catching sharp-eared moths.
Ter Hofstede demonstrated this by tethering moths in small arenas, and hooking electrodes to the neurons in their ears that detect bat calls. These neurons typically fire when bats are around 19 meters away, giving the moths plenty of time to react. But those same neurons only detect barbastelles when they are 2 meters away, giving them just half a second to dodge. By then, it’s probably too late.
Probably. In these experiments, the bats never actually got to attack the moths—and Corcoran wanted to know what would happen if they did. He chose moths that are known to jam bat echolocation and tethered them to fishing lines, hanging them in large outdoor arenas that were surveilled by cameras and microphones. Then he waited for bats to approach.
The results were clear. Compared to the long-legged myotis, a similarly sized bat and a fairly typical echolocator, the stealthy Townsend’s big-eared made calls that were 20 to 40 decibels quieter. And as a result, they catch moths on 80 percent of their attacks. “That’s pretty unheard of for a bat attacking well-defended prey,” says Corcoran. “And the moths almost never exhibited their normal defenses. They very rarely do diving maneuvers, and they never made their jamming clicks.” And when the moths did try to dodge, they did so at a third of the distance for the big-eared than the myotis.
But these whispers come at a cost: The big-eared bat can only detect moths at half the range of the louder myotis. This might explain why the former species isn’t very common. “Maybe you’re doubling your chance of capturing a moth, but you’re reducing your ability to find that moth by half,” says Corcoran. “The math may or may not work in your favor.”
There might be other costs, too. Many of the moths that make jamming clicks also do so to advertise the poisons in their bodies, says ter Hofstede. She wonders whether stealthy bats might be more likely to attack toxic prey. “Bats that catch these kinds of moths generally let them drop without eating them, so it is not dangerous to the bats, but it would represent wasted effort,” she says.
For now, it seems that the moths haven’t evolved a response to the stealthy sonar—and that’s probably because it’s not worth it. “At the sites where I work, there’s 15-plus species of bat, and this one is taking advantage of the fact that all the others echolocate very loudly,” says Corcoran. “Being a rare enemy that uses this odd trick, there’s not enough pressure for the prey to evolve a counter. If the moths focused on this one predator, their response would be all off for all the other bats.” In other words, Townsend’s big-eared bat is a hipster ninja: Its stealth only works because it’s the only bat that uses it.


You know the story, or at least some of it. Sixty-five million years ago, a rock about the size of Mount Everest careened out of space and slammed into modern-day Mexico. It opened a Hawaii-sized hole in the crust, launching soot and sulfur high into the atmosphere, blocking out the sun.
Within days, air and water temperatures plummeted around the world. Within weeks, the food chain on both the land and the ocean had collapsed. Within years, the dinosaurs—the rulers of Earth for more than 150 million years—had perished (except, of course, for the birds).
This moment, the Cretaceous-Paleogene mass extinction, is the most recent widespread die-out in our planet’s history. The dinosaurs’ departure opened the gates for a new class of creature—that is, mammals, like us—to inherit the top of food chains around the Earth. And the moment was also, according to a paper published Thursday in Nature Scientific Reports, quite improbable.
When the asteroid slammed into Mexico 65 million years ago, there was only a 13 percent chance that it would trigger a mass extinction, argue Kunio Kaiho, a geoscientist at Tohoku University, and Naga Oshima, a senior researcher at the Japan Meterological Agency.
“The probability of significant global cooling, mass extinction, and the subsequent appearance of mammals was quite low after an asteroid impact on the Earth’s surface,” they write in their paper.
Odds were 87 percent that the asteroid wouldn’t have prompted the evolution of mammals. In other words, the odds were longer that mammals would triumph than that Donald Trump would, on the eve of the 2016 president election (at least according to FiveThirtyEight).
What set these odds? The location of the asteroid’s impact with Earth—and the mechanism that actually killed the dinosaurs in the days after the impact.
Researchers once believed that the Chicxulub asteroid (so named because it struck the modern-day location of Chicxulub, Mexico) was so devastating because it prompted forest fires around the world. Those fires released soot and ash into the high atmosphere, blocking out the sun’s rays and cooling global temperatures. Indeed, all around the world, geologists find the remnants of soot and sulfur at the rock layer that signifies this moment in geological history.
But geologists and climate scientists have questioned whether forest fires alone could have set off a mass global cooling. Two years ago, a team led by Kaiho looked at the molecular structure of the soot. They argued that all that soot came from one origin point, and that its ratio of elements suggested a higher-energy burn than would normally occur in a wildfire.
The asteroid, they said, must have struck a site rich in hydrocarbons, like crude oil, natural gas, or kerogen. Only about 13 percent of the Earth’s surface contains enough subterranean fuel to set off a mass global cooling.
“Hence, if you assume that such an impact occurred at a random place on Earth, there would have been a 87 percent change that there would have been no mass extinction at all,” said Johan Vellekoop, a paleoclimatologist at the University of Leuven in Belgium who was not connected to this paper, in an email.
“It turns out the dinosaurs just happened to be very, very unlucky. This aspect of chance is often neglected in our field, and I think it is really cool that Kaiho and Oshima have tried to put a number on it,” he said.
Kaiho and Oshima are not the only researchers who have looked at how contingent the timing of the asteroid’s impact was on its consequences. Earlier this year, a group of British and American scientists showed that if the asteroid had entered Earth’s atmosphere mere seconds earlier or later, it would have collided with the sea instead of North America. An aquatic impact would have sent much less dust and soot into the sky, dampening its effects.
Vellekoop took issue with how Kaiho and Oshima arrived at their 13 percent number. They used climate models to estimate global temperature decline with the amount of soot in the atmosphere. He said this linked climate change too directly with the mass extinction:
For their calculations, they basically assume that all the extinctions that occurred at the Cretaceous-Paleogene boundary can be ascribed to climate change, and then go on to put a number on it: “Approximately 8–10 degrees Celsius temperature changes causes a mass extinction.” In doing so, they basically dismiss all other detrimental consequences of the Chicxulub impact, such as the blocking of sunlight and surface-water acidification.
The severity of climate change–driven extinctions greatly depend on the rate of change. Does it occur over days, months, years? Also, different species of plants and animals react differently to changes. Many species will already go extinct with a 5-degree change, some others might require much more. Putting actual reliable numbers on this would require a whole new study, probably involving a complex biological modeling component.
But he said their experiment was valid as a “thought experiment,” and as a first-pass estimation of how unlikely the rise of mammals may have been.
Kaiho’s work is not the only work being done on the mass extinction of the dinosaurs. Last year, the evolutionary biologist Manabu Sakamoto argued that dinosaurs began declining 40 million years before the infamous asteroid collision. For tens of millions of years, more species of dinosaurs disappeared than were replaced by new species.
The time scale, by the way, points to the longevity of dino dominance on Earth. Dinosaurs first appeared on this planet about 230 million years ago. They declined for 40 million years before they went extinct, at least in Sakamoto’s telling. But only 65 million years separates us from that asteroid collision. During that interval, a tiny shrewlike creature branched into horses and antelopes, sloths and polar bears, orangutans and blue whales—and the entirety of human history took place.
“Their long decline had made dinosaurs vulnerable to sudden environmental changes, such as those caused by the asteroid impact,” said Sakamoto in an email. “A sudden catastrophe followed by a substantial global cooling may have been too much of a stressor for dinosaurs to recover from.” He said that Kaiho’s paper did not conflict with his team’s hypothesis.
Lucky for us, the odds that an asteroid collision prompts a mass extinction haven’t changed much in the past 50 million years. In an email, Kaiho said there’s still about a 13 percent chance that any asteroid impact would toss up enough soot and ash to cool global temperatures.


The chatty green parrots had a front-row seat to a spine-chilling show. Tethered to a tree branch not far from their cage, another parrot, similar in appearance but of a different species, was armored in a small leather vest. As the green parrots looked on, a man approached the lone parrot with yet another bird leashed to his arm: a red-tailed hawk. The hawk lunged at the parrot in the vest, wrapping its talons around it. The parrot screamed, a sound only made when death is imminent. Satisfied, the man pulled the hawk off.
This simulated attack—don’t worry, the parrot was unscathed thanks to the vest—was a ruse, aimed at Puerto Rican parrots about to be released into the wild for the first time. A critically endangered species found nowhere else in the world, those chatty green birds had never known life outside of captivity. So they were being taught what to look out for when they headed into hawk territory.
“We wanted an experience that would instill in those birds a very real fear and recognition of a red-tailed hawk as a deadly predator,” says Tom White, a wildlife biologist with the U.S. Fish and Wildlife Service’s Puerto Rican Parrot Recovery Program. In 2001, White helped spearhead this fear-based training regimen for parrots before reintroducing them into the island’s forests. The training involves several other phases before the simulated attack, including “flying” a hawk-shaped cutout over the parrots’ aviary, playing recorded hawk calls, and having a live hawk attack their cage.
For endangered species, even a small boost in numbers can help shift the needle away from extinction. But isolating animals for safe breeding comes with a cost. Captive “insurance populations” of endangered species, like the protected group of Puerto Rican Parrots, can lose their survival edge, both behaviorally and genetically, in just a few generations. Once they become easy prey, they’re less likely to survive if released.
This situation has left some conservationists in an ethical bind. So far, wildlife-reintroduction programs have been hesitant to embrace an approach where live predators possibly injure or kill one animal for the sake of teaching many—even if, as in the safety-vested parrot’s case, the animal that’s attacked isn’t endangered. But White’s results seem promising. And researchers behind these programs know that pampering wildlife in captivity won’t help if the species can no longer make it on its home turf.
Earlier this year, on an island thousands of miles from Puerto Rico, another group trying to bring back a critically endangered bird called White for advice. The group, based primarily on Hawaii’s Big Island, had released five Hawaiian crows—raven-like birds with bristly feathers framing dark beaks—from a captive flock into their native forest, a place the species had not been in nearly 15 years. But within a week, three of the crows had died. Two were killed by their native predator, the Hawaiian Hawk, or ‘io.
Having some deaths in a reintroduction program, especially early on, is not unusual, says Alison Greggor, a researcher with San Diego Zoo Global, one of a few institutions collaborating to reintroduce the crows. Nevertheless, this initiative—known as the the ‘Alalā Project, after the crows’ native name—collected the surviving birds back into captivity, and vowed next time would be different. While the group had attempted antipredator training before releasing the crows, they realized it wasn’t rigorous enough. For these ‘alalā, approximately five generations removed from the wild, to have a chance at survival, the species needed to relearn to outwit its old adversary.
White advised his colleagues in Hawaii to show the ‘alalā that hawks mean business, and recommended a similar approach to the training he employs with the Puerto Rican Parrots. If the ‘alalā were going to watch a non-native bird get attacked, “I told them that you don’t need to worry about a vest,” White says. “Just go ahead and have the real experience.”
But the “real experience”—allowing a hawk to actually kill a bird in front of the ‘alalā, as opposed to merely simulating an attack—might not go over well with everyone. Animal-welfare permits and public perception are concerns, and the ‘alalā is revered as a family guardian in Native Hawaiian culture. Besides, the ‘Alalā Project wanted to take a “data-driven approach” to the training and adapt it to the learning style of crows, which are notoriously perceptive, social birds. To narrow down the best techniques, the group performed pilot trials, observing how the crows reacted to multiple scenarios, including one in which a hawk attacked live prey.
When they settled on what would go into the actual training regimen, the ‘Alalā Project tried to incorporate as many of the elements of a real attack as they thought would be effective. In line with many of White’s techniques, a taxidermied ‘io with wings spread wide was rigged to “fly” across the crows’ aviary while they heard its cries, along with ‘alalā alarm calls. A live ‘io appeared just outside their cage, flapping its wings as if swooping toward them. Finally, they saw and heard the consequences of ignoring those signals: the sound of ‘alalā shrieking in distress and the body of a dead American Crow.
Throughout these experiences, the researchers watched for what they considered appropriate reactions from the ‘alalā: calling in alarm, acting vigilant, flying, or making themselves bigger and more threatening, a behavior called “mobbing.” The approached seemed to work. Even though the crows didn’t observe the gruesomeness of an actual kill, Greggor, who oversaw the training, thinks the birds still got a frightening enough picture of a predator. In fact, during the training, one ‘alalā that had been taken back into captivity after the last release started mobbing the hawk outside their cage, which prompted a second round of training to see if other birds picked up this behavior. That time, three other crows joined in.
In the end, the group didn’t feel that a real kill would be “more effective or more reliable” than their staged attack, Greggor says. Their approach, she notes, “doesn’t mean that that’s now the perfect formula forever moving forward, but this was our attempt at taking an evidence-based approach to designing a training scenario that would be most relevant to the birds.”
Katherine Moseby, a wildlife-reintroduction biologist at Australia’s University of New South Wales who isn’t involved with the ‘Alalā Project, was encouraged to hear that Greggor and her colleagues used a live ‘io in the training. But Moseby remains somewhat skeptical of just how much of an effect simulated encounters without actual attacks can have. “Here is a question, what was the first animal that bit you?” she asks. “I bet you still remember it, even though it likely happened when you were young. Real experiences with real animals that truly scare you are likely to have stronger and longer lasting effects.”
White acknowledges that some people might think his recommendation—in which a live bird is attacked or even killed by a live predator—is cruel or drastic. “But in my opinion, it’s more cruel and more irresponsible to release captive-reared animals that have not had the best training possible,” he says. He stands by this view even if not training some of the released animals at the same time would be a more scientific way of discerning what about that training does and doesn’t work. “If we are then going to expose them to the real world, then we need to prepare them for the real world,” he says.
The ‘Alalā Project may soon have a better idea of how much its training has sunk in. Over two occasions this fall, they released 11 total ‘alalā into Pu‘u Maka‘ala Natural Area Reserve on Hawaii’s Big Island. Still, Moseby points out, “most practitioners train all of the released animals, so it is difficult to isolate the effect of training.” Not training a few of them would mean allowing some crows to encounter their first hungry hawk after flying blithely into the forest for the first time. But with less than 130 ‘alalā left on the planet, the species may not be able to afford that.


To understand the lengths Vaquita CPR has gone to rescue vaquitas from the brink of extinction, consider that the team mobilized not just fellow humans but also four U.S. Navy–trained dolphins.
Andrea, Fathom, Katrina, and Splash—chosen for their “gentle nature”—arrived in Mexico in early October. Since then, they have assisted in the delicate task of locating vaquitas, a species of porpoise of which fewer than 30 are left in the world. They are finding vaquitas because the Mexican government-led Vaquita CPR project wants to capture and breed them away from the dangers of the wild.
This is an incredibly risky plan. When they set out, no one had ever raised vaquitas in captivity or successfully mated them or even, to start, captured one alive. “Vaquitas are exceptionally shy,” says Barbara Taylor, a marine-mammal researcher with NOAA, who is also involved in Vaquita CPR. And the 30 or so surviving vaquitas might be the most elusive of all; they are the few who have managed to avoid the fishing nets that have entangled and killed the rest of their species.
In late October, the team managed to catch its first vaquita. That was the good news. The bad news was that the young vaquita quickly became distressed and had to be released. But the team tried again, and on Saturday, Vaquita CPR had caught an adult female. Things again got worse and quickly. The team decided to release the vaquita, but she died—seemingly of cardiac arrest, though it’s too early to say for sure.
The scientists always knew this was a theoretical risk, but here was an actual dead vaquita on their hands. “A devastating setback. There are no words to express how sad I feel,” Andy Read, a marine conservation biologist on the Vaquita CPR project, wrote on Twitter. If the team can’t keep vaquitas alive in captivity, then they can’t breed vaquitas. And if they can’t breed them, then the species will almost certainly die out in the wild. This might just be the end.
What makes it worse is that some of Vaquita CPR’s marine biologists had experienced this before—a fruitless field expedition, the dawning realization that it was too late—and they had vowed to never let it happen again.
* * *
When I asked Taylor to tell me the history of Vaquita CPR, she began with the story of the baiji, a rare white dolphin that lived in China’s Yangtze River. Like the vaquita, it was becoming entangled in fishing nets and dying. Through the ’80s and ’90s, the number of baiji fell precipitously, but lack of survey data obscured the extent of the problem. In 2006, scientists finally convinced a wealthy Swiss man to fund a six-week expedition in search of baiji in the Yangtze River. Taylor was on that expedition. They hoped—and this plan will sound familiar—to gather information so they could later capture baiji and breed them in a reserve before returning them to the river.
But they found nothing. They didn’t see or hear a single baiji. “This species that had been on planet Earth for 30 million years was gone in an eyeblink,” says Taylor. The team concluded the baiji was functionally extinct—the first documented megafauna extinction in half a century. Then the international media started to pay attention. The baiji “only became a story when it was gone,” another scientist on the expedition later observed to The New York Times.
It was too late for the baiji, but there was still hope for the vaquita. “We were very committed to not let that happen to the vaquita,” says Taylor. Within a year of their failed baiji expedition, many of the scientists behind what would become Vaquita CPR published a paper with a call to action, titled “Saving the Vaquita: Immediate Action, Not More Data.” In its opening line, they noted that the likely extinction of the baiji made the vaquita the most endangered cetacean remaining in the world. It was time to act.
At first, the scientists focused on getting nets out of the Gulf of California, the one place where vaquitas live. They lobbied the Mexican government to ban gill nets, which it did in 2015. But the main threat to the vaquitas now are illegal nets used to poach a fish called totoaba, whose swim bladders are a delicacy in China. The gill-net ban wasn’t enough to slow the decline of vaquita. “Once the population dropped below 100, it became evident we needed to consider radical and risky conservation actions,” says Frances Gulland, a senior scientist at the Marine Mammal Center, one of the organizations assisting in Vaquita CPR.
Captive-breeding programs have saved other species from the brink of extinction before. California condors and black-footed ferrets both rebounded from populations in the low double digits. But large marine mammals like the vaquita are much more difficult to keep in captivity. Since the death of a vaquita on this weekend, the team has made no additional attempts to capture the animals and a spokesperson says they will not for the three remaining days of scheduled field operations. It’s unclear what will happen in the future.
How We Almost Lost the Island Fox
“Everyone here recognizes it’s risky,” says Gulland about Vaquita CPR. But they went ahead because the alternative, doing nothing, would mean vaquitas dying one by one in the wild.
Gulland cited other reasons for the risky plan, too. Having Vaquita CPR’s boats in the water, she hoped, would discourage illegal nets. And if conservation is also a fight for public attention, then an audacious plan like Vaquita CPR could get more people to pay attention. “This effort to develop captive breeding has really increased awareness in the community,” says Gullan. At the very least, the vaquita will not be like the baiji: forgotten until it went extinct.


In the interest of cutting costs, airlines have taken away everything that used to be free—so the refrain goes. Gone are the meals that used to be included in the ticket price for flights that verged on mealtime hours. First checked bags started carrying an additional charge, and now sometimes you have to pay for a carry-on.
On planes, I like to joke to whoever is seated next to me that soon we’ll have to pay extra if we want oxygen masks to fall from the ceiling in case of emergency. My seatmates pretend to be listening to their headphones, because I’ve been talking a lot, but I think they get the point.
The one thing that hasn’t been taken away, I continue, is soft-drink service. And with every drink comes a three-inch-by-three-inch cocktail napkin—which, unlike pretty much anything else, the airlines really, really want to give you.
Have you ever tried to decline a cocktail napkin on an airplane?
It seems straightforward. What you do, basically, is ask for a cup of water or coffee or something, then say, “I don’t need a napkin, thank you.” I’ve probably tried a hundred times, but I’ve only succeeded a few. Surely sometimes giving me the napkin is just muscle memory on the part of the very busy flight attendant, which I totally understand. But some flight attendants look at me like I have no idea what I’m talking about, like I’ve just asked to take a drink straight from their bottle of non-dairy creamer, and hand me a napkin. Sometimes it becomes a conversation, where my sanity is called to question. “Are you sure about that? You really should have a little napkin. What if you spill? Here.”
It’s a plane, and I really might spill my drink, but what good would a cocktail napkin be? Cocktail napkins were designed to handle rings of condensation, but are wholly ineffective in spill scenarios. It’s like tossing a sponge into a pond. Yet millions are given out and thrown away every day, and in aggregate their effect on the world isn’t inconsequential.
“It’s really just how we’ve always done it,” said Ivan Noel, the president of a cabin-crew training organization called the In-Flight Institute, which staffs airlines around the world. “It’s probably a holdback from the past. I haven’t put a great deal of thought into it.”
It’s not clear to me anyone has.
“They’re light, they’re inexpensive, and they can also be used as marketing,” he continued, explaining to me that during training, flight attendants are told the logo on the cocktail napkin goes up and toward the passenger. Some low-cost carriers have begun printing actual advertisements on the napkins.
“It does have a utilitarian use, as well,” he said. “Particularly if you’re on a bumpier flight and you’re stuck in your seat, and if you don’t have a napkin on you, I would suggest that it would be uncomfortable to have fluid on your table tray.”
That’s when I would ring the call button and say there is fluid on my tray and ask for a napkin.
“That’s a very good point as well,” he said. “However, let’s say that you just have condensation.”
Commercial airplanes have plastic tray tables, which aren’t damaged by water rings. [Editor’s note: What about damage to a magazine you put on the table? JH: How much condensation do you get in airplanes? I feel like the climate control keeps it minimal. Ed.: Just looking out for magazines.] It takes concerted effort to damage a tray table, in my experience.
“Well let’s say you have a ring on your table tray, which now you’re going to put away, and it’s going to get the inside of the seat back wet,” Noel said. “That could be another way to look at it.”
The other argument in favor of the cocktail napkin is that it prevents the drink from sliding across the tray table. That’s true—assuming you don’t have one of those tray tables with a recessed well that’s made to sort of secure a cup. Like the idea of cupholders on planes is so impossible. But that’s a screed for another time.
If the idea is that the napkin prevents the cup from sliding, the scenario in which the napkin actually prevents a spill is a very rare one. The force of turbulence or a bumped tray would have to be great enough to overcome the friction of the cup-tray interface, but not great enough to overcome the cup-napkin-tray interface.
I don’t know what made me start getting anxious about wasted airplane napkins, but once I started thinking about them, they became really important. Every day, 2,587,000 people fly in and out of U.S. airports alone. I started seeing people as napkins.
Of course, the fuel that’s burned to carry all these planes and people and luggage is a vastly more significant source of environmental influence than billions of wasted napkins, so this could seem like a drop in the bucket. But since the proverbial bucket is actively overflowing at the moment, every drop counts. If every plane stopped carrying boxes of cocktail napkins, the effect on the weight of the aircraft could add up. It could mean less wasted fuel, or at least an improved ability to fly in increasingly hot climates. As The New York Times recently reported, “Hot temperatures cause air density to decrease, reducing lift and forcing airlines to either reduce weight on flights or move departures to cooler hours of the day. Experts say that will most likely pose a long-term economic challenge for airports.” A recent study in Climatic Change found that by the middle of the century, as many as 30 percent of flights will face additional weight restrictions to take off during midday heat.
So could getting rid of cocktail napkins help?
“It would be actually an interesting cost-benefit analysis,” said Noel. “You could be saving some money and space. Though it would be interesting to see how many people would then use the motion-sickness bags for their garbage or their gum—and those bags are not cheap. They’re lined.”
In economics I believe this is referred to as the vomit-bag externality. And of course one must also consider the nuanced sociocultural milieu into which a change like this is introduced.
“I would wonder how the passenger would perceive the loss of a napkin,” said Noel. “Someone who’s not thinking directly about the environmental impact might see that as just one more thing that’s being taken away.”
That’s probably true. Agency is important in a change like this. If people are asked if they need a little napkin, many would probably decline. Those same people might grumble if the expected napkin had simply disappeared.
The nice thing about life is you get to choose whether you want to see stuff like this as one more thing taken away or one fewer thing wasted.
After years of declining napkins, as I was writing this—on a plane—I was drinking coffee and the person next to me bumped my arm. I spilled just the tiniest drop onto my shirt. And I thought, you know what would be perfect right now? So I commandeered his little napkin.


The death of a big star, much more massive than our sun, usually proceeds like this: After millions and millions of years of shiny existence, the star starts to run out of hydrogen. Without this fuel, the star can’t power the nuclear fusion that produces its light. Its core shrinks and heats up, spawning heavier and heavier elements until mostly iron remains. Within a second, the core collapses and sends star material flying in a spectacular light show—a supernova—that fades after several months. The dead star leaves behind a neutron star, a very dense object, or a black hole, the light-gobbling lurkers of the universe.
What a star isn’t supposed to do, however, is stay alive.
A team of international astronomers announced Wednesday the discovery of a supernova that occurred in 2014 and continued to erupt for more than 600 days, making it the longest such explosion ever observed. The light of typical supernovae usually lasts only 100 days, and anything more than 130 days is extremely rare. When astronomers dug into archival data, they found evidence of another supernova in the same location in an image from 1954. The star responsible for both explosions, the researchers say, somehow managed to survive a blast and explode again 60 years later.
The latest supernova was detected in September 2014 by the California Institute of Technology–operated Palomar Transient Factory, which is designed to spot new objects in the sky. The explosion, named iPTF14hls, was classified as an ordinary Type II-P supernova. Astronomers at the Las Cumbres Observatory, a global network of robotic telescopes, were studying the Palomar data in their search for supernovae at their peak. They took note of it, but moved on after they saw the supernova appeared to be fading. Later, in early 2015, Iair Arcavi, an astronomer at Las Cumbres, asked an intern, Andrew Wong, to go through the whole data set again. Wong found something unusual: iPTF14hls, once fading, had flared up. He asked Arcavi if this was normal.
“I said, absolutely not,” Arcavi said. “That’s very strange.”
After all, supernovae are a one-time deal. “A supernova gets bright and then fades,” he said. “It’s not supposed to get bright again.”
Arcavi thought iPTF14hls was more likely to be a nearby twinkling star that was swept up in a survey of supernovae. He and his colleagues decided to run some forensics on the object. They used telescopes to study its spectra, which provide information about its chemical composition, the speed of the material, the age of the blast, and other important properties. The results showed that this wasn’t a healthy, living star, but a supernova. “I was really surprised,” Arcavi said. “The last thing I expected to see was a spectrum of the most typical supernova, the most boring kind of supernova you can get.”
Arcavi directed Las Cumbres telescopes to stare nonstop at the mystery explosion. Every few days, new data came back. “As time went on, it just got more and more mysterious,” he said. The supernova was challenging every known theory about the end of a star’s life that astronomers have.
Over two years, the brightness of iPTF14hls varied over time in such a way that it seemed like it was exploding over and over again. The ejected material appeared to travel at the same speed over time rather than slowing down. The temperature of the explosion remained unchanged, too. One by one, every theoretical model for star death was thrown out, unable to explain all of the explosion’s bizarre attributes. The discovery of the image from 1954 further confounded the researchers.
The astronomers eventually settled on a theoretical model that describes the potential existence of a never-before-observed event with the strange name “pulsational-pair instability supernova.” In this model, very massive stars, as much as 100 times the mass of the sun, become so hot at their cores that they convert energy into matter and antimatter. An explosion follows, ejecting the star’s outer layers but leaving its core intact. More explosions could continue like this for decades before the core finally collapses.
But there are limitations to this explanation. iPTF14hls produced far more energy than the theory predicts it should. Arcavi said every scientist to whom he’s shown the findings is stumped. “And still now, even with the paper being published, we still don’t have any theory or any model that fully explains the observations of it,” he said. It even took some convincing for Nature to publish a paper that didn’t have a concrete answer for such a significant conundrum, he said. But to truly explain the supernova, astronomers around the world need to see the data and come up with a completely new model. Perhaps someone will even discover earlier images of the supernova in their own archival data, Arcavi said.
Sarah Sadavoy, an astronomer at the Smithsonian Astrophysical Observatory at Harvard University who was not involved in the study, called the long-lasting supernova a “strange event.” The biggest mystery in the findings, she said, is the detection of hydrogen at the scene of the cosmic explosion. “Hydrogen is found in the outermost layer of these massive stars, and as such, should be lost during the first burst,” Sarah said. “This particular event is just as powerful as most other supernova of this type, so it doesn’t make sense for it to still have hydrogen.”
But cosmic mysteries, while frustrating, are a good thing. “There are many things that we still don’t understand about the explosion mechanisms of massive stars and the associated element production,” said Anna Frebel, an astrophysicist at the Massachusetts Institute of Technology who studies stars. “Finding this object appears to cause more puzzling ‘problems’ since it’s never been seen before, but it’s those kind of challenges that help astronomers understand more about the death of stars.”
Arcavi said the discovery will force astronomers to reexamine the supernovae they’ve encountered in their observations. “Anytime someone reports a supernova of this type, we have to look at it again and check if it’s not one of these weird ones,” he said. “We don’t know how many of these we might have missed.”
Some researchers are calling the star responsible for iPTF14hls a zombie star, an undead being glowing in the cosmos. Arcavi said he has some mixed feelings about the nickname. The Walking Dead and many other depictions of the undead have shown that zombies, after considerable effort, can be killed. And when they’re dead, they usually stay dead, save for, perhaps, one final gasp when the unlucky human who took it out has turned away. This star, on the hand, won’t stop dying.
The supernova has begun to fade slowly in the last year, however. Astronomers hope that as the ejected material expands, the area may become transparent enough for powerful telescopes to glimpse the heart of the explosion. Hubble, the premier star-observing space telescope, will take a look next month. The future of iPTF14hls is unclear. Arcavi and his colleagues don’t know whether the star survived the explosion.
“It is weird that the current explosion is going on for so long, but that doesn’t necessarily mean that the star is still alive,” Arcavi said. “I wouldn’t bet my car on it either way.”


At the age of 7, Hassan had already seen more than his fair share of hardship. A week after he was born in Syria, a blister appeared on his back. The doctors there diagnosed him with a genetic disorder called epidermolysis bullosa, or EB, which leaves one’s skin extremely fragile and prone to tearing. There was no cure, they said. When Hassan’s family fled Bashar al-Assad’s regime and moved to Germany as refugees, the doctors there said the same thing. Meanwhile, the blisters were getting bigger.
In June 2015, Hassan was admitted to the burn unit of a children’s hospital in Bochum, Germany. By that time, around 60 percent of his epidermis—the top layer of his skin—was gone. His back, flanks, and limbs had become a continuous landscape of open wounds, red and raw. Much of it was badly infected. The pain was excruciating. “Why do I have to live this life?” he asked his father.
Five weeks later, Hassan’s doctors had run out of options, and were planning to start end-of-life care. But after his father asked about experimental treatments, they contacted Michele de Luca, a stem-cell biologist at the University of Modena and Reggio Emilia. Over the past decades, de Luca had been working on a way of giving EB patients fresh skin. He would collect stem cells from their body, edit the faulty genes that were causing their condition, use the corrected cells to grow healthy epidermis, and graft these new layers back onto the patients.
He and his colleague Graziella Pellegrini had tried this once before in 2006, but on a smaller scale. Back then, they successfully treated a 49-year-old woman with a large EB-induced wound on her right leg. Hassan’s condition was much worse—and he was just a child. Still, there was nothing else to try.
In August, De Luca and Pelligrini got the green light to try their technique. In September, they collected a square inch of skin from Hassan’s groin—one of the few parts of his body with intact skin. They isolated stem cells, genetically modified them, and created their gene-corrected skin grafts. In October and November, they transplanted these onto Hassan, replacing around 80 percent of his old skin.
It worked. In February 2016, Hassan was discharged from the hospital. In March, he was back in school. He needs no ointments. His skin is strong. It doesn’t even itch. “He hasn’t developed a single blister,” says de Luca, who shared the details of Hassan’s story with me. “He’s gaining weight. He’s playing sports. He’s got a normal social life.”
EB has been called “the worst disease you’ve never heard of.” In the United States, it affects around one in every 20,000 people, and the many types and subtypes are caused by mutations in at least 18 possible genes. But all of these mutations have one thing in common: They impair the molecules that strengthen skin, making it extremely fragile. For some people, the symptoms are mild, while others are afflicted with massive wounds and blisters. When it comes to junctional EB—one of the most severe types, and the one Hassan had—around 40 percent of people die before adolescence.
“Whenever I speak about EB, I find myself saying: until there is a cure ...” says Kimberly Morel, who directs an EB clinic at Columbia University. “Now it seems there is more hope on the horizon for this terrible disease.”
Hassan’s treatment is “a sea change to the world of EB,” says Brett Kopelan, the executive director of the Dystrophic Epidermolysis Bullosa Research Association of America. Even though it’s only one instance of success, “it’s made such a change in that one subject that I think it’ll be very much welcome by the EB community. You’re looking generally at a desperate patient population. I have a daughter with recessive dystrophic EB. If something like this were to be commercially viable, it’s certainly something we would pursue.”
Hassan’s miraculous recovery mirrors that of two young boys who, in 1983, accidentally set themselves on fire and burned the skin off more than 97 percent of their bodies. Their lives were saved by the physician Howard Green, who had inadvertently discovered a way of regenerating skin so it could be grafted onto burn victims. Green’s technique has since saved countless lives, and fueled the entire fields of stem-cell biology and regenerative medicine.
De Luca studied with Green in his early career, and his procedure is almost identical to the one his mentor developed, except for one crucial part: He added gene therapy to the mix, modifying the stem cells behind the regenerating skin to correct the mutations behind Hassan’s condition.
“It’s a landmark in the field of stem-cell therapy,” says Elaine Fuchs, from The Rockefeller University. It also helps resolve what she calls “a brewing controversy in the field” about the exact nature of the stem cells behind the substitute skin. In some cases, burn victims who are treated with regenerated skin do really well. In others, the new skin proves to be unstable. By analyzing the DNA in Hassan’s new skin, de Luca’s team showed why: Almost all of the new cells are produced by a small but elite group of long-lived stem cells. In the future, scientists like de Luca would need to check for such cells when they grow their grafts.
A second group led by Peter Marinkovich and Jean Tang, both from Stanford University School of Medicine, have also tried to use gene-corrected skin grafts to treat four patients with a different type of EB. They replaced smaller patches of skin with variable degrees of success, but their results were promising enough to start recruiting for a larger clinical trial.
So far, both teams haven’t seen any negative side effects in their patients—a real concern, when it comes to gene therapy. Hassan’s EB was caused by a faulty version of a gene called LAMB3, and de Luca’s team used a virus to insert the normal version of the gene into the DNA of the boy’s stem cells. But if the virus inserts the gene into the wrong place, it could cause new mutations that might ultimately lead to cancer. Fortunately, when de Luca’s team sequenced DNA from Hassan’s new skin, they found that the normal LAMB3 gene had landed in safe zones. There were no signs of any potentially cancerous changes in either Hassan’s new skin cells or those from de Luca’s earlier 2006 patient.
De Luca’s team is now running two separate clinical trials to test their gene-corrected skin grafts on around two-dozen children with EB. His ultimate goal is to develop an effective and standardized procedure that could be carried out during early childhood, to prevent the painful blisters before they happen, rather than restoring lost skin after the fact. “It will take years to get there but it’s clearly doable,” he says. “Maybe this will be the last thing I’ll do in my career.”


Anna Phillips is delighted because I’ve just found her favorite parasite, which she had misplaced a year ago.
We are walking through what, at first glance, could be mistaken for an oddly macabre Italian deli. The shelves around us are full of chaotically arrayed jars, which contain what look like formless bits of meat and coiled balls of pasta. But this is actually part of the largest collection of parasites in the country, and on closer inspection, a bundle of tagliatelle is actually a tapeworm. A tangle of capellini is actually a cluster of nematodes. “You can make a lot of food references,” says Phillips. “I try to avoid that, because it ruins food for people.”
But there’s one jar that Phillips is especially keen to find, and after 20 minutes of sweeping the dimly lit shelves with the light of our phones, I spot it. It seems to contain nothing more than a lump of grisly, amorphous tissue, preserved in yellow liquid. But its label, in just a few terse and unconnected words, tells a rich story.
A century ago, the lump was once part of a zebra that was captured in Mombasa, Kenya, and imported to the United States for Barnum and Bailey's Circus. During its stay, it fell sick, and eventually died in quarantine. Upon dissecting its arteries, parasitologist H. W. Graybill found bloodworms—a common and occasionally fatal parasite of horses. Those worms—and a part of the zebra they fed upon—are now in the jar that Phillips is gushing over. She is fascinated by the creatures, but she also loves the way their stories thread through history. She picks up another jar containing a fish tapeworm that was pulled from a dog in New York in 1922. “That was someone’s pet,” Phillips says. “There are so many stories here.”
Phillips is the curator of the National Parasite Collection, a 125-year-old hoard of bloodsucking, fluid-draining, flesh-eating, mind-controlling, and host-castrating creatures that all make a living by living off other animals. These smaller members of this gruesome menagerie are housed in a room full of slides at the Smithsonian National Museum of Natural History. The bigger ones are kept in a separate building on the outskirts of Washington, D.C., which houses much of the Smithsonian’s collection.
The building’s walkways are lined with taxidermied polar bears, leopards, cougars, anacondas, and other superstars of the animal kingdom. But its real treasure is smaller and less charismatic. Put it this way: The largest museum collection in the world belongs to the Smithsonian Institution; the largest Smithsonian collection belongs to the National Museum of Natural History; the largest collection in that museum belongs to the Department of Invertebrate Zoology; and the largest collection in that department is the National Parasite Collection.
The collection was founded in 1892 by two parasitologists from the U.S. Department of Agriculture—Albert Hassall and Charles Wardell Stiles (who waged a successful eradication campaign against hookworms in the American South.) The duo began with whatever they had personally collected, but they were soon flooded with incoming specimens. Doctors would call them up with worms they had yanked out of patients. The local zoo would offer the chance to conduct necropsies on their dead animals.
At first, the collection focused on parasites that infect people and livestock, but it widened its scope after the 1940s. During World War II, U.S. troops found themselves in places with unfamiliar parasitic diseases, and scientists realized that treating those diseases meant understanding the complicated ways in which parasites travel from one host to another. They started looking at parasites in wildlife, and they began appreciating these creatures as important parts of their respective ecosystems—worthy of study in their own right.
Indeed, parasitism is the norm rather than the exception. This lifestyle has evolved independently at least 200 times in the animal kingdom, and around 40 percent of modern animal species are parasites. Most of them are small, but their numbers are astronomical and their influence enormous. In California, a painstaking census of three estuaries revealed that the local parasites weighed as much as all the fish, and outweighed the birds by up to 9 times. Meanwhile, in Japan, researchers estimate that freshwater trout get 60 percent of their energy from suicidal insects, which are compelled to drown themselves by the mind-controlling horsehair worms inside them. And all around the world, parasitic wasps and flies keep pest insects in check, to the delight of farmers and gardeners.
So the National Parasite Collection isn’t a selection of dismissible oddities—it’s really an assembly of the animal kingdom’s unsung heroes and secret overlords. And it’s perhaps one of the largest such assemblies in the world, rivaled only by those in Geneva and London. It’s hard to say exactly how big the collection is because many of its specimens are stuffed into containers by the hundreds. “When you look at a jar of hookworm larvae, it looks like jelly beans in a jar,” says Phillips, going back on her promise to avoid food comparisons. “We say there are around 20 million parasites, but we could easily say 87 million.”
Over the decades, the collection has grown haphazardly, such that samples gathered for a researcher’s dissertation might be sitting pretty next to a tray of rabbit parasites. In 2014, when the Smithsonian took charge of the collection from the USDA, its curators began reorganizing everything. That work will take another year or two. For now, the chaos is frustrating—you can eyeball a row of shelves with no idea what you’re going to find.
The chaos is also delightful for exactly the same reason. People fear parasites, avoid them, kill them. But here, among these shelves, you get to browse them. There’s a tapeworm that was extracted from a python at D.C.’s National Zoo in 1961. There’s a fish tapeworm that was wrenched from the body of an Antarctic seal in 1948. There’s a whole set of tapeworms that were pulled out of reindeer, and Phillips points to the fifth of them. “Dasher, Dancer, Prancer, Vixen, Comet ... that’s Comet’s tapeworm,” she says. “We don’t mention that on the kid tour.”
Some parasites are preserved with the tissues of their hosts, like the bloodworm that fed off the Barnum and Bailey zebra. We see part of a horse stomach with fly larvae attached to it, and flatworms clinging to a piece of cow gut. “It’s important to know how the parasite is associating with the host,” Phillips says. “But also, it would take a hell of a long time to pick each of these out.” Impatience was clearly a problem for yesterday’s parasitologists, who often shoved entire animals into jars. We see a bird, some crabs, a frog in really bad shape, a rat. “There’s something in it, I assume,” says Phillips. Her colleague Kayce Bell reads the label. “Subcutaneous nematode,” she says. “Those are fun.”
Phillips was always interested in animals, and during her college years, she applied for an internship at the American Museum of Natural History in New York. “There were projects on bees, scorpions, and catfish, and I thought, no one’s going to apply for leeches,” she says. That kicked off a career-long fascination with leeches and other parasitic worms that has taken her to six continents, and led to some memorable discoveries. In 2010, she described a new two-inch-long species of leech that had been found inside the noses of three Peruvian children. She called it Tyrannobdella rex—the tyrant leech king, or T. rex for short.
To the study of parasites, Phillips has often given blood, sweat, and tears—sometimes, very literally. For example, she collects leeches “in the usual way”—by finding the right watery habitat, donning shorts, wading in, and making a lot of movement. The leeches come to her. When one bites, she has to lift her leg very slowly because some of them will drop off if they leave the water too quickly. “This is why I do yoga,” she tells me.
Leeches are exceptional, though. In most cases, to collect a parasite, you need to collect its host. Kayce Bell studies the parasites of mammals, and she’s spent a lot of time trapping chipmunks. After euthanizing the animals, she plucks them of fleas, mites, ticks and lice, and then systematically dissects them, scanning every gastrointestinal organ for tapeworms, flukes, and more. Each rodent is more than just an individual: It’s a world, ripe for exploring. “I can do a chipmunk in about 30 minutes,” she says.
This work matters, because parasites are ecologically vital, and poorly studied. And much like the rest of nature, they’re in danger. As hosts die, so do their parasites—and sometimes, conservationists deliberately usher the latter into oblivion. In one notable case, the same people who saved the Californian condor from extinction also wiped out the harmless condor louse, by delousing the last surviving birds.
Humans can also make things harder for parasites in subtler ways. Many of these creatures have free-living stages, where they travel through the world in search of hosts. On these journeys, they’re vulnerable to the same changes in temperature and rainfall that are affecting the entire planet. Global climate change is also forcing animals to move into new areas; that could be a problem for the parasites that depend on those animals, especially if they have complex life cycles that involve several hosts.
Phillips and a team of like-minded scientists recently used the National Parasite Collection to simulate these changes, by looking at how the whereabouts of parasite groups have shifted over time, and how those ranges relate to climate. They estimated that between 5 and 10 percent of parasite species will go extinct by 2070 because of climate change, as will 30 percent of parasitic worms.
These are conservative figures, born of research that has been heavily skewed toward North America, says Carrie Cizauskas from the University of California, Berkeley, who was involved in the study. “There’s hardly any data across Africa. There’s very little parasite research in global-biodiversity hot spots, which are also likely to be parasite-biodiversity hot spots.” With a more detailed portrait of our parasitic planet, Cizauskas says that scientists can work out which groups are most in need of saving.
Save the Parasites (Seriously)
The very concept of saving parasites is so counterintuitive that Cizauskas and her colleagues found it hard to publish their studies on parasite extinction risk. Reviewers would say, “Why should we conserve these things that everyone thinks are gross?” This attitude seems especially trenchant, but as I’ve written before, history suggests that it can change. Microbes were once seen as germs; now, we’re starting to appreciate how important those in our bodies are for our health. Top predators were once seen as competitors or trophies; now, conservationists talk about protecting or even reintroducing them. Parasites could get the same reputational makeover.
“People ask me if I dreamed about being the curator of the National Parasite Collection when I was five,” Phillips says. “No, not at all. But dreams change over time.”


It’s official. When it comes to climate change, there’s now literally everyone else—and then there’s the United States.
Syria, the last remaining holdout from the Paris Agreement on climate change, announced at a United Nations meeting in Germany Tuesday that it will sign the agreement. The Syrian Arab News Agency, a state-sponsored news outlet, also reported that the country’s legislature voted to accept the agreement last month.
Its declaration means that the United States is the only country in the world that has rejected the treaty and promised to withdraw from it.
If the news isn’t exactly pleasant for the Trump administration, which announced the intent to pull the United States out of the Paris Agreement in June, it’s also something of a poor advertisement for the treaty itself. That Syria—war-torn, war crime–committing Syria—has acceded to the Paris accord does not make an obvious case for the United States doing the same.
At the same time, Syria is committing to Paris now because 195 other countries have already signed on. In Britain, France, Germany, Japan, and South Korea, the Paris Agreement is considered a relatively uncontroversial international achievement.
“With Syria on board, now the entire world is resolutely committed to advancing climate action—all save one country,” said Paula Caballero, a climate-policy specialist at the World Resources Institute. “This should make the Trump administration pause and reflect on their ill-advised announcement about withdrawing from the Paris Agreement.”
“Syria’s participation puts an exclamation point on the fact that the U.S. actions are contrary to the political actions, and the sincerely held beliefs, of every other country on the face of the Earth,” said Michael Oppenheimer, a professor of geosciences at Princeton University and a longtime observer of UN climate negotiations.
The United States is “the only powerful country” that has disavowed the treaty, he said. “And that was the case from the day it withdrew.”
“I find it ironic that the government of Syria would say that it wants to be involved [in the Paris Agreement] and that it cares so much in climate and things like CO2 gases,” said Heather Nauert, a spokesperson for the State Department, at a briefing on Tuesday. “If the government of Syria cared so much about what was put in the air, then it wouldn’t be gassing its own people.”
The People’s Council of Syria may not have made the decision to enter the Paris Agreement in the first place. Syria has been engaged in a horrific civil war since 2011, and the areas under government control are tightly ruled by President Bashar al-Assad. The United Nations has implicated Assad in war crimes, including sarin-gas attacks on Syrian children. Assad’s family has run the country since 1971.
Syria has not yet submitted a plan to reduce its greenhouse-gas emissions, as the Paris Agreement requires. In fact, preparing a plan to emit less carbon pollution is just about the only thing the Paris treaty requires.
“I don’t know what the reasons are that Syria chose to focus on this now. It’s got enough problems and doesn’t want to be seen as an outlier in any other way,” Oppenheimer told me.
Its acceptance of the accord can also be seen as a kind of geopolitical troll. Since Nicaragua signed onto Paris last month, Syria was the only remaining country left out of the Paris process. Nicaragua gets almost all of its energy from renewable sources, and it declined to join the treaty in 2015 because it said the accord did not go far enough.
Some analysts have argued that Syria’s decision to join the Paris treaty shows that the agreement is a weak or toothless document. The Paris Agreement, after all, sets few limits on its signatories. Most importantly, it allows countries to set their own emission-reduction goals, rather than imposing them as part of the treaty text.
The United States has never taken issue with the treaty’s toothlessness. In fact, the Paris accord adopts a “bottom-up” strategy in part because the United States has long insisted on it. In 2007, President George W. Bush’s administration began arguing for a “pledge-and-review” climate treaty, similar to the one adopted in Paris. Half a decade later, the Obama White House and the United Nations used a broader version of that plan as they began work on what became the Paris Agreement.
Trump’s critiques of the agreement have focused on its alleged strength. In June, the president said that his “solemn duty to protect America and its citizens” required him to withdraw from the agreement. He and other officials have insisted, to the contrary, that the document is impossibly strong and that it benefits China and India at the expense of the United States.
“We intend to withdraw from the Paris Agreement as soon as we’re eligible to do so, unless the president—and he’s been very clear about saying this—unless he’s able to identify terms of engagement that he feels are more favorable to American businesses, workers, and taxpayers,” Nauert said.
Under the original terms of the Paris Agreement, the United States can announce a new greenhouse-gas reduction plan at any time, and it can elect to remain in the treaty at any time up to 2020. Trump has so far not used this power. Roughly seven out of 10 Americans want America to remain in the Paris accords, according to a Yale University poll conducted earlier this year.
Syria’s accession to the treaty is the first major news to emerge from this year’s UN climate talks, which are being held in Bonn, Germany, this month. The United States cannot formally withdraw from the Paris Agreement until 2020, so a team from the State Department is attending the talks. But experts say its negotiating power will be undercut by America’s looming withdrawal from the process.
“The United States stepped aside from fighting at the beginning of two world wars,” Oppenheimer said. “Then it found it had to join the fight, and it led the fight, and it saved the world from terrible calamities. The United States is going to have to join this fight, too, eventually, and it will have to lead the world in the direction of solving a terrible calamity.”
“The Paris Accord isn’t strong, but it’s a way forward. The world is better with it than without it,” he continued. “I think we’re going to cut emissions, and we’re going to get away from fossil fuels. It’s just a matter of how much damage we bake in in the meantime.”


With the recent drought in the American West, ranchers may be forced to find more sustainable options. And some researchers are looking to a breed of cattle from the past to find the answers.
Jon and Susie Eickoff are used to working together and going home together. Jon says that over three decades they have only been apart a couple of times.
“Except for a business trip to Houston and a stint that I spent in the hospital for 10 days, I can say that in 30 years those are the only times we have not been together,” said Jon Eickoff.
That stint in the hospital was due to a heart attack Jon suffered while being the CEO of a fast-paced, successful engineering-services company that he managed with Susie. The couple say they decided just years afterward that it was time to leave the “pressure-cooker” career and trade it in for a slower-paced ranching life near Faywood, New Mexico.
“Being out here in a beautiful place with beautiful scenery and around some nice docile animals, it’s a nice place to be,” said Jon Eickoff.
Those animals are criollo cattle.
Susie Eickoff says while trying to decide on a cattle breed, they watched as ranching friends struggled during New Mexico’s recent drought.
“Friends of ours had a large cattle ranch and they were selling off their cattle because there was no grass for the cattle, there was no water for the cattle, it was very, very severe on them,” she said.
The Eickoffs say they came across research being done on criollo cattle at the U.S. Department of Agriculture’s Jornada Experimental Range that showed the breed could adapt to the arid desert environment in southwest New Mexico.
Susie Eickoff says they read up on a lot of the studies being done on the criollo before selecting a breed that has been in the region for hundreds of years.
“This is a sustainable animal. It requires less food, less supplementation, less water, all of those things that the British breeds require more of,” she said.
She says that once a rancher goes through the struggles of drought, it’s very difficult to go through it again.
“They can survive these droughts and you know, it will get us through the tough times,” she said.
Jon Eickoff says that they are lucky to have 141 acres of land with irrigable water.
“Water is a premium in this part of the country,” he said.
How much of a premium? Research published in the peer-reviewed journal Environmental Research Letters found one in 30 wells in the West is probably dry. The study by Stanford University and the University of California, Santa Barbara, looked at data in 17 Western states covering more than 2 million groundwater wells.
In eastern New Mexico, there is concern in Curry and Roosevelt Counties about increased use of the Ogallala Aquifer.
Alex Rinehart, a hydrogeologist with the New Mexico Bureau of Geology and Mineral Resources, says he worries about the lack of recharge to the aquifer.
“Even if the drought reverses and we end up in wet period for a while, it’s not really going to change anything,” said Rinehart.
Rinehart says officials are looking into piping in water to the area as a solution.
As for a possible solution for ranchers facing drought, researchers at the USDA’s Jornada Experimental Range north of Las Cruces continue their work on the criollo.
Alfredo Gonzalez, an animal scientist at the range, drives out to check on a group of criollo cattle on the range in a pasture that he says is around 6,000 acres.
Gonzalez is comparing how this group of criollo compares to a group of Angus and Hereford crossbred to see how the cattle respond to the desert temperatures.
“They seem better adapted to the hot temperatures,” said Gonzalez.
Gonzalez says not only does the criollo thrive in the desert temperatures, but by monitoring the cattle with GPS tracking collars, they found the breed can travel further from water.
“We’ve found that they go days more without water, again, they are responding by going out further and so they distribute a lot better than the Angus. So those are things that the researchers are continuing to collect data on the breed,” said Gonzalez.
That data also includes understanding the criollo’s diet better. He says the breed is helping restore grasslands that have disappeared due to overgrazing in the state.
“So with them using these areas a lot better, a lot more diverse plants, [then] they’re utilizing water in a better manner,” Gonzalez said. “I think this is good for the remediation of our grasslands.”
The Eickoffs say the breed of cattle has improved the grasslands on their ranch.
“Because they’re grazing constantly, they’re spreading seeds for us, they’re fertilizing seeds, even with their small hooves they’re doing very little damage to what’s here,” said Jon Eickoff.
The Eickoffs also say criollos birth more easily—they’ve never had to pull a calf—and the breed seems to be great at avoiding disease. It’s another insurance policy for this three-person ranching operation facing conditions that ranchers in the West continue to deal with.
“I’m not a pessimist, I’m just a disillusioned optimist. It’s really difficult to look into the negative side of things, but if it does go negative, I’m sure going to be glad there is going to be criollos around,” said Jon Eickoff.
This post appears courtesy of High Country News.
This story is part of the State of Change project, produced in partnership with the Solutions Journalism Network.


Every big, ambitious project has to start somewhere, and for U.K. Biobank, it was at an office building south of Manchester, where the project convinced its very first volunteer to pee into a cup and donate a tube of blood in 2006.
U.K. Biobank would go on to recruit 500,000 volunteers for a massive study on the origins of disease. In addition to collecting blood and urine, the study recorded volunteers’ height, weight, blood pressure; tested their cognitive function, bone density, hand-grip strength; scanned their brains, livers, hearts; analyzed their DNA. In breadth and depth, the study is the first of its kind.
Handling all the samples was a logistical challenge. To process thousands of tubes of blood, for example, U.K. Biobank’s lab needed a new robotics system. (This ultimately came from a company that builds machines for packing sausages, not unlike tubes of blood in shape.) Each tube of blood was split into its component parts—red blood cells, white blood cells, plasma—and run through a battery of tests. White blood cells contain DNA, which the project had analyzed, too. When all was said done, U.K. Biobank had assembled one of the largest single genetic data sets ever. It all took a while.
This spring, 11 years after the first volunteer gave up a tube of blood, U.K. Biobank announced it would release its full genetic data set to registered scientists in July. This huge amount of genetic information, combined with the thousands of other characteristics tracked by U.K. Biobank, allows scientists to look for the genetic determinants of virtually any disease. Geneticists marked their calendars. “We heard stories that people who head groups had canceled holidays,” says Jonathan Marchini, a statistical geneticist at the University of Oxford. “Everyone has been waiting for this for so long.”
U.K. Biobank had done data releases before, including an earlier subset of the genetic data set with just over 100,000 people. In the past, research groups using the data wrote up their papers, submitted to journals, waited for peer review, and eventually their papers trickled out to the public. In the last year, however, an increasingly popular website called bioRxiv—pronounced “bio archive”—has changed the game. BioRxiv allows biologists to publish preprints, or preliminary drafts of their papers that have not yet been peer-reviewed.
Preprints based on the latest U.K. Biobank data started to come out almost immediately. Within two weeks, David Howard and Andrew McIntosh, psychiatry researchers at the University of Edinburgh, had posted not one but two preprints, one on genetic variants linked to depression and the other to neuroticism. Their team subsisted on pizza and worked “constantly.”
Others soon followed, and the flood of preprints has continued ever since. Never had genetics research moved so fast.
* * *
Ask scientists what’s so revolutionary about U.K. Biobank and they’ll say it’s big. But they’ll also say this: Nobody gets preferential access.
In the past, research groups that had gone through the trouble and expense of building DNA data sets have hoarded it for themselves, so that they could be the first to mine it for publishable insights. U.K. Biobank, however, is supported by the United Kingdom’s National Health Service. Its data is open to anyone in the world, as long as they are a legitimate researcher and pay a fee commensurate with the amount of data they want to access—a couple thousand dollars for the full genetic data.
When it came to releasing the 500,000-person data set, making sure everyone got the huge file (12 terabytes uncompressed) at the same time was no trivial matter. U.K. Biobank decided to allow registered researchers to start downloading the data weeks before its official July release. The catch: It was encrypted. The decryption keys went out to all research groups simultaneously on the official release date. Nobody got a head start of a few days, or even a few hours. Even Marchini, who helped U.K. Biobank process some of the data, was not allowed to analyze it for his own research purposes until it was available to all.
“The vision for providing the data to any bona fide researcher without preferential access was really a game changer,” says Manny Rivas, a biostatistician at Stanford University. Rivas, who is an assistant professor, noted it is a real boon for junior faculty, who haven’t had years to amass their own data. The availability of a data set as rich and deep as U.K. Biobank democratizes genetics research.
On top of this shared data set, several research groups have now built freely available tools to help other scientists make use of U.K. Biobank’s data. Marchini’s group made a web browser dedicated to parsing genetic and brain data from U.K. Biobank. Albert Tenesa, from the University of Edinburgh, created GeneATLAS, which accounts for family members in the database, the presence of whom usually screw up the math used to find links between genetic variants and disease. Rivas made the Global Biobank Engine, which is essentially a search engine for genes potentially associated with any disease. The Global Biobank Engine, in turn, is partly based on calculations done by Ben Neale, a geneticist at the Broad Institute, who looked at nearly 2,500 traits and disorders and how they corresponded with genetic variants in the U.K. Biobank.
(Unlike U.K. Biobank’s full data, these tools are accessible to anyone with an internet connection, but they show only aggregate data, so study participants should not be individually identifiable.)
In the past, looking at how a single trait corresponded with a set of genetic variants could be a paper in itself. It’s called a genome-wide association study, or GWAS. Neale’s group did 2,500 GWASs in a single day—and he didn’t even bother to write a paper. It’s a blog post on his website. Neale says it didn’t quite feel like a discrete journal article. It’s more a starting point for scientists interested in specific genes or traits. He’s since heard from both pharmaceutical companies and academic researchers using his GWAS data.
Tenesa, who uploaded a preprint describing GeneATLAS on bioRxiv in August, says he has also heard from a couple dozen researchers using the tool. Some have asked him to run calculations for specific traits. This is happening as he’s still working to publish a paper about GeneATLAS in an official journal. It’s the way things are now. “When I get my email from Nature Genetics these days, and they tell you what papers have just been published, I’ve often seen the papers nine months earlier on bioRxiv,” says Marchini.
But is there such a thing as too fast? Jeffrey Barrett, a geneticist at the Sanger Institute, has cautioned against hastily posting preprints based on a quick GWAS. “I understand why,” he says. “It’s the quickest way to get out the stamp that you’ve done this analysis first.” But it’s easy to miss possible artifacts or mistakes in a data set as big and complex as this one. And now that it’s easy to identify genetic variants linked to a disorder, says Barrett, simply enumerating the variants doesn’t add much value. U.K. Biobank has made genetics research easier, but it is also raising the bar.
Huge DNA Databases Reveal the Recent Evolution of Humans
To publish, researchers increasingly will have to tease out how a genetic variant found via GWAS may be causing a disease—perhaps by tracking how it’s expressed in different parts of the body or sequencing the gene. U.K. Biobank did not fully sequence the DNA in its blood samples, which would have been far too expensive; it used a technique called genotyping that spot-checked 820,967 sites in the genome. In March, though, it signed a deal with the pharmaceutical companies Regeneron and GSK to sequence the DNA of everyone in the study. This deal gives the pharma companies exclusive access to the sequences for nine months—a change in access policy. Eventually, the data will be available to the wider research community.
U.K. Biobank is still following its 500,000 volunteers, and will continue to do so for many years as they age. The technology available to scientists will advance over time, too. When planning for the study was going on, says U.K. Biobank’s principle investigator Rory Collins, studying 820,967 genetic markers for 500,000 people seemed unlikely: “No one envisaged that being possible so soon.” A decade on, any scientist in the world can do it.


Climate change is real. It’s caused by greenhouse-gas pollution released by human industrial activity. Its consequences can already be felt across every region and coastline of the United States—and, unless we stop emitting greenhouse gases soon, those consequences will almost certainly get worse.
 Those are the headline findings of the Climate Science Special Report, a sweeping and more than 800-page examination of the evidence. The report was published Friday by four agencies of the U.S. government and academics from across the country.
Their conclusions form the first volume of the new National Climate Assessment, a report on the science and impacts of global warming that Congress requires agencies to complete every four years. A draft version of the second volume, on the human impacts of climate change, was also released Friday.
“This is the most comprehensive assessment of climate science currently available in the world, and it reaffirms what we’ve already known,” said Robert Kopp, one of the lead authors of the report and a professor of climate science at Rutgers University. “If we want to do something like stay under 2 degrees Celsius of warming, the window to do that is closing in the next couple decades.”
The two-degree limit is a rough target used by the United Nations to signal the point where dangerous climate change could begin. The report finds that the world can only continue to emit carbon for roughly another 23 years at current levels before it will have a more than two-thirds chance of going over the limit.
The Climate Science Special Report comes at an auspicious time in the history of global warming and the United States. The report’s conclusions do not deviate wildly from the last 20 years of consensus in climate science. They do not shock anyone who follows the field. And they don’t break new ground: The authors have synthesized the best available papers; they have not conducted new research for this report.  
But simply by affirming the science of climate change, the authors—and the interagency bureaucrats who shepherded the writing of the document—provide a contrast to the actions and statements of political figures in the Trump administration. Scott Pruitt, the director of the Environmental Protection Agency, has cast doubt on the idea, fundamental to climate science, that the amount of carbon dioxide in the atmosphere significantly controls Earth’s climate.
President Trump and Pruitt have dismantled the aggressive policies advanced by President Barack Obama meant to reduce U.S. carbon pollution. In October, Pruitt repealed the Clean Power Plan, which would have reduced greenhouse-gas emissions from the power sector. In June, Trump withdrew the United States from the Paris Agreement—and, with it, pulled back from Obama’s decision to apply the full force of the U.S. foreign-policy apparatus to reducing carbon pollution worldwide.
The assessment is a scientific achievement by itself. The last major synthesis of climate science, as a field, was published in 2013 by the UN Intergovernmental Panel on Climate Change. Much has changed over those four years, including significant updates to how the field understands the interaction between global warming and hurricane strength.
It is also the end result of a colossal amount of work. The National Academy of Sciences and the U.S. Global-Change Research Program both convened expert panels to comb through the report line by line and subject it to meticulous comment and approval. During the writing process, scientists from four federal agencies—including NASA, the National Oceanic and Atmospheric Administration, the Department of Energy, and the Environmental Protection Agency—contributed significant writing and expertise. They were joined by researchers from the U.S. Army Corps of Engineers and 11 academic institutions, including Columbia University, Texas Tech University, and the Naval Postgraduate School. It is not an overstatement to say that a vast swath of the profession of American climate science played a hand in this report.
Its conclusions span the Earth system. Modern climate change is primarily caused by the release of carbon dioxide and other gases into the atmosphere, which prevent the sun’s heat from escaping back into space. Over the past several decades, this heat has accumulated in the atmosphere and oceans. Now, the United States sets many more extreme heat records than extreme cold records; it sees more intense heat waves and weaker, briefer cold snaps.
That heat is shrinking the ice sheets at the planet’s north and south poles and causing the oceans to expand. The report includes forecasts of how much the sea level will rise around the world. The sea level worldwide has risen by about seven or eight inches since 1900, with three of those inches coming in the last 25 years. This will intensify: The world ocean is almost certain to rise one to four feet by the end of this century. And if some of the fastest scenarios for the melting of the Antarctic come to pass, then the sea could rise as much as eight feet.
The report sketches out what that sea-level rise will do to different regions of the United States. The East Coast will likely experience even more of that sea-level rise than the world average. The West Coast will likely experience less—unless Antarctica begins to melt in its entirety, in which case it will also see above-average rise.
It also dives into “potential surprises” that the United States may encounter in a climate-changed world. In that chapter, which previous National Climate Assessments did not include, the authors warn of “compound extremes,” the risk of multiple unusual weather conditions coming to pass at the same time. For instance, scorching heat and a lack of rain during the same summer will intensify a drought (and increase the chance of wildfires) far more than either one would alone. If excess rain falls on waterlogged ground, then the chance of a devastating flood also rises. It’s hard for scientists to predict how the various extremes of climate change will overlap with each other.
The authors caution that current climate models are more likely to underestimate future warming than overestimate it. While climate models have accurately predicted the past few decades of warming, they struggle to describe warmer climates that occurred millions of years ago. Models suggest that these climates should be colder than archaeological and climatic evidence tells us that they were, which means that they may fail to capture how warm Earth can get. There may be tipping points in the climate system—difficult-to-predict points of no return—that researchers may not fully understand.
Environmentalists and some government scientists had worried that the Trump administration would try to suppress the release of the report. But some of the authors, speaking anonymously so as not to distract from the release, said they saw little evidence of political interference during the writing process.
Some references to the Paris Agreement were removed from the final report, as compared to a draft version leaked over the summer, sources indicated. Environmental groups plan to conduct a line-by-line comparison of the final report with the leaked drafts in the coming days.


Leslie Bolen was there for every one of her son Michael’s medical tests. He’d had more than his fair share in his 14 years, but this last one was almost too much for his mother to bear.
That test was on a Saturday night in April 2016. Eight days earlier, Michael had had a seizure at the residential facility where he lived. He had autism, and he’d had seizures almost every week since he was 8 years old, but never one this bad. He stopped breathing, and his heart stopped beating. Medics managed to restart his heart with a shot of adrenaline, but his brain never recovered.
Michael spent the following week on life support. His parents rarely left his side, ducking out of the hospital only three times for clean clothes and to spend time with their then-13-year-old daughter, Rachel. Sitting in a chair beside Michael’s bed, Bolen played back events from the previous Friday in her mind: the frantic call from Michael’s caseworker, the seemingly endless 10-minute drive to the hospital, the look on her husband Chad’s face when he arrived at the hospital.
As part of the final test, doctors pinched Michael’s chest and poured ice water in his ear. Michael showed no response to these normally painful stimuli, confirming that he would never regain consciousness. For some of that time, his mother had prayed in the next room; heartbroken and exhausted, she focused on practicalities: She and her husband had already decided that they would donate his organs, because Rachel had benefited from a corneal transplant as an infant. But what about Michael’s brain?
Bolen Googled “autism brain donation” on her phone and dialed the number on the screen. About 16 hours later, she joined the ranks of more than 350 families that have donated a loved one’s brain to autism research. “Michael died for a reason. I have to believe that, otherwise I can’t go on; maybe this was [the reason],” she says.
These decisions, however painful for families, are invaluable to researchers. Much of autism research has relied on mice, whose behaviors and brains are a far cry from those of people. Studying postmortem brains has revealed the pathological hallmarks of many neurological conditions—including Alzheimer’s disease, Parkinson’s disease, and multiple sclerosis. Postmortem autism studies present unique challenges, however. Autism is much more heterogeneous than most neurological conditions. Researchers don’t yet know of any obvious features that distinguish the brains of people with autism. Whatever is present is likely to be subtle, and to vary from brain to brain. “You can be sure that every case is different,” says Patrick Hof, director of the Seaver Autism Center Tissue Program at the Icahn School of Medicine at Mount Sinai in New York. “If you want to look for neuropathology and find patterns of cell pathology in these brains, you’re going to need numbers.”
Finding brains for autism research has another, thornier complication. Whereas most people with Alzheimer’s or Parkinson’s diseases die in their 70s or 80s, autism is a developmental condition: The brains of children are most pertinent to research, but those who die usually do so unexpectedly, after drowning or having a seizure, for example. Getting enough numbers to definitively tie any feature to autism means reaching out to families in their darkest hour. A brain that sits unpreserved after death rapidly decays: After 24 hours, genetic messages inside the tissue start to fade; after 36 hours, the walls of each cell start to crumble. Depending on where the family is, and their proximity to a medical examiner and a tissue bank, there might not be a minute to spare.
Given all of these difficulties, there have been few brains available for autism research. In 1998, the advocacy group Autism Speaks launched the Autism Tissue Program—an effort to collect and distribute brain tissue for autism research. In 2013, the National Institutes of Health launched its own brain bank, the NIH NeuroBioBank, to collect brains for studying a wide range of conditions, including autism. But the two efforts had different protocols for processing and distributing samples. Neither had a wide reach across the United States, which meant some families were unable to donate even if they wanted to. “A lot of times, we would get brain authorization and not have a research home for it,” says Jason Bridge, vice president of eye and tissue services for Unyts, an organ-procurement organization based in Buffalo, New York.
New initiatives in the United States and Europe aim to boost the number of autism brains by streamlining the donation process. They try to engage families long before death occurs, and they are working with hospitals, organ-procurement organizations, and medical examiners around the country to reach families immediately after the loss of a loved one. “People have to know us and be convinced that this is a good thing to do,” Hof says.
* * *
The call for autism brains grew particularly urgent five years ago. In May 2012, a freezer at Harvard University containing about 50 specimens from the Autism Tissue Program—roughly one-third of the collection—malfunctioned. The freezer had two separate alarm systems, neither of which activated when the temperature began to climb above -80 degrees Celsius. The contents slowly thawed, destroying the cells in the tissues and degrading their scientific value. Scientists realized they needed better procedures to protect and distribute this limited resource.
“My first thought really was, ‘Oh my gosh, this is a tragedy,’” says Eric Courchesne, a professor of neuroscience at the University of California, San Diego. “You can always collect new DNA samples, you can always collect new RNA, you can always do new brain scans,” he says. But a physical brain? “It’s irreplaceable.”
The living brain is guarded by three layers of mesh-like tissue called meninges, the hard bones of the skull, and a thin cushion of fluid in between that helps absorb shocks. But these protections were no match for Michael’s self-injurious urges. He once smashed his head against the windshield of his school bus so hard that the glass fractured into a spiderweb of cracks.
Michael was diagnosed with autism at 21 months. Bolen, who has an older son from a previous relationship, knew something was off when Michael swapped babbling and pointing for spinning and “stimming”—self-stimulating behaviors. He loved to watch liquids seep into the carpet or couch, and so he would sometimes empty jugs of milk or orange juice or urinate right in the living room. Within a year of his diagnosis, he began hurting himself, at first slapping his head and then banging it against the wall or the floor.
Michael’s self-injury worsened as he grew. When he was 6, a neurologist ordered brain scans, which revealed that part of Michael’s brain extended down into his spinal column. This “Chiari malformation” may have been contributing to his dangerous behavior, the neurologist said, and recommended surgery to fix it. Before opting for surgery, the Bolens tried giving Michael risperidone, a drug that can ease aggression and self-injury in children with autism. It helped for a few weeks, but then the effects waned. Boosting the dose restored the drug’s effects temporarily, but within five months they had reached the maximum dose.
About a year after Michael was diagnosed with the Chiari malformation, he underwent a three-and-a-half-hour procedure to remove a small piece of his skull and a piece of a vertebra to ease the pressure on his brain. At the same time, the surgeon repaired a portion of his dura, the outermost layer of the meninges. Dura should be tough, but the surgeon reported that Michael’s, for some reason, had the consistency of wet newspaper.
Even so, the surgery seemed to go well. Within 24 hours, though, Michael developed meningitis, a rare infection of the meninges. He recovered, but 10 weeks after the surgery, he began to have seizures. At first, they were sporadic, but within two years Michael was having a severe seizure every week. He would suddenly stop moving and then slowly begin to circle the room in a trancelike state. His lips would sometimes turn blue, signaling a lack of oxygen. Doctors suggested giving him Diastat (diazepam)—an anticonvulsant drug—if a seizure lasted more than five minutes. Even with the medication on hand, his mother sometimes panicked and called 911.
In June 2014, when Michael was 13 years old, he had his first convulsive seizure. His father was just drying him off after a shower when his 180-pound body went limp. His parents realized they could no longer keep him safe at home. That October, Michael moved into a residential facility in Media, Pennsylvania, a 10-minute drive from their house. Michael had his final seizure 18 months later.
When Michael died, Bolen’s Google search led her to Autism BrainNet, a network of brain banks set up in 2014 to preserve the brains of people with autism for research. (Autism BrainNet is funded by the Simons Foundation, Spectrum’s parent organization.) She called the network’s main number, and the organization worked with a local agency to collect Michael’s brain and ship it to Hof’s lab at Mount Sinai.
* * *
Brain banking is a finely tuned science, aimed at maximizing the research potential of each precious sample. Each brain is first dissected into its two halves, or hemispheres. One hemisphere is then cut into slabs about 1 centimeter thick; the slabs are quickly frozen in liquid nitrogen and stored in labeled bags. These can be kept in a deep freezer for years, if not decades.
The other hemisphere is typically submerged in a pungent formaldehyde bath that hardens the tissue and fixes each cell in place. Some brain banks keep this fixed hemisphere whole. Others cut it into slabs of tissue, also about a centimeter thick. They can store these slabs in tubs of formaldehyde in a walk-in refrigerator or embed them in paraffin wax and maintain them at room temperature. They can later shave the waxed blocks into delicate slices for inspection under a microscope as needed.
Imaging studies can provide clues to when and where brain development is altered in autism, but only postmortem tissue can reveal how development goes awry. “You have to be looking at the individual cells in those brain regions that appear to be developing differently,” says Cynthia Schumann, an associate professor of psychiatry and behavioral sciences at the University of California, Davis, MIND Institute.
One of the first autism studies of postmortem brain tissue, published in 1986, analyzed tissue from four males with the condition, ranging in age from 10 to 22 years. The researchers focused on the cerebellum—a wrinkly lobe at the back of the brain that helps to coordinate movement, among other things—and found a dearth of a type of neuron called Purkinje cells. The results prompted Courchesne to wonder whether the same feature would be visible in brain scans from people with autism. Using magnetic resonance imaging, he confirmed in 2001 that a wormlike structure known as the cerebellar vermis, which is normally rich in Purkinje cells, is smaller in people with autism than in controls.
Courchesne also showed that young boys with autism have an unusually large prefrontal cortex—a brain region involved in attention, decision-making, and social behavior. The scans hinted at an early overgrowth of neurons in the prefrontal cortex. To confirm this, his team manually counted neurons in samples of brain tissue from seven boys with autism and six controls. They discovered in 2011 that the boys with autism had, on average, 67 percent more neurons in the prefrontal cortex. “We were astonished,” Courchesne recalls. Neurons in the prefrontal cortex develop during the first and second trimesters of pregnancy, so the findings provided some of the first evidence that autism originates before birth.
Brain tissue has also shed light on which genes are involved in autism. A 2011 study found, for example, that brains from people with autism show an increased expression of immune genes and diminished expression of genes involved in neuronal connections. A 2014 study with more brains, including some used in the 2011 study, confirmed these findings. The team behind the first study later discovered that people with classic autism share patterns of brain-gene expression with those who have dup15q duplication syndrome—a rare genetic condition associated with autism. In a separate study, they showed that the brains of people with classic autism and those with dup15q syndrome bear a similar pattern of chemical tags that affect gene expression.
Together, the studies hint at a common molecular signature across multiple forms of autism, regardless of the cause or clinical features. “I don’t think we would have been able to find that result without looking at gene expression in autistic brains,” says Dan Arking, an associate professor of medicine at Johns Hopkins University in Baltimore, who led the 2014 study.
* * *
The tissue for Arking’s analysis came from two brain banks: one in Maryland that is now part of the NeuroBioBank, and the one at Harvard University. Arking says he could tell the Harvard tissue was of poorer quality, thanks to quality-control steps he included in his analysis. “We could, based on our data, go back and identify which brain bank the samples came from,” Arking says. He suspects the Harvard freezer may have been warming for months, explaining the low quality of some of the specimens they had received.
Two years after the freezer malfunction, in 2014, several nonprofit organizations came together to form Autism BrainNet. The idea had been in the works before the malfunction but quickly gained momentum, says David Amaral, the director of Autism BrainNet and a distinguished professor of psychiatry at the MIND Institute. (Autism BrainNet was jointly funded by the Simons Foundation and Autism Speaks initially, but the Simons Foundation became its sole funder in 2016. Another nonprofit organization, the Autism Science Foundation, coordinates outreach.)
Autism BrainNet has 164 autism brains from the Autism Tissue Program. As of October, it has collected an additional 41 autism brains, 56 control brains, and 13 brains from people with other genetic conditions, including dup15q syndrome. It has five collection “nodes”—teams trained to process brain tissue—that partner with local hospitals, organ-procurement programs, and medical examiners, who each raise the possibility of brain donation with grieving families. To reach families earlier, the program runs a campaign called “It Takes Brains.” Families can register regardless of whether any members have an autism diagnosis because having control brains is important for research. Autism BrainNet also works closely with advocacy groups for related syndromes.
“We’re all hoping to find a common cell-system defect,” says Carol Tamminga, the chair of psychiatry at the University of Texas Southwestern in Dallas and director of the Texas node of Autism BrainNet. Finding that common alteration could point to treatments for multiple types of autism, which is most families’ primary motivation for donating brains. “They mourn their family member’s passing, for sure, but they want to think that that life will also be useful in helping other people,” she says.
Jeanine and Bill Rossbach heard about Autism BrainNet through the Dup15q Alliance. Their son, Michael, began having seizures at 7 months and was diagnosed with dup15q syndrome around his second birthday. He didn’t receive a formal autism diagnosis until he was 15, but his mother—a special-education teacher—had suspected it since he was an infant. The Rossbachs constantly worried about his safety. Michael would sometimes bolt, or “elope,” out the door, and he had excruciating gut pain that would make him bite his hands until they bled. His digestive problems made him small and frail. His body seemed to weaken with every seizure and his related health issues intensified before his death at age 35.
One morning in December 2015, Michael’s father opened the door to his bedroom and found him lying, as he usually did, on his side with his head resting on his right arm. “He must have died probably no later than midnight,” Bill Rossbach recalls.
The Rossbachs knew they wanted to donate their son’s brain, but time was of the essence. The family’s home in Acme, Michigan, was nowhere near a brain bank. Jeanine Rossbach called Autism BrainNet, kicking off the search for someone who could harvest Michael’s brain. Eventually, BrainNet found someone in Wisconsin willing to make the four-and-a-half-hour drive over the upper peninsula of Michigan in the snow to collect Michael’s brain and arrange for its delivery to Mount Sinai. “I’m forever grateful,” Jeanine Rossbach says. “I wanted to make his life as meaningful to others as possible.”
* * *
The Netherlands Brain Bank launched a campaign with the slogan “We need brains” last year, encouraging people with a range of psychiatric conditions to register as brain donors. As of October, 179 adults with autism have registered to donate their brains, according to Mignon de Goeij, who manages the Netherlands Brain Bank in Amsterdam. But the bank has yet to collect an autism brain. “It’s the only disorder we have tried to collect [for] that we haven’t gotten a brain [for] yet,” says de Goeij. More than half of the registrants with autism were born after 1970, so brain donation is still years away for many of them, she says.
As of October, more than 4,000 people are registered donors with Autism BrainNet. John, a 54-year-old man with autism who lives in Connecticut, registered last year. He learned about the initiative through a lawyer who helped him write his will. His job in the power industry is extremely dangerous. (He requested that we use only his first name because his employers do not know he is on the spectrum.) He says he wants to know that if he dies on the job, his organs and brain will go to good use. John has already donated 13 gallons of blood to the American Red Cross—enough to fill his own body about eight times. Donating his brain, he says, “goes right along with that.”
Autism BrainNet’s growing collection will be available to researchers in 2018, Amaral says. A panel of experts will review each request for tissue, considering its scientific merits and the type of tissue available. Some brain regions are large, and there is plenty of tissue to go around; others are quite small, and competition for those tissues is fierce. The fusiform face area, for instance, occupies a piece of the brain’s surface no bigger than a chickpea. Hof and his team have found a shortage of neurons in this area in people with autism. The difference in neuron number between autism and control brains is subtle, he says. “You can find it, but you have to look for it; it’s little bit of a needle-in-a-haystack search.”
Another brain region, the amygdala, is about the size of an almond and helps to process emotions, a function altered in people with autism. A 2006 study by Amaral and Schumann that looked at 9 autism brains and 10 control brains revealed a decrease in neurons in the amygdalae of people with autism. But subsequent analyses with 31 more brains have painted a more complex picture: Adults with autism have a dearth of neurons in the amygdala, but children with the condition have a surplus. The findings have yet to be published, but they reinforce the need for more postmortem brains. “To this point, all studies of the autistic brain have been underpowered,” Amaral says.
To help boost the studies’ power even with limited samples, Amaral and others are developing a new method to process the tissue. Instead of storing the fixed hemisphere whole, they plan to slice it into about 2,000 paper-thin sections that can be rolled up like scrolls and stored in fluid-filled test tubes. They can then cut these sections further to isolate certain brain regions, or slice them for various types of microscopy. That way, a single brain region the size of the amygdala could be cut into 215 sections that multiple labs can then use.
The brain banks also collect clinical data for every brain donor—including diagnostic records, brain scans, and genetic sequences. In addition, the clinical coordinator from Autism BrainNet visits each donor’s family and interviews the members to piece together a detailed portrait of the deceased person’s clinical profile. Collecting these data could help to cut through autism’s heterogeneity problem: A string of studies over the past three years have shown, for example, that some mutations linked to the condition affect only a subset of a person’s cells. The proportion of brain cells affected, and where they are in the brain, may influence the type and severity of the person’s condition.
A host of new techniques is making it possible to analyze gene expression in individual cells from postmortem tissue. “There is some really exciting research and techniques available now; we are simply limited by the number of brains available,” Schumann says. She notes that one brain donated before Autism BrainNet launched has already been used in more than 30 studies. “Every single donation makes a big difference.”
The researchers who study donated brain tissue understand the sacrifices families have made. “We do appreciate that our success is based on a family having to go through a really difficult time,” Amaral says. “Knowing that, we try to do everything in our power to make it as straightforward and easy as possible for families to make a donation.”
Hope that something good will come of her son’s brain donation is the only thing that brings Leslie Bolen comfort. Bolen wears a heart-shaped locket that is etched with Michael’s fingerprint and contains a pinch of his ashes. Advocating for Michael was her full-time job for 14 years. In a way, donating his brain lets her continue that work, helping children like her son.
Bolen had one request when she made the donation: She wanted to see Michael’s brain at the bank and keep a photograph of it. It’s not something many parents request, and even her husband, Chad, didn’t feel he could accompany her. But the clinical coordinator connected Bolen with Hof to make the arrangement. “[Hof] was just so warm and understanding and empathetic,” Bolen says. “He knew where I was coming from.”
Bolen got to visit Hof’s lab in New York twice. She held Michael’s brain in her hand. And felt at peace.
This article appears courtesy of Spectrum.


Climate change is already afflicting human health worldwide, exposing tens of millions of elderly people to excess heat while possibly reducing the ability of hundreds of millions of workers to do their jobs, according to an expansive new synthesis from The Lancet, one of the world’s oldest and most widely cited medical journals.
The report examines dozens of statistics from around the planet and finds that the long-predicted effects of climate change have already become a reality in many places. Heat waves now last longer, reaching more people and broiling more territory, than they did in the 1980s and 1990s. In the United States, this spike in warmth is lengthening the allergy season, sometimes by weeks, and helping infectious diseases to spread.
But one of its techniques is questionable. Its findings about the global economy raised eyebrows among academic economists who more formally study climate change.
“The human symptoms of climate change are unequivocal and potentially irreversible—affecting the health of populations around the world today,” warn the paper’s coauthors, who are drawn from nonprofits and 24 different academic institutions. “Whilst these effects will disproportionately impact the most vulnerable in society, every community will be affected.”
The report, dubbed “Countdown 2017,” represents the first coherent attempt to unify many of the different climate-related threads in medical and global-health research. In 2015, a commission of researchers—also convened by The Lancet—warned that climate change could “undermine the last 50 years of gains in public health.” They also called for more comprehensive tracking of important climate signals.
This report, which will now be updated annually, is the result. The coauthors have pulled together figures and estimates from across the medical literature and attempted to tabulate single, worldwide figures for many values.
Some of its techniques may have significant weaknesses. The Lancet report makes an eye-popping assertion about the global economy, arguing that climate change has already significantly harmed labor capacity around the world. Between 2015 and 2016—which are the second- and first-hottest years ever recorded—it argues that “outdoor-labor capacity” fell by 2 percent. Since the year 2000, outdoor-labor capacity has fallen by 5.3 percent overall, it claims.
Academic economists who study climate change were very doubtful of this estimate. “I would back way off the claim that [the data] show any of these things, since they don’t have or use any actual data on labor,” said Solomon Hsiang, a professor of public policy at the University of California, Berkeley, and a member of the Climate Impact Lab.
The Lancet authors, he said, used a technique drawn from a 2013 Nature Climate Change article to estimate “labor capacity.” They effectively interpolated temperature data with conclusions from U.S. military and industrial experiments, from 1996 and 2003, that researched how much work soldiers could do in different temperature conditions. But their findings do not align to a type of labor measurements that economists usually track.
“The numbers presented are (in the most generous interpretation) actual measurements (although still not sure what kind) from a few military lab experiments using U.S. soldiers as subjects and then extrapolated to people across Africa, Latin America, the Middle East, and South Asia,” Hsiang told me in an email.
He continued: “There’s lots of ways this could go wrong, since we don’t know if people adapt, we don’t know if U.S. soldiers are good proxies for normal people in Indonesia, and we don’t have any idea how the “labor-capacity” index these guys put together actually connects to “labor productivity” or earnings as any economist would recognize it.”
Climate change will almost certainly harm global labor output, but a lack of empirical observations around the world makes the kind of measurement The Lancet is attempting difficult. The controversy points to one of the most difficult aspects of finding climate change’s effects on human life: While scientists can measure weather worldwide, measuring the fingerprint of climate change on all human activity is far more complicated.
The Lancet report makes other broad claims about global public health that were more widely accepted. It finds that many more older people experience heat waves now than did two or three decades ago. The report says that roughly 175 million more people older than 65 worldwide were exposed to excess heat in 2015 as compared to several decades ago. On average, 125 million more older adults are exposed to heat than were in previous decades.
In the United States, an additional 14.5 million people older than 65 were exposed, a number larger than the population of Pennsylvania.
Howard Frumkin, one of the coauthors of the report and a professor of public health at the University of Washington, said that heat’s health effects are insidious. While a heat wave may only seem to directly kill a couple thousand people via heat stroke, researchers find that hotter months—when studied after the fact—have many more deaths than would otherwise be expected.
“Heat does a lot of things to people. It disrupts sleep and contributes to sleeplessness. It triggers violence—crime goes up during heat waves. It may trigger self-harm. And there’s very strong evidence that occupational injuries rise during heat waves,” he told me. Heat appears to function as a general stressor on people, wearing down the most vulnerable among them.
The European heat wave of 2003, one of the worst natural disasters ever experienced, is estimated to have ultimately killed more than 70,000 people.
The report also examines climate-related migration from around the world. It finds that a minimum of 4,400 people have definitively been forced to leave their homes because of climate change. “The total number for which climate change is a significant or deciding factor is much higher,” it adds (and it exempts events like the Syrian Civil War, which some experts think climate change helped aggravate).
While this number may seem small, it presages tens of thousands more relocations to come. It also shows how poorly documented most of the relocations are: Most of the 4,400 come not from inundated islands or low-lying coasts in the tropics, but from indigenous villages in northern Alaska.
The report does pull out a spot of good news: Despite a 44 percent increase in the number of extreme-weather events since 2000 (as compared to the decades before that), there’s been no equivalent rise in the number of deaths. Frumkin said that suggested that—if it plans ahead—society may be able to adapt to some of the consequences of climate change.
A separate report from The Lancet pulls out specific findings about how climate change has already altered the public health of the United States. The allergy season here is getting much longer: Nebraska’s ragweed season has extended by 17 days since the early 1990s, and Minneapolis has seen it lengthen by 21 days. The ragweed season in Kansas City, Missouri, extended by 23 days and it now nearly encompasses a quarter of the year.
And certain insect-borne infectious diseases are already showing climate-related spikes. Doctors found three times as many Lyme cases in the United States were diagnosed in 2016, compared to 1990. And mosquitoes that carry dengue fever now transmit the virus 5 percent more effectively than they did a couple decades ago.
Public-health groups in the United States used the report to make a larger point about the direction of American climate policy. “The report emphasizes the scale of the threats to human health—and makes clear that reducing dangerous climate pollution is critical to protect health and save lives,” said Harold P. Wimmer, the president of the American Lung Association, in a statement.
“The U.S. Environmental Protection Agency must listen to this latest warning and implement strong tools, including many already in place, to fight climate change and protect the health and safety of all Americans. We urge the EPA to halt its efforts to repeal the Clean Power Plan,” he added.
Frumkin said it made sense to link the report’s conclusions to recent policy changes. “In my view, and the view of most health professionals, the current administration’s policies are nothing short of tragic,” he said. “They’re misguided; they will cause more suffering and deaths in the population; and they will only delay us in our attempts to tackle climate change.”
“The impacts of climate change are not some distant future event. They’re happening now,” he added.


By the time they got to the orangutan, it was already dying.
In the Batang Toru forest, on the western flank of Sumatra, orangutans will often venture from the jungle to pick fruit from nearby gardens—a habit that puts them in conflict with villagers. In November 2013, the conservationist Matthew Nowak got word of one such conflict, and his veterinary colleagues went to investigate. They arrived to find a male orangutan, badly beaten, his face and hands riddled with cuts. Despite the team’s efforts, he died from his injuries eight days later.
With just 120,000 orangutans left in the wild, the loss of any one is a tragedy. But this particular ape has a significance that will transcend his death. Based on a close analysis of his skeleton, and a study of several orangutan genomes, Nowak and his colleagues think that the dead individual belongs to a different species of orangutan than those that we’re familiar with. If they’re right, there are actually three species of these orange-haired apes. And the newly described one would be the most endangered great ape alive.
When I was a child, an orangutan was an orangutan was an orangutan. But in 2001, after years of debate, scientists formally agreed that there actually two species—one from the Indonesian island of Borneo, and the other from neighboring Sumatra. The Sumatran species is slimmer and paler, with fur that’s closer to cinnamon than maroon. It spends more time in trees (perhaps because Sumatra, unlike Borneo, has tigers). And it’s rarer, with about 14,000 remaining individuals, compared with 105,000 in Borneo.
Most of the Sumatran orangutans live on the northern part of the island. But there’s another small group that lives in Batang Toru—100 kilometers to the south, on the other side of the sizable Lake Toba. A few obscure reports from the 1930s hinted at the existence of this splinter cell, but the group was only formally described in 1997, by a team led by the conservationist Erik Meijaard. These orangutans always seemed a little unusual. They live in more mountainous forests, and they eat different kinds of food.
Their genes are also distinct. In 2013, Michael Krützen, from the University of Zurich, analyzed the DNA of 123 Sumatran orangutans, and found that, in at least one part of their genome, the Batang Toru (or Tapanuli) orangutans were distinct. If anything, they seemed more closely related to the Bornean orangutans on a different island than the Sumatran ones just a day’s walk to the north. “We didn’t expect that,” says Krützen. “It was peculiar, but we needed more data.”
He later mentioned this peculiarity while giving a talk at a conference, where both Meijaard and Nowak happened to be in the audience. The three talked, and, suspecting that these orangutans might belong to their own distinct species, they teamed up to test that idea.
Krützen’s team analyzed the entire genomes of 37 orangutans, including two from Batang Toru. This more thorough analysis confirmed that these animals are indeed genetically distinct from both the Bornean and Sumatran species—and closer to the former than the latter.
They think that the ancestors of all modern orangutans traveled from mainland Asia into Sundaland—a continuous landmass that includes what is now Sumatra, Borneo, and other islands. Around 3.4 million years ago, these ancestral apes split into two populations, one of which gave rise to the current Batang Toru lineage. The other group spread throughout Sundaland; around 670,000 years ago, they split again into two new lineages, which we now know as the Bornean and Sumatran orangutans. The Batang Toru population occasionally crossbred with their Sumatran cousins, but those interspecies shenanigans stopped almost completely 100,000 years ago, when an erupting volcano cut them off.
It’s often said that humans differ from chimps by just 1 percent of our genome, so I wondered how close the three orangutan species are. Krützen disabused me of that question; the number depends on the history of each pair of species, and there’s little to be gained by comparing between different pairs. “There’s no magic number where, beyond that level, it’s a different species,” he says. And besides, genetic differences aren’t the only line of evidence the team has.
At the time Krützen was analyzing orangutan DNA, Nowak got word of the dying male who had been attacked in Batang Toru. And when his team compared its skeleton to those of 33 other orangutans, whose remains are housed in museums, they found clear differences in the shapes of its skull, teeth, and jaw. Even from the outside, they look distinctive, with frizzier fur and a more prominent mustache, and beards on the females. They behave differently too: They eat different plants than the northern populations, and the males have a higher-pitched call.
Not everyone is convinced, though. “It’s premature to consider this a separate species based on one cranium and two genetic samples,” says Rebecca Stumpf, from the University of Illinois at Urbana-Champaign. “I’d suggest that more evidence is needed before adopting separate species designations.”
But Nowak and Krützen argue that it’s the weight of all their evidence that matters. The physical measurements “are based on a sample size of one, which is a pity but we can’t change that,” says Krützen. But there’s also the genetic data, and the unique behaviors. Besides, other primates have been billed as distinctive species on the basis of much less. Bonobos, for example, were proposed as a distinct species from chimpanzees based on a single female specimen and five skulls—without any supporting genetic information to begin with. “There’s no one smoking gun that this is a species,” says Nowak, “but we kept looking, and we kept on finding unique things.”
“Time will tell,” Krützen says. “We’ll continue to work on this, and it might be that in 10 years’ time, we’ll say we have new data that doesn’t support a species status. That’s okay. That’s scientific progress.”
The apes might not have 10 years, though. The Sumatran orangutans were already critically endangered, and now their population might be even smaller than anyone suspected. The newly identified Batang Toru orangutan is rarer still, with an estimated 800 individuals left. Like the other two species, they are killed as agricultural pests, hunted for the pet trade, and rendered homeless as their forests are felled.
The good news is that since 2006, biologist Gabriella Fredriksson from the Sumatran Orangutan Conservation Program has been pushing the local government to spare the Batang Toru forest from logging. Thanks to her efforts, around 85 percent of the forest is now at least partially protected.
Forest Animals Are Living on the Edge
The bad news is that the unprotected 15 percent includes land that’s being set aside for a hydroelectric dam. If built, the dam would cut off two large chunks of forest where the Batang Toru orangutans live, splitting this already small population into even smaller factions. That would be devastating. “It’s probably one of the most endangered great apes we know,” says Krützen. “With just 800 individuals, there’s not much leeway for any mistakes.”
Marc Ancrenaz, co-director of the Kinabatangan Orangutan Conservation Project, is hopeful, though. “I hope that this new status will foster conservation efforts to make sure that the population doesn’t go extinct shortly after being described,” he says. “It’s definitely good news in these times where conservation is more often than not gloom and doom.”


On the Giza Plateau in Egypt rise three large pyramids—the tallest and oldest of which is the Pyramid of Khufu. It is also known as simply the Great Pyramid of Giza. You know what it looks like. It’s one of the seven great wonders of the world.
Yet, for all its fame and antiquity, so many questions remain. How was it built? Why is there nothing in the pyramid, except a broken sarcophagus missing its lid? Could there be anything else hidden inside this massive structure? In the absence of information, there is of course ferocious speculation. And now, an intriguing new piece of information: the discovery, announced today, of a large, previously unknown “void” in the Great Pyramid.
This discovery comes by way of cosmic rays. When these high-energy rays hit atoms in the Earth’s atmosphere, they send subatomic particles called muons shooting toward the ground. The muons can be slowed down by large masses—like the rocks that make up the Great Pyramid. And if muons pass through a cavity inside a large mass, that cavity will show up on muon detectors, too. Three groups of particle physicists using three different techniques patiently tracked muon patterns over several months—gathering evidence that a large cavity lurked in the middle of the pyramid.
It is an incredible—and incredibly expensive—technical feat. ScanPyramids is a project of Cairo University and the Heritage Innovation Preservation (HIP) Institute, the latter of which is funded by a number of private technology and media companies.
As for what it all means, Egyptologists are being very cautious. “The significance of it is still an open question. Even the shape of the void is not quite clear yet,” says Peter Der Manuelian, an Egyptologist at Harvard University, who was not involved with the study.
In fact, the study’s authors exhorted journalists, please, please do not call it a secret chamber. “We know it is a void, but we don’t want to use the word ‘chamber,’” says Mehdi Tayoubi, president of the HIP Institute and an author on the paper. Their caution maybe sharpened by the reaction to a press release last October extolling their preliminary results, which media reports quickly indeed turned into speculation about “secret chambers.”
The new void is above the Grand Gallery—a passage with 28-foot vaulted ceilings leading to the King’s Chamber. The ScanPyramids group first saw hints of a void when they placed nuclear-emulsion film in the Queen’s Chamber, the room below the King’s Chamber. Nuclear-emulsion film records muons, not unlike how ordinary photographic film records photons. The team could see the Grand Gallery and the King’s Chamber in their muon pattern, but they also saw an anomaly. Two other teams of physicists—using instruments that detect muons passing through plastic arrays or argon—then verified this anomaly.
Using muons to study pyramids isn’t an entirely new idea. In the 1960s, future physics Nobel Prize winner Luis Alvarez took his early muon detector to the Pyramid of Khafre. He did not find any secret chambers or even unexpected anomalies. But the idea has lived on, and scientists have used muography to study volcanoes and man-made structures.
The ScanPyramids paper published in the scientific journal Nature is heavy on particle physics and deliberately light on archaeology. Hany Helal, an engineer at Cairo University and a member of the ScanPyramids team, says he is organizing a seminar in Egypt later this year, where archaeologists can come and debate the significance of the void for the pyramid’s construction.
Mark Lehner and Zahi Hawass, two members of the Egyptian Ministry of Antiquities’ scientific committee, to whom ScanPyramids presented it results earlier this year, both told me they suspected the void to be a “construction gap.” All of the chambers and major passageways of the pyramid are aligned along one vertical plane. In order to build the chambers and fill in the rest of the pyramid simultaneously, workers may have worked along what is essentially a trench that allowed them continual access to the King’s Chamber and Grand Gallery. A construction gap could be a remnant of the trench. So it is not surprising, they say, that a void from the construction gap might appear in the space above the Grand Gallery.
In contacting Egyptologists for this story, I could sense a weariness and wariness in their responses. Weariness because claims about hidden chambers in pyramids surface all the time.
The thing to understand, says Lehner, is “the pyramid is more Swiss cheese than cheddar.” That’s only a slight exaggeration, he adds. The inside of the Great Pyramid is filled with stones of irregular sizes, so there are numerous small gaps. In this case, he agrees the void appears to be large enough as to be deliberate, like a construction gap. But many people before have found evidence of a small cavity in the pyramid and gone on to speculate wildly about secret chambers. Lehner said he found ScanPyramids’ characterization of a different anomaly on the pyramid’s north face as a “corridor” to be premature.
The wariness, on the other hand, seems to stem from the project’s origins. Tayoubi, the president and cofounder of the HIP Institute, is also a VP at Dassault Systèmes, a French 3-D-design software company. In 2005, he teamed up to visualize the Great Pyramid construction site with architect Jean-Pierre Houdin, whose idea that the pyramids were built using a series of ramps is not accepted by mainstream archaeologists. (He has since also worked with Der Manuelian now at Harvard to reconstruct the Giza Plateau in 3-D.) Funding for the HIP Institute comes from a number of companies, including: Dassault, Japan’s national broadcasting agency, a watch company, a VR company, and a hotel near Giza.
Hawass, who is also a former Egyptian minister of antiquities, and an outsized, outspoken, and sometimes controversial figure in Egyptology, was blunt—his bluntness perhaps the result of longstanding frustration. “Everybody who comes to the pyramid,” he says, “either they’re looking for fame or they want to make experiments with their equipment and the equipment belongs to a company, and the company can make money.”
The Coolest Thing About King Tut’s Space Dagger
In an interview, Tayoubi acknowledged he is no Egyptologist, and he now assiduously avoided speculation on how the pyramid was built. He did want to tout the technologies used in the study, though not by company name. “We love innovation,” he says, “This mission is about better understanding the pyramid, but above all it’s about innovation,” he says. He likened studying the pyramid to space exploration—an endeavor driven by pure wonder that may nevertheless result in practical innovations in fields like muography and robotics. In fact, the ScanPyramids project is already designing its next piece of technology, a robot to explore inside the pyramid.
New technology might one day crack some of the questions about the Great Pyramid. But so much of its appeal may just be how little we know, despite its prominence and endurance. A mystery right in front of us, daring us to solve it.


Between 1956 and 1962, the University of Cape Town psychologist Kurt Danziger asked 436 South African high-school and college students to imagine they were future historians. Write an essay predicting how the rest of the 20th century unfolds, he told them. “This is not a test of imagination—just describe what you really expect to happen,” the instructions read.
Of course, everyone wrote about apartheid. Roughly two-thirds of black Africans and 80 percent of Indian descendants predicted social and political changes amounting to the end of apartheid. Only 4 percent of white Afrikaners, on the other hand, thought the same. How did they get it so wrong?
Students’ predictions were more like fantasies. “Those who were the beneficiaries of the existing state of affairs were extremely reluctant to predict its end,” Danziger explains, “while those who felt oppressed by the same situation found it all too easy to foresee its collapse.”
Psychology research indeed suggests that the more desirable a future event is, the more likely people think it is. When the sociologists Edward Brent and Donald Granberg studied wish fulfillment in U.S. presidential elections between 1952 and 1980, they found that 80 percent of each of the major candidates’ supporters expected their preferred candidate to win by a ratio of around four to one. “People distort their perception of an election's closeness in ways that are consistent with their preferences,” a later paper concluded. Likewise, after the 2008 election, researchers analyzed survey predictions from 19,000 Americans and found that Democrats tended to think Barack Obama was more likely to win, while Republicans assumed John McCain would.
Conversely, the more someone dreads or fears a potential outcome, the less likely they think it is to happen. In November 2007, economists in the Philadelphia Federal Reserve’s Survey of Professional Forecasters predicted just a 20 percent chance of “negative growth”—read: decline—in the U.S. economy any time in 2008, despite visible signals of an impending recession. There is, the economist Sergey Smirnov wrote in a review of economists’ botched predictions on the 2008 recession, “some deep inherent unwillingness to predict undesirable things.”
But people’s thinking isn’t as simplistic as “I wish it to happen, so it will” or, “I don’t want it to happen, so it won’t.” Self-interest influences our predictions in subtler ways.
* * *
The Rutgers University psychologist Neil Weinstein discovered unrealistic optimism by accident. It was the late 1970s, and for no particular reason he had asked study subjects to rate their likelihood of experiencing certain negative future events, like getting divorced, fired, or mugged, from “below average risk” to “above average risk.” This was back when data were manually entered on yellow punch cards. So he was sitting at his punch-card machine punching people’s responses when he realized that “all the responses were on the below-average side of the scale.”
Unrealistic optimism is thinking that good things are more likely to happen to you than to other people, whereas bad things are less likely. It’s not outright denial of risk, says Weinstein. “People don’t say, ‘It can’t happen to me.’ It’s more like, ‘It could happen to me, but it’s not as likely [for me] as for other people around me.’” People predict that they’re less likely than others to experience illness, injury, divorce, death, and other adverse events—even when they’re exposed to the same risk factors. For instance, someone might think she’s less prone to diabetes than others, even if she weighs the same, eats the same, shares similar family history with, and has the same lifestyle as the people she's comparing herself to.
Take these Pew Research Center findings: In 2015, 65 percent of a representative sample of American workers predicted that automation would monopolize most of the work currently performed by humans within 50 years. But 80 percent of workers believed that their own jobs would remain intact. In essence, they admitted that automation posed a threat to workers but assumed they were less susceptible than average to its effects.
Anxiety affects people’s predictions subliminally. For example, they may unwittingly only gather and synthesize facts about their prediction that support the outcome they want. This process may even be biologically ingrained: Neuroscience research suggests that facts supporting a desired conclusion are more readily available in people’s memories than other equally relevant but less appealing information. Our predictions are often less imaginative than we think.
They’re also more self-absorbed. Unrealistic optimism occurs in part because people fail to consider others’ experiences, especially when they think a future outcome is controllable. Imagine you’re trying to find a job. You do everything you can to make yourself an appealing candidate: get relevant experience, fix up your resume and cover letter, network. You might conclude that these measures will make you more likely to get a job than other job seekers, not taking into account that others are likely doing the same things you are to boost their chances, too. People may think their odds are better than average because they don’t know what the average really is.
Weinstein tried to curb this bias in his lab with limited success. He told students to list the factors that influenced their chances of experiencing certain events, like a heart attack, and then read other students’ responses to the same prompt. When students realized that their risk factors matched everyone else’s, their unrealistic optimism was reduced but, oddly, not eliminated. They still thought they were less likely than the average student with the same risk factors to experience negative events, even without any objective justification. Other research indicates that people resist revising their estimations of personal risk even when confronted with relevant averages that explicitly contradict their initial predictions.
Despite attempts to remedy unrealistic optimism, Weinstein insists that it isn’t all bad. Thinking things are going to turn out well may actually be adaptive, a way to soothe our fears about the future. “It keeps you from falling apart.”
* * *
Before Ray Kurzweil became a high-profile inventor, author, and futurist, he was a kid who lost his father. Kurzweil has kept every physical memento of his dad—records, notes, pictures, electric bills, 50 boxes in total—in a storage facility in Massachusetts in preparation for his prediction: By the mid-2030s, “we will be able to create avatars of people who have passed away from all of the information they have left behind.”
Kurzweil himself plans on immortality. He takes 90 supplements per day, gets regular blood tests and infusions, and has been working with the famed longevity doctor Terry Grossman for the last two decades to arrest his aging. But Kurzweil thinks that someday he’ll transcend the need for these antiaging measures. He predicts that in the 2030s our brains will connect directly to the cloud to augment our existing intelligence, and that our biological bodies will be replaced with machines part by part.
Research suggests that far-off events, like death, are particularly vulnerable to overly optimistic predictions. Moreover, predictions appear to be most influenced by whether “the event in question is of vital personal importance to the predictor.”
The author, speaker, and global-trends expert Mark Stevenson says that people who predict the future are victim to their own prejudices, wish lists, and life experiences, which are often reflected in their predictions. When I asked Stevenson for an example, he told me to consider at what point in time any futurist approaching 50 predicts life extension will be normal—“quite soon!”
But Kurzweil tells me that he makes his predictions based on what he calls the “law of accelerating returns,” which says that technology follows “a predictable and exponential trajectory.” Kurzweil says that he controls for wishful thinking in his predictions by using that trend to project out a future curve. It’s math. In 2045, the year Kurzweil predicts humans will become effectively immortal after they’ve merged with machines, Kurzweil will be 97.
* * *
In 2010, Pew Research Center found that 41 percent of Americans—70 percent of whom identify as Christian—believed that Jesus would probably or definitely return to Earth by 2050. These 50 million Americans might be right this time, but the oft-prophesied Second Coming has failed to come before. Jesus predicted his own return within a single generation in Matthew 24:34; 500 years ago, Martin Luther predicted that Judgment Day would occur within the next 300 years.
University of Pennsylvania’s Philip Tetlock, who studies the art and science of forecasting, says that strongly held beliefs are a big reason people make bad predictions.
For example, the Western faith in progress—the idea that, as sociologist Robert Nisbet put it, “mankind has advanced in the past, is now advancing, and may be expected to continue advancing in the future”—biased early predictions about the internet. In 2005, only 32 percent of hundreds of internet experts surveyed by Pew agreed that by 2014 most people “will use the internet in a way that filters out information that challenges their viewpoints on political and social issues.”
Sometimes the self-interest influencing people’s predictions is simply their desire to be right. People predict outcomes that will affirm their beliefs about the world: that democracy is winning, that death is a needless tragedy, that there is or isn’t a God. “Once you commit to anything you then have a vested interest in the outcome,” Kurzweil says. Strongly held beliefs become self-interested beliefs.
People aren’t so naïve as to think that just because something is important to them, it will happen. Rather, they tend to think most other people share their beliefs, and thus the future they endorse is likely. What researchers call “projection bias” explains why individuals so often bungle election predictions. Because they assume that others have political opinions similar to their own, people think their chosen candidate is more popular than she or he actually is. Liberals’ underestimates of the true scale of Trump support during the 2016 election may have at least partially resulted from this bias.
People who are abnormally good at predicting the future—“super-forecasters”—skillfully ban their prejudices from their probability equations. As Tetlock told Steven Dubner on the Freakonomics podcast, super-forecasters “try not to have too many ideological sacred cows.”
* * *
Sometimes self-interested predictions pan out. When Cold War historians predicted the imminent Westernization of the world amid the threat of nuclear war with the Soviet Union, they were praying they wouldn’t witness the alternative. They ended up being right. And of course, the black and Indian South Africans who predicted—and hoped—that apartheid would end were right, too.
Still, on the whole people would make better predictions with more objectivity and awareness. And good predictions matter. If you think you won’t get an STI, you may not practice safe sex or get tested regularly, thereby increasing your risk. On a larger scale, people may settle in regions prone to natural disasters because they assume they won’t be affected. Or, as Weinstein observed, people may think Trump’s proposed tax reform will help them more than it actually will, or at least that it will help them more than it will help other people. These beliefs could sway public support and congressional votes and, as a result, the future of American taxes.
In short, how we predict the future is important because it affects what we do in the present. So how do you forfeit your fantasies?
Faith Popcorn, the CEO of “a future-focused strategic consultancy,” advises shaking up your perspective: “Learn how the other side thinks,” she says. Chat up interesting people; go to readings, talks, and fairs to “expand your horizons.”
Tetlock says forecasting tournaments can teach people how to outsmart their shortcomings and instead play “a pure accuracy game.” (His wife runs one such tournament, called the Foresight Project.) He says that many people “have a little voice in the back of their heads saying, ‘Watch out! You might be distorting things a bit here,’ and forecasting tournaments encourage people to get in touch with those little inner voices.”
But Weinstein has been around long enough to know that extinguishing unrealistic optimism isn’t so simple. “It’s hard because it has all these different roots,” he says. And human nature is so obstinate.
In one study, Weinstein and his collaborators asked people to estimate the probability that their Texas town would be hit by a tornado. Everyone thought that their own town was less at risk than other towns. Even when a town was actually hit, its inhabitants continued to believe that their town was less likely to get hit than average. But then, by chance, one town got hit twice during the study. Finally, these particular townspeople realized that their odds were the same as all the other towns. They woke up, says Weinstein. “So you might say it takes two tornadoes.”


At the very beginning of his book The Song of the Dodo, the author David Quammen invites us to imagine a fine Persian carpet, which we then slice into 36 equal pieces. “What does it amount to?” he writes. “Have we got 36 nice Persian throw rugs? No. All we’re left with is three dozen ragged fragments, each one worthless and commencing to come apart.” He wrote that almost two decades ago, and it’s still the perfect metaphor for the state of the world’s forests.
Humans chop down an estimated 13 million hectares of trees every year, but even that huge number doesn’t fully capture the destruction we inflict. We don’t just destroy forests—we fragment them, turning unbroken stretches of green into ragged patches fraying at the edges. There are only two places on Earth—the Amazon and the Congo—where forests have retained their old, continuous glory. Everything else has been partitioned into green islands, separated not by water but by roads and farmland.
It matters, this slicing of the world. Just as Quammen’s 36 swatches of fabric don’t add up to a full carpet, fragmented forests are less conducive to life than their total area might suggest. Each green island is limited in how many species it can sustain. And since these populations are cut off from their neighbors, they’re uniquely vulnerable to disease, disaster, or dumb, bad luck. For these reasons, fragmented habitats tend to lose half their animal and plant species within two decades.
Many of the survivors are, quite literally, living on the edge. About 70 percent of the world’s remaining forests lie within a kilometer of an edge, and about half lie within 500 meters. And in a new and unprecedentedly detailed study, a team of scientists led by Marion Pfeifer at Newcastle University has shown that 85 percent of forest animals are affected by the presence of an edge. Some benefit, others suffer. Either way, the results show that humans, by fragmenting forests, have radically restructured them—and that the edge of a forest is almost a completely different world than its core.
Many researchers have tried to assess edge effects in simple ways, such as measuring how the abundance of a species changes as it gets closer to an edge. This ignores the fact that a patch of forest might be within reach of many edges, each of which compounds the hazards of the others. It also treats land simplistically, as if it’s either forest or nonforest; in reality, the land surrounding a group of trees can vary a lot in its usefulness and attractiveness to local species.
To improve on these methods, Pfeifer’s colleague Véronique Lefebvre from Imperial College London developed two new metrics for making sense of fragmented landscapes. The first—edge influence—reflects the variations in tree cover around a given point; roughly speaking, it tells you how “edgy” a particular bit of forest is. The second metric—edge sensitivity—reflects how edgy a habitat has to get before a species avoids it. They then calculated edge sensitivities for 1,673 vertebrate animals around the world. “The study sets a new standard of how edges should be measured,” says Nick Haddad from Michigan State University, who was not involved in the work.
They found that around 39 percent of these species steer clear of edges while 46 percent seem to gravitate toward them. This doesn’t mean that edges are a net good, though. The edge-seeking animals are more likely to be generalists or invasive species, such as green iguanas and boa constrictors. By contrast, the species that stick to the forest core, such as the Sunda pangolin and Baird’s tapir, are almost four times as likely to be on the endangered list. They’re the creatures most at risk of extinction, and their shrinking habitat is even smaller than it first seemed. There’s a lot of them too. About 57 percent of mammals decline in number as you get closer to edges, as do 41 percent of birds, 30 percent of amphibians, and 11 percent of reptiles.
“Edge effects have long been studied but not with the level of detail in this study,” says Isabel Rosa from the German Centre for Integrative Biodiversity Research. “It highlights the importance of considering not only the habitat amount that is available for the species, but also its quality.”
For example, the team’s calculations show that forest-core species only thrive at least 200 to 400 meters away from an edge. To a long-billed black cockatoo, a circular clump of trees that’s 800 meters in diameter is not 200 hectares of inviting greenery—it’s nothing. To a Sunda pangolin, a strip of forest that’s extremely long but just 800 meters wide might as well be no forest at all. “We need to manage patches in a way that maximizes the forest core,” Pfeifer says. Her team has developed a freely available tool called BioFrag to help them, by calculating the edginess of their forests and the edge sensitivities of the local animals.
Conservationists do already think about edge effects, Haddad says, but typically on the scale of tens of meters—not hundreds. Pfeifer and her colleagues have shown that edges are more pernicious, and that their influence extends far deeper into a forest. Only half the world’s forests lie more than 500 meters from an edge, and conservationists should focus on the species that persist in this invisible corral. If they ignore these edge effects, Haddad says, “they may be missing the forest for the trees.”


Joshua Plotkin’s dive into the evolution of language began with clarity—and also a lack of it.
Today, if you wanted to talk about something that’s clear, you’d say that it has clarity. But if you were around in 1890, you would almost certainly have talked about its clearness.
Plotkin first noticed this linguistic change while playing with Google’s Ngram Viewer, a search engine that charts the frequencies of words across millions of books. The viewer shows that a century ago, clearness dominated clarity. Now the opposite is true, which is strange because clarity isn’t even a regular form. If you wanted to create a noun from clear, clearness would be a more obvious choice. “Why would there be this big upswing in clarity?,” Plotkin wondered. “Is there a force promoting clarity in writing?”
It wasn’t clear. But as an evolutionary biologist, Plotkin knew how to find out.
The histories of linguistics and evolutionary biology have been braided together for as long as the latter has existed. Many of the earliest defenders of Darwinism were linguists who saw similarities between the evolution of languages and of species. Darwin himself wrote about these “curious parallels” in The Descent of Man. New words and grammatical rules are continually cropping up, fighting for existence against established forms, and sometimes driving those old forms extinct. “The survival ... of certain favored words in the struggle for existence is natural selection,” Darwin wrote.
Darwin, Plotkin says, used the way language changes “to popularize his heretical theory and explain for a broad audience what natural selection means. The process wasn’t easy to observe in organisms, but it was easier to see in words.”
But natural selection is just one force of evolutionary change. Under its influence, genes become more (or less) common because their owners are more (or less) likely to survive and reproduce. But genes can also change in frequency for completely random reasons that have nothing to do with their owner’s health or strength—and everything to do with pure, dumb luck. That process is known as drift, and it took decades for evolutionary biologists to recognize that it’s just as important for evolution as natural selection.
Linguists are still behind. It’s easy to see how languages can change through drift, as people randomly pick up the words and constructions they overhear. But when Darwin wrote about evolving tongues, he said, “The better, the shorter, the easier forms are constantly gaining the upper hand, and they owe their success to their own inherent virtue.” That’s a view based purely on natural selection, and it persists. “For the most part, linguists today have a strict Darwinian outlook,” Plotkin says. “When they see a change, they think there must be a directional force behind it. But I propose that language change, maybe lots of it, is driven by random chance—by drift.”
To see whether that was true, he and his colleagues developed statistical tests that could distinguish between the influence of drift and of natural selection. They then applied these tests to several online repositories, such the Corpus of Historical American English—a digital collection of 400 million words, pulled out of 100,000 texts published over the past 200 years.
The team focused first on the past-tense forms of verbs, and found at least six cases where natural selection is clearly in effect. In some cases, the verbs were regularized, losing weird past forms in favor of more-predictable ones that end in –ed. Wove, for example, gave way to weaved, while smelt lost ground to smelled. That’s not surprising: Many linguists have suggested that verbs tend to become more regular over time, perhaps because, like Darwin theorized, these forms are just easier to learn.
But Plotkin found just as many instances where selection drove verbs toward irregularity: Dived gave way to dove, lighted to lit, waked to woke, and sneaked to snuck. Why? Perhaps because we like it when words sound alike, and we change our language to accommodate such rhymes. For example, dove began to replace dived at the same time that cars became popular, and drive/drove became common parts of English. Similarly, the move from quitted to quit coincided with the rise of split, which became much more widely used when it acquired a new meaning—to leave or depart. In both cases, changes in one irregular verb—drive or split—may have irregularized others. “We can’t definitively say that’s the reason, but it’s coincident,” Plotkin says.
“It gets you to think harder about the motivation for change,” says Salikoko Mufwene, from the University of Chicago. “The general claim is that there has been an evolution toward regularization, and they’re showing that this hasn’t always been the case. Now we need to think harder about when irregular forms are favored over regular variants.”
That is, if anything is favored at all. The team found that the changes that have befallen the vast majority of our verbs are entirely consistent with drift. You don’t need to invoke natural selection to explain why we say spilled instead of spilt, burned instead of burnt, and knit instead of knitted.
In other cases, drift and natural selection work together to shape languages. For example, Plotkin’s team also looked at the rapid rise of do in the 16th century, when phrases like “You say not” quickly changed into “You do not say.” They concluded that at first, the word randomly drifted its way into questions, so that “Say you?” gradually became “Do you say?” Once it became common, natural selection started pushing it into new contexts like declarative sentences, perhaps because it was easier for people to use it consistently.
The team also analyzed a third and more obscure grammatical change called Jespersen’s Cycle. In Old English, spoken before the Norman Conquest, speakers would negate a verb by putting a not in front of it. In Middle English, spoken between the 11th and 15th centuries, the negatives would surround the verb as they do in modern French (“Je ne dis pas”). And in Early Modern English, spoken between the 15th and 17th centuries, the negative followed the verb—the Shakespearean “I say not.” Now, we’ve come full circle, back to “I don’t say.”
Jespersen’s Cycle exists in many unrelated languages. In French, for example, the formal “Je ne dis pas” is giving way to the colloquial “Je dis pas.”
Natural selection still explains Jespersen’s Cycle far better than drift does, according to Plotkin's analysis. Perhaps it’s due to emphasis, he says. If one form is common, speakers could emphasize their disagreement by adding or subtracting words (“I don’t say that at all,” versus “I don’t say that”). As the emphatic forms become more common, they lose their sting, and are themselves replaced.
These results are part of a wider trend where linguists are starting to use these massive online corpora to address long-standing puzzles in language change. “This is an excellent trend,” says Jennifer Culbertson, from the University of Edinburgh. “Linguists have uncovered many really fascinating cases of language change, but the explanations on offer sometimes read like just-so stories. Random processes are simply underappreciated, because we want to come up with interesting explanations.” But by considering drift, too, linguists could “focus our energies on providing interesting explanations where they are really warranted.”
What about the change from clearness to clarity, which set Plotkin onto this quest in the first place? He says that he’s found signs of natural selection’s hand, but that will have to wait for another publication. “There’s lots to be done,” he says. “This is just the beginning of an investigation, which need not stop at written texts. Spoken records are just as ready and ripe for scrutiny.”


The Plague of Cyprian, named after the man who by AD 248 found himself Bishop of Carthage, struck in a period of history when basic facts are sometimes known barely or not at all. Yet the one fact that virtually all of our sources do agree upon is that a great pestilence defined the age between AD 249 and AD 262.
Inscriptions, papyri, archaeological remains, and textual sources collectively insist on the high stakes of the pandemic. In a recent study, I was able to count at least seven eyewitnesses, and a further six independent lines of transmission, whose testimony we can trace back to the experience of the pestilence.
This article is adapted from Harper’s recent book.
What is starkly lacking, however, is a Galen. The previous century’s dumb luck of having a great and prolific doctor to guide us has run out. But, now, for the first time, we have Christian testimony. The church experienced a growth spurt during the generation of the plague, and the mortality left a deep impression in Christian memory. The pagan and Christian sources not only confirm one another. Their different tone and timbre give us a richer sense of the plague than we would otherwise possess.
The lack of a medical witness like Galen is partly compensated by the vivid account of the disease in Cyprian’s sermon on the mortality. The preacher sought to console an audience encircled by unfathomable suffering. It took no mercy on his Christians.
“The pain in the eyes, the attack of the fevers, and the ailment of all the limbs are the same among us and among the others, so long as we share the common flesh of this age.” Cyprian tried to ennoble the victims of the disease, likening their strength in pain and death to the heroic intransigence of the martyrs. Cyprian conjured the symptoms for his hearers.
These are adduced as proof of faith: that, as the strength of the body is dissolved, the bowels dissipate in a flow; that a fire that begins in the inmost depths burns up into wounds in the throat; that the intestines are shaken with continuous vomiting; that the eyes are set on fire from the force of the blood; that the infection of the deadly putrefaction cuts off the feet or other extremities of some; and that as weakness prevails through the failures and losses of the bodies, the gait is crippled or the hearing is blocked or the vision is blinded.
Cyprian’s account is central to our understanding of the disease. The pathology included fatigue, bloody stool, fever, esophageal lesions, vomiting, conjunctival hemorrhaging, and severe infection in the extremities; debilitation, loss of hearing, and blindness followed in the aftermath. We can complement this record with more isolated and frankly uncertain hints from other witnesses. According to Cyprian’s biographer, the disease was characterized by acute onset: “carrying off day by day with abrupt attack numberless people, every one from his own house.”
The course of the infection and illness was terrifying. This impression is confirmed by another North African eyewitness, a Christian not far removed from the circle of Cyprian, who insisted on the sheer unfamiliarity of the disease. “Do we not see the rites of death every day? Are we not witnessing strange forms of dying? Do we not behold disasters from some previously unknown kind of plague brought on by furious and prolonged diseases? And the massacre of wasted cities?” The pestilence, he argued, was a manifest encouragement to martyrdom, since those who died the glorious death were spared the “common fate of others amidst the bloody destruction of ravaging diseases.”
The Plague of Cyprian was not just another turn through the periodic cycle of epidemic mortality. It was something qualitatively new—and the evocation of its “bloody” destruction may not be empty rhetoric, if hemorrhagic symptoms are implied.
The disease was of exotic origin and moved from southeast to northwest. It spread, over the course of two or three years, from Alexandria to other major coastal centers. The pandemic struck far and wide, in settlements large and small, deep into the interior of empire. It seemed “unusually relentless.” It reversed the ordinary seasonality of death in the Roman Empire, starting in the autumn and abating in the following summer. The pestilence was indiscriminate; it struck regardless of age, sex, or station. The disease invaded “every house.”
One account predictably blamed the “corrupted air” that spread over the empire. But another chronicle tradition, going back to a good contemporary historian in Athens, recorded that the “disease was transmitted through the clothes or simply by sight.” The observation is notable; in a culture without even a rudimentary sense of germs, the comment betrays a pretheoretical sense of contagion. The concern that the disease could be transmitted by clothing or eyesight suggests at least a dim awareness of an infectious origin. And it just might provide a further hint that the disease affected the eyes.
The ancients harbored plenty of eccentric notions about the powers of eyesight, among them that it was tactile, ejecting a flow of particulates from the eye of the looker. The bloody eyes of Cyprian’s victims may have presented a terrifying visage, in a culture where the eyes had the power to reach out and touch.
The death toll was grim. We have an intriguingly specific report from the bishop of Alexandria, who claimed that:
This immense city no longer contains as big a number of inhabitants, from infant children to those of extreme age, as it used to support of those described as hale old men. As for those from 40 to 70, they were then so much more numerous that their total is not reached now, though we have counted and registered as entitled to the public food ration all from 14 to 80; and those who look the youngest are now reckoned as equal in age to the oldest men of our earlier generation.
The reckoning implies that the city’s population had declined by about 62 percent (from something like 500,000 to 190,000). Not all of these need be dead of plague. Some may have fled in the chaos. And we can always suspect overheated rhetoric. But the number of citizens on the public grain dole is a tantalizingly credible detail, and all other witnesses agreed on the scale of the mortality. An Athenian historian claimed that 5,000 died each day. Witness after witness—dramatically if imprecisely—testified that depopulation was invariably the sequel of the pestilence. “The human race is wasted by the desolation of pestilence.”
These haphazard clues do not equip us well to identify the pathogenic agent of the Plague of Cyprian. But the range of suspects capable of causing a disease event of this scope is not large, and some possible agents can be almost certainly exculpated.
Bubonic plague does not fit the pathology, seasonality, or population-level dynamics. Cholera, typhus, and measles are remote possibilities, but each poses insuperable problems. Smallpox must be a serious candidate. The two-generation lapse between the episode under Commodus and the Plague of Cyprian means that effectively the entire population would have been susceptible again. The hemorrhagic form of the disease might also account for some of the features described by Cyprian.
But in all the case for smallpox is weak. A North African author claimed it was an unprecedented disease (though whether he would have had any memory of previous smallpox epidemics is of course questionable). None of our sources describe the full-body rash that is the distinctive feature of smallpox. In the church history of Eusebius, written in the early fourth century, an outbreak more like smallpox was recounted in AD 312–13. Eusebius both called this a “different illness” than the Plague of Cyprian and also distinctly described the pustular rash. The exotic origins of the third-century event, again from beyond the Roman Empire, do not suggest the eruption of a now-endemic pathogen. Finally, the putrescent limbs and permanent debilitation of the Plague of Cyprian are not a fit for smallpox. None of these clues are conclusive, but collectively they militate against the identification of smallpox.
Any identification must be highly speculative. We would offer two candidates for consideration. The first is pandemic influenza. The influenza virus has been responsible for some of the worst pandemics in human history, including the “Spanish flu” epidemic that carried off some 50 million souls at the end of World War I. The lack of clear evidence for influenza from the ancient world is puzzling, because the flu is old and it was undoubtedly not a stranger in the ancient world. Influenza is a highly contagious acute respiratory disease that comes in many forms. Most types are relatively mild, causing familiar cold-like symptoms. Other rare types of influenza are more menacing.
Zoonotic forms of the disease, especially those native in wild aquatic birds, can be pathogenic to other animals, including pigs, domestic fowl, and humans; when these strains evolve the capacity to spread directly between humans, the results are catastrophic. There have been four global outbreaks in the last century, and avian influenza (which includes some dreaded strains such as H5N1) remains a terrifying threat today.
Pathogenic zoonotic influenzas are viciously lethal. They induce an overheated immune response which is as dangerous as the viral pneumonia itself; hence, the young and healthy are paradoxically put at risk by the vigor of their immune response. The lack of any respiratory symptoms in the account of the Plague of Cyprian is a strike against the identification. But it is worth reading some observations of the 1918 pandemic.
Blood poured from noses, ears, eye sockets; some victims lay in agony; delirium took away others while living ... The mucosal membranes in the nose, pharynx, and throat became inflamed. The conjunctiva, the delicate membrane that lines the eyelids, becomes inflamed. Victims suffer headache, body aches, fever, often complete exhaustion, cough ... Often pain, terrific pain ... Cyanosis ... Then there was blood, blood pouring from the body. To see blood trickle, and in some cases spurt, from someone’s nose, mouth, even from the ears or around the eyes, had to terrify ... From 5 to 15 percent of all men hospitalized suffered from epistaxis—bleeding from the nose.
Pandemic influenza might indeed account for the horrifying experience of the Plague of Cyprian.
The winter seasonality of the Plague of Cyprian points to a germ that thrived on close interpersonal contact and direct transmission. The position of the Roman Empire astride some of the major flyways of migratory birds, and the intense cultivation of pigs and domestic fowl such as chickens and ducks, put the Romans at risk. Climate perturbations can subtly redirect the migratory routes of wild waterfowl, and the strong oscillations of the AD 240s could well have provided the environmental nudge for an unfamiliar zoonotic pathogen to find its way into new territory. The flu is a possible agent of the pestilence.
A second and more probable identification of the Plague of Cyprian is a viral hemorrhagic fever. The pestilence manifested itself as an acute-onset disease with burning fever and severe gastrointestinal disorder, and its symptoms included conjunctival bleeding, bloody stool, esophageal lesions, and tissue death in the extremities. These signs fit the course of an infection caused by a virus that induces a fulminant hemorrhagic fever.
Viral hemorrhagic fevers are zoonotic diseases caused by various families of RNA viruses. Flaviviruses cause diseases like yellow fever and dengue fever, which have some resemblance to the symptoms described by Cyprian. But flaviviruses are spread by mosquitoes, and the geographic reach, speed of diffusion, and winter seasonality of the Plague of Cyprian rule out a mosquito-borne virus.
The speed of diffusion points to direct human-to-human transmission. The belief that caring for the sick and handling the dead were fraught with danger underscores the possibility of a contagion spread between humans. Only one family of hemorrhagic viruses seems to provide a best match for both the pathology and epidemiology of the Plague of Cyprian: filoviruses, whose most notorious representative is the Ebola virus.
Filoviruses are millions of years old. Fragments of their genetic material are anciently embedded in mammalian genomes, and for millions of years they have infected bats, insectivores, and rodents. Yet filoviruses, like Ebola virus and Marburg virus, were only recognized in the second half of the 20th century during a series of small-scale outbreaks. The Ebola epidemic of 2014 brought further attention to the family. The natural host of the Ebola virus remains unconfirmed, although bats are suspected. Ebola virus grabs public attention because of its ghastly clinical course and extreme case-fatality rates.
To cause an epidemic, the Ebola virus must first leap from its host species to a human; this probably occurs when humans come into contact with infected bats or apes. Once infected, after a brief incubation period (on average four to 10 days, sometimes longer), victims suffer intense fever and a disease that breaks down multiple systems simultaneously, including gastrointestinal and vascular involvement. Conjunctival injection and severe hemorrhagic symptoms could well account for the disturbing reports of Cyprian. Tissue necrosis and permanent disfigurement of the limbs might reflect Cyprian’s description of extremities turning putrid and becoming irreversibly disabled.
Case-fatality rates, even with modern treatment, are grotesquely high: 50–70 percent. Death usually comes between days six and 16; survivors are thought to possess immunity. The Ebola virus is transmitted by bodily fluids, but not aerial droplets; it spreads easily within households. Caregivers are at special risk, and cadavers remain a potent source of infection. The observance of traditional burial rites has been a problematic risk factor even in recent outbreaks.
Retrospective diagnosis from anguished reports of nonmedical personnel across nearly 2,000 years is never going to offer great confidence. But the hemorrhagic symptoms, the shocked sensibilities, and the insistence on the novelty of the disease all fit a filovirus. An agent like Ebola virus could diffuse as quickly as the Plague of Cyprian, but because of its reliance on body fluids for transmission, it could exhibit the slow-burning, “unusually relentless” dynamics that so struck contemporary observers. The obsession with deadly corpses in the third-century pandemic strikes a profound chord, given the recent experience of the Ebola virus. The uncertainty lies in our profound ignorance about the deep history of pathogens like Ebola that never became endemic in human populations.
As historians, we understandably default to the familiar suspects. But our broadening awareness of the incessant force of emerging disease, at the frontier between human society and wild nature, suggests a place for significant disease events in the past, like the Plague of Cyprian, caused by zoonotic diseases that wreaked havoc and then retreated back to their animal hosts.
By the time of the Plague of Cyprian’s appearance in AD 249 there was much that was different. The empire’s stores of reserve energy were depleted. Perhaps this microbial enemy was just more sinister. In this event, the center could not hold. There is much that must remain uncertain about the Plague of Cyprian, but not this: In its immediate wake, anarchy was loosed on the world.
This post is adapted from Harper’s recent book, The Fate of Rome: Climate, Disease, and the End of an Empire.



For most of us, it’s unthinkable: Human is never what’s for dinner. Sorry to burst any bubbles, but in this episode, we discover that not only is cannibalism widespread throughout the natural world, it’s also much more common among our own kind than we like to think. Spiders and sharks do it; so do both ancient and modern humans. So why it sometimes make sense to snack on your own species—and what are the downsides? From Hannibal Lecter to the Donner party, cannibals are now the subject of morbid fascination and disgust—but how did eating one another become such a taboo? Join us this episode for our Halloween special: the science and history of cannibalism!
According to the zoologist Bill Schutt, the author of Cannibalism: A Perfectly Natural History, until recently, “the party line was basically that if you saw cannibalism in nature it was because of a lack of nutrition or cramped captive conditions.” In the past three decades, however, scientists have come to realize that cannibalism is surprisingly common, and that it occurs for a variety of different reasons: Male spiders who become their consort’s dinner gain a reproductive advantage, Schutt explains, while sand tiger sharks take advantage of their spare siblings in utero to hone their hunting skills before they’re even born.
Fewer species eat members of their own kind as you move through the animal kingdom toward primates—but, according to the archaeologist James Cole, cannibalism seems to have been a reasonably regular part of early human behavior too. His question was: Why? Were ancient humans eating one another out of hunger, or for more complicated reasons that have to do with spiritual beliefs about the soul and the body? To find out, Cole determined how many calories a raw male would provide, and then compared that with the number of calories in early humans’ other dinner options, such as mammoth, boar, and deer. He reveals his findings on Gastropod, which include a macabre organ-by-organ guide to the human body—useful for anyone who’d like to try cannibalism but is worried about their weight.
Cannibalism became increasingly taboo in modern history, as mainstream religions have typically frowned on the practice, labeling it as barbarous and driving it almost to extinction—while simultaneously using the accusation of man-eating as justification for colonial exploitation. Questionable morality aside, there are good reasons to avoid eating members of our own species. One of the bizarre medical mysteries of modern times is kuru, a fatal neurodegenerative “laughing” disease that began killing large numbers of the Fore people in the 1960s. This episode, we talk with Shirley Lindenbaum, the anthropologist whose fieldwork, carried out in remote Papua New Guinea in her 20s, uncovered the cause of the disease in the cannibalistic Fore funerary rituals. Today, however, despite the risks and the taboo, one kind of cannibalism is having a resurgence among celebrities and natural-birth advocates alike: placentophagy. It may be endorsed by Kim Kardashian West, but is there any scientific evidence behind the trend? Answers to all this and much more in this week’s episode!
This post appears courtesy of Gastropod, a podcast co-hosted by Cynthia Graber and Nicola Twilley that looks at food through the lens of science and history.


Between 7,000 and 9,000 years ago—during the middle Holocene—the Four Corners area went through a slow but dramatic climatic shift. As the region became hotter and drier, stream and lake levels dropped, and larger game animals and firewood became harder to find. Indigenous communities had to rely on foods that were less nutritious and took more time to prepare, such as grass seeds and chenopodium seeds, a tiny grain similar to quinoa.
But recently, archaeologists working with local tribes have recognized a surprising addition to these early food sources: Eleven millennia ago, communities at the North Creek Shelter—a rock overhang in southern Utah’s Escalante Valley—began harvesting a unique species of potato. That’s the earliest known use of a potato in North America, and the evidence suggests that the nutritious tuber helped communities adapt to climate change during the middle Holocene, even as other food sources disappeared.
Now, this potato—and the Westerners who still harvest it—may hold answers to present-day challenges that face the region, including climate change, food security, and reestablishing tribal connections to the land.
* * *
Archaeologist Lisbeth Louderback researches the effects of the middle Holocene climate shift on humans’ diets by sifting through plant remains from ancient hearths. By microscope, she identifies the charred seeds, grains, and other leftovers from meals cooked thousands of years ago to understand how people’s diets reflected their shifting worlds. But one set of samples from fire pits built at North Creek Shelter defied her expertise, resembling nothing she could identify. So Louderback tried something unusual: She analyzed the residues left over millennia on manos and metates, grinding stones and surfaces used to prepare corn and other foods, found at the site. She and her colleagues came to a surprising realization: Almost 11,000 years ago, people in the region were eating a species of potato, known as Solanum jamesii, that’s different from the potatoes widely eaten today, Solanum tuberosum.
Every type of potato purchased in the store today, whether white, red, or purple, is Solanum tuberosum. The Four Corners species, though, is unique, producing small, hardy tubers from long underground shoots, explains Bruce Pavlik, director of conservation at the University of Utah’s Red Butte Garden. The plant, which has white flowers shaped like stars, grows in the mid-elevation pinyon-juniper and oak forests of the Mogollon Plateau in central Arizona and New Mexico. Though starch grains turned up at North Creek Shelter, it’s not a common plant in Utah.
After identifying the starch grains on manos and metates as Solanum jamesii, Louderback teamed up with Pavlik and living descendants of the region’s tribal communities to understand how it got there, and where the potato lives today. As Louderback surveyed archaeological sites and materials for traces of plants that were on the menu thousands of years ago, Pavlik inventoried the vegetation growing on the landscape surrounding past and current sites of human habitation. The research team—with help from citizen scientists—has found more potato plants growing near places of human habitation in Utah, with further evidence of ancient use tucked into university herbarium collections. As expected, the potato has been widely collected from the Mogollon Plateau, Pavlik says. But it also has been collected in Mesa Verde, Chaco Canyon, the Escalante Valley, and other regions inhabited by ancient peoples.
By talking with descendants of pioneer families and researching the diaries of cavalry soldiers, Louderback and Pavlik learned that pioneers called Escalante Valley “Potato Valley” during the 1800s. Delane Griffin, an Escalante man in his 90s, recollected for the researchers how he and other kids would collect the potato during the Great Depression. Griffin directed the researchers to a place where he’d harvested the potato as a child. “Sure enough, it was still there,” Pavlik says.
The evidence suggests that ancient peoples introduced the potato around the Four Corners area. “If you were going on a long journey, these tubers were a perfect food source,” Pavlik says. “And then if you arrived at your destination, the thought might occur to you, ‘I could plant these and come back here sometime in the future and have food.’” Pavlik doubts that the story of how the potato originally came to the Four Corners region is a simple one, though. Over the thousands of years that people have lived in the region, communities have shifted. The very shelter where Louderback first collected manos and metates to analyze has been used intensely and abandoned multiple times over the millennia. “How many events were required to establish a (potato) population, if that’s what happened?” Pavlik wonders. It’s a question he hopes to answer through genetic analysis.
Pavlik cautions that the researchers need to do more work before they can be confident that ancient peoples intentionally brought the potato to these sites. “We want to look at sites that (the potato) isn’t as well as sites that it is,” he explains. “We can’t just be biased and only look in certain places, because it is still a possibility that it’s a remnant of a natural distribution.” Still, the genetics of the plant also suggest a human connection. After monsoon flowering, wild potatoes in the central part of their range produce fruit resembling tiny tomatoes or eggplants. But the potatoes near archaeological sites only reproduce clonally, by growing more tubers. They seem to have lost the ability to use their flowers for sexual reproduction. “It suggests that those Utah populations—the archaeological ones—don’t have all the genes with them they need in order to complete the sexual process,” Pavlik says. “That would be indirect evidence of having been transported because it means they’ve gone through a genetic bottleneck.” He’s testing this idea in greenhouse experiments by crossing potato plants from archaeological sites with potato plants from the Mogollon Plateau and seeing whether they produce fruit.
* * *
Either way, this research has revealed this potato to be an enduring food source, capable of persisting through major environmental changes such as drought, Louderback says. These characteristics make it a valuable food crop for all Westerners in a warming, drying climate. The potato’s genetic diversity also could protect the more common Solanum tuberosum from future blights, so something like the Irish potato famine doesn’t happen again.
Tribal collaboration has been essential to this research. Louderback and Pavlik work with Utah Diné Bikéyah traditional-foods program director Cynthia Wilson to document the occurrence and uses of the potato. Both Louderback and Pavlik support Wilson’s more general goal of reconnecting tribal communities to traditional food resources. “Native Americans have managed the potato for thousands of years,” Louderback says. “It still exists because of them. This is their resource.” Utah Diné Bikéyah is a nonprofit organization that supports the Bears Ears Intertribal Coalition, a group of five tribes working to protect Bears Ears National Monument. Connecting the potato’s current distribution to ancestral history has profound meaning for many of the region’s contemporary tribal inhabitants, some of whom still grow Solanum jamesii today, many others of whom were cut off from traditional food practices through forced relocations. For Wilson, the potato is yet another example of traditional wisdom that’s protected by Bears Ears National Monument, along with petroglyphs, potsherds, and ancestral structures. Wilson learned about the potato from her grandfather. He called it leeyi’ naa’ mááz, or “rolling around under the soil” in the Diné language.
Tribal communities use the potato less now than they historically did, which Wilson attributes to forced removal from their homelands and the arrival of grocery stores. Yet the potato persists in home gardens of Zunis, Hopis, Jemez people, and Navajos, among others. In addition to further documenting how tribes moved through the Four Corners region over millennia, Wilson says, the potato demonstrates how the designation and design of Bears Ears National Monument assists the region’s tribes in once again accessing traditional—and nutritious—food sources. Louderback has found that S. jamesii has twice the amount of protein and calcium and three times the amount of zinc, iron, and manganese as the potatoes most people eat today. And, according to very unscientific trials by Louderback, the hardy spud is quite delicious if roasted in butter and served with salt and pepper. “It was kind of nutty and earthy and it was really fluffy on the inside and crispy on the outside,” she recalls. It can be bitter, especially if eaten raw; the Hopi prepare it with white clay to draw out the bitterness.
The researchers want to make potato plants available to the tribal communities that originally stewarded them, Pavlik says, as well as the communities in which they still exist, such as Escalante. “That’s a place where if you serve the potato there, it would have been a food that’s been served there for nearly 11,000 years,” he says. “There’s nowhere else I can think of where a claim like that could be made. So it really could be pretty cool.”
This post appears courtesy of High Country News.


“The entire paper is an enormous bummer,” Chelsea Wood told me during the annual meeting of the Ecological Society of America in Portland, Oregon, in August. She was referring to research she had published in June as part of a special theme issue, which she had helped to compile and edit, of the Philosophical Transactions of the Royal Society B. The issue explored the link between biodiversity and infectious disease, and Wood had just presented her findings before a packed room. It was a tough audience, she said.
“The news that ecologists want to hear is that biodiversity is something that we can market as an intervention that has co-benefits—an intervention that pays for itself,” she said. “And we don’t see any evidence of that in our data set. Not even a hint of it.”
Wood is an assistant professor at the University of Washington’s School of Aquatic and Fishery Sciences, and she is a prominent voice on one side of what has become a contentious and heated ecological question: Is biodiversity beneficial to human health or not? The argument centers around what’s known as the “dilution effect”—the idea that preserving and protecting an abundance of other species can help to dilute the risk of diseases spreading among humans.
The notion was first proposed in 2001 in relation to the spread of Lyme disease, and in some circles, it has become settled science. “The evidence is already in,” Newsweek declared in 2009. “The loss of biodiversity is itself a threat to public health.” In 2015, the Earth Island Journal asserted unequivocally that “biodiversity limits disease outbreaks among humans and wildlife.”
Proponents of the theory believe the dilution effect is widespread, and they champion human-health policies that include conservation initiatives. But critics say the evidence for this remains thin, and they call such ideas both panglossian and irresponsible. Both sides have sizable support among researchers, and the back and forth in scientific journals has become acrimonious at times.
That’s how science is supposed to work, of course. The problem is that very little coverage of this debate has migrated into the popular-science press—and in that sense, science journalists are succumbing to the same sort of publication biases that we bemoan among scientists themselves: Studies that support appealing ideas get widespread coverage, while ambiguous findings or ones that counter “feel-good” stories are ignored.
Presenting a false balance, as is often seen with climate-change reporting, is problematic. But so, too, is presenting a false consensus. It undermines the public’s understanding of science while providing ammunition to those claiming the media is biased and unreliable.
Intuitively, the dilution effect makes sense.
Take West Nile virus, for example. It’s a bird disease, but people can catch it when bitten by Culex mosquitoes. Where there are more birds overall, it would be statistically less likely for a female mosquito to bite an infected animal, reducing the number of infected mosquitoes buzzing around. Similarly, mosquitoes that are infected would be more likely to come across and seek out a blood meal from another bird than from a human. And some bird species will be more resistant to the infection, helping to check its overall spread.
All of these mechanisms would lead to a “dilution” of the disease and, ultimately, a reduction in the number of human disease cases—or at least that’s the theory. It’s also exactly what researchers in a 2008 study published in PLOS One found. When they compared bird-species richness to West Nile transmission during the 2002 outbreak in the eastern United States, they found a strong, negative correlation between the diversity of birds and cases of the virus. And they aren’t the only ones to find evidence for a dilution effect. A 2015 meta-analysis boldly states so in its title: “Biodiversity Inhibits Parasites: Broad Evidence for the Dilution Effect.”
Jason Rohr is an associate professor at the University of South Florida and senior author of that paper. “There was general support for the hypothesis that when you increase biodiversity you get decline in parasites per host or in parasite prevalence,” he told me in an interview. The results are convincing enough that Rohr says conservation should be seen as a prophylactic. Much like exercise and a balanced diet are considered effectively safe methods of supporting health, “general biodiversity conservation,” he said, “should be approached as a general proactive strategy to disease management.”
But that’s where critics of the dilution effect strongly disagree. They see the literature very differently.
“A key element of their argument is that our previous meta-analysis, which failed to find unequivocal support for dilution effects in zoonotic disease systems, should be discounted,” write Daniel Salkeld and his colleagues in a critique of the paper by Rohr and his colleagues. Their work focused in on just human pathogens, and found “weak support at best for the dilution effect” and “evidence of publication bias toward publishing reports of a negative relationship between biodiversity and disease,” Salkeld argued.
The bitter debate isn’t just about interpretation of results. Critics also say the research to date is flawed because it has only examined a small subset of human pathogens. Studies focus on accessible disease systems like Lyme and West Nile virus—ones that Wood says almost exclusively affect affluent Westerners. Rohr and his coauthors included just 14 human diseases, for example (though the analysis included schistosomiasis, which predominantly affects the developing world).
By broadly claiming the majority of infectious diseases will follow the patterns of West Nile and Lyme, Wood says Rohr and other proponents are trying to apply a “pretty thinly supported idea that’s been demonstrated only in diseases of rich people” to diseases of poverty in developing countries.
That’s why she and her colleagues decided to focus on the infectious diseases tracked by the World Health Organization’s global burden of disease database instead. “We felt like it was really important to figure out whether dilution worked for these more-important diseases because people were starting to call for conservation as this intervention,” Wood told me, “and we felt that was really irresponsible because the evidence is so thin.”
Wood and her colleagues examined the change in disease burden for 24 infectious diseases over two decades, including notorious killers like HIV/AIDS, tuberculosis, and malaria. Using per-person disability-adjusted life years as their measure of lethal and nonlethal health impacts, they examined a variety of potential drivers of these diseases in 60 intermediate-sized countries, including forestation and urbanization—which Wood considers proxies for biodiversity tied to conservation action—as well as a calculated measure of biodiversity per unit area.
“Contrary to the dilution-effect hypothesis, increases in biodiversity over time were not correlated with improvements in human health, and increases in forestation over time were actually associated with increased disease burden,” the authors concluded. Or, as Wood puts it, “If anything, conservation is potentially a problem for infectious-disease transmission.”
Of course, Rohr and other proponents were quick to find flaws with this analysis.
“I could go on and on and on and talk about all of the factors that are conflated with biodiversity by using urbanization or deforestation as a proxy,” he told me. To Rohr, the study doesn’t “provide a real solid test of the hypothesis relative to studies that do a better job of isolating the effects of biodiversity.”
And so continues the disagreement among scientists—a debate that remains invisible for many general readers.
Indeed, based on media coverage over the last 10 years, it would seem the dilution effect is universally supported.
“A new study shows that biodiversity serves as a bulwark against the transmission of diseases and parasites,” Conservation Magazine’s Sarah DeWeerdt writes in an article covering the 2015 meta-analysis by Rohr and his coauthors. DeWeerdt’s piece was titled “The Bloodsucking Consequences of Biodiversity Decline.”
Such stories sometimes include conditional words like “may” and “might,” but they often fail to provide voices of critics like Wood. And while there have been instances of balanced coverage—by NPR, for example, or TakePart, or a nuanced piece, which included Wood’s perspective, published by Smithsonian magazine in May—such articles tend to take their inspiration from new research and papers that support the dilution hypothesis, relegating Wood and her colleagues to counterpoints instead of leads.
Sometimes, counterpoints are lacking altogether. “Biodiversity protects ecosystems against infectious diseases, researchers have concluded,” a 2010 Nature News story boldly stated. The only outside comment comes from Conservation International senior scientist Will Turner, who did not doubt the universality of the findings. “The clear message is that we degrade ecosystems at our own peril,” Turner told Nature. Similar coverage came from NPR.
But the most glaring issue seems to be what studies are chosen for coverage. Those that find evidence for the dilution effect are readily picked up by a diversity of outlets. A search for “dilution effect” brings up article after article on such studies, including coverage in prestigious outlets like The New York Times. Studies that find amplification of disease by biodiversity or no relationship at all are generally overlooked. I could only find these two old pieces from Futurity and Bioscience Technology reporting the meta-analysis that didn’t find support for the dilution effect, and a few articles in outlets like The Scotsman and Farmers Weekly covering a recent paper suggesting biodiversity may actually increase risk of Lyme disease.
“When papers that support the dilution effect come out, they tend to get tons and tons of press coverage,” said Wood. “In contrast, when you’re telling the story that nature is actually dangerous for people, that has less legs.”
The bias in reporting gives the impression that the dilution effect is fairly uncontroversial. But ecologists as a group are far less certain than such coverage would suggest. “We have a lot of people now who are of the opinion that sometimes you see dilution, and sometimes you see amplification, and sometimes neither,” says Skylar Hopkins, a postdoctoral associate with the National Center for Ecological Analysis and Synthesis at the University of California, Santa Barbara. Andrew MacDonald, a National Science Foundation postdoctoral research fellow at Stanford University, echoed this idea. “At least at this stage, there’s a lot more that we need to learn about the ecology of disease,” he said, “and how it relates to diversity.”
Both researchers say they are somewhat frustrated that the press coverage they’ve seen so far is lopsided. “It’s not necessarily exciting to share work that has a whole bunch of caveats,” MacDonald said. “The challenge for science journalists in general is how to talk about scientific uncertainty and how to get that across to the general public, who might be more interested in a yes/no answer,” MacDonald says.
Hopkins adds that hypotheses like the dilution effect need to be handled with extra care. “The argument here is that we can reduce human risk of disease and that’s a really big deal,” she said, “so we don’t want to promise that if that’s not something that can actually be delivered as a solution.”
Ultimately, that’s Wood’s biggest concern, too. “We need to not close our eyes to the fact that conservation can sometimes increase human disease risk,” she said.
* * *
Of course, Wood would be upset if her research was used as an excuse to halt conservation efforts. “I didn’t get into this business because I want people to cut down forests,” she emphasizes. But she also fears that claiming conservation will broadly reduce disease could lead to unintended consequences.
“My position is not that the dilution effect never happens—that is absolutely the opposite of what I think,” said Wood, pointing out several studies that have found evidence. “But there’s also a whole bunch of parasites that show the opposite response.” Wood says she “would be 100 percent behind efforts to use conservation to control infectious diseases,” if those efforts put the time and resources into examining how their actions affect the diversity of human pathogens in the area.
“Our argument is that you really need to understand the whole spectrum of responses of disease to environmental changes, because otherwise, conservation is a roll of the dice in terms of public health,” she explained. “We want people to go into conservation with their eyes wide open so that they can plan for potential collateral impacts.”
Any associated costs with implementing disease monitoring or prophylactic measures given that risk, Wood says, are “very minor when compared to the potential damage you could do through a conservation project that initiates an epidemic.”
It’s a sentiment that journalists would be wise to heed as well. After all, when covering hypotheses like the dilution effect that are hotly debated amongst scientists, the costs of diligent coverage are minor compared to the potential harms of shoddy reporting.
One-sided coverage not only undermines the media’s credibility and the public’s trust in the process of science. Such weighted stories imply advocacy, even if carelessly. And if, as Wood supposes, an outbreak occurs after our tacit approval, then we, too, would share in the blame for the lives destroyed by it.
This post appears courtesy of Undark Magazine.


In 1986, the social psychologist David Sears warned his colleagues that their habit of almost exclusively studying college students was producing a strange and skewed portrait of human nature. He was neither the first to make that critique, nor the last: Decades later, other psychologists noted that social sciences tended to focus on people from WEIRD societies—that is, Western, educated, industrialized, rich, and democratic. The results of such studies are often taken to represent humanity at large, even though their participants are drawn from a “particularly thin and rather unusual slice” of it.
The same concerns have been raised in virtually every area of science that involves people. Geneticists have learned more about the DNA of people in Europe and North America than those in the rest of the world, where the greatest genetic diversity exists. The so-called Human Microbiome Project was really the Urban-American Microbiome Project, given that its participants were almost entirely from St. Louis and Houston.
Neuroscience faces the same problems. When scientists use medical scanners to repeatedly peer at the shapes and activities of the human brain, those brains tend to belong to wealthy and well-educated people. And unless researchers take steps to correct for that bias, what we get is an understanding of the brain that’s incomplete, skewed, and,  well, a little weird.
Kaja LeWinn, from the University of California, San Francisco, demonstrated this by reanalyzing data from a large study that scanned 1,162 children ages 3 to 18 to see how their brain changed as they grew up. The kids came from disproportionately wealthy and well-educated families, so LeWinn adjusted the data to see what it would look like if they had been more representative of the U.S. population. That's called “weighting,” and it’s a common strategy that epidemiologists use to deal with skews in their samples. As an easy example, if you ended up recruiting twice as many boys as girls, you’d assign the girls twice as much “weight” as the boys.
When LeWinn weighted her data for factors such as sex, ethnicity, and wealth, the results looked very different from the original set. The brain as a whole developed faster than previously thought, and some parts matured earlier relative to others.
Natalie Brito, from New York University, says that this study “clearly shows how our interpretation of brain development changes based off who is being represented within the sample.” She adds that most neuroscientists would acknowledge or agree that representative samples are a good thing, but that there are practical reasons why such samples are hard to get. Most obviously, brain-scanning studies are very expensive, so most of them are small and rely on “samples of convenience”—that is, whoever’s easiest to recruit.
“Neuroimaging research is also complex and difficult to conduct, and because of this, I think there is a tendency to focus on its technological aspects,” says Duke Han, from the University of Southern California. That’s symptomatic of a problem in neuroscience that I’ve written about before: an inclination to focus on the technological innovations that allow us to study the brain, and to forget about the people whose brains are being studied.
Perhaps the brain itself invites this lapse. We intuitively understand that our thoughts and behavior vary considerably from person to person. But when it comes to the lump of gray tissue behind those behaviors, it’s easy to forget that variation. “To a degree, I think there’s a sense that a brain is a brain is a brain,” says LeWinn. “That’s problematic. Everyone’s brain is shaped by their experiences, and we want to capture the diversity of people’s experiences rather than just a few kinds.”
For example, in the study she reanalyzed, around 35 percent of the children had parents with college backgrounds, and around 38 percent had parents who earned more than $100,000 a year. If the sample had been truly representative of the U.S. population, those proportions would have been 11 percent and 26 percent, respectively. And weighting the data to account for these biases produced a different picture of brain development.
Brains get bigger as we get older, before shrinking again during later childhood. In the unweighted data, the brains hit their peak volume at 6 years on average, and their peak surface area at around 12 years. But in the weighted data, the brains hit those milestones 10 months and 29 months earlier, respectively. The pattern of development across the brain also changed. In the unweighted data, three of the brain’s four lobes hit their maximum area from the ages of 12 to 13, with only the parietal lobe peaking earlier at around 10 years. But the weighted data showed more of a wave of maturation, from the back of the brain to its front, and going from 9 years to 11.
“These results shine a much-needed spotlight on an issue that does not receive enough attention in neuroimaging research—the lack of diversity among study participants,” says Han. “Unless these issues are adequately addressed, it would be wise to show temperance in discussing the implications of a study.”
Jim Coan, from the University of Virginia, learned the same lesson in his own work. A decade ago, he put 16 women in a brain scanner, promised to give them an electric shock, and looked at parts of their brains that respond to threats. He found that these areas are less active if the women held the hand of a stranger, even less active if they held their spouse’s hand, and less active still if they were in an especially happy relationship. “I had to raise $30,000 to do that experiment and everyone was white, wealthy, well educated. And yet, we thought: Here’s the story,” he says. “By yourself, you’re maximally responsible for meeting the demand of the threatening situation so you have more of a threat response. If you’re with your trusted romantic partner, you’re minimally responsive because you outsource.”
Years later, he got more money to do a bigger and more representative study of racially and socioeconomically diverse people drawn from the local community. “And the findings changed,” he says. The romantic partners still reduced the threat response, but a stranger’s hand had no effect at all. Why? Perhaps it’s because, as he showed in another study, the wealth of the neighborhood you grow up in affects the way your brain weighs up rewards and threats. “This shouldn’t surprise anybody,” he says. “The context in which you develop shapes the way your brain functions and probably the way it’s structured.”
Neuroscientists are increasingly grappling with this self-evident truth. Brain-scanning studies are getting bigger, and researchers are making more of an effort to recruit samples that are at least representative of the local community—if not America as a whole.
The Adolescent-Brain Cognitive Development study—the largest study of childhood brain development in the United States—is perhaps doing the best job. It’s looking to recruit around 11,500 children, ages 9 and 10, and follow their brain development over the next 10 years. The plan is to get a truly representative sample, and to deal with any small skews with the same weighting approach that LeWinn uses. That has many advantages, says Wes Thompson, from the University of California, San Diego, who is involved in the study. You not only can see what the average American brain looks like as it grows up, but you can see how different subgroups differ from this population-wide norm, and how individuals differ from their particular subgroup.
“Since bigger studies are now coming online, this is the time to think about the sampling issue,” says LeWinn. “We’re finally doing studies that are large enough to get representative samples.”


Americans have a complicated relationship with fear.
On the one hand, we enjoy fear enough to dedicate a holiday to it. This year, we will spend an estimated $9.1 billion celebrating Halloween. Horror films gross nearly half a billion dollars per year, and are known in Hollywood to have the best return on investment in the movie business. Quasi-dangerous activities like roller coasters are a big industry as well, following Hunter S. Thompson’s famous exhortation, “Faster, faster, until the thrill of speed overcomes the fear of death.”
These pursuits are occasions of “fake” fear. They simulate frightening circumstances that lie outside the realm of ordinary life, providing a fun shot of adrenaline without putting anyone in actual danger.
Real threats, however, are far less enjoyable. Not even roller-coaster fans look forward to losing their car’s brakes on a steep hill. To enjoy genuine mortal danger is considered abnormal: Indeed, in psychology, the “fear-enjoyment hypothesis” holds that pleasure from authentic fear increases along with sociopathic traits.
Given that real fear can be scarring and unpleasant, there’s a temptation to believe that the best way to deal with it is to avoid it at all costs. But science and philosophy often suggest otherwise. Fear can be one of the great sources of personal improvement. In particular, fear can help people cultivate several classic virtues that religious figures, sages, and secular moral traditions have all seen as essential for living a well-ordered life.
One such virtue is courage. The University of British Columbia psychologist Stanley Rachman, a leading expert on fear, has studied people in the world’s most dangerous professions, from bomb defusers to paratroopers. He has concluded that courage is misunderstood when it is defined as complete fearlessness. In his book Fear and Courage, Rachman makes the case that courage is not the absence of fear, but the decision to go forward in spite of it. Brave people are not merely numb to danger or discomfort; they feel and acknowledge fear, and just refuse to allow it to dominate their behavior.
By this definition, fear is not the antithesis of courage. It is courage’s necessary precondition. The German philosopher Josef Pieper writes in The Four Cardinal Virtues that a man can only show courage when he “walks straight up to the cause of his fear and is not deterred from doing that which is good.” For Pieper, this final qualifier is important: The confrontation of fear must be oriented toward the common good. In practice, this could mean confronting your fear on behalf of people weaker than you—for instance, risking physical harm to bring someone else to safety in an emergency, or speaking up to stop bullying. (This rules out extreme thrill seeking and other fear-provoking situations that are ultimately just entertainment.)
Fear also can signal where people need to do moral work on themselves. Buddhists, for example, believe that fear is a sign of attachment. According to the Buddha, the key to freedom from fear is to abandon “passion, desire, fondness, thirst, fever, and craving for sensuality.” There is a famous Zen Buddhist story about a band of samurai who ride through the countryside causing destruction and terror. As they approach a monastery, all the monks scatter in fear, except for the abbot. The samurai enter to find him sitting in the lotus position in perfect equanimity. Drawing his sword, the leader snarls, “Don’t you see that I am the sort of man who could run you through without batting an eye?” The master responds, “Don’t you see that I am a man who could be run through without batting an eye?”
For non-Buddhists, indifference to death might entail a bit more nonattachment than is optimal. But virtually every major faith and moral tradition preaches the same core principle. Christians and Jews see a similar connection between fear and the deadly sin of pride—“an excessive desire for one’s own excellence,” in the formulation of Thomas Aquinas. Modern research might back a connection like this up: Rankings of Americans’ top fears consistently reveal that one of their most prevalent social fears is public speaking. Presumably, the explanation is that people are abjectly terrified of humiliation in front of others. As Rousseau phrases it in his Confessions: “I was not afraid of punishment, I was only afraid of disgrace; and that I feared more than death, more than crime, more than anything else in the world.”
A few jitters before a big presentation is one thing. But a paralyzing horror of being judged by others? That seems to be a bright-red warning that too much of your identity is tied up with others’ esteem for you.
If fear is often a weed growing over the roots of attachment and pride, what can we do? Try taking inventory of your daily anxieties and worries. See which ones boil down quickly to your wealth, your looks, your reputation, your social status, or your influence. Then attack these inordinate attachments—and be grateful for the fear that led you to them.
John Bunyan wrote in The Pilgrim’s Progress in 1678 that fear “keeps the soul tender.” Properly understood, fear is good. So this Halloween, don’t settle for fake fear. Enjoy the haunted house, but then embrace the real thing.


The first thing to know about crows is that a group of them is called a murder.
In America, crows count as a Halloween decoration, like skeletons and mini-gravestones. Homeowners perch plastic ones in their trees to instill fear in passersby. People in many cultures consider the crow to be an omen, a harbinger of war and death. In Islamic hadith, reports about Muhammad’s sayings and practices, crows are one of the five animals “for which there is no blame on the one who kills them.” On the Faroe Islands, virginal women once had to throw a stone, a bone, and a clump of dirt at a crow for some reason. Farmers literally erect mannequins in their fields to scare them.
Can we give crows a break?
The world’s treatment of crows is grossly unfair. When I was a kid, my parents told me about their friend Linda, an elementary-school teacher in Illinois. One day, Linda and her class decided to adopt a zoo animal as a service project. When she was reading to them from a list of available animal adoptees, “crow” came up as an option. “Who would want a stupid old crow?” one little boy asked. The class adopted a polar bear instead.
When I heard this story as a child, it devastated me. I knew what it was like to get picked last. I would feel a pang of pity whenever I saw a crow perched on the power lines behind my house or pecking through the yard. Stupid old crow, I would say to myself sadly. But in fact, crows are not stupid. That little boy in Illinois can eat crow—because these birds are really, really smart.
Crows, along with magpies, jays, and ravens, are members of the family Corvidae, a group known for two things: exhibiting complex behaviors, and having massive brains. A New Caledonian crow named Betty once made a TED Talk audience go bananas by bending a wire to make a hook. And researchers believe the Hawaiian crow is likely to have the same talent.
Scientists Have Found Another Crow That Uses Tools
“One of the reasons I got so excited by them was because hooked-tool use is something that only New Caledonian crows and ourselves have invented,” said Alex Taylor, a senior lecturer in psychology at the University of Auckland. “The great apes haven’t invented using hooks, and [humans] only invented using hooks probably around 100,000 years ago. It’s possible that crows beat us to that bit of technology.” Taylor says the birds have a high encephalization quotient, which means their brains are really big for their bodies. The crows he works with aren’t spooky; they’re observant: “Basically it feels like you’re constantly being observed and assessed,” Taylor says. “You’re dealing with a very smart and watchful animal.”
Kevin McGowan, an ornithologist at the Cornell Lab in New York, told me that the crow’s spooky reputation is pretty unwarranted. “It’s because they’re black and they’re scavengers,” McGowan said. Their bad rap started in northern Europe, where there are no vultures, so ravens and crows were always the first to show up to snack on animal carcasses. When Europeans came to North America, they brought their crow prejudice with them. “That whole combination of being near death, not having a very pretty song, that was all a big negative stigma for these guys to overcome.” He also wishes The Birds had never been made.
“Crows are beautiful,” McGowan says. “A big adult in the sunshine is beautiful, the way the light shines off their wings ... They’re just a really nice-looking bird.”
But sometimes crows are victims of mistaken identity. Grackles look like crows, except they are smaller and shinier, more of an iridescent purple. They bully other birds at feeders, and they can damage crops. They also make a horrible sound, like a screen door closing, or a rusty gate swinging open. Grackles are the birds that you hear and you think ugh. Crows, though. Crows are matte black, and they make a sharp, clear caw caw. Ravens, like the one who barges into the chamber in the Edgar Allan Poe poem, are also not crows. They are much larger, roughly the size of a red-tailed hawk.
While humans might sometimes conflate crows with their avian relatives, research shows that crows rarely forget a face. John Marzluff, a wildlife biologist at the University of Washington, once conducted an experiment with two masks—a “dangerous” mask and a “neutral” mask. His assistants wore the dangerous mask to capture and band several crows, and for months afterward, he and his students wore both disguises around campus: The ones in the dangerous masks were repeatedly scolded by crows, while the ones in neutral masks were left alone. The crows also appeared to warn their friends of the danger at hand.
They have “great family values,” McGowan told WBUR in 2015. “They do neighborhood watch. They help each other out.” McGowan said. “They are very interested in working together to make the world a safer place for other crows.”
Kaeli Swift, a Ph.D. student at the University of Washington, researches crow “funerals.” On her blog Corvid Research, Swift writes that crows “appear to respond strongly once they discover a dead member of their own species.” They use death as a teachable moment, gathering around their fallen comrades in an attempt to identify potential threats. It’s something only a handful of nonhuman animals do.
And they can be generous gift givers, too: In 2015, an 8-year-old girl in Seattle named Gabi Mann started feeding her neighborhood crows scraps of food. In return, they left her dozens of little tokens—buttons and paper clips and pieces of colored glass. Gabi told the BBC that her favorite gift from the crows was a tiny heart charm. “It's showing me how much they love me,” she said.
It’s high time we give these resourceful, community-minded creatures some love in return. McGowan, who has spent more than 30 years studying and teaching people about crows, says he’s working to do just that. “I do try to tell people they’re not evil birds,” he said. “They’re just trying to make their way and do the best they can.” He tells me that this year at the Cornell Lab, the tagging color is orange, which means every crow identified by his team will get a little pumpkin-colored tag pinned to its wing. “It looks real nice, the orange with the black feathers,” McGowan says. “It looks really nice.”


In 1928, after returning from a countryside holiday and examining a stack of petri dishes that he had left in the sink, British chemist Alexander Fleming discovered a new type of bacteria-killing mold. From that mold, he isolated a chemical called penicillin, and ushered in the modern antibiotic era—an age when humans could finally keep infectious diseases at bay. But in 1945, two years after penicillin became widely used and shortly after Fleming won a Nobel Prize for its discovery, he issued a stark warning against overusing these wondrous chemicals. “The thoughtless person playing with penicillin treatment is morally responsible for the death of the man who succumbs to infection with the penicillin-resistant organism,” he told The New York Times.
His words were prophetic. The year before, scientists had identified penicillin-resistant strains of Staphylococcus aureus—a bacterium that commonly lives on our skin and in our noses, but sometimes causes life-threatening infections. As penicillin became more widely used, these resistant strains also became more common. To deal with these incipient superbugs, scientists turned to methicillin—a chemical relative of penicillin. But its usefulness was also short-lived. A year after it made its way into British clinics in 1959, Margaret Patricia Jevons isolated three strains of methicillin-resistant Staphylococcus aureus, or MRSA.
MRSA is now a global problem, and has become something of a poster child for the superbug threat. It supposedly showed how bacteria can quickly evolve to resist a drug that comes into wide use—a process that’s illustrated in the video below, the second in a series of online films produced by HHMI Tangled Bank Studios, which adapt the stories in my book, I Contain Multitudes.

But the MRSA origin story always had a few glaring plot holes. For a start, the three initial strains of resistant staph all came from patients who had never been exposed to methicillin, and who were treated in a hospital that had only ever used the drug once. On top of that, MRSA appeared in India and some Eastern European countries before those nations started using methicillin. How exactly did the bacteria evolve to resist a drug that they had never actually seen?
Catriona Harkins and Matthew Holden at the University of St. Andrews have the answer, in a study that turns the history of MRSA on its head, and makes it even scarier than before.
They sequenced the DNA of 209 MRSA samples that were collected between 1960 and 1989, including the earliest resistant strains ever identified. By comparing these strains and reconstructing their evolutionary history, the team calculated that they all descended from a common ancestor that first acquired the ability to resist methicillin in 1946—13 years before people started using the drug to treat infections. “Methicillin use was not the original driving factor in the evolution of MRSA as previously thought,” they write.
That original driving factor was, ironically, penicillin. It turns out that mecA, the same gene that allows staph to shrug off methicillin, also confers some measure of resistance to penicillin. When penicillin became widely used in the 1940s, it likely fueled the rise of staph strains that carried mecA, and were already resistant to methicillin. Initially, those strains were rare—despite its benefits, mecA doesn’t spread easily between staph strains, and slows the growth of the microbes that carry it. But as soon as methicillin came into use, the advantages of carrying mecA outweighed the disadvantages, and the gene became more common.
So bacteria can begin evolving resistance to antibiotics that they haven’t even encountered yet. And new drugs can be neutralized by adaptive genes that are lurking in the environment, waiting for the chance to rise to the occasion. As Hsu Li Yang from the National University of Singapore writes, “Antibiotic resistance is a web of unintended consequences, rather than a simplistic cause-effect model that we often find (too much) comfort in.”
We might be able to predict some of these unintended consequences by routinely sequencing the DNA of wild bacteria, identifying the resistance genes that are already out there. “However, this will be difficult because it is not currently possible to confidently predict resistance from genome-sequence data alone,” says Jessica Blair from the University of Birmingham. “We can only detect resistance mechanisms we already know about.”


Twenty-five years ago, James Anderson discovered a fungus that expanded the possibilities of life on Earth.
It was a single fungus of the genus Armillaria, weighing an estimated 22,000 pounds and spread over a remarkable 15 hectares. The organism had been growing for around 1,500 years, more than a millennium before the land under which it grew even became the state of Michigan. When Anderson and his collaborators wrote it up in Nature, they suggested it was “among the largest and oldest living organisms” in the world.
This suggestion, in its use of superlative, set off a competition—naturally. Scientists all over the world were soon hunting for Armillaria, or honey mushrooms, in their own forests. The title of the largest fungus in the world eventually went to one in the Malheur National Forest in Oregon: nearly 1,000 hectares big, as many as 8,650 years old. This “humongous fungus,” as it’s sometimes called, is by some counts still the largest living organism ever found.
Fungi normally grow as mycelia—soft, white, cottony tufts that you may have seen on food kept in the fridge too long. Some of them also form mushrooms. But Armillaria, somewhat uniquely, can also grow thick, black, rootlike rhizomorphs whose networks can extend miles through the soil in search of wood to eat. The rhizomorphs, scientists think, are what allow a single Armillaria organism to get so big. A comprehensive new genetic study takes up the question of how Armillaria got its rhizomorphs.
“Ever since I since I was a graduate student, I wanted to do exactly the study that was just published,” says Anderson, who is now a professor of biology at the University of Toronto. Anderson contributed a couple of genomes for the study, but the bulk of the research and analysis was done by György Sipos and László Nagy, of the University of Sopron and the Hungarian Academy of Sciences, respectively.
Sipos and Nagy not only sequenced four species of Armillaria, but they also pinpointed genes active in the fungus’s rhizomorphs and mushrooms. To identify those genes, they had to figure out how to grow at least one species of Armillaria in a lab.
The rhizomorphs were the easy part. Once Armillaria took to their growing medium of rice, sawdust, tomato, and orange—“this fungus has really weird tastes,” notes Nagy—they spontaneously formed rhizomorphs. The mushrooms were much trickier. They had to trick the fungus into thinking it was autumn, which they did by moving their fungi to colder temperatures with progressively less light in Nagy’s lab. Sipos says his colleagues “did a really excellent job. It used to be be difficult to make this fungus produce the mushroom.” They succeeded in getting one species—Armillaria ostoyae, also the species of the giant Oregon fungus—to produce mushrooms.
All that trouble paid off though. When the team got the sequencing data back, they noticed that the same networks of genes appear to be active in both the fungus’s rhizomorphs and its mushrooms. It suggests one potential evolutionary origin for rhizomorphs in this genus: Armillaria could have gained its rhizomorphs—and consequently its ability to spread so wide—by co-opting genes originally used to grow mushrooms. Nagy speculates the rhizomorphs may be akin to mushroom stems that failed to sprout and grow a cap, instead growing long and thin underground.
The rise of Armillaria has come at the expense of trees. The fungus actually grows into trees and spread under the bark. At first they digest living wood and when they’ve done enough damage, they continue to feast on the dead wood. “You can basically see entire hills wiped out, entire forests wiped out,” says Nagy. You can’t see much of the humongous fungus in the Malheur Forest in Oregon since the Armillaria is mostly underground, but you can see all the trees it has killed.
Armillaria as a genus not a particularly picky eater either, and it attacks all sorts of plants. Understanding how the fungus spreads could impact many agricultural industries. For example, Kendra Baumgartner, a plant pathologist at the U.S. Department of Agriculture, studies Armillaria that specifically attack California vineyards. She was ecstatic to see the new study, which also catalogues the genomes, proteins, and active genes in Armillaria. “They generated an incredible amount of data,” she says. When we spoke last week, she told me she had the article’s official publication date marked on her calendar, so she could start digging through the data as soon as it’s out.
How a Guy From a Montana Trailer Park Overturned 150 Years of Biology
This fall, Anderson went back to the Michigan fungus whose discovery he first reported in the 1990s. He is also sequencing this fungus now—specifically, different parts of the same fungus—in hopes of understanding how it has mutated over its 1,500 years of life. DNA-sequencing technology has come a long way in 25 years.
But there is another thing Anderson told me he wishes he could do that he knows he never will. For all the estimates of how big Armillaria can grow, no one has really seen it in full. “I wish all of the substrate”—the soil and matter the fungus grows in—“would be transparent for five minutes, so I could see where it is and what it’s doing. We would learn so much from a five-minute glimpse.”


To many cosmologists, the best thing about neutron-star mergers is that these events scream into space an otherwise close-kept secret of the universe. Scientists combined the gravitational and electromagnetic signals from the recently detected collision of two of these stars to determine, in a cleaner way than with other approaches, how fast the fabric of the universe is expanding—a much-contested number called the Hubble constant.
In the days since the neutron-star collision was announced, Hubble experts have been surprised to find themselves discussing not whether events like it could settle the controversy, but how soon they might do so.
Scientists have hotly debated the cosmic-expansion rate ever since 1929, when the American astronomer Edwin Hubble first established that the universe is expanding—and that it therefore had a beginning. How fast it expands reflects what’s in it (since matter, dark energy, and radiation push and pull in different ways) and how old it is, making the value of the Hubble constant crucial for understanding the rest of cosmology.
And yet the two most precise ways of measuring it result in different answers, with a curious 8 percent discrepancy that “is currently the biggest tension in cosmology,” said Dan Scolnic of the University of Chicago’s Kavli Institute for Cosmological Physics. The mismatch could be a clue that cosmologists aren’t taking into account important details that have affected the universe’s evolution. But to see if that’s the case, they need an independent check on the measurements.
Neutron-star collisions—newly detectable by the Laser Interferometer Gravitational-Wave Observatory (LIGO) and Virgo detectors—seem to be just the thing.
“This first [collision] gives us a seat at the cosmology table,” Daniel Holz, an astrophysicist with the University of Chicago and LIGO who was centrally involved in the new Hubble measurement, said in an email. “And as we get more, we can expect to play a major role in the field.”
In an expanding universe, the farther away an astronomical object is, the faster it recedes. The Hubble constant says how much faster. Edwin Hubble himself estimated that galaxies move away from us 500 kilometers per second faster for each additional megaparsec of distance between us and them (a megaparsec is about 3.3 million light-years). This was a gross overestimate; by the 1970s, astrophysicists favored values for the Hubble constant around either 50 or 100 kilometers per second per megaparsec, depending on their methods. As errors were eliminated, these camps met near the middle. However, in the past year and a half, the Hubble trouble has reheated. This time, 67 stands off against 73.
The higher estimate of 73 comes from observing lots of astronomical objects and estimating both distance and velocity for each one. It’s relatively easy to see how fast a star or galaxy is receding by looking at its “redshift”—a reddening in color that happens for the same reason the sound of a receding ambulance’s siren drops in pitch. Correct for an object’s “peculiar velocity,” caused by the gravitational pull of other objects in its neighborhood, and you’re left with its recessional velocity due to cosmic expansion.
Historically, however, it has proven much, much harder to measure the distance to an object—the other data point needed to calculate the Hubble constant.
To gauge how far away things are, astronomers build up rungs on a “cosmic-distance ladder” in which each rung calibrates more-distant rungs. They start by deducing the distances to stars in the Milky Way using parallax—the stars’ apparent motion across the sky over the course of the year. With this information, astronomers can deduce the brightness of so-called Cepheid stars, which can be used as so-called standard candles because they all shine with a known intrinsic brightness. They then spot these Cepheid stars in nearby galaxies and use them to calculate how far away the galaxies must be. Next, the Cepheids are used to calibrate the distances to Type Ia supernovas—even brighter (though rarer) standard candles that can be seen in faraway galaxies.
Each jump from one rung to the next risks miscalculation. And yet, in 2016, a team known as SH0ES used the cosmic-distance ladder approach to peg the Hubble constant at 73.2 with an accuracy of 2.4 percent.
However, in a paper published the same year, a team used the Planck telescope’s observations of the early universe to obtain a value of 67.8 for the current expansion rate—supposedly with 1 percent accuracy.
The Planck team started from the faint drizzle of ancient light called the cosmic microwave background (CMB), which reveals the universe as it looked at a critical moment 380,000 years after the Big Bang. The CMB snapshot depicts a simple, nearly smooth, plasma-filled young universe. Pressure waves of all different wavelengths rippled through the plasma, squeezing and stretching it and creating subtle density variations on different length scales.
At the moment recorded in the CMB, pressure waves with particular wavelengths would have undergone just the right fraction of an undulation since the Big Bang to all reach zero amplitude, momentarily disappearing and creating smooth plasma densities at their associated length scale. Meanwhile, pressure waves with other wavelengths undulated just the right amount to exactly peak in amplitude at the critical moment, stretching and squeezing the plasma to the full extent possible and creating maximum density variations at their associated scales.
These peaks and troughs in density variations at different scales, which can be picked up by telescopes like Planck and plotted as the “CMB power spectrum,” encode virtually everything about the young universe. The Hubble constant, in particular, can be reconstructed by measuring the distances between the peaks. “It’s a geometric effect,” explained Leo Stein, a theoretical physicist at the California Institute of Technology: The more the universe has expanded, the more the light from the CMB has curved through expanding space-time, and the closer together the peaks ought to appear to us.
Other properties of nature also affect how the peaks end up looking, such as the behavior of the invisible “dark energy” that infuses the fabric of the cosmos. The Planck scientists therefore had to make assumptions about all the other cosmological parameters in order to arrive at their estimate of 67 for the Hubble constant.
The similarity of the two Hubble measurements “is amazing” considering the vastly different approaches used to determine them, said Wendy Freedman, an astrophysicist at the University of Chicago and a pioneer of the cosmic-distance ladder approach. And yet their margins of error don’t overlap. “The universe looks like it’s expanding about 8 percent faster than you would have expected based on how it looked in its youth and how we expect it to evolve,” Adam Riess of Johns Hopkins University, who led the SH0ES team, told Scientific American last year. “We have to take this pretty darn seriously.”
The 67-versus-73 discrepancy could come down to an unknown error on one side or both. Or it might be real and significant—an indication that the Planck team’s extrapolation from the early universe to the present is missing a cosmic ingredient, one that changed the course of history and led to a faster expansion rate than otherwise expected. If a hypothesized fourth type of neutrino populated the infant universe, for instance, this would have increased the radiation pressure and affected the CMB peak widths. Or dark energy, whose repulsive pressure accelerates the universe’s expansion, might be getting denser over time.
Suddenly, neutron-star collisions have materialized to cast the deciding vote.
The crashing stars serve as “standard sirens,” as Holz and Scott Hughes of the Massachusetts Institute of Technology dubbed them in a 2005 paper, building on the work of Bernard Schutz 20 years earlier. They send rushes of ripples outward through space-time that are not dimmed by gas or dust. Because of this, the gravitational waves transmit a clean record of the strength of the collision, which allows scientists to “directly infer the distance to the source,” Holz explained. “There is no distance ladder, and no poorly understood astronomical calibrations. You listen to how loud the [collision] is, and how the sound changes with time, and you directly infer how far away it is.” Because astronomers can also detect electromagnetic light from neutron-star collisions, they can use redshift to determine how fast the merged stars are receding. Recessional velocity divided by distance gives the Hubble constant.
From the first neutron-star collision alone, Holz and hundreds of coauthors calculated the Hubble constant to be 70 kilometers per second per megaparsec, give or take 10. (The major source of uncertainty is the unknown angular orientation of the merging neutron stars relative to the LIGO detectors, which affects the measured amplitude of the signal.) Holz said, “I think it’s just pure luck that we’re smack in the middle,” between the cosmic-distance-ladder and cosmic-microwave-background Hubble estimates. “We could easily shift to one side or the other.”
The measurement’s accuracy will steadily improve as more standard sirens are heard over the next few years, especially as LIGO continues to ramp up in sensitivity. According to Holz, “With roughly 10 more events like this one, we’ll get to 1 percent [of error],” though he stresses that this is a preliminary and debatable estimate. Riess thinks it will take more like 30 standard sirens to reach that level. It all depends on how lucky LIGO and Virgo got with their first detection. “I do think the method has the potential to be a game changer,” said Freedman. “How fast this will occur [or] what the rate of these objects will be ... we don’t yet know.”
Scolnic, who was part of SH0ES, said his team’s tension with Planck’s measurement is so large that “the standard siren approach doesn’t need to get to 1 percent to be interesting.”
As more standard sirens resound, they’ll gradually home in on the Hubble constant once and for all and determine whether or not the expansion rate agrees with expectations based on the young universe. Holz, for one, is exhilarated. “I’ve dedicated the last decade of my life in the hopes of making one plot: a standard-siren measurement of the Hubble. I got to make my Hubble plot, and it is beautiful.”
This article appears courtesy of Quanta Magazine.


There’s a decent chance, if I may blow your mind for a moment, that your favorite pumpkin-pie recipe does not contain any actual pumpkin—at least, not the way you think. Scoop that autumnal goop out of a can, even one labeled “100 percent pure pumpkin,” and you just may be living a delicious lie.
The canned pumpkin you buy in the grocery store often contains little to no amount of the bright-orange, jack-o’-lantern kind of pumpkin. So what, exactly, is in there? And does it still count as a pumpkin?
Debating canned-pumpkin labels is becoming something of a perennial exercise. Snopes investigated the debacle last year, building its case on a Food and Wine article that got the whole internet’s harvest knickers in a collective twist. But this trivia tidbit has been bouncing around the web since at least 2012, when Heirloom Gardener revealed in a shocking exposé that the Libby’s brand of canned “pumpkin” contains a squash variety called the Dickinson pumpkin. In fact, Libby’s even acknowledges that they use the “Dickinson variety of pumpkin” on their website.
If you Google the Dickinson pumpkin, it will quickly become clear that no one would ever make it into a jack-o’-lantern. It looks like a pale, slightly misshapen butternut squash. Heirloom Gardener describes the Dickinson pumpkin as “uniform, smooth, tan”—no ridges, no familiar orange hue.
But wait! you cry, desperate to know if, like the Food and Wine author, your “whole life is basically a lie.” “Dickinson pumpkin” has pumpkin in the name. So how is it not a real pumpkin?
It depends who you ask.
In this case, the most relevant definition might come from the FDA, which authorizes food sellers to label as pumpkin any article “prepared from golden-fleshed, sweet squash or mixtures of such squash and field pumpkin.”
“Field pumpkin,” as you might have guessed, is the canonical, jack-o’-lantern type, the embodiment of fall and Halloween and resident of hay-strewn patches across America. It’s actually a subspecies of Cucurbita pepo, along with acorn squash, zucchini, and decorative gourds.
What’s not in the same species as field pumpkin? You guessed it: the dastardly Dickinson.
Okay, but maybe the species the Dickinson does belong to—Cucurbita moschata, to be precise—could also be included under the umbrella term of “pumpkin,” right? This is where the line between pumpkins and squash starts to break down.
Heirloom Gardener claims “there is only one species of ‘true’ pumpkin, the Cucurbita pepo.” Encyclopedia Britannica disagrees, saying that pumpkins are the “fruit of certain varieties of squash (namely, Cucurbita pepo and Cucurbita moschata)”—the field-pumpkin species and the Dickinson species. And if you think the Brits just don’t know what they’re talking about, know that Merriam-Webster is on their side.
The Missouri Botanical Garden takes an entirely different tack, saying that “pumpkin” as a category “really has no botanical meaning, as they are actually all squash.” Where, then, do we draw the line? The first jack-o’-lanterns were carved out of potatoes; does that mean we have to start calling potatoes pumpkins too?
Cindy Ott, who wrote Pumpkin: The Curious History of an American Icon, told me that, up until the 19th century, the words “pumpkin” and “squash” were used interchangeably. “It’s the same damn vegetable!” she said. The difference is all about symbolism. (Or, in some cases, about color: Pumpkin-growing competitions across the country will only consider specimens that are at least 80 percent orange.)
The whole reason canned pumpkin isn’t field pumpkin is that field pumpkin isn’t good to eat. Colonial Americans only ate them when they couldn’t afford anything else. It’s icky. So when the Industrial Revolution came along, according to Ott, “the pumpkin stayed behind on the farm.” Unlike squash, which were sold in markets and remained “part of people’s everyday life, just like how we see squash now,” no one tried to sell pumpkins in the city. They were just used as a cheap alternative to animal feed on small family farms. The pumpkin thus became a symbol of agrarian values, a bright-orange beacon to Americana; what we call a pumpkin comes down to what we decide fulfills that ideal.
As for the stuff that comes out of a can, a spokesperson from Libby’s confirmed to The Atlantic that “Libby’s canned pumpkin contains 100 percent Dickinson pumpkins”—you know, the kind that isn’t even the same species as the big round orange ones. You can even watch a video of Libby’s harvesting their pale, oblong fruit. (Yes, pumpkins are fruit; don’t get me started.)
And to settle any worries that your cans are full of butternut squash, Snopes confirmed that most canned pumpkin is not majority-butternut, despite what much of the internet suggested in its autumnal freakout. So we don’t have to start calling that a pumpkin.
But beyond that, we’ve ended up right back where we started. We may lack any botanical, scientific, or regulatory unanimity, but the popular consensus remains that the field pumpkin is the One True Pumpkin. I’m not about to disabuse myself of this notion just because a bunch of botanists can’t make up their minds.
Adrienne LaFrance contributed reporting to this article.


What new responsibilities should a rich and liberal democracy take on in the 21st century? If you ask almost any American progressive, they will talk about the need to regulate greenhouse-gas emissions and lessen the blow of climate change. Lately, a growing number of conservatives have glommed onto another policy. They suggest that such a democracy should also be pronatalist: It should take children as a virtue and a public good (even a blessing), and it should direct its welfare state toward encouraging human reproduction.
Given that some environmentalists seem to treat low population growth as just one more variable in the climate equation, the two policies may seem incompatible. In fact, they support each other; they may even need each other. Aggressive climate policy is pronatalist.
I’ve been thinking about this because two writers have gone back and forth lately on what they call modern liberalism’s “doom loop.” Derek Thompson identified the idea first, here at The Atlantic, describing a three-step process. Step one: Liberal and richer democracies tend to have lower birth rates and aging populations, which strains their respective welfare states. Step two: Those same democracies admit more immigrants, in order to reinforce their welfare state and beef up their working-age population. But—step three—as an increasing amount of a state’s population becomes foreign-born, its aging population becomes more xenophobic and less supportive of the same welfare state.
And that “leads to stratification, further discontent and an authoritarian turn, which presumably slows growth further, etc., etc., until liberalism goes kaput,” writes Ross Douthat, picking up the idea at The New York Times. Part of what makes the loop so insidious is that—much like a plane’s real-life death spiral—a polity’s knee-jerk instinct to escape the loop only reinforces it.
Thompson names the loop but doesn’t quite describe a way out of it, though he suggests that guiding people out of bigotry is a possible exit in the long term. “The liberal cause,” he says, “requires Americans learning to break the catch-22 of diversity and equality.”
Douthat, seizing on that sentence, says that maybe nations should consider trying to tinker with another variable:
It could also be that there’s a policy mix that would make that combination a little less of a catch-22, and require less in the way of eliminating the inherent human bias in favor of one’s own posterity in order to succeed.
Maybe, for instance, if you cut immigration rates modestly while also spending heavily to encourage old-fashioned procreation, you would have a better chance of getting faster growth while also creating a socioeconomic environment—youthful, future-oriented, optimistic—that makes assimilation easier.
“Youthful, future-oriented, optimistic”—who wouldn’t want a country like that? It even sounds like the United States, once upon a time.
How do we get back there? By addressing the full scope of the problem. Americans have to talk about climate change—because climate change will simply be happening in the background of all of this. If liberalism’s doom loop is one condition of the 21st century, then the accumulating degradation of Earth’s only climate is another. Its political economy is as specific to this century as the unusually high number of old people kicking around is. And it specifically informs the issues here in two ways.
First, the warming of the world will set off more waves of mass migration. No matter how the doom loop churns, global warming will very likely increase the foreign-born population of the northern and richer democracies all by itself. The immigration reporter Dara Lind has argued that there’s a tendency to take diversity’s threat to nationalism for granted, even though racism corrodes democratic solidarity at least as much as a foreign-born population does:
I would love to read a defense of nationalism that takes the threat racism poses to it at least as seriously as the threat diversity does.
The point holds for climate migration as well. As I argued last year, reducing racism and neutralizing the threat it poses to democratic solidarity constitutes climate adaptation as much as raising a seawall or protecting a coastal wetland does.
But here Douthat would insert that racism isn’t the only variable at play in the doom loop; there’s also “the inherent human bias in favor of one’s own posterity.” He proposes pronatalist policy—that is, incentives for procreation and family formation—as a solution.
I think it’s good to have kids; I think taking care of kids gives rise to responsibility, generosity, and selflessness among adults and communities; and I agree that the longer horizons of child-rearing promote a “youthful, future-oriented, optimistic” politics. I also suspect that pronatalist policy will soon make for good U.S. electoral politics: By the time of the next presidential election, the biggest single-age cohort in America—that is, my own, the 1991-er—will be narrowing in on prime parental age. Perhaps our progeny-concerned parents could even be converted into a new and coveted bloc.
But if you want to make American politics more future-oriented, you first need to fill the climate-shaped hole in our future-oriented-ness. As a polity, we have little collective vision for the future, and little public determination to make sure it’s a better place to live.
When you talk to people about why they don’t want kids, they don’t always talk about their aversion to children or child-rearing; they often talk about how bleak they think the future will be. Between the breakdown of the global liberal order and the ongoing degradation of the planet’s climate, the next few decades don’t seem like a particularly sociable place, for them or their hypothetical children. To many potential parents, it feels like older folks are pressing down the accelerator, trying to burn all the gas in the engine before the car goes careening straight off the side of the canyon.
But wait: Haven’t people birthed kids into hopeless environments since, well, forever? How is climate-change doom now any different from doom over nuclear weapons at the height of the Cold War? After all, in 1962, the same year as the Cuban Missile Crisis, the U.S. fertility rate was nearly double its current number. (In fact, there’s some evidence that the crisis itself slightly increased births in states near Cuba or with large military installations.) And the most radical of anti-procreative climate-themed projects, like the Conceivable Future Project, record only dozens of examples of people swearing off child-rearing.
Well, sure. But much of the conservative argument for pronatalist politics respects the fact that cultural changes—and important medical and political advances—have altered childbearing decisions. Isn’t the general anxiety about climate change as a cultural phenomenon—and the lack of political amelioration of it—one of those changes?  Potential parents undertake a complex and often spiritual calculus when they plan their families, when they decide to have zero, or one, or five kids. It seems reasonable to me that if you want to coax people back into having larger families, or families at all, you may have to soften that calculus by assuring them the future will be a good place to live.
Because right now, the future does not seem like a very pleasant place to live at all. Economists who study climate change say that, at best, the phenomenon will exact persistent and troubling costs on the poorest parts of the United States; at worst, it will initiate one of several globally destabilizing crises. Global warming will also degrade Earth in plenty of hard-to-calculate ways, wrecking the gentle rhythm of the seasons and strangling the natural biodiversity of every stream and mountain. This last seems like an impoverishment to which pronatalists should be especially sensitive.
This line of argument is somewhat awkward, because Douthat himself (and many of his fellow pronatalists) does not believe in the bleakness of a climate-changed future. He has written that climate change is not “a crisis unique among the varied challenges we face,” and he has doubted the apocalyptic future that some climate advocates describe. Many conservatives seem to reject climate change in part because of its anti-procreative supporters.
And perhaps many conservatives will never buy into a future that bleak. But could they take seriously that many of their younger peers do? A recent poll taken by, alas, an energy-reform interest group found that a majority of young Republicans, ages 18 to 30, support a carbon tax. Young Americans overall, ages 18 to 34, are more concerned about climate change than other age cohorts.
I go into these details not to make a generational plea but to assert a public reality. For voters, climate change is unmistakably a low-intensity issue: Even though millions of Americans say they care about it, that concern doesn’t seem to change which candidate they support. But global warming can have political and cultural consequences that are not electoral. As our climate problem goes unfixed, as it seems to be intentionally aggravated by federal policy, it corrodes confidence in the future.
Planning for the future has many benefits that do not help the future alone. If you want a society that encourages people to have kids, you must first tell them that you are working to make the future modestly more hospitable. Not a perfect place, not a problem-free place, not a place where everything will be okay. Just a modestly better one. Pronatalist policy and aggressive climate action could reinforce a vision of a healthy, thriving world, full of human beings.
Optimism can break the doom loop—and even if it fails in doing so, at least our grandchildren under the Third People’s Junta will pay less for their flood insurance.


It’s been two years since an epidemic of Zika began in Brazil, three since the largest Ebola outbreak in history erupted in West Africa, eight since a pandemic of H1N1 flu swept the world, and almost a hundred since a different H1N1 flu pandemic killed 50 million people worldwide. Those viruses were all known, but no one knew when or where they’d trigger epidemics. Other diseases, like SARS, MERS, and HIV, emerged out of the blue.
Sick of being perpetually caught off guard, some scientists want to fully catalogue all viral threats, and predict which are likely to cause tomorrow’s outbreaks. The PREDICT project has been doing that for 8 years; with $100 million in funding from the U.S. Agency for International Development, they’ve discovered nearly 1,000 new viruses. The Global Virome Project is even more ambitious. Proposed in 2016, and still existing in concept only, it aims to find and sequence almost all the viruses in birds and mammals that could potentially spill over into humans.
The GVP estimates that around half a million such viruses exist, and finding them would cost $3.4 billion. With that hefty price tag would come security. In lofty language, the project promises to switch the world “from responding to outbreaks to proactively preparing for them” and to “mark the beginning of the end of the Pandemic Era.”
There’s just one problem, say Jemma Geoghegan and Edward Holmes, two virologists based on Sydney. It won’t work.
In a new paper, Geoghegan and Holmes argue that these projects aren’t going to help preempt pandemics, for the simple reason that there are just too many viruses. About 4,400 have been identified; millions more have not, and only a tiny fraction of these could conceivably jump into humans. “The GVP will be great for understanding more about viruses and their evolution, but I don’t see how it’ll help us work out what’s going to infect us,” says Geoghegan. “We’re only just coming to terms with the vastness of the virosphere.”
There are ways of narrowing down the culprit list. Many teams have tried to map geographical hotspots from which diseases are most likely to emerge, pinpointing areas with tropical forests and lots of mammal species. Others have tried to find features in viruses that make it easier for them to spread between people. But having tried this approach themselves, Geoghegan and Holmes argue it’s not very useful.
Partly, that’s because the results of such studies are too broad to narrow down the list of suspicious viruses in a helpful way. Partly, it’s because such work is based on past epidemics—events that are relatively rare, and so difficult to draw reliable patterns from. For example, Saudi Arabia comes out as mostly cold in maps of disease hotspots, and yet it’s where MERS virus recently jumped into humans from an unlikely host: camels. “We’re trying to predict really, really rare events from not much information, which I think is going to fail,” Geoghegan says.
Ultimately, the odds that a given virus will cause an outbreak depend on the virus itself, the animals that host it, the people who stand to contract it, and the environment that all of them live in. “Within each of these categories, there are so many variables that could influence disease emergence,” says Jennifer Gardy, from the University of British Columbia. “It’s hard enough to model the effect of any one, and these factors likely interact in ways that we can’t possibly understand just by looking at each of them discretely.”
It’s even difficult to work out whether the viruses we already know about are going to cause outbreaks. Ebola and Zika, for example, were discovered in 1976 and 1947 respectively, but both managed to catch the world unawares this decade. “This is the easiest kind of prediction to make,” says Kristian Andersen, from the Scripps Research Institute, and we’re still about 10 to 20 years from doing it well. Next up in difficulty: predicting whether a virus like H7N9 bird flu, which can infect humans but isn’t known to cause major outbreaks, will eventually do so. Again, Andersen says that this isn’t feasible now, but should be with more research.
But predicting whether a newly discovered animal virus could jump into humans and cause a pandemic “is simply impossible,” he says. “What you’re trying to predict is likely something that happens maybe once out of tens of billions of encounters, with one virus out of millions of potential viruses. You will lose your fight against the numbers.” Even machine learning—using computers to divine patterns in data that humans might miss—won’t solve the problem because there isn’t enough good data for the computers to sift through.
Proponents of predictive initiatives say it’s too early to discount such approaches. If the same complaints had been raised in meteorology a century ago, “we wouldn’t have created the data that lets us forecast the weather, which we can do pretty well now,” says Jonna Mazet, the global director for PREDICT who also sits on the Global Virome Project steering committee.
“Can we predict pandemics? The answer right now is no. But just because something is hard to predict does not mean we cannot quantify its risk in a useful, actionable way—a logic that the insurance industry profits from,” adds Barbara Han, from the Cary Institute of Ecosystem Studies. No predictions are perfect, but at the very least, we can put boundaries on what is likely.
Resources aren’t infinite, though, and public health is an area that’s historically underfunded. Geoghagen argues that it would be best to channel efforts into approaches that would do the most good. For her, that involves looking at the “fault lines” where humans and animals meet—regions where people are more likely to be exposed to animal viruses because they are chopping down forests, or setting up dense animal markets, or hunting wild creatures for meat, or moving around a lot because of political instability.
Mazet agrees, and says that the Global Virome Project plans to look for viruses precisely at such fault lines. They want to, for example, search blood and meat samples of bushmeat, or the urine or saliva of rodents that share human homes. “It’s not aimed at detecting every virus out there,” she says, but she admits that the team hasn’t done the best job in explaining that to their fellow virologists.
But Geoghagen and Holmes argue that searching for these viruses in animals is still “a Sisyphean exercise.” You’d find too many, with no way of accurately assessing their risk of jumping into us. The project, they say, would be better off focusing on people—the workers in the bushmeat trade rather than the meat itself, for example. “Humans are the best sentinels: A virus discovered in humans very obviously can replicate in that host, which will not be the case for myriad viruses identified through biodiversity surveys of other [animals],” they say.
Andersen agrees. For the moment, preempting pandemics isn’t possible; what matters is catching them as early as possible. “Forget about detecting the virus before it jumps. Forget even about detecting the first patient,” he says. “Detect the first cluster of cases.” That’s possible if health workers routinely search for viruses in people who live at disease fault lines, and the advent of portable, pocket-size DNA sequencers could make such searches a reality.
These goals shouldn’t be seen in opposition, though. Kevin Olival from EcoHealth Alliance, who works with PREDICT, says that it would be impractical to study all fault-lines. “We need tools to help us narrow down and target our resources to the locations, host species, and viruses of greatest concern,” he says. Projects like PREDICT and the Global Virome Project may not act as crystal balls for future outbreaks, but they “help us prioritize on-the-ground disease surveillance.”
And PREDICT, through its work on detecting new animal viruses, has also helped to develop analytics tools and strengthen labs in developing countries, which will make it possible to do the kind of surveillance that Geoghagen, Andersen, and others are calling for. Everyone agrees that’s vital. “If we can’t even get routine surveillance working in hot-spot settings, we have no chance of getting something even more complex, like prediction, in place,” says Gardy.


Imagine you’re Edwin Hubble in 1923, about to prove that the Milky Way is just one galaxy in a universe filled with them. You have just spotted a faraway variable star. You write down a note about that star on a photographic plate: “VAR!”
Or imagine it’s 1977, and you’re reading a printout from a radio telescope that listens for aliens, red pen in hand, when you find a long, strange, still-unexplained signal. “Wow!” you write.
Or imagine it’s August 2017, you’re signed on to Slack, and you’ve just seen the smoldering wreckage of a collision of two neutron stars. “!” you type to your colleagues, unable to muster anything else.
Each of these astronomical classics highlights one particular aspect of discovery: the thrill of knowing something about nature that no one else does. But these moments from the highlight reel of astronomy’s history minimize the more prosaic aspects of research, the tedium of peering at a screen for hours on end, blinking, clicking, or executing a computer script, again and again, forever, and maybe not finding anything noteworthy at all.
But now AI is here to do the boring part.
In a new paper published by the journal Monthly Notices of the Royal Astronomical Society, a neural network has successfully flipped through images of more than 20,000 galaxies and pulled out a few hundred of the most intriguing. “I think it will become the norm since future astronomical surveys will produce an enormous quantity of data,” said Carlo Petrillo of the University of Groningen in the Netherlands, in a statement. “We don’t have enough astronomers to cope with this.”
Petrillo’s specific quarry was gravitational lenses: rare patterns in the night sky that let astronomers fathom the depths of many of modern physics’ most pressing mysteries. Picture a galaxy so massive that space is distorted by its gravity, as Einstein’s theory of general relativity predicts. As rays of light from galaxies far beyond it pass through that warped space around the foreground galaxy, they curve, bending toward the Earth. This makes the foreground galaxy like a lens made of space, not glass.
Astronomers often call gravitational lenses cosmic telescopes, whose lenses amplify the light from very distant galaxies, opening a window to older, more primitive parts of the universe than we could otherwise see. That is, if they can find them. Since galaxies needs to fall directly behind each other by random chance, lenses are rare, and graduate students have only finite time to comb through endless images hunting them down. They all look a certain way, like broken arcs of light ringing a galaxy or a galaxy cluster. But they’re all different, and arms of spiral galaxies look like arcs too, confounding the search. It’s the kind of thing you’d think you need human intuition to do, that is until an AI pulls it off.
Petrillo’s group turned to convolutional neural networks, which are often used for other image-analysis tasks like facial recognition. They showed their network 100,000 images of gravitational lenses. Since fewer than a thousand real lenses are known, they used fake ones simulated in a computer. They let their network build up a feeling for how lenses look. Then they turned it loose on real galaxies, where it found 761 candidates, which the team winnowed down to 56 after checking by human eye.
According to astrophysicist Brian Nord at Fermilab, Petrillo’s work is part of a boom in applying artificial intelligence to this exact problem. From January to March, he estimates, about 8 different papers came out on the subject from different groups—including a draft of this one. While most of these efforts limited themselves to looking only at simulated pictures, as a proof of concept, Petrillo’s network and another made by graduate student Colin Jacobs seem to have identified real lenses that no human had ever seen.
And AI can do more than just find lenses. It can use them. In April, Nord says he visited Stanford and talked about how making changes at just “two little places” in a neural network’s code could let a network switch from recognizing lenses to measuring them—specifically, it could allow them to add up the mass that was creating the lens effect. By August, Stanford researchers did it. Their network analyzes lenses some 10 million times faster than older, simulation-based methods. Like any good scientist, the network had graduated from simply classifying—lens or not a lens?—to measuring.
How far this goes is an open question. In the past few years, cosmologists have started touting gravitational lenses as the solution to many of the field’s woes. Besides just magnifying distant galaxies, lenses work like cosmic bathroom scales: Their exact curvature traces the mysterious, unidentified dark matter that surrounds all galaxies. They are all also hyper-accurate yardsticks for astronomers who hope to clock the speed at which the universe is expanding. Better still, it seems like new AI methods and faster-than-ever computers can understand them.
Sending AI after lenses might only be a fad, a technique with pros and cons that is folded into other kinds of computer classification, Nord thinks. Just another part of the astronomer’s tool kit. Or it could transform discovery. It might soon be possible to give a neural network images of galaxies and simply set it free—allowing it to find the lenses, assess them, and to report back with some overarching measurement like the expansion rate of the universe. Provided astronomers could trust it, of course.
“Now you’re getting into the era that is scary for scientists, or may someday be scary,” Nord says. There would be no written notes for the history books, no exclamation points. The excitement of finding something no one else had would go unfelt, or would at least be unreadable.
It’s striking to consider what gravitational lenses and neural networks have in common. Science uses them both as tools—whether made by random chance or by design, they give us a way to pry open nature’s secrets. But here it might get recursive. Someday, not too far from now, we could be empowering a scientific instrument to find telescopes out in the void, to look through them on its own, and to tell us what it learned.


On April 22, more than a million people took to the streets, in Washington, D.C., and over 600 satellite locations around the world, to march for science. But six months later, the eponymous organization behind those gatherings—March for Science (MFS)—is still struggling with many of the same issues that have troubled it since its conception.
On Monday, Aaron Huertas, the former communications lead for MFS, posted an open letter that called out the group’s leaders for creating a culture beset by miscommunication, opacity, and disorganization. “Though the organization calls itself an open, grassroots movement, it is run like a closed, hierarchical organization,” the letter says. Seven other people told The Atlantic that their experience of working with March for Science was consistent with the open letter. “I really do think everyone has the best intentions, but not everyone has the skill sets they need to run a grassroots organization,” Huertas says.
As an example, Angela Carpio, a volunteer who helped to organize a satellite march in the Twin Cities, says that the leadership team ignored her repeated requests for help in moderating the MFS Facebook group—a community of almost 800,000 members that Carpio single-handedly oversaw for months. Eventually, she just picked three people from the community to help. “We just do our own thing. We have no direction,” she says. “There’s no transparency and no one knows what’s going on.” (Koren Temple-Perry, the newly hired communications director for MFS, says that the organization has had internal discussions about how to improve the moderation process, and that a newly hired director of social-media engagement is working with the team to bring on more moderators.)
“This is what happens when you have a group of very passionate, well-meaning people without the organizational experience who take on a tremendous amount of work, with this sort of Herculean mission of saving science,” says Jacquelyn Gill, who volunteered for the March for Science in its early stages, left the organization in April, and had signed the new open letter. “It set the stage for a culture that was big on enthusiasm and energy but weak on logistics.”
In a statement responding to the letter, Temple-Perry notes that the organization has taken several steps to address these problems, including soliciting feedback from partners and volunteers, running a retreat in May, issuing an open invitation to join an internal communications network, and staging biweekly calls with satellite organizers and partners. “Unfortunately, individuals on the letter have not yet called in to participate,” she notes. “That being said, the concerns brought up in this letter are being discussed by the board. We will continue to work toward greater transparency in all stages of our development as an organization and movement.”
But several of these measures were only initiated after the damage was done and volunteers had walked away, says Huertas. “People had stopped interacting with the national organization because they weren’t being listened to,” he says. “Volunteers don't see it as an effective use of their time.”
Rufus Cochran, who led the march’s satellite group in Indianapolis, has seen evidence of this through his attempts to unite leaders from the Midwest groups into a unified network. He says that around 8 groups have unplugged themselves from the national team because “their voices weren’t being heard” and several have become largely inactive. The central team, Cochran says, “are not 100 percent responsible for that but they’re a major factor in that loss of momentum.”
Despite the changes, confusion still abounds around the organization’s structure and finances. Huertas has pushed for MFS to publicly publish details of its expenditures and revenue, which he says is good practice for grassroots organizations—and a standard that several satellite marches have adhered to. Stephani Page, who was formerly on the national leadership team but left in April, says that even before the march, “I and others asked questions specifically about what was happening with the money. The response was always: All the money is going toward making the event itself happen. I never saw a budget.” When I asked about details of the organization's funding, I was directed to this page which breaks down revenue according to donations from organizations, individual donations, and merchandise sales, with no further information on financial structure and outputs.
The open letter also alleges that Caroline Weinberg—one of the three MFS cochairs—had been promoted to a paid director position, contrary to promises made that such positions would be open and competitive, and that the cochairs would only serve on the board or as advisors. After five rounds of circuitous emails, some of which wrongly stated that Weinberg is still simply a cochair, the MFS finally confirmed with The Atlantic that Weinberg is currently acting as executive director in an interim capacity; this was agreed to by the board of directors, and an openly posted search for a permanent ED will be conducted in the future. Weinberg is leading a team of eight other part-time staff working for MFS, including Terry Kush, the new chief operating officer. None of this information is currently available on the MFS website. Temple-Perry, the comms director, notes that the site will be updated soon.
Huertas and others claim that the March’s leaders have consistently ignored the views and contributions of people of color. As previously reported, diversity statements were repeatedly tweaked and pulled back after online pushback. Several individuals within the organization told The Atlantic that diversity concerns from people of color were met with hostility on internal forums, culminating in the resignations of roughly half of the MFS diversity committee in the week before the march. They requested anonymity because they feared harassment. Some of their colleagues who spoke out publicly said they faced online vitriol and threats.
Concerns around diversity strained relationships with partner organizations like 500 Women Scientists, a grassroots effort to foster more diversity, inclusion, and accessibility in science. “We were early supporters and partners of the MFS and it seemed, at first, that our values were aligned,” says Kelly Ramirez, a cofounder of 500 Women Scientists. So in February, when MFS drew heavy criticism for repeatedly posting messages that some thought were tone-deaf on Twitter, Ramirez and her team offered to help. “We had someone who would volunteer their time, but they weren’t added to any meetings or discussions. They used us as an example of how they were highlighting diversity, but we weren’t given a voice at the table.” By the time the volunteer was finally invited into MFS discussions, the group had had enough; they officially severed their ties with MFS in April. (“We are sorry to lose that connection, though we were excited to see that they continued to participate in the actual march as an organization,” said Temple-Perry in a statement. “We deeply respect their work and hope that there are additional opportunities to collaborate in the future.”)
On the day of the march itself, many speakers at the D.C. event emphasized the importance of all kinds of diversity, and several of the new recruits to the national team are people of color. But the signatories on the open letter feel that these changes are superficial. “From the outside, it looks like we’ve hired a racially diverse group of people with different social backgrounds,” says Amber Ying, who helps to moderate the Facebook group and volunteered for a largely defunct Boston chapter. “It feels like they’re only paying lip service to this idea of inclusivity. The actual practice of making sure people are heard is a much harder process and they’re not very good at it.”
Lucky Tran, who serves on the board, counters that “many on our board have served in and built grassroots organizations and social-justice movements, and we’re committed to creating a world in which science serves, and is inclusive of, everyone.” He says that MFS has worked with local leaders to organize aid for communities affected by Hurricanes Harvey and Maria, and has spoken up about social-justice issues like the white-supremacist rally in Charlottesville and Nature’s since-retracted editorial on the removal of a statue of J. Marion Sims. (Carpio notes that the Charlottesville post only happened after she pushed MFS leadership to act—and after raising the matter on an internal forum, she was billed as “hysterical and divisive” by a satellite organizer.)
“The concerns voiced in the letter and elsewhere are highly valued, and dialogue like this is needed to make movements better and more accountable,” he adds. “We are all committed to working hard to address them and building a better and more just world.”
That’s what the signatories of the open letter want, too. They all feel that MFS has tremendous promise, and a large and motivated base. But they also felt that these opportunities were being wasted. “There are lofty goals and ideas, but not enough organization to distill that into action on the ground,” says Ying.


One of my first jobs was to keep a lookout for lions. There are some occupations that are not suitable for someone with untreated narcolepsy and this is probably one of them. I was 22, a recent zoology graduate studying meerkats in the Kalahari desert in South Africa. We worked in pairs, one of us on foot, walking with meerkats, the other in the jeep scanning the horizon for signs of leonine danger. On many occasions, I awoke with the imprint of the steering wheel on my forehead, realizing that meerkats and colleague had wandered out of sight. I would look for signs of life and, as the panic grew, signs of death. I can tell this story now only because nobody got eaten.
I have not always been like this. For the first 20 years of my life, I had a healthy relationship with sleep. Shortly after my 21st birthday, though, I began to experience symptoms of narcolepsy, a rare but not-so-rare disorder thought to affect around one in 2,500 people. If people know one thing about narcolepsy, it’s that it involves frequent bouts of uncontrollable sleepiness. This is true, but the condition is so much more disabling, often accompanied by cataplexy (where a strong emotion causes loss of muscle tone and a rag doll–like collapse), trippy dreams, sleep paralysis, frightening hallucinations, and, paradoxically, fractured nighttime sleep. There is no cure. Yet.
In the Kalahari, back in 1995, I was new to these symptoms. I had little sense of the incalculable toll that fighting a never-ending battle against sleep (with defeat the inevitable outcome) would take on mind, body, and soul. I was not alone. Few family doctors had heard of the disorder, let alone encountered a patient. Some neurologists knew what to look for, but many did not. Not even sleep specialists could explain why this disorder would suddenly strike, with peak onset at around 15 years of age.
A lot has changed in 20 years. There is now overwhelming evidence that by far the most common cause of narcolepsy is an autoimmune attack, where the body’s immune system mishandles an upper respiratory infection and mistakenly wipes out the estimated 30,000 neurons in the center of the brain.
In an organ of up to 100 billion cells, this might not sound like too much to worry about. But these are no ordinary cells. They are found in the hypothalamus, a small, evolutionarily ancient, and unbelievably important structure that helps regulate many of the body’s basic operations, including the daily seesaw between wakefulness and sleep. The cells in question are also the only ones in the brain that express the orexins (also known as hypocretins). This pair of related peptides—short chains of amino acids—were completely unknown at the time of my diagnosis in 1995.
The story of their discovery, beginning in the 1970s, is a brilliant tale of chance and luck, imagination and foresight, risk and rivalry, and involves a colony of narcoleptic Doberman pinschers to boot. It might even be the perfect illustration of how science works.
Yet while there are drugs that can help manage the worst of the symptoms of narcolepsy, none of these comes close to repairing the underlying brain damage. It is remarkable that a lack of two chemicals results in such a bewildering constellation of symptoms. The answer to my problems appears to be simple—I just need to get the orexins (or something similar) back inside my brain. So why am I still waiting?
* * *
In April 1972, a toy poodle in Canada produced a litter of four. Eager families were quick to snap up the cute puppies, but one of them, a silver-gray female called Monique, soon developed what her owners described as “drop attacks” when she tried to play. These did not look like sleep; they were mostly partial paralyses: Her hind legs would go weak, her bottom would slump to the floor, and her eyes would become still and glass-like. At other times, particularly when fed, Monique would be struck by a full-blown attack.
When vets at the University of Saskatchewan observed Monique, they suspected these were bouts of cataplexy, and hence figured this might be a case of narcolepsy with accompanying cataplexy. As luck would have it, Monique’s diagnosis coincided with the arrival of a peculiar circular from William Dement, a sleep specialist at Stanford University. He was on the lookout for narcoleptic dogs. The Saskatchewan vets wrote back to him immediately. With Monique’s owners persuaded to relinquish their pet, all that was needed was to figure out a way to get her to California.
I met Dement, now 89, to find out what he remembers about those early years. He retired several years ago, but still lives in a leafy neighborhood on the edge of the Stanford campus. His office is a large, shedlike structure attached to the main house and not unlike a Scout hut.
The walls are wood-clad and covered with framed posters, photographs, and miscellaneous memorabilia from an illustrious career in sleep medicine. Dement’s desk is a picture of organized chaos. Among all this is a water pistol. I ask him why. “It’s for when students fall asleep in class,” he explains, referring to an incredibly popular lecture series on sleep and dreams he instigated in the early 1970s.
In 1973, Dement approached Western Airlines to see if they could fly Monique down from Saskatchewan to San Francisco. They had a strict “no sick dogs” policy. “It’s not a sick dog. It’s a dog with a brain abnormality,” he told them. “It’s an animal model of an important illness.” Eventually, with some political lobbying, Dement succeeded in persuading the airline to help. Once in San Francisco, Monique quickly became something of a celebrity.
“Monique is very likely to collapse when she’s eating something she especially likes, or when she smells a new flower outside, or romps around,” Dement’s colleague Merrill Mitler told the Associated Press for a story that ran in dozens of newspapers across the United States. “We hope to discover exactly where in the brain the dysfunction occurs that causes narcolepsy,” Mitler had told the newspapers soon after Monique’s arrival at Stanford. “This could be the first step toward developing a cure.”
Mitler is now a forensic examiner based in Washington, D.C., specializing in litigation arising from fatigue-related accidents. I ask him if the story of the discovery of narcolepsy is really as good as it appears. “In a word, yes,” he says. “In the ’70s, we didn’t know what we didn’t know about narcolepsy.” There is simply no way anyone could have anticipated how profitable the research into Monique and other dogs would turn out to be. The plan at that stage, he admits, was simply to use the animals to test new drugs that might improve treatment of the symptoms and to carry out autopsies in case there were some obvious physical changes to the brain.
Word began to spread, and soon Dement and Mitler were looking after Monique alongside several other narcoleptic dogs, including a Chihuahua-terrier cross, a wirehaired griffon, a malamute, Labrador retrievers, and Doberman pinschers. The fact that narcolepsy appeared to be more common in some breeds than others suggested there could be some kind of genetic basis to the disorder. Then came the breakthrough: a litter of around seven Doberman puppies, all of them with narcolepsy and cataplexy. “Within 24 hours or less we saw the first of the litter and then the last of the litter all collapse,” says Mitler. “There was a large group of us at Stanford and we collectively had our chins on the floor.”
It turned out that in Labradors and Dobermans, the disorder was inherited. Dement made the decision to focus on Dobermans and, by the end of the 1970s, he was the proud custodian of a large colony and had established that narcolepsy in this breed was caused by the transmission of a single recessive gene. By the 1980s, methods of genetic analysis had advanced just enough to contemplate an effort to hunt down the defective Doberman gene.
* * *
I can never reconstruct the combination of factors that led to the onset of my own narcolepsy, but the stage was set at the moment of my conception in 1972, at around the time of Monique’s birth in Saskatchewan. My one-cell self inherited a particular version of a gene (known as HLA-DQB1*0602) that forms part of a set that helps the immune system distinguish friend from foe. HLA-DQB1*0602 is pretty common—around one in four people in Europe boasts a copy—but it plays a key role in many cases of narcolepsy, and is present in 98 percent of those with narcolepsy and cataplexy.
On top of this genetic background, there may have been some bad timing too. People with narcolepsy are slightly but significantly more likely to be born in March (as, indeed, I was). This so-called “birth effect” is seen in other autoimmune disorders and is probably explained by a seasonally variable infection at a particular moment in development. In the case of narcolepsy, it seems that those of us born in March are just a little bit more vulnerable than others.
While other infections during my childhood, hormonal fluctuations, and emotional stress may also have played a part, it was in late 1993 that I probably encountered a key pathogen—an influenza virus or Streptococcus perhaps. It was this that took me to an autoimmune tipping point and resulted in the rapid dismantling of my orexin system. In short, most cases of narcolepsy are probably the result of an unfortunate combination of events that create the perfect immunological storm.
Around this time, the Doberman project in Stanford was on the verge of unraveling the genetic basis of narcolepsy in this breed. The man tasked with hunting down the mutation responsible was Emmanuel Mignot, who subsequently succeeded Dement as director of the Stanford Center for Sleep Sciences and Medicine. We meet in his office there, joined by Watson, a narcoleptic Chihuahua he adopted a few years ago. “It’s such a silly breed,” he says, holding down Watson’s ears to prevent them from burning, then setting him on the floor. “Not one I would ever have chosen myself.”
At first, Watson is wary of me, keeping his distance and growling. When I get down to his eye level, he yaps and jumps in at me, then out, pretending he is fiercer than he is. I can empathize, even across the gulf that separates his species from mine. I know about the excessive daytime sleepiness. I know about the cataplexy, how it feels to have emotions short a neurological circuit in the brainstem and cause a muscular collapse (just as occurs in the rapid eye movement, or REM, stage of sleep, when most dreaming takes place). I wonder if Watson suffers the total terror of sleep paralysis and the supernatural hallucinations that often accompany it.
As he looks back at me, his eyelids close and open with a dullness I recognize. He turns, daintily steps into his basket, and curls up for the rest of the interview.
Back in the 1980s, the idea of locating the gene for canine narcolepsy was off-the-scale ambitious. Breeding narcoleptic Dobermans is harder than it sounds, as the afflicted tend to topple over mid-coitus, temporarily paralyzed by a cataplectic thrill (a so-called orgasmolepsy that can occur in humans too). This impracticality aside, there was also the task of locating a gene whose sequence was not known, in a genome that was, at the time, a no-man’s-land. “Most people said I was crazy,” says Mignot. In a sense, they were right, because it took him more than a decade, hundreds of dogs, and over $1 million. And he was nearly beaten to it.
In January 1998, after more than a decade of painstaking mapping, and just as Mignot’s team was closing in on the gene, a young neuroscientist called Luis de Lecea at the Scripps Research Institute and colleagues published a paper describing two novel brain peptides. They gave them the name “hypocretins”—an elision of hypothalamus (where they were found) and secretin (a gut hormone with a similar structure). They appeared to be chemical messengers acting exclusively inside the brain.
Just weeks later, a team led by Masashi Yanagisawa at the University of Texas independently described the exact same peptides, but called them “orexins” and added the structure of their receptors into the bargain. They speculated that the interaction of these proteins with their receptors might have something to do with regulating feeding behavior. “We didn’t even think about sleep at all,” admits Yanagisawa, now director of the International Institute for Integrative Sleep Medicine at the University of Tsukuba in Japan.
Back at Stanford, Mignot heard about the two papers, but there was no reason to imagine this new pathway had anything to do with narcolepsy or sleep. By the spring of 1999, however, he and his team had worked out that the recessive mutation had to lie in one of two genes. One was expressed in the foreskin. “It didn’t look like a candidate for narcolepsy,” says Mignot. The smart money was on the other gene, which encoded one of the two orexin receptors. When he got wind that Yanagisawa had engineered a mouse lacking orexins that slept in a manner characteristic of narcolepsy, the race was on.
Within weeks, Mignot and his team had submitted a paper to the journal Cell, revealing a defect in the gene encoding one of the orexin receptors. “This result identifies hypocretins [orexins] as major sleep-modulating neurotransmitters and opens novel potential therapeutic approaches for narcoleptic patients,” they wrote. Kahlua—one of a litter of Dobermans all named after alcoholic beverages—lay sprawled across the cover of the issue. Yanagisawa and colleagues added their experimental evidence to the mix just two weeks later, also in Cell.
* * *
Under normal circumstances, a chemical messenger and its receptor work a lot like a key and lock. A key (the messenger) fits into a lock (its receptor) to open a door (cause a change within the target cell). In the case of Mignot’s Dobermans, a massive mutation had effectively jammed the lock of the orexin receptor, rendering the orexin useless.
Whether it’s the lock that doesn’t work, as in this case, or that the keys are missing, as they were in Yanagisawa’s mice, the upshot is the same. The door won’t open. The orexin system is broken. In human narcolepsy, there are many ways to break the orexin system. Occasionally, a brain tumor or head trauma is sufficient to do the damage. In most cases, however, narcolepsy is caused by the series of unfortunate events outlined above.
The orexin neurons are a very big deal, and not just for those like me who’ve lost them. Present in every major class of vertebrate, they have to be doing something seriously important. When de Lecea first described the orexins in 1998, he was in his mid-20s and had only recently moved from Barcelona to San Diego. In 2006, he made the move from there to Stanford to be closer to the sleep action. “To be honest, I thought we’d understand the system much better at this point than we actually do,” he says.
But we have found out a lot, particularly thanks to optogenetics, a technique de Lecea helped pioneer. By deploying a virus, a promoter, and a gene found in blue-green algae, it is possible to render a particular population of neurons sensitive to light.
To illustrate this wizardry, de Lecea brings up a video on his laptop. There is a mouse in a cage that has been engineered so its orexin neurons will fire in response to light. There is a thin fiber-optic cable running into its brain. “The mouse is asleep,” he says, waves of electrical activity characteristic of deep sleep spooling across an inset video at the top of the screen. The optic cable comes alive, a pulse of bluish light flashing for precisely ten seconds. The light-sensitive orexin neurons release their neuropeptides and, all of a sudden, the mouse wakes up. When the light goes off, it falls asleep as rapidly as it awoke.
There can be few more striking illustrations of the power of the orexins than this. Completely unexpectedly, I feel my tear ducts tingling and for a split second I almost envy the mouse.
Using optogenetics and other methods, de Lecea has been able to show that the orexins have a powerful effect on many important neurological networks. In some settings, they act like neurotransmitters, crossing gaps in neurons to activate target neurons that release a chemical called norepinephrine throughout the brain’s cortex.
In other settings, the orexins act more like hormones, working further afield in the brain. This is how orexins influence other brain chemicals, including dopamine (essential for the processing of reward, in planning, and for motivation), serotonin (strongly associated with mood and implicated in depression), and histamine (an important alerting signal).
“In most other neural networks, there are parallel and multiple layers of security,” says de Lecea, so if something isn’t working properly, there are systems that can step in and pick up the slack. In the case of the orexins, however, there appears to be little or no backup at all. So, manipulating this system produces the kind of clear-cut response that scientists can work with. “It is a brilliant model for understanding neural networks more generally,” says de Lecea.
What we now know about orexins also helps explain why losing just a few tens of thousands of cells should result in a disabling, multi-symptomatic disorder like narcolepsy—something that messes with wakefulness and sleep, body temperature, metabolism, feeding, motivation, and mood. These proteins are giving us a privileged insight into how the human brain does what it does.
All this makes the orexin story sound like the archetypal double helix–like tale of scientific discovery, the perfect illustration of how science works. There’s an underlying puzzle (narcolepsy), an origin story (Monique), foresight (Dement), ambition (Mignot), technological developments (genetics), a photogenic animal (Dobermans), a race (with Yanagisawa), it looks like science (optogenetics), and there’s a still higher purpose (sleep and the brain).
It is elements like these that can transform everyday scientific events into a compelling cultural narrative, says Stephen Casper, a historian of neurology at Clarkson University in New York. “It has all the ingredients of something that I think physiologists and neurologists in the early part of the 20th century were looking for and hoping they would find, something that would bring together heredity, biochemistry, biophysics, neurology, and psychology.”
But there is a pattern in biomedical research of niche disorders opening up promising avenues of research that never end up helping the patients themselves, Casper adds. The narrative around narcolepsy has something missing, he says: “A good story should have a clear happy ending.”
* * *
We are still waiting for that happy ending. Even if I could get my hands on a vial of orexin-A or orexin-B, how would it get into my brain? Swallowed in solution, the enzymes in my gut would make short shrift of it, plucking off the amino acids like beads off a necklace. Injected into muscle or the bloodstream, not enough would make it through the blood-brain barrier. There have been some experiments on a nasal delivery, suggesting that sniffing orexins may be a way to smuggle some of them into the hypothalamus via the olfactory nerve, but there has been relatively little investment in this approach.
This does not mean that the pharmaceutical industry has ignored the discovery of the orexin pathway. Far from it. Within just 15 years of the Cell publication by Mignot and colleagues that linked a loss of orexin to narcolepsy, Merck had received U.S. Food and Drug Administration approval for suvorexant (or Belsomra as it’s known in the trade), a small molecule capable of getting through the blood-brain barrier and blocking orexin receptors.
A drug that promoted sleepiness was not the application that most people with narcolepsy were looking for. By preventing the orexins from binding to their receptors, Belsomra effectively creates an acute case of narcolepsy, but where the fog, ideally, will have started to lift by the morning.
Sleeping pills commonly used to treat insomnia tend to work by depressing the central nervous system as a whole, says Paul Coleman, a medicinal chemist who works at Merck’s laboratories at West Point, Pennsylvania, and who was instrumental in the development of Belsomra. “What’s so exciting about Belsomra is that it is very selective for blocking wakefulness, so it doesn’t affect the systems that control balance, memory, and cognition,” he says.
In his career, Coleman has developed drugs to treat a range of different infections, illnesses, and disorders, but the orexin system stands out. “Narcolepsy has given us a thread we can pull on to unravel a lot about what underlies the systems that govern wakefulness and sleep,” he says.
“Wakefulness is a pretty central process for everybody, whether you are a healthy person or have narcolepsy or insomnia. It’s the most exciting thing I’ve had a chance to work on.” The applications of Belsomra may be wider still, with clinical trials proposed to investigate its potential to help shift workers’ sleep during the hours of daylight, improve the sleep of Alzheimer’s patients, help those suffering from post-traumatic stress disorder, combat drug addiction, and ease human panic disorder.
I am delighted to see these developments, but the millions of us with narcolepsy are still hoping for a drug that could work in the brain to rouse rather than silence the orexin system.
This has been a long-term project for Masashi Yanagisawa, who was in the race with Mignot to link the orexins with narcolepsy 20 years ago. But designing and synthesizing a compound that will make it through the gut intact, that has what it takes to find its way from blood to brain, and that boasts the perfect configuration to activate one or both of the orexin receptors is “a very, very high challenge” he says, one that is “significantly” greater than finding a compound to interfere with the receptor as Belsomra does.
Earlier this year, Yanagisawa and his colleagues published data on the most potent such compound to date, a small molecule called YNT-185. Injections of this molecule into narcoleptic mice significantly improves their wakefulness and cataplexy and reduces the abundance of the REM stage of sleep in which most dreaming occurs (one of the characteristics of narcolepsy). This, says Yanagisawa, is a “proof of concept.” Although the affinity of YNT-185 (how strongly it binds to the orexin receptor) is not great enough to warrant a clinical trial, Yanagisawa’s team has already hit upon several other potential candidates. “The best one is almost 1,000 times stronger than YNT-185,” he says.
While the symptoms of narcolepsy can vary wildly from one person to the next, the underlying pathology—the absence of orexins—is still the same. “If this compound works, it’ll work for all those patients,” he says. “In that sense, it’s a relatively simple clinical trial compared to many other disorders.”
A still more futuristic avenue involves stem cells. Sergiu Paşca has the office next to Emmanuel Mignot at Stanford and in 2015, he and his colleagues developed a way to take induced pluripotent stem cells (fashioned from skin cells) and direct them toward a new life as brain cells. “You can use this system to derive various brain regions and like a Lego game, assemble them to form circuits in a dish,” he says.
Recently, his lab has developed methods to do something similar for people with narcolepsy, starting with a skin cell and ending up with a fully functional orexin neuron. In theory, it should be possible to transplant this into the brains of people with narcolepsy and restore some of the function. This is, however, not something to be taken lightly. For a start, the cells themselves are unlikely to be exactly the same as orexin cells, inserting a needle into the brain is not a risk-free exercise, and there’s always the possibility that the immune system might make another assault on the transplanted cells.
So, will the tale of the orexins really have a happy ending? The translation of basic research into the clinic is notoriously difficult and expensive, says Casper. (The cost of the current best available treatment for narcolepsy—sodium oxybate, or Xyrem—is such that it is not routinely available for adults in England, even though it could transform the lives of many.)
There is a widespread perception that narcolepsy is a rare disorder with a small
market, so any pharmaceutical research and development in this area would be unlikely to reap a significant return. This ignores the fact that narcolepsy is probably undiagnosed in many people, and that someone who develops narcolepsy in their teens and lives into their 80s would need some 25,000 doses over their lifetime.
Even more compellingly, perhaps, the orchestrating role that the orexins play in the brain suggests the market for such a drug would go far beyond narcolepsy. Something that tickled up the orexins would be useful for any condition where excessive daytime sleepiness is an issue, not to mention the myriad other situations where low levels of these messengers may play a role, including obesity, depression, post-traumatic stress disorder, and dementia.
There is, I believe, one other reason why this story has not yet reached its conclusion. For too long, sleep has been undervalued, seen as an inconvenient distraction from wakefulness. With this mindset, research into the neuroscience of sleep does not seem like it should be a priority. Nothing could be further from the truth. There is now abundant evidence that poor sleep can have devastating consequences for physical, mental, and psychological health. Sleep is not incidental. It is fundamental, a matter of serious public health. Investing in sleep research is not just about the few with demonstrable sleep disorders. It is about everyone.
This post appears courtesy of Wellcome and Mosaic Science.



It was the nightmare that wasn’t.
On July 20, 2014, as West Africa struggled to quash a historically large outbreak of Ebola, an infected man carried the virus to Lagos, Nigeria—Africa’s largest city. In that dense throng of 21 million people, many of whom travel extensively, it seemed that Ebola would be impossible to track and contain.
The Threat of Polio in the Badlands of Boko Haram
But Nigeria was ready. In the previous years, it has been using investments and support from the United States and other countries to boost its efforts to eradicate polio. When Ebola came, it swiftly redirected all of that infrastructure at the problem, including an emergency operations center, a crack team of epidemiologists trained by the U.S. Centers for Disease Control and Prevention (CDC), and GPS systems that could be used to track potential cases. In the end, Nigeria brought Ebola to heel in just three months, with only 19 cases and eight deaths.
The country’s spectacular success is a testament to the decisive actions of its government and health workers. But it also shows how important it is for rich countries to bolster the capacities of poorer ones, where outbreaks are most likely to begin due to weaker health systems and dense populations. No nation can tackle the problem of epidemics alone. In a world in which someone with a deadly virus can fly to any other continent in less than a day, the United States is connected to the entire planet’s diseases. And so, to protect itself, it must protect everyone else. As Rebecca Katz from Georgetown University once said to me, “If your desire is to keep disease out of your country, the best way to do that is to contain it at the source.”
In the years since the Ebola outbreak, the United States has channeled billions toward this goal, and has led other countries in doing the same. But some of that funding is set to disappear. This is a common pattern with epidemics. Diseases flare up and we throw money and resources and troops at them. The crisis ends and peacetime brings complacency. “We’ve got to get out of this cycle of panic and neglect,” says Carolyn Reynolds, vice president for policy and advocacy at PATH, a nonprofit working in global health.
In a new report called Healthier World, Safer America, launched today, PATH calls for the United States to redouble its commitment to global health. The timing is no coincidence: Tomorrow, delegates from 50 countries will gather in Kampala, Uganda, for a ministerial meeting of the Global Health-Security Agenda—a five-year international partnership that aims to improve the health security of developing nations. PATH wants America’s support for the GHSA to continue, and to be backed up by strong leadership and a firm plan.
Barack Obama convened the GHSA in 2014 with strong bipartisan support, and since then, the United States has committed more than $1 billion to the program. These investments have already made tangible differences. As just one example, Cameroon’s response time to recent outbreaks of cholera and bird flu shortened from 8 weeks to just 24 hours. “That’s a really significant change,” says Reynolds, when thinking thinking about whether a disease remains a localized outbreak or flares into a globe-spanning pandemic. “We feel that the Ebola epidemic was a tipping point—a global wake-up call that the world is not prepared.”
This is as uncontroversial a position as exists in public health, and one also shared by many other major organizations. Since the Ebola epidemic, the National Academies of Sciences released a report urging that the “U.S. government should maintain its leadership position in global health as matter of urgent national interest.” The World Bank released its own report saying that “investing in preparedness is not a one-off, but an ongoing requirement” and that “every expert commentary and every analysis in recent years tells us that the costs of inaction are immense.” Meanwhile, an international coalition called CEPI—the Coalition for Epidemic-Preparedness Innovations—has raised $460 million to develop vaccines against potential pandemic diseases. “We have taken some steps, but there’s always another crisis that shifts political attention,” says Reynolds. “We can’t afford to take our eye off the ball.”
America has a long and bipartisan history of supporting global health. In 2003, George W. Bush created the President’s Emergency Plan for AIDS Relief (PEPFAR), which has since used over $72 billion in funding to distribute antiretroviral drugs to almost 11.5 million people in sub-Saharan Africa. In 2014, Barack Obama deployed thousands of troops and health workers to fight Ebola in West Africa, and secured an emergency budget of $5.4 billion to deal with the epidemic.
The Trump administration’s attitudes toward global health have been harder to gauge. The president’s penchant for isolationism, from his America First rhetoric to his thrice-attempted travel ban to his NATO skepticism, sit uneasily with an ethic of international cooperation, and the administration’s proposed budget threatened to slash $2.2 billion from global health programs. But recent signs from administration officials have been more reassuring.
For example, the U.S. delegation at the GHSA meeting in Kampala will be led by Tim Ziemer, a retired rear admiral who led George W. Bush’s President’s Malaria Initiative, and has been described as “one of the most quietly effective leaders in public health.” In July, at the Aspen Security Forum 2017, Thomas Bossert, assistant to the president for homeland security and counterterrorism, said the United States would “continue our full-throated support” of the GHSA. “The weakest country among us with the ... least preventative-care capabilities [is] going to be the patient-zero outbreak source,” he said. “And they're going to end up killing and infecting the world, and so we need to put money into places that don't have the money to do it themselves to prevent loss of life here. So that's it.” Secretary of State Rex Tillerson backed up this rhetoric  in early October, saying that “the United States advocates extending the GHSA until the year 2024.”
Reynolds argues that these strong verbal commitments must be backed up by equally strong financial ones. Currently, the United States spends around $450 million on global health security programs—less than 0.1 percent of what it spends on military defense. Congress has averted the worst of Trump’s proposed cuts, but its budget still reduces the total spent by 10 percent in fiscal year 2018.
An even steeper fiscal cliff looms in 2019, when $1 billion, which was diverted from the money allocated to fighting Ebola, runs out. That money has been used well, to train epidemiologists, buy equipment, upgrade labs, and stockpile drugs. If it disappears, progress will halt, and potentially reverse. The CDC, for example, would have to pull back 80 percent of its staff in 35 countries, breaking ties with local ministries of health. “We’re in the middle of substantial capacity-building efforts and it’s the wrong time to pull out,” says Reynolds.
Research and development into drugs, vaccines, and diagnostic tests for neglected diseases has also been underfunded. Until the Ebola money was appropriated, such funding was at its lowest since records began in 2007. It will fall back to that historical nadir unless new commitments are made.
Of course, budgets aren’t infinite, and the United States has any number of domestic health crises too, from environmental catastrophe in Puerto Rico to a nationwide opioid epidemic. But “it can’t be an either/or,” says John Monahan at Georgetown University, a former adviser to the State Department. “We have to do multiple things to protect the health of the American people, and one of them is investing in health security abroad.”
The word “investing” is key. A severe outbreak, equivalent to the flu pandemic of 1918, could cost the world up to $6 trillion and kill between 50 and 80 million people. It is far more cost-effective to prepare for one than to react to it.
Nor does preparedness involve huge amounts of money, doled out in perpetuity. PATH calculates that USAID and CDC could continue the work that was supported by the Ebola money with an extra $250 million over the next three years. That would go a long way toward helping the countries that are assisted by the GHSA to meet development benchmarks laid out by the World Health Organization, and reach a point when they can sustain their own progress.
“The American government, the American people, have done so much in global health,” said Awa Marie Coll-Seck, Senegal’s minister of health, at a recent event hosted by PATH. “It’s important for the United States to understand that supporting our countries to have strong preparedness and response to epidemics will protect the United States. It’s a win-win situation. If we are weak, everything will come. There are no borders, no passports, no visa if it’s a disease.”


New York is a city on the water. For hundreds of years, its rivers and harbor have worked to its advantage, bringing it speedy transportation and pleasant temperatures.
The next couple hundred years may not be as smooth sailing. Global warming, caused by the release of carbon-dioxide pollution into the atmosphere, will cause the seas to rise and the storms to intensify around the city. A new study from an all-star list of climate scientists attempts to estimate how a few of climate change’s symptoms—higher seas, large storm surge, and more intense hurricanes—will intersect in New York over the next 300 years.
It isn’t pretty. Sea-level rise will make every tropical cyclone that hits New York more likely to release damaging floods. For instance, storm floods of nearly seven-and-a-half feet once occurred only a couple times per millennium. In today’s somewhat warmed climate, 7.5-foot floods are projected to happen every 25 years. By 2030, these floods will occur every five years.
New York City has experienced 7.5-foot floods several times in the past decade. Superstorm Sandy loosed 10- or 11-foot floods on much of Manhattan, Brooklyn, and Staten Island, killing 43 people and inundating more than 88,000 buildings.
Not all of the news from the study’s estimates was bad. In a climate-changed world, the effects of storm surge on New York may remain about the same. Even though future hurricanes are likely to be more intense—and thus more likely to “push” more water in front of them, as storm surge—the hurricane models also showed the same storms avoiding New York Harbor. In other words, global warming seems to redirect some of the largest hurricanes eastward. It’s unclear why this may be the case.
Andra Garner, a climate scientist at Rutgers University and the first author of the paper, described the shift as “the biggest surprise from this work.”
“The eastward shift in storm tracks at the latitude of NYC in future simulations [was] a result that we did not expect to find, and something that we hope to investigate further with future research,” she told me in an email.
The study is the first to compare data from three sources: models of storm surge in New York City; probabilistic projections of sea-level rise; and advanced climate models that include high-resolution hurricane simulations. Its coauthors include Kerry Emanuel, a hurricane researcher at the Massachusetts Institute of Technology; Bob Kopp, a climate economist at Rutgers University; and Michael Mann and Richard Alley, two geoscientists at Pennsylvania State University.
The study also investigates how hurricane flooding will be affected by the possible collapse of the West Antarctic Ice Sheet. The timing of the ice sheet’s collapse remains debated: While some models suggest that a warmed world will erode the ice sheet between the years 2200 and 3000, newer and more aggressive studies argue it could collapse within decades. Its collapse will have profound consequences for the U.S. Eastern Seaboard, as its powerful gravitational pull currently keeps sea levels on that coast unusually low.
If it does begin to fall into the sea, ocean levels worldwide could rise five to six feet by 2100. “To our knowledge, this is the first study of its kind, looking at flood risk associated with tropical cyclones, to incorporate such ‘worst-case scenario’ sea-level rise projections,” said Garner.
The study doesn’t capture every kind of flooding event that New York will experience—only spinning tropical storms. It doesn’t account for systems like Superstorm Sandy, for instance, which became a more disorganized “extratropical cyclone” before it made landfall.



In just over a month, the world’s first theme park devoted entirely to Italian food will open its doors—and Gastropod has the scoop! Among Eataly World’s delights will be hunt-your-own truffles, baby lambs, beach volleyball, and custom Bianchi shopping bike-carts. But there’s a bigger story, and it’s that Oscar Farinetti, the founder of the Eataly empire, has somehow managed to make money by merging two businesses—grocery stores and restaurants—that are both incredibly challenging when it comes to turning a profit. In the process, he’s transforming the way we shop for food. Join us this episode as we tell the story behind the life and death of the great American supermarket—and take a trip to Italy for a sneak peek at its future.
The supermarket is such a mainstay of daily life throughout most of the developed world that it’s hard to imagine that someone invented it. But that is exactly what Michael Cullen did when he opened King Kullen in Queens, New York, on August 4, 1930. Before that, as Michael Ruhlman, the author of Grocery: The Buying and Selling of Food in America, explained, a grocery store was simply a dry-goods store, selling boxed and canned goods. To get their hands on perishable goods like meat, produce, and dairy, Americans had to either grow their own, await delivery (remember the milkman?), or visit one of the specialist shops that started appearing in cities in the late 1800s and early 1900s—butchers, greengrocers, and creameries.
The supermarket of today is many, many times bigger than the first King Kullen: It stocks 200 times as many products, and is often surrounded by a vast car park. But as impressive as it seems, the supermarket is in trouble. As Ruhlman explained, the typical grocery-store chain operates on a tiny 1 percent margin. “It’s a crazy business model,” he told us. “Nobody in their right mind would get into it.” On every front, the supermarket is under attack: It is losing the price wars against Walmart, and the battle for convenience against Amazon, Instacart, and meal-delivery services. And yet Farinetti, the founder of the Eataly chain of grocery mega-stores, has somehow managed to make selling food profitable again. How?
To find out, this episode we speak with Sunil Gupta, a Harvard Business School professor and the author of a recent case study on the Eataly chain, as well as CEO Farinetti himself. The secret? It lies in a blend of good, old-fashioned synergy—combining the best of the restaurant and retail business models to overcome the weaknesses of both—and Farinetti’s own creativity and passion for experiential storytelling. “I’d call him a maverick or a visionary,” said Gupta, who sees the lesson of Eataly as “how you can take an old, traditional industry and completely reimagine that.” Listen in as we tease apart Eataly-nomics, experience its ultimate expression at Eataly World, and discover what Farinetti’s example can tell us about the future of shopping for food.
This post appears courtesy of Gastropod, a podcast cohosted by Cynthia Graber and Nicola Twilley that looks at food through the lens of science and history.


When Chris Lowe first saw the buck stoop to lick the small, silver-speckled fox, he thought his eyes might be playing tricks on him. He’d just gotten back from a run on Santa Catalina, a remote Southern Californian island where he studies sharks, and came upon the two animals in the scrub. Mule deer and island foxes, the rascally miniature descendants of gray foxes, are everyday sights on Catalina’s grassy hills. But to see them nuzzling was downright weird.
Was the buck simply nibbling on a plant behind the fox? Had the fox happened to hop in front of the buck’s face? Lowe dashed into his apartment to grab his camera, and made it to the window to catch the deer taking another lick. The fox, docile in the shade of its antlered friend, wasn’t just tolerating the apparent cleaning, Lowe realized. “It looked like it was actually enjoying this,” he says.
How We Almost Lost the Island Fox
Lowe tweeted a picture of the curious scene a few hours later, and it quickly racked up several thousand likes and retweets. In the image, the buck has its pursed lips planted on the fox’s forehead. The fox, its eyes closed, resembles a dog getting a good behind-the-ears scratch. People responding to Lowe’s tweet were captivated by the strange pairing. It was adorable—in one person’s words, “a Disney moment!” And no one had ever seen anything quite like it.
Well, no one except Michael Cove. To match Lowe’s tweet, Cove offered picture of his own: a lean doe in a forest rubbing noses with a cat. “We get this all the time in the Keys ... interesting that it is happening on islands,” he wrote. Then he brought the party down: “Certainly a pathway for disease transmission.”
Cove, a mammologist at North Carolina State University who spends several months each year on the Florida Keys, has in fact spotted several peculiar meetings between the islands’ diminutive Key deer and other creatures. Motion-triggered cameras he’s set up around a wildlife refuge on one of the islands have photographed a deer dancing around a peacock, and a deer getting its face groomed by raccoon. There are a few more cases with cats, including a time off-camera that Cove passed a dumpster and saw two deer licking the same cat at once. (“The Florida Keys are an interesting place,” he says.)
Cove speculates that cut-off places like the Keys and Catalina, which is one of California’s eight Channel Islands, have two features that could encourage such interspecies intermingling. The most prominent is a lack of large predators. The islands’ deer have lived for generations on verdant floating worlds devoid of wolves, mountain lions, and other sharp-toothed threats. It’s possible their isolation has granted them a peace of mind that mainland deer can’t afford. Perhaps by now they don’t even know they could be afraid of other curious creatures.
The second is geography. Since an island’s inhabitants have limited land to roam, it’s easy for them to bump into each other. And as Cove points out in a new research paper in the journal Mammalian Biology, the scattered centers of human activity in the Keys attract animals that can find easy meals, pulling them into an even tighter orbit. The paper focuses on the Key deer and raccoons, but the same could likely be said of cats and island foxes, the latter of which are known to beg tourists for food and sneak off with your peanut butter even though you left it safely on your campsite’s picnic table, you swear.
These two factors account for a greater probability of animal-to-animal encounters on islands, but they don’t explain what would convince a deer to run its tongue over a cat or fox in the first place. Moreover, there’s evidence that this licking isn’t an island thing exclusively: Deer are occasionally spotted giving tongue baths to cats, at least, in mainland backyards. The exact motivation behind this behavior is much harder to pin down.
There’s a temptation to describe their interactions as mutually beneficial, in line with the natural world’s other astounding instances of species-to-species symbiosis. When Lowe first saw the buck and fox together, for example, he was reminded of underwater “safe zones,” where “predators and prey all line up to get cleaned” by small fish that munch on parasites.
Yet as Gary Roemer, an ecologist at New Mexico State University, points out, scientists reserve the concept of mutualism specifically for relationships in which both sides benefit in ways that help them survive. The dynamic is conceivable for Key deer and raccoons; in Cove’s camera-trap photos, a slinking raccoon takes a doe’s snout into its paws and nibbles around the patient animal’s eyes and ears, probably hungry for a snack of ticks. Neither Cove nor Roemer, who spent years studying island foxes earlier in his career, however, are convinced licking does much for the ecological fitness of deer, foxes, or cats.
Both researchers suggested what might be a more obvious benefit for the foxes and cats: Getting licked feels good. “Maybe deer are getting those hard to reach places,” Cove says. As for the deer, ocean breezes cover islands—foxes and cats included—in salt. Cove has a theory that deer on islands particularly might be lured into the cleanings by a little extra seasoning.
But Cove’s tweet about disease transmission also underscores the much more ominous way these pictures can be read. Even if it feels and tastes nice, contact between animals isn’t necessarily positive, because it can cripple populations by passing along rabies, roundworms, and plenty of other viruses and parasites. These dangers are especially threatening in locations where the entirety of a species resides. Key deer and island foxes, both endemic to their respective coastal islands, have each been pushed to the edge of extinction in the past. If their newly observed canoodling sessions hint at any larger changes in island ecosystems, they conceivably are causes of concern.
But Roemer cautions against the impulse to read anything more into a few documented instances of deer licking smaller and probably salty animals than what they perhaps most clearly seem to be: two wild creatures inquisitive enough to give each other a closer look. “This is probably a novel, random, curious interaction,” he says of the buck and the fox. “It probably doesn’t have much significance either from an evolutionary or an ecological standpoint.”
(Roemer doesn’t buy the salt theory, either: If plants and rocks are also coated by the breeze, he reasons, it wouldn’t make sense for a deer to go through the trouble of tracking down a moving, claw-possessing island resident for tastiness alone.)
Still, randomness leaves open two opposite conclusions about interspecies encounters like these. It’s possible—if not certain—that animals bump into each other in all kinds of undiscovered ways. “[Camera traps] are opening our eyes to just how fascinating the natural world is,” Cove says. “There are tons of species interactions that we might have never noticed just casually walking around the woods and stuff.” Key deer might not lick cats for a reason, but that doesn’t mean they don’t do it often.
Then again, it might be misguided to say what all deer on an island do in the first place. “More and more, we’re recognizing that, just like us, animals have different personalities,” Roemer says. “Sometimes they do bizarre things.” While working on one of the Channel Islands, he befriended an exuberant island fox named Josie, who made a game of goading a nature conservancy’s surly hunting dog into chasing her up trees.
So maybe it was an especially bold buck and a uniquely lonely fox that met under that fading afternoon sun on Santa Catalina Island. They neared each other in the brush of the only land they’ve ever known. And when they were close enough to touch, they were both filled with enough wonder to decide: why not?


The rain began on August 25, and it would fall, remarkably, for four more days. We know now that Hurricane Harvey dumped as much as 60 inches of rain over parts of Texas. Twenty trillion gallons in all. The equivalent of the entire Chesapeake Bay. Enough to push the Earth’s crust down two centimeters.
All of that water eventually had to go somewhere. It made its way to the Gulf of Mexico, and its volume was so massive that it did not immediately mix with the ocean. Nearly two months after the hurricane, a distinct blob of freshwater from Harvey is still moving through the Gulf. “We’ve literally never see that much freshwater added to the Gulf of Mexico at once,” says Kathryn Shamberger, an oceanographer at Texas A&M University.
Because it’s so unprecedented, scientists do not know exactly what effects the plume is having. Will it bring a path of destruction through marine ecosystems unused to freshwater? Will contaminants from land sweep through? Or will marine life quickly rebound? In the weeks after Harvey, scientists like Shamberger have mobilized to study the effects of this giant blob of freshwater. Shamberger and her colleagues transected the Gulf sampling water in late September, and they are embarking on a second research cruise this Friday to study the potential impacts on coral reefs.
What oceanographers do know about the interface of freshwater and ocean comes from studying rivers that naturally empty into the sea. The key is density. Because freshwater lacks dissolved salt, it is less dense and floats atop seawater. It becomes a barrier between the air and the ocean water, which can have nasty consequences. “The freshwater sitting on the salty water cuts off the oxygen from the atmosphere getting into the ocean, and then you get the dead zone,” says Steve DiMarco, one of Shamberger’s colleagues at Texas A&M.
Freshwater doesn’t move in one uniform blob, either. It can squirt and jet far away from the main body. Think of what happens when you add milk to your coffee, says DiMarco. In the first few seconds, the milk squirts through the coffee before fully mixing. Eventually, Harvey’s freshwater blob will mix into the ocean, too. DiMarco says he expects for it to take a few more weeks; the winter winds will stir it all up when they come in.
Meanwhile, a number of scientists have received or are expecting Rapid Response Research grants from the National Science Foundation to study Harvey’s freshwater pulse. Lisa Campbell, another oceanographer at Texas A&M, has received one such grant to look at the effects of Harvey’s freshwater on phytoplankton. “Previously when we seen slugs of freshwater, it changes the type of phytoplankton that grow,” says Campbell. And since phytoplankton are at the bottom of the food chain, that can have knock-on effects up the food chain from the copepods to the coral to the fish.
Shamberger and DiMarco are going out to the Flower Garden Banks National Marine Sanctuary, a coral reef about 100 miles off the coast of Texas. Because winds have blown Harvey’s freshwater blob south and west, Shamberger says the corals have likely escaped the worst of it. But the reef may still be susceptible to the aforementioned squirts and jets of freshwater or more subtle changes in phytoplankton. In July 2016, recreational divers had noticed unusually hazy waters and dying corals in the Flower Garden Banks, which scientists suspect were the result of floodwaters that went through Houston during April. In the early days after Harvey, satellite pictures captured brown sediment-rich runoff pouring out of Galveston Bay. The team will also collect samples to test for contaminants.
The Houston Flooding Pushed the Earth's Crust Down 2 Centimeters
“Often when we think about hurricanes and their impact on a coral reef, we are thinking about wave energy and storm surge and physical breakage of the 3-D reef structure,” says Adrienne Correa, a marine biologist at Rice University. But because Harvey dumped so much freshwater into the ocean, it might have this whole other set of effects, and this is a unique opportunity to study them. Correa is leaving on the cruise Friday as well, despite the fact that her home was flooded during Harvey and it’s still “pretty much a mess.”
DiMarco was actually on a research boat in the Gulf when Hurricane Harvey started threatening Texas. When the storm hit, they pulled into port west of Louisiana and started driving back to Texas through the rain. “It was just incredible how much water there was,” say DiMarco. “Every time we went over an overpass, the people in my car—and I was driving—were like, ‘I wonder if we’re going to come down in the middle of lake.’” That was when the planning of his post-Harvey research began. And now, nearly two months later, they’re still tracking the effects of that water through the Gulf.


It took 130 million years for astronomers to see the light. On August 17, scientists observed through telescopes a small, glowing orb, the remnants of a collision between two neutron stars in another galaxy that triggered universe-bending gravitational waves. They watched as the sphere changed from royal blue to crimson red, as lighter chemical elements in the cloud of radioactive debris gave way to heavier ones, like gold, platinum, and silver. About a week later, it faded.
The light show may be over in the night sky, but it can be found on the internet and replayed, over and over, as a dreamy short video:

Luís Calçada created the video for the European Southern Observatory, whose fleet of telescopes in Chile tracked the aftermath of the collision. Calçada is a member of ESO’s education and public-outreach department, a team of astronomers and science-communication specialists in Munich.
“We wanted to have something striking, but we wanted it to be correct,” Calçada said.
The clip is another addition to a rapidly growing volume of illustrations and animations of wondrous astronomical objects and phenomena. As the rate of discovery of exoplanets has picked up in the last several years, so has the production of visualizations of these worlds. Often, scientists and illustrators have only a few pieces of the puzzle, like the mass, temperature, and orbit. They look carefully at how these factors have shaped the celestial bodies that we can see, and use it as inspiration to create a full picture of those we can’t. When astronomers discovered the presence of seven Earth-sized planets in a star system 39 light-years away, illustrators turned tiny blips in data into colorful alien worlds.
Calçada has worked at ESO for about 11 years. He was studying astronomy in school when he started experimenting with computer graphics. He eventually decided that he wasn’t going to be a scientist, and he went into science communication instead. The ESO gig, he said, is the perfect mix.
When exciting research papers come in at ESO, it’s up to Calçada and the rest of the team to create compelling visuals out of the data. They come up with designs for illustrations and videos, send it to the paper authors for feedback, and exchange notes until everyone’s happy with the product. When scientists have few specifics, the animators have some creative freedom. “But of course, some other times, they say no, that wouldn’t happen because the star is too big or the temperature is too high,” Calçada said.
On some rare occasions, illustrators get to check their designs against the real thing—they just have to wait for technology to catch up with them. In 2009, astronomers using ESO’s Very Large Telescope published some research about Pluto’s thin atmosphere, which is mostly nitrogen with some methane. Calçada whipped up a short video to accompany the research. The clip, “filmed” from the perspective of a camera panning over the surface of the dwarf planet, showed a hazy, bluish atmosphere over gray, Arctic-esque terrain. When the New Horizons spacecraft arrived at Pluto in 2015, it returned images of the dwarf plant that looked eerily similar.
Illustrations and animations of scientific research make the cosmos look like a radiant place bursting with color. But it’s worth remembering that the universe doesn’t always look that way to the scientists doing the work, Calçada notes. “They’re usually looking at boring code on the computer,” he said.
Many astronomers rely on spectroscopy, a technique that involves measuring the light across the entire spectrum of electromagnetic radiation. Astronomers use spectroscopic instruments in telescopes to observe different wavelengths coming from an object, from radio waves to visible light to gamma rays. The data, known as spectra, appears to astronomers as a simple squiggly line on a plot, like a cosmic electrocardiogram. But the spikes and dips can reveal a lot about the object in question, whether it’s a single star or an entire galaxy, like its mass, temperature, and chemical composition. In other fields of scientific study, Calçada said, “you can go outside, you can study the rocks, you can study the plants, you can look at the fossils. But astronomers, they only have light.”
Light was exactly what astronomers wanted to find in this kind of discovery. Before this week, scientists had detected gravitational waves four times, but the ripples came from collisions of black holes, which don’t emit any light. Astronomers had no way of knowing where exactly the waves originated. The new finding marks the first time we have seen the source.
Space illustrations sounds like they should evoke wonder at all times, but for the people making them, they can seem pretty routine. Calçada said his first few exoplanet projects were exciting, but as more and more were discovered, the work of animating them began to feel a little mundane. Oh, look! Another rocky planet. Eventually, perhaps soon, the work of visualizing cosmic collisions might lose some of its thrill, too; the scientists running the gravitational-wave detectors predict the extremely sensitive instruments will detect one or more mergers a week.
“But don’t worry, there’s going to be more exciting astrophysics and astronomy,” Calçada said. “There’s always going to be something more exciting and new coming out. I’m not worried of getting bored.”


The Centers for Disease Control and Prevention (CDC) keeps a Most Wanted list for flu viruses. The agency evaluates every potentially dangerous strain, and gives them two scores out of 10—one reflecting how likely they are to trigger a pandemic, and another that measures how bad that pandemic would be. At the top of the list, with scores of 6.5 for emergence and 7.5 for impact, is H7N9.
Influenza viruses come in many flavors—H5N1, H1N1, H3N2, and so on. The H and N refer to two proteins on their surface, and the numbers refer to the versions of those proteins that a particular virus carries. H1N1 was responsible for both the catastrophic pandemic of 1918 that killed millions of people, and the most recent (and much milder) one from 2009. H5N1 is the bird-flu subtype that has been worrying scientists for almost two decades. But H7N9? Until recently, it had flown under the radar.
H7 viruses infect birds, and only very rarely jump into humans. H7N9 in particular had never been known to infect humans at all before 2013, when it caused an unexpected epidemic in China. It was billed as low-pathogenic (or “low-path”) because it only caused mild disease in chickens. But in humans, the story was different: Of the 135 people infected, around a quarter died.
Every year since, there’s been a new epidemic, and the current one is the worst. H7N9 has evolved, acquiring mutations that allow other flu strains to reproduce more effectively in both birds and mammals. It has started killing birds. In one year, H7N9’s highly pathogenic (“high-path”) strains have caused as many human infections as the previous four epidemics put together. As of September 20, there have been 1,589 laboratory-confirmed cases, and 39 percent of those people have died. “It was a matter of time,” says the flu expert Yoshihiro Kawaoka, from the University of Wisconsin-Madison. “It wasn’t surprising to see this change.”
Kawaoka and his colleagues have now studied the new high-path strains collected from one of the people who died this year. They’ve shown that these strains reproduce efficiently in mice, ferrets, and monkeys, and cause more severe disease than their low-path ancestors. They can spread through the air between captive ferrets, and in some cases, kill the animals they land in. Perhaps most worrying, some strains have already evolved the ability to resist Tamiflu, a frontline drug that’s used to treat flu infections.
These are, of course, just animal studies, and they’re an imperfect reflection of how the high-path viruses behave in humans. “The little data available to date does not reveal an obvious increase in virulence for humans,” says Malik Peiris, from the University of Hong Kong, “but this is very difficult to assess because we only see the more severe infections who present to hospitals. This is an issue that needs to be closely monitored in the upcoming winter season.”
“When you compare H5 and H7 viruses, I think H7 are more worrisome,” says Kawaoka. That’s because the H5 viruses need several further mutations to spread between mammals, as Kawaoka showed in controversial lab experiments where he engineered strains with those mutations. But H7 strains apparently don’t need such tweaks. The strains that are out there right now are already capable of spreading between ferrets.
And yet, there’s no strong evidence that they’re hopping from person to person. Some of the cases this year have occurred in family groups, but it’s hard to say if they passed H7N9 between them or simply acquired it from the same birds. For now, the CDC still notes that “the risk to the general public is very low,” since most people who were infected had been in direct contact with birds, whether in poultry markets, vehicles, or their own homes.
“Clearly this is a virus that we don’t want to become any more transmissible between humans,” says Wendy Barclay, from Imperial College London. “But it’s not already transmissible enough to cause a pandemic—otherwise, we would have seen one.” She also notes that, in Kawaoka’s study, the high-path strains didn’t spread any more easily between ferrets than their low-path cousins. Even though this year’s epidemic is unprecedentedly big, the viruses don’t seem to be any more transmissible than when they first emerged in 2013.
There’s also a silver lining to the Tamiflu-resistant strains that Kawaoka identified. The mutation behind this resistance works by changing the shape of a protein on the virus’s surface—a protein that Tamiflu normally attacks. But the same protein is also part of the infection process; by changing its shape, the strains weaken themselves. They cause milder disease in both mice and ferrets (although they still spread with the same ease as the drug-sensitive strains).
That’s good news, but it’s no reason to rest on our laurels. In 1999, scientists discovered a mutation called H274Y that made H1N1 strains resistant to Tamiflu, but that also reduced their ability to infect mouse and ferrets. The scientists thought that this mutation was “unlikely to be of clinical consequence.” They were wrong. H1N1 picked up other mutations that compensated for H274Y, creating flu strains that were infective and resistant. By 2008, almost all the seasonal strains of H1N1 had become resistant to Tamiflu. With H7N9, history could well repeat itself.
But Tamiflu isn’t our only weapon against influenza. There’s an experimental new drug called Avigan (or favipiravir) that, rather than going after a surface protein, attacks an enzyme that the virus uses to copy its genetic material. Even Tamiflu-resistant strains of H7N9 fall to this drug, as do other kinds of flu that Kawaoka has looked at—at least in animals. “Whether that’s also the case in humans, we don’t know,” he says.
The viruses could eventually evolve to resist this new drug, too. But, Kawaoka says, “many people, including us, have looked for viruses that are resistant to favipiravir, and I don’t think anyone has found one yet.” And Barclay suggests that scientists should start running clinical trials that test both drugs together. “It still astonishes me that we continue to treat flu patients with a single drug when we know that the virus is highly mutable,” she says. “It’s almost inevitable that drug-resistant viruses can evolve.”
In the meantime, vaccines are being developed to match the viruses seen in the fifth and current epidemic. Other control measures have waxed and waned. When the first of the epidemics struck, Chinese health ministries closed markets and slaughtered birds. But as Helen Branswell reports in STAT, some of those containment efforts became more lax in 2015 and 2016.
Again, there is some good news: H7N9 infects chickens very well, but unlike H5N1, it seems to avoid ducks. That matters because Chinese ducks are often housed outside, and domestic birds can mingle with wild ones. Aboard ducks, bird flu can easily spread from one infected farm to other parts of the world. “That may be a major difference that may make it easier to control H7N9 compared to H5N1.”
It might also be a blessing in disguise that the high-path strains have emerged. The low-path strains were very hard to detect because they didn’t cause symptoms. But the high-path viruses kill infected birds, which means “they might be easier to eradicate from chickens since they can be more easily detected,” says Adolfo García-Sastre, from the Icahn School of Medicine at Mount Sinai in New York. “However, one would need a very well-organized eradication campaign to eliminate them from poultry before they spread to other areas beyond China. I’m afraid that this will not happen, since it did not happen with the H5N1 viruses, which were first detected in 1997, and finally disseminated to most of the rest of the world starting in 2003.”


The bottles were getting emptier: That was the first sign that something awful was happening.
Since 1989, scientists from the Entomological Society Krefeld had been collecting insects in the nature reserves and protected areas of western Germany. They set up malaise traps—large tents that funnel any incoming insect upward through a cone of fabric and into a bottle of alcohol. These traps are used by entomologists to collect specimens of local insects, for research or education. “But over the years, [the Krefeld team] realized that the bottles were getting emptier and emptier,” says Caspar Hallmann, from Radboud University.
By analyzing the Krefeld data—1,503 traps, and 27 years of work—Hallmann and his colleagues have shown that most of the flying insects in this part of Germany are flying no more. Between 1989 and 2016, the average weight of insects that were caught between May and October fell by an astonishing 77 percent. Over the same period, the weight of insects caught in the height of summer, when these creatures should be at their buzziest, fell by 82 percent.
“We were expecting declines, but the extent of them was tremendous,” says Hans de Kroon, who was involved in analyzing the Krefeld data. “If this was in agricultural settings, we wouldn’t be quite so surprised. But it’s especially alarming that it happened in nature reserves.”
There have long been signs of such a decline. Studies have also shown that populations of European butterflies have halved since 1990, honeybee colonies have fallen by 59 percent in North American since World War II, and populations of British moths have dropped by 30 percent per decade. But most of these surveys focused on particular groups, whereas Hallmann’s group looked at the entire spectrum of flying insects. “It confirms the widespread, windscreen phenomenon,” he says. “Any truck driver in the developed world will tell you that they used to squash a lot of insects on the windscreen. Now the windscreens stay clean.”
“The study makes visible what otherwise has been an invisible decline in insect abundance,” says Michelle Trautwein, from the California Academy of Sciences. “Our mistreatment of the planet has been recognizably bad for elephants and coral reefs, but it seems likely that it has also been just as bad for flies, moths, beetles.”
This is, to put it mildly, a huge problem.
Insects are the lynchpins of many ecosystems. Around 60 percent of birds rely on them for food. Around 80 percent of wild plants depend on them for pollination. If they disappear, ecosystems everywhere will collapse. But also, insects are the most diverse and numerous group of animals on the planet. If they’re in trouble, we’re all in trouble.
There’s a debate about whether the Earth is in the middle of a sixth extinction—an exceptionally severe period of biological annihilation of the kind that has only happened five times before. One of the talking points in this debate is that, as Peter Brannen recently wrote, “when mass extinctions hit, they don’t just take out big charismatic megafauna, like elephants ... They take out hardy and ubiquitous organisms as well—things like clams and plants and insects.”
And. Insects.
But remember that the German study only looked at one particular region. And it raises a question: If insects have disappeared by such a large degree, wouldn’t other species that depend on them be in much worse shape? Wouldn’t Germany’s flowers, birds, spiders, and reptiles also be plummeting? “We see great declines of insectivorous species—but not to this extent in most cases,” de Kroon acknowledges. “Some species could switch food sources, but we don’t really know what’s going on. We do know that we see declines in even common species, like blackbirds, starlings, and sparrows.”
Another unanswered question: Are all groups declining equally? “It would be interesting to see the list of species they collected, as Malaise traps are very good at collecting certain species and poor at collecting others, like dragonflies,” says Jessica Ware, from Rutgers University. “If insect [groups] vary in their response to climate change, temperature, habitat change, or other factors,” that could change the implications of the study’s stark percentages. (Hallmann notes that identifying the thousands of individuals in a single trap, let alone all 1,503, would mean months of work for a team of specialists. That’s why they focused on total weight.)
Also, what’s behind the insect downfall? Pollutants and pesticides are likely to be a problem. Neonicotinoids—the world’s most popular insecticides—can mess with bees in myriad ways, impairing their memory, befuddling their spatial skills, and preventing them from finding food.
More surprisingly, the German team couldn’t find any evidence that the two usual suspects—habitat loss and climate change—were important culprits. The declines were similar in every kind of habitat, whether healthy grasslands or nutrient-poor wastelands. And although weather patterns in the region could explain the numbers of insects across a season, they couldn’t account for the year-on-year decline.
But neither line of evidence is clear-cut. The team didn’t look at larger-scale climate events, like prolonged droughts, and they couldn’t measure the effect of habitat fragmentation—cutting up the land available to insects rather than merely reducing it. Indeed, the nature reserves in the German study are small, too distant from each other for insects to travel between, and locked in by agricultural land. Those are “hostile environments” for insects, de Kroon says, so species that thrive in the reserves could drain into the surrounding no-man’s-land—and be lost.
Ware wonders if some of the vanished insects are simply migrating into other areas. “We know that certain dragonflies, for example, are changing their ranges in response to climate change,” she says. “So are neighboring countries experiencing a similar loss, or are specific species moving northward?”
That’s why researchers need to do similar surveys in other countries, says Crystal Maier, from Chicago’s Field Museum. “We could actually do that. We have similar samples here at the museum, for similar ranges and time periods. You could spend a lot of time identifying species but they just weighed the samples. That’s something we don’t usually do but it’s so simple, and it would be interesting.”
In the meantime, “we should use anything we have to enhance insect populations, like adding flower-rich areas around the margins [of agricultural land],” says Hallmann.
“We don’t want people to get depressed,” says de Kroon. “Ecosystems are very resilient. They’re still functioning quite well despite this loss. Let’s make use of that resilience. We can’t wait till we know exactly what’s leading to these losses. We have to act.”


Last year, a 77-year-old woman traveled to a clinic in Georgia to have stem cells injected in her eyes. She came in hope of a cure—or at least something that could help her macular degeneration, which causes a dark spot to appear in the center of vision.
The procedure was supposed to work like this: The clinic would take fat from her belly, separate out stem cells that naturally occur in fat, and inject them into her eyes to regenerate damaged tissue. The procedure cost $8,900. It had not been approved by the Food and Drug Administration and was not covered by insurance. To pay out of pocket, she had to raise money on a crowdfunding site.
Her vision did not get better. It got much worse. Within three months, her retinas—the eye’s layer of light-sensitive cells—had peeled away from the rest of her eyes. As a result, she can only make out hand motions in her right eye and light in the left, according to a recent case report. She could no longer walk on her own.
In March, eye doctors based primarily at the Bascom Palmer Eye Institute in Miami had published a widely covered report describing three eerily similar cases: Three elderly women with macular degeneration got stem cells derived from their own fat injected into their eyes at a different stem-cell clinic in Florida. The same thing happened: Their retinas became detached, and they went blind. The doctors ended up examining the 77-year-old woman too, which led to the recent case report describing her condition.
And there are likely even more cases. Since writing the first report, says Ajay Kuriyan, an author on the report and now a retinal specialist at the University of Rochester, eye doctors around the country have come forward with similar stories of stem-cell injections gone awry. They are now preparing an article describing the additional cases.
* * *
Stem-cell clinics that offer seeming miracle cures for everything from back pain to erectile dysfunction have proliferated in the United States in the past decade. These cases of blindness now cropping up in the medical literature point to the potential dangers of letting hundreds of such clinics operate without oversight.
In August, the FDA moved toward a crackdown. It posted a warning letter to the Florida clinic that had treated the first three women and called the fat-derived stem cells an unapproved treatment. On the same day, the agency announced that federal marshals had seized live-virus vaccines from a California company that was injecting the viruses along with stem cells into cancer patients. After the news broke, says Mark Berman, a plastic surgeon and the California company’s director of stem-cell implantation,“I’ve actually had patients call me up, cancel their surgery, demand their money back, and tell me what a disgusting human being I am and I should be removed from this planet.” He criticized the initial news reports as “classic leftist kind of propaganda, fake news.”
Berman also cofounded the Cell Surgical Network, of which the Georgia clinic that treated the 77-year-old woman is an affiliate. The network trains affiliated doctors to use their equipment and follow their stem-cell therapy protocols.
The case report does not name the Georgia clinic, but The Atlantic has independently confirmed it is the Stem Cell Center of Georgia, which operates within the Ageless Wellness Center in Peachtree City. The clinic declined to comment for this story. A local news report from June 2016 quotes the center’s doctor as saying, “We have an ophthalmologist who is going to treat three people with macular degeneration with intraocular injections.”
Berman says that his network’s affiliates have performed about 15 eye procedures total. They stopped offering it after the woman went blind. (Cell Surgical Network and the Stem Cell Center of Georgia both still list macular degeneration on their websites.) The injections, he says, were part of a study approved by an institutional review board. At the clinic, the 77-year-old woman received injections into her two eyes one day apart. Berman concedes that they should have waited longer to make sure there were no serious side effects after the first eye. “That’s a pretty good lesson learned. Unfortunately it was learned by doing them,” he says.
Others say the clinic should have known better. “It’s just not a professional thing to take an unproven intervention and inject it in both eyes,” says Leigh Turner, a bioethicist at the University of Minnesota, who tracks stem-cell clinics. Kuriyan says that injecting both eyes and asking patients to pay out of pocket for their treatment are both highly unusual for clinical trials. “Those are all big red flags,” he says. A better approach, he says, would have been to test the injections in animals for safety first.
The Dangers of Snake-Oil Treatments for Autism
It’s unclear exactly why the stem-cell injections caused such a bad reaction in these women. Perhaps the stem cells had differentiated into cells that formed a membrane and then contracted, peeling the retina away from the rest of the eye. Or perhaps there was scarring caused by immune cells, which are part of the mix of cells in fat that can be injected along with stem cells into the eye.
Given the growing number of retinal detachments coming to light, Turner wonders if there are other consequences of stem-cell injections that have not been reported. Berman says that the Cell Surgical Network has performed 7,000 stem-cell injections into various parts of the body, and the 77-year-old woman’s case is “the only real significant problem we’ve had.” A FDA inspection report from July chastised his network for failing to investigate and report four “serious adverse events” to the FDA, including the 77-year-old woman’s case as well as a hip infection, another “severe infection,” and a case where a patient was hospitalized for confusion and headache.
The FDA has drafted guidelines on how to oversee stem-cell clinics. The agency says stem cells do not have to be regulated as drugs as long as clinics follow certain standards, like if they only minimally manipulate the cells and don’t change their purpose in the body. For example, transferring fat from the belly to breasts would not fall under FDA purview because the fat is still acting as fat. But if stem cells are being separated from fat and then injected to treat a disease, then the FDA may have reason to step in.
“The question to me is, ‘Will the FDA really dedicate the resources that are needed?’” says Turner. The FDA very publicly criticized a couple stem-cell clinics in August as a warning shot, but there are hundreds of such clinics in the Untied States, and overseeing them all would require a significant investment. In just the five years Turner has been closely tracking the stem-cell industry, he says, business seems to have been booming.


Roses are red, violets are blue. Except they’re not. They’re, er, violet.
True blue flowers are exceedingly rare, and not for lack of effort. Plant breeders have repeatedly tried to nudge roses and chrysanthemums into blueness, but doing so is really hard (at least, without the use of dyes). These flowers get their colors from pigments called anthocyanins, which typically look pink or red. A flower must chemically tweak these pigments to make them bluer, and even if they did, the results are essentially purple.
Only a few flowers, like cornflowers and Himalayan blue poppies, have achieved true blue, and all by using special chemical tricks like adding metals to their pigments, or making their petals more alkaline. “All of this is chemically quite difficult and not many species have evolved the enzymes to do it,” says Beverley Glover from the University of Cambridge. “Even with genetic modification, people have managed to make purple, bluish roses, but true blue isn’t happening.”
So imagine her surprise when she found that many flowers have secret blue halos in their flowers.
The halos are rings at the bases of the flowers’ petals. Sometimes, they’re visible to us, especially if the petals are dark. But in most cases, they’re so faint that we can’t see them. Glover only detected them with the aid of laboratory equipment. And yet, they’re there—and they’re visible to bees, whose eyes are more sensitive to blue wavelengths of light than ours are.
Glover discovered the first of these halos back on a sunny day in 2009. While walking through the Cambridge University Botanic Garden, she came across Hibiscus trionum—a white flower whose petals have dark-red bases. And those bases, Glover noticed, were shiny. If you moved them around, they would take on blue, yellow, or green metallic sheens on top of the red undercoats.
The cells on the surface of these petals are mostly flat and smooth, but those at the base have many microscopic ridges, like the grooves on a vinyl record. When light hits each ridge, part of it reflects away, and the rest passes through to the other ridges. And because the ridges are regularly spaced, the reflected beams amplify each other to produce colors that are unusually vivid from certain angles. That’s iridescence. It’s color produced not by chemical pigments, but by microscopic structures. “We wanted to know how common this was,” Glover says. “Was this just one or two weird plants—or more?”
More, it turned out. Her team, including Edwige Moyroud and Tobias Wenzel, found at least 11 families of flowers in which at least one species has the same microscopic ridges, and the same iridescent halos. At first, they were puzzled because the iridescence is inconsistent—strong in some species, but weak in others. But weak or strong, it always has a blue component. “If we had just looked at one plant, we wouldn’t have thought anything of it,” Glover says. “It’s only when we looked at a dozen species, and they were all making the same blue, that we thought: Maybe it’s the blue we should be looking at.”
And when they took a closer look at the petals producing the blue halo, they found something stranger.
To create a vivid sheen, those microscopic ridges should all have the same dimensions, and be equally spaced apart. But biology is messy, and plants can’t manufacture structures to such exacting specifications. So the actual ridges come in a variety of heights, widths, and spacing. Glover’s team found that the degree of this variability, which they term “disorder,” is the same across flowers. And for reasons that are still unclear, this particular amount of disorder scatters blue light at specific angles away from the petals. Hence the halo.
The team tested this by creating artificial ridges of their own, with varying degrees of disorder built in. If the ridges are massively disordered and very different from each other, they don’t produce any interesting optical effects. If they’re all the same, they’re iridescent, but there’s no blue scattering. But in the Goldilocks zone, where the ridges are disordered but not too disordered, they create a blue halo. “It’s a different way of making blue,” Glover says.
If the halo lies over a black, purple, or red base, it’s possible for humans to see it. If it lies over white or yellow, we can’t. But Glover’s team showed that bumblebees can see the pattern no matter the background, and in lab conditions, they find flowers more quickly if they have a blue halo.
“When the same trait evolves over and over again, it is strong evidence that it’s adaptive,” says Lena Hileman from the University of Kansas. And since the blue halo is “very widespread in flowering plants, that suggests it is a reliable signal to pollinators about the nectar reward of the flower.”
But “being able to perceive the pattern doesn’t necessarily mean the pollinators actually care about it, or prefer it to simple, pigment-based colors,” says Yaowu Yuan from the University of Connecticut. “The importance of this blue halo to pollinators is still an open question.”


In addition to the cars flipped, the roofs collapsed, and the streets flooded, this season’s unprecedented string of devastating hurricanes also hit a much grander target: a volcano. The Caribbean island of Montserrat, home to the active Soufrière Hills volcano, twice suffered blows: first an almost-direct hit by Category 5 Hurricane Irma, then a direct hit by Category 5 Hurricane Maria.
The volcano’s peak this year was, thankfully, more noisy than explosive. No volcano-related calamities were reported from the storm. But the collision of the two disaster-movie terrors reveals the possibility of an unnerving scenario: What happens when a superstorm lands on a volcano that’s erupting?
While there’s no record of a top-of-the-scale Category 5 hurricane or super typhoon ever running over such a volcano, there are hundreds of active volcanoes in the tropics—including 20 in the Caribbean—where some of the worst storms start tearing things apart. And evidence suggests a warming climate only intensifies these storms. No one is certain about what would happen in a matchup, but most likely neither of the two primeval forces could take the stuffing out of the other. Even so, the battle could prove especially deadly for people caught nearby.
Some meteorologists and geologists speculate that the collision would be like a pair of agitated gorillas: two giants making a lot of noise, shaking the ground, and then parting ways. It’s unlikely the volcano would make much of a dent in this battle. “As massive as the impact of a volcanic eruption can be, it’s hard to grasp the scale of a hurricane,” says David Nolan, an atmospheric scientist at the University of Miami. “The eye wall of a hurricane typically has a radius of 30 miles. Off the top of my head, I'd say the primary updraft from a volcano is just one mile across.”
As a result, he explains, almost anything a volcano can do to disrupt a storm would be localized and likely erased as it moved along. Tracy Gregg, a geologist at the University at Buffalo, also emphasizes how little impact typical volcanic eruptions would likely have on such a powerful storm. “A volcano punching up through [a hurricane] would be a smudge on the windshield,” she says.
Nevertheless, there are certain circumstances that could be much more extreme. It’s possible that a volcano’s intense heat—lava can register 2,200 degrees Fahrenheit—could intensify a tropical cyclone. Heat evaporates seawater, which rises to create thunderstorms. No heat, no storm. And an explosive eruption might still temporarily disrupt areas within a hurricane. For instance, dust shot into the atmosphere could delay or accelerate rainfall, Nolan says. One study suggests the dust would contribute to more lightning. In fact, massive eruptions often generate cinematic lightning within their boiling black clouds.
More intriguing—and terrifying—is a theory that a submerged, massive volcano could heat enough ocean water to supercharge a Category 5 hurricane or super typhoon. Heating the already abundant heat of tropical waters would make storms bigger and stronger, and do it in less time. Jeff Masters, the meteorology director at Weather Underground, says such an underwater eruption might even spin up something called a hypercane, with winds reaching the speed of sound.
It would take “an awful big coincidence” for this to happen, Masters reassures: The volcano would have to be big enough and hot enough to heat hundreds of miles of the ocean’s surface to 122 degrees. Standard weather dynamics would take it from there. “It’s theoretically possible,” Masters says. So is the possibility that a hypercane could inject enough water into the stratosphere to block the sun, creating a years-long global winter.
Team Hurricane’s fever dream is more mundane. Some think the low air pressure in a hurricane’s eye might weakly draw up the contents of a volcano, like a TV cowboy sucking venom from his cowpoke friend's snakebite wound. But “that’s really unlikely,” Gregg says. After all, volcanoes begin 1,800 miles down, where temperatures exceed 6,700 degrees, and liquid rock (or magma) is squeezed by pressures that are 1.4 million times greater than air at sea level. A change in air pressure over a volcano is like a fly buzzing off an elephant’s back.
The feet of water that a storm can drop also isn’t likely to have any effect. The entire state of Hawaii, for example, is a collection of gurgling volcanoes that rose through 20,000 feet of water from the floor of the Pacific. Volcanoes experiencing a Category 5 hurricane might as well be enjoying a music-festival mist station.
Major hurricanes slamming into volcanoes could still have a devastating impact, but the victims would be the environment below the crater—and anyone who lives there. As often happens even with lesser hurricanes, rain soaks into the enormous deposits of fine ash resting precariously on a steep volcano’s flank until the resulting warm mud suddenly breaks free. The rush of debris, known as a lahar, roars down the slope at 120 miles per hour like a runaway freight train. Anything—and anyone—not washed away is encased in the mud.
In 1991, Typhoon Yunya hit Mount Pinatubo on the Philippines island of Luzon and triggered lahars that, according to Masters, killed as many as 300 people. Yunya, barely a typhoon at the time, delivered a glancing blow to Mt. Pinatubo as the mountain—coincidentally—ended almost a year of fitful activity with the 20th century’s second-most-monstrous volcanic explosion. The lahars easily inflicted more damage than the eruption.
At lease one positive could result from a geological-meteorological grudge match, Gregg points out. The hurricane likely would act like an air filter, trapping motes of dust in raindrops. Dust then would fall to earth almost immediately, instead of into the atmosphere, as typically happens with all large, explosive volcanoes. Up there, the dust would circle the globe, reflecting sunlight back into space and measurably disrupting air temperatures for years.
Asked if he would ever buy a ticket to Montserrat, the island struck by Irma and Maria, to witness such a matchup, Masters paused a surprisingly long time before laughing. “Maybe when I was younger, I would. I don't know.” It sounded like a “no,” but as with hypercanes, he couldn’t rule it out completely.


A researcher at a Swedish university says that Viking burial clothes bear the word “Allah”—and some people really want to believe her.
Annika Larsson, a textile researcher at Uppsala University who was putting together an exhibit on Viking couture, decided to examine the contents of a Viking woman’s boat grave that had been excavated decades ago in Gamla Uppsala, Sweden. Inspecting the woman’s silk burial clothes, Larsson noticed small geometric designs. She compared them to similar designs on a silk band found in a 10th-century Viking grave, this one in Birka, Sweden. It was then that she came to the conclusion that the designs were actually Arabic characters—and that they spelled out the name of God in mirror-image. In a press release, she described the find as “staggering,” and major media outlets (including The New York Times, The Guardian, and the BBC) reported the story last week.
But other experts are not sure the silk bears Arabic script at all, never mind the word “Allah.” They warn that people being credulous of Larsson’s claim may be guided less by solid evidence than by a political motivation: the desire to stick it to white supremacists.
“Everybody wants a counter-narrative for the narrative that’s been put forward by white supremacists,” said Stephennie Mulder, an associate professor of Islamic art and architecture at the University of Texas at Austin. She was referring to the tendency of white supremacists to appropriate the symbols of Vikings, whom they claim constituted a pure-bred white race; in Charlottesville, for example, neo-Nazis were seen toting banners with Viking runes. The idea that Vikings were influenced by Muslims would likely be anathema to them. “The Vikings are every white supremacist’s favorite white guy.”
Mulder took to Twitter on Monday to debunk Larsson’s claim. In a 60-tweet thread, she described her three main issues with it.
First, the style of Arabic that Larsson says she has identified—square Kufic—is not known to have been used in the 10th century; it only became common about 500 years later.

It’s a style called square Kufic, and it’s common in Iran, C. Asia on architecture after 15th c., ex: Safavid Isfahan w/Allah and Ali 9/60 pic.twitter.com/pbGJNFITGk

Second, even if you read the script as Arabic, it does not say “Allah” but “lllah,” a meaningless non-word. In place of an alif or “a,” it has a lam or “l.”

The word “Allah” in Arabic looks like this: الله. It has an upright alif, two more uprights (lam), and a final ـه 'ha' 29/60

Third, the end of the word “Allah” does not actually appear in the artifact; instead, it’s part of what Larsson imagines might have existed beyond the edges of the frayed fragment we have today. Larsson is looking at the pattern that’s visible and extrapolating what may have been beyond it, as part of her attempt to reconstruct what the artifact might have looked like in full.

There is a small triangular shape, but no final ha ـه. Frag. was published in 1938 by Agnes Geijer, original drawing looked like this: 31/60 pic.twitter.com/DxDossuWzs

But reconstruction drawing by @UU_University textile archaeologist Annika Larsson shows extensions on either side that include a ha. 32/60 pic.twitter.com/1NyQzcqDV2

These extensions practically double width of band. Not mentioned in press accounts: Larsson’s extensions are entirely conjectural. 33/60

This reconstruction is unfounded, according to textile expert Carolyn Priest-Dorman, who told me the artifact could not have extended farther out (to include the end of the word “Allah”) given how narrow its borders are: “Larsson’s saying the artifact was wider than it is.”
“She might be indulging in some fanciful readings that aren’t justified by the evidence,” agreed Paul Cobb, a professor of Islamic history at the University of Pennsylvania. He clarified that it’s already an established fact that the Viking world and Muslim world were closely integrated through trade and travel; he and other experts like Mulder and Priest-Dorman aren’t disputing that. They’re only disputing whether these specific burial clothes truly bear Arabic script.
“People want to see Arabic there, because it resonates today with a dream of a more inclusive Europe. There’s a real desire to document that Vikings had interactions, not to mention intermarriages, with many non-Vikings,” Cobb said. “That flies in the face of the white supremacists, who see Vikings as Nordic warriors defending Europe from foreign pollution, when nothing could be further from the truth. They were one of the great international societies of the Middle Ages.”
In fact, for Vikings, Arabic may have come with cultural cachet. They circulated coins bearing Arabic inscriptions, as well as weights for measuring silver bearing pseudo-Arabic inscriptions (writing that imitates the look of Arabic but doesn’t get it quite right). In a journal article for Current Swedish Archeology, scholar Lotta Fernstal writes that Vikings may have used the language to “‘spike’ certain objects with additional meaning” as part of constructing their self-image. “It seems likely that the Eastern, Oriental, Arabic and/or Islamic was alluring and desirable, perhaps as an ideal image of the ‘Other’ as part of a Viking Age Orientalism,” she adds.
Given this exoticizing attitude, Mulder said, it wouldn’t be surprising if Vikings were to have bought funeral clothes with Arabic inscriptions. “It would be like, for us, buying a perfume that says ‘Paris’ on it,” she told me. “Baghdad was the Paris of the 10th century. It was glamorous and exciting. For a Viking, this is what Arabic must have signaled: cosmopolitanism.”
Priest-Dorman added that it wouldn’t be unusual to find an eclectic mix of styles in the burial finery of a single Viking woman. “Everything beautiful goes with everything else beautiful—that is the Viking aesthetic.”
Still, the critics believe there isn’t yet enough evidence to support Larsson’s claim, and are concerned by how quickly her non-peer-reviewed findings went viral. “If stories like this are not fully fact-checked, white supremacists can then say, ‘Look, that wasn’t Arabic at all, the journalists are just pushing their PC agenda,’” Mulder said. Or as Cobb put it: “The story might well support my political views about Europe, but if it’s poorly documented, that makes it an easy target.”
In an email, Larsson indicated to me that additional details of her research are forthcoming. Another word that she says she discovered in the burial clothes—“Ali,” the name of the fourth caliph of Islam, revered especially by Shia Muslims—does not appear in the same artifact that purportedly bears the name of God. “Ali is not depicted in this ribbon. It is to be found on other ribbons that I’m working with and that is to be published in a coming work.”
Responding to critics who say that the burial clothes say “lllah” and not “Allah,” Larsson wrote, “If it is another word, it is still Kufic … that’s interesting.” She does not agree with the experts who say that there’s a dating issue with that claim and that “lllah” is a senseless jumble of letters.
“The meaning of research is to open questions,” Larsson added. “This discovery opens new questions.”
That, at least, is certainly true. Perhaps unusually for questions in medieval archeology, these questions feed directly into a contemporary heated political debate. The answers and debunkings of those answers are sure to be used as fodder by the left and the right alike.


In between its silly chatrooms and custom emojis, Slack is a place where real work gets done. But in some offices—no offense—the projects managed on the messaging platform are way cooler than others. Some even have cosmic significance.
On August 17, observatories in the United States and Italy detected gravitational waves, forces that bend the fabric of the universe, as they washed over Earth. Space telescopes observed a short gamma-ray burst, a powerful beam of radiation, coming from about the same part of the sky about two seconds later. Astronomers around the world quickly jumped into action, mobilizing dozens of ground-based and space telescopes to search for the source of these mysterious events.
There were calls and emails and, in the case of a few scientists who work for the University of California, Santa Cruz, and their associates, there were Slack messages. Ryan Foley, an astronomer at the university, was in Copenhagen when the alert went out. He started giving orders at 8:51 a.m. Pacific Time, then jumped on a bike and headed to his office at the city’s Dark Cosmology Center, where Dave Coulter and his fellow astronomers were already working.
A few hours later, they would find the source of the mysterious observations: a massive collision of neutron stars in a galaxy 130 million light-years from Earth. The discovery, announced Monday, would produce the first-ever image of a cosmic merger powerful enough to generate gravitational waves, spawn scientific reports from some 3,500 scientists, and open a new chapter in astronomy.
But Foley, Coulter, and their colleagues didn’t know that yet.
The team drew up a list of galaxies in the region of sky where the near-simultaneous events were detected and started looking for the source. Screenshots of their messages, which they published along with their paper in Science, show a cascade of real-time coordination and information. Several people were typing. (The full exchange starts on page 26 here.) The astronomers searched images from the Carnegie Institution’s Swope telescope in Chile, looking for a “transient,” an object that hadn’t been observed in the data before.
The first few images didn’t turn up anything, said Charles Kilpatrick, a postdoctoral researcher at UC Santa Cruz, who was in California at the time.
Then, in their ninth photo, they found something.
There it was, the spectacular afterglow of a cosmic explosion.
The Swope telescope was the first to capture the light from the merger. In the coming days, dozens of observatories would study the event at every wavelength of light, from gamma rays to radio waves.
Chat services like Slack have become ubiquitous in many offices in the last several years, tethering modern workers to their jobs in new ways that, by now, seem pretty standard. There’s nothing special these days about real-time, and often nonstop, communication among coworkers, whether they’re down the hall or across the pond. But there’s something particularly interesting, even mesmerizing, about watching extraordinary moments unfold in an ordinary space. When all is said and done, the participants can return to this strange day in their chat history and look at the exact moment when things started to change.
Michele Bannister, an astronomer at Queen’s University Belfast, has a similar record, from January 2016. Back then, Bannister and a small group of researchers, spread out in locations in Canada, the United States, France, and Taiwan, were working on the Outer Solar System Origins Survey, a search for objects beyond the orbit of Neptune. Bannister’s colleague was processing images from the Canada-France-Hawaii Telescope in Hawaii when he noticed something unusual and pinged her on Flowdock, a group-chat client.
Bannister headed to J.J. Kavelaars’s office, which was two floors down from hers, and examined the picture. There was a tiny spot of bright light. “We found ourselves our very own dwarf planet,” Bannister said. The dwarf planet, they eventually determined, is among the 20 largest worlds that have been discovered so far beyond Neptune.
Bannister said she enjoyed seeing the messages that preceded UC Santa Cruz’s big find. Before they knew what they had, the chat seemed to resemble just another day at the office.
“It’s so much a typical workday for all of us,” she said. “And most of the time, it’s something that you expect to get in your data. And every so often, it’ll be something like, oh, okay. That’s fun.”


“Imagining the future is a kind of nostalgia.” This is a line from John Green’s young-adult book Looking for Alaska. It’s pretty, and melancholy, and very popular on Tumblr. It’s also scientifically accurate.
Imagining the future is a kind of nostalgia, because humans predict what the future will be like by using their memories. This is how things you do over and over again become routine. For example, you know generally what your day will be like at the office tomorrow based on what your day at the office was like today, and all the other days you’ve spent there. But memory also helps people predict what it will be like to do things they haven’t done before.
Say that you are imagining your future wedding (if you’ve never gotten married before). You probably see it as a scene—at a church, or on the beach, or under a wooded canopy in a forest with the bridal party all wearing elf ears. There are flowers, or twinkling lights, or mason jars everywhere. You can envision the guests, how they might look, what your soon-to-be spouse is wearing, what look they have on their face. All of these details come from your memory—of weddings you’ve been to before, as well as weddings you’ve seen depicted in pop culture, or in photo albums. The scene also relies on your memory of your friends and family.
“When somebody’s preparing for a date with someone they’ve never been on a date with before, or a job interview—these situations where we don’t have past experience, that’s where we think this ability to imagine the future really matters,” says Karl Szpunar, a professor of psychology at the University of Illinois at Chicago. People “can take bits and pieces, like who’s going to be there, where it’s going to be, and try to put all that together into a novel simulation of events.”
The first clue that memory and imagining the future might go hand in hand came from amnesia patients. When they lost their pasts, it seemed, they lost their futures as well. This was the case with the famous patient known by his initials, “H.M.” H.M. had epilepsy, and to treat it, he received an experimental surgery in 1953 that removed several portions of his brain, including almost his entire hippocampus, which is a vital brain structure for memory. After the surgery, H.M. had severe amnesia, and also appeared to struggle with the future. A researcher once asked H.M., “What do you think you’ll do tomorrow?” He replied, “Whatever is beneficial.”

Since then, functional MRI scans have allowed researchers to determine that many of the same brain structures are indeed involved in both remembering and forecasting. In a study Szpunar did, he and his colleagues looked at activity in the brain’s default network, which includes the hippocampus as well as regions that involve processing personal information, spatial navigation, and sensory information. They found that activity in many of these regions was “almost completely overlapping” when people remembered and imagined future events, Szpunar says.
Researchers are still trying to pin down exactly how different brain regions are involved in these processes, but much of it has to do with the construction of scenes. You can remember facts, sure, and you can make purely informational predictions—“We will have jet packs by 2050”—but often, when you remember, you are reliving a scene from your memory. You have a mental map of the space; you can “hear” what’s being said and “smell” smells and “taste” flavors; you can feel your emotions from that moment anew. Similarly, when you imagine something you might experience in the future, you are essentially “pre-living” that scene. And just as memories are more detailed the more recent they are, imagined future scenes are more detailed the nearer in the future they are.
“It takes so much cognitive effort to come up with detailed simulations,” Szpunar says. “So it’s like, why would you spend all that time when it’s not going to happen for 30 or 40 years? Whereas if it’s something happening this weekend, and you’re like ‘How’s this date going to go?’—those things, people just anguish over them and really come up with these detailed simulations.”
When people try to imagine the more distant future—that classic interview question, “Where do you see yourself in 10 years?”—they tend to rely heavily on something called a cultural life script. This is the progression of events that a life in a certain culture is expected to contain. In much of the West, the cultural life script is something like: go to school, move out of your parents’ house, get one or more college degrees, find a job, fall in love, get married, buy a house, have kids, retire, have grandchildren, die. Not everyone expects their life to contain all of those events, but they’re aware of those milestones and will generally tell their life story using them as a framework. The further into the future you try to imagine, the more unknowns there are, so people reach for these events.
“We can’t really imagine or think that far into the future, and we can’t remember that far back, if we don’t have this cultural life script as a kind of skeleton for our life story,” says Annette Bohn, a professor of psychology at Aarhus University in Denmark. In studies Bohn has done with adolescents, their conception of a script seemed to develop in parallel with their ability to remember the past and imagine the future. (At the other end of the life course, older people’s ability to imagine the future declines in tandem with their memory.)
It’s not hard to see how this ability to imagine the future gives humans an evolutionary advantage. If you can plan for the future, you’re more likely to survive it. But there’s are limitations as well. Your accumulated experiences—and your cultural life script—are the only building blocks you have to construct a vision of the future. This can make it hard to expect the unexpected, and it means people often expect the future to be more like the past, or the present, than it will be.
In a similar vein, people tend to underestimate how much their feelings and desires will change over time. Even though they know that their personalities have changed a lot in the past, they have a tendency to think that the person they are now is the person they will be forever. This applies more broadly, too. You can see it in the technological advances imagined in science fiction. As my colleague Adrienne LaFrance wrote, while Back to the Future II (made in 1989 and set in 2015) made a lot of canny predictions—it got videoconferencing and drones right—it also thought people would still be using pay phones and fax machines. Which makes sense, given how ubiquitous those technologies were at the time the film was made.
There’s also an “optimistic, extreme positivity bias toward the future,” Bohn says. To the point that people “always say future events are more important to their identity and life story than the past events.” Talk about being nostalgic for the future.
But it might help people temper their expectations if they keep in mind that even though they can dream up detailed, novel scenes of things yet to come, their imagined futures are really just projections of their pasts. The future holds more surprises—and, potentially, more disappointments—than we might predict.


The deadliest and most destructive spate of fires in California’s modern history continues to burn. The string of massive fires in the Napa and Sonoma Valleys have little precedent: They have killed 40 people, destroyed more than 5,700 structures, and incinerated more than 200,000 acres of land. The Washington Post reports that hundreds of people remain missing.
“This is truly one of the greatest tragedies California has ever faced,” said Jerry Brown, the state’s governor.
The fires come amid a rash of unusual weather disasters across the world: Hurricanes Harvey, Irma, and Maria; a landfalling superstorm in Ireland; and deadly wildfires from British Columbia to the Iberian peninsula. 2017 is almost certain to be the second-warmest year ever measured, second only to an El Niño–intensified 2016.
Northern California suffered through a record-breaking heat wave earlier this year. In late August, Napa Valley saw day after day of temperatures above 100 degrees Fahrenheit. Those hot days are blamed in part for desiccating the region’s foliage, essentially preparing the area for this month’s wildfires.
But the Napa-Sonoma wildfires have surprised experts for reasons other than their size or intensity. Driven by hot, dry winds from the state’s interior, the North Bay blazes resemble Southern California’s wildfires, which often burst into life in the Santa Ana winds. Those winds, like the gusts whipping the north, are also hot, dry, and east-to-west. (And the subject of a dynamite Joan Didion essay, but that’s another story.)
In their entirety, the fires—from their speed to their wind-driven nature—suggest a much more southwest-like climate than the region today. Is this the future? Will climate change turn Northern California into Southern California?
“The comparison is not a bad one,” said Alex Hall, a professor of atmospheric science and the director of the Center for Climate Science at the University of California, Los Angeles.
It might be literally true for temperature. If greenhouse-gas emissions continue on roughly their current trajectory, then Northern California’s temperatures will warm by between 6 and 11 degrees Fahrenheit by 2100.
“That’s approximately the current temperature difference between Southern and Northern California on the coast,” says Hall. “The warming you would expect by the end of the century would be equivalent to a shift in latitude.”
He added that it’s not clear whether wind-driven fires will increase in Northern California in a warmed world. In Southern California, researchers predict that the amount of area burned by wildfires will double through the end of the century, as the Santa Ana winds become even drier.
Michael Wehner, a senior staff scientist at the Lawrence Berkeley National Laboratory, said that while the north’s future temperatures might resemble Los Angeles, its future precipitation would not. “Precipitation in Northern California will not likely decrease to Southern California levels. Future precipitation in NorCal will undergo a phase change, i.e., a larger fraction in the Sierra will fall as rain instead of snow due to warmer temperatures,” he told me in an email.
Some climate models suggest that storm systems will also make landfall further north in California as the century goes on, bringing more precipitation to the state’s north half and Oregon. But Wehner cautioned: “This is yet to be documented in the observational record however, whereas increases in temperature, decreases in snowpack, and drier conditions all have to varying degrees.”
“It is likely an oversimplification to state that future NorCal landscapes become SoCal current landscapes. It would likely be something completely different than either,” he added.
There’s some precedent for swapping the two regions’ temperature profiles—though it last happened in the other direction. When glaciers covered the top third of North America 26,000 years ago, the climate of modern-day Los Angeles was 6 to 11 degrees Fahrenheit cooler—comparable, in other words, to San Francisco.
California as a whole is projected to be drier and hotter in the decades to come. The U.S. government projects the Sonoran, Mojave, and Great Basin deserts to expand as climate change continues to take hold. Since water evaporates faster into hotter air, it also estimates that the Sierra Nevadan snowpack will shrink and as a whole California’s water problems will get worse.
Outside of California, there’s widespread evidence that global warming has already made the West’s wildfires more intense and more expansive. As I wrote last month, a study in Proceedings of the National Academy of Sciences last year found that the size of the area burned by wildfires has doubled over the past 33 years. And the area of forest burned annually in the Pacific Northwest has increased by 5,000 percent since the early 1970s.


The origin of humanity’s best friend is still murky, but here’s a likely scenario. In the distant past, wolves started skulking around human campsites, scavenging the carcasses left behind by hunters. Perhaps, to make the most of these opportunities, they evolved to be more docile. They transformed in both physique and temperament, and eventually became domestic dogs.
As they changed, they gained the ability to interact with humans, becoming increasingly attuned to our gestures, expressions, and emotions. But they lost something, too.
Sarah Marshall-Pescini, from the University of Vienna, has now found that dogs are terrible at a simple task that requires them to cooperate. Working together, they have to pull on two pieces of rope to bring a piece of distant food within reach—and they mostly fail. Wolves, however, are far more successful, dramatically outperforming their domestic peers.
This simple experiment contradicts the common belief that dogs are just friendlier wolves. “The idea is that we’ve changed their psychology to make them into super-cooperative beings,” says Marshall-Pescini. But that’s only true for their relationships with us. By domesticating dogs (or rather, providing the conditions for them to domesticate themselves), humans ruined the pack instinct that makes wolves some of the most gregarious and cooperative hunters on four legs. “They adapted to the niche we provided for them and it changed their sociality,” Marshall-Pescini says.
“The assumption that dogs are a tamer version of wolves is an oversimplification,” says Anindita Bhadra, from the Indian Institute of Science Education and Research in Kolkata. “This is all the more reason why we need to carry out similar studies on free-ranging dogs.”
Around 80 percent of dogs, in fact, are free-ranging, and their behavior shows just how different they are to wolves. They’re mostly solitary, scavenging alone on human garbage. When they do form packs, these groups are usually small and loose-knit. They might hunt together, but they mostly congregate to defend their territory. By contrast, wolves live in extremely tight-knit family groups. They rely on their pack-mates to bring down large prey, and they work together to rear each other’s pups. The strength of the pack is the wolf, and the strength of the wolf is the pack, as Rudyard Kipling’s poem goes.
Fifteen wolves currently live in Vienna’s Wolf-Science Center, along with 15 dogs. “The center was established to look at the differences between wolves and dogs in as fair a way as possible,” says Marshall-Pescini. “They’re raised in exactly the same way, with a lot of human contact. This allows us to test a lot of different things without the confounding variables of wolves not being used to humans and pet dogs being super-used to humans.”
She and her colleagues challenged their canines to a simple task, which other scientists have used on all kinds of brainy animals—chimps, monkeys, parrots, ravens, and even elephants. There’s a food-bearing tray that lies on the other side of their cage, tempting and inaccessible. A string is threaded through rings on the tray, and both of its ends lie within reach of the animals. If an individual grabs an end and pulls, it would just yank the string out and end up with a mouthful of fibers—not food. But if two animals pull on the ends together, the tray slides close, and they get to eat.
All in all, the dogs did terribly. Just one out of eight pairs managed to pull the tray across, and only once out of dozens of trials. By contrast, five out of seven wolf pairs succeeded, on anywhere between 3 and 56 percent of their attempts. Even after the team trained the animals, the dogs still failed, and the wolves still outshone them. “We imagined that we would find some differences but we didn’t expect them to be quite so strong,” Marshall-Pescini says.
It’s not that the dogs were uninterested: They explored the strings as frequently as the wolves did. But the wolves would explore the apparatus together—biting, pawing, scratching, and eventually pulling on it. The dogs did not. They tolerated each other’s presence, but they were much less likely to engage with the task at the same time, which is why they almost never succeeded.
“The dogs are really trying to avoid conflict over what they see as a resource,” says Marshall-Pescini. “This is what we found in food-sharing studies, where the dominant animal would take the food and the subordinate wouldn’t even try to approach. With wolves, there’s a lot of arguing and it sounds aggressive, but they end up sharing. They have really different strategies in situations of potential conflict. [With the dogs], you see that if you avoid the other individual, you avoid conflict, but you can’t cooperate either.”
“Amazingly, no one had ever studied whether carnivores could solve this type of cooperative task, and it’s fun to see that the wolves coordinated,” says Brian Hare from Duke University, who studies dog behavior and the influence of domestication. He has argued that during the domestication process, dogs began using their traditional inherited mental skills with a new social partner: humans.
Simultaneously, dogs perhaps became less attentive to each other, adds Marshall-Pescini. After all, wolves need to work together to kill large prey, and sharing food helps to keep their social bonds intact. But when they started scavenging on human refuse, they could feed themselves on smaller portions by working alone. If they encountered another forager, “maybe the best strategy was to continue searching rather than to get into conflict with another dog,” she says.
But dogs can be trained. When owners raise dogs in the same household, and train them not to fight over resources, the animals start to tolerate each other, and unlock their ancient wolflike skills. This might be why, in 2014, Ljerka Ostojić, from the University of Cambridge, found that pet dogs, which had been trained in search and rescue, had no trouble with the string-pulling task that flummoxed Marshall-Pescini’s dogs.
“It speaks to the fact that living among other dogs, without interaction with humans, is arguably less natural for dogs—as if domestication both refined attention, coordination, and even pro-sociality between species, and weakened social skills within the species,” says Alexandra Horowitz, who studies dog cognition at Barnard College. “A pack of dogs living together, without human intervention, is impaired compared to dogs living with humans.”
But Laurie Santos, from Yale University, says that there’s growing evidence that dogs also differ from wolves in their physical understanding of the world. “Maybe dogs are just as good at cooperating as wolves, but they don’t get that the only way to solve the task is to pull at the same time,” she says. “They just don’t get it in terms of the physics. The cool things is that both of these interpretations have exciting implications for the evolution of dog intelligence.”
“If I ask people to close their eyes and think of a dog, everyone thinks of a pet dog,” Marshall-Pescini says. “But pet dogs are a really recent invention and free-ranging dogs are more representative of the earlier stages of domestication. We need to base our theories on a different understanding of what a dog is.”


The 19th-century American scientist Joseph Leidy has been described as the “last man who knew everything.” An extraordinary polymath, Leidy was a scholar of parasites, a discoverer of dinosaurs, a collector of gemstones, a curator of museums, an exceptional illustrator, and the first person to use a microscope to solve a murder mystery. But learned though he was, he was still shocked by what we saw when he cut open some termites in an attempt to find out what they ate.
Gazing at the dissected insects through his microscope, Leidy saw hordes of small specks evacuating their corpses, like “a multitude of persons from the door of a crowded meetinghouse,” he wrote. He billed them as parasites, and for good reason. It was 1889, 30 years after the publication of Charles Darwin’s provocative opus, On the Origin of Species by Means of Natural Selection, and biologists had begun to see nature as a red-blooded gladiatorial arena, where only the fittest survive. And in previous decades, scientists had shown that many infamous diseases, like gonorrhea and tuberculosis, were the work of microscopic bacteria. With nature framed in terms of conflict, and bacteria framed as germs, any microbes living in the bodies of animals were instantly vilified as parasites.
We now know that this view is deeply incorrect. Every individual animal is a thriving community of microbes, most of which are harmless, and many of which are beneficial. These microbiomes bestow their owners with amazing abilities.
Those of termites, for example, help them to digest their food. The ones that Leidy saw are protists—microbes that have more in common with us than with bacteria, but that still consist of a single, tiny cell. They produce enzymes that help termites to digest the otherwise indigestible chemicals in the wood that they eat, to survive on a rich and abundant food source that would otherwise be inaccessible to them.
You can learn more about that microbiome in the video below—the first in a series of online films produced by HHMI Tangled Bank Studios, which adapt the stories in my book, I Contain Multitudes.

There are different kinds of termites, too. The misleadingly named lower termites rely on protists as partners in digestion. The higher termites, which evolved later, rely more on bacteria, which they house in a series of cowlike stomachs. And the macrotermites—the most recent of these groups—rely on both architecture and agriculture. They feed bits of wood to a fungus, which breaks them down and creates a compost that the termites can swallow. In their guts, bacteria digest the wood pulp even further.
This is true for every member of a macrotermite colony except the huge and grossly swollen queen. Her distended body lacks gut microbes, but she can eat because her daughters regurgitate predigested fluids into her mouth. She has effectively turned her entire nest—towering, fungus-laced walls; thousands of diligent laboring daughters, and their billions of attendant microbes—into her gut.
Even the microbes of termites can have their own microbiomes. One species of Australian termite carries a protist in its gut called Mixotricha paradoxa—a pear-shaped microbe whose name means “paradoxical being with mixed-up hairs.” Those “hairs,” which propel the microbe through the gut, are actually bacteria—and close relatives of those that cause syphilis. Lynn Margulis, one of the most vocal proponents of the interconnectedness of life, showed that Mixotricha carries around no fewer than four types of bacteria—one inside itself, and three on its surface. As she and her son once wrote:
“Scrutinizing any organism at the microscopic level is like moving ever closer to a pointillist painting by Georges Seurat: The seemingly solid figures of humans, dogs, and trees, on close inspection, turn out to be made up of innumerable tiny dots and dashes, each with its own attributes of color, density, and form.”


Some 130 million years ago, in another galaxy, two neutron stars spiraled closer and closer together until they smashed into each other in spectacular fashion. The violent collision produced gravitational waves, cosmic ripples powerful enough to stretch and squeeze the fabric of the universe. There was a brief flash of light a million trillion times as bright as the sun, and then a hot cloud of radioactive debris. The afterglow hung for several days, shifting from bright blue to dull red as the ejected material cooled in the emptiness of space.
Astronomers detected the aftermath of the merger on Earth on August 17. For the first time, they could see the source of universe-warping forces Albert Einstein predicted a century ago. Unlike with black-hole collisions, they had visible proof, and it looked like a bright jewel in the night sky.
But the merger of two neutron stars is more than fireworks. It’s a factory.
Using infrared telescopes, astronomers studied the spectra—the chemical composition of cosmic objects—of the collision and found that the plume ejected by the merger contained a host of newly formed heavy chemical elements, including gold, silver, platinum, and others. Scientists estimate the amount of cosmic bling totals about 10,000 Earth-masses of heavy elements.
Neutron stars are the collapsed cores of dead stars, the sole survivors of supernovae. They are the densest known objects in the universe; one neutron star measures about the size of a bustling city, but has about the same mass as our sun. A teaspoon of its contents would weigh about 10 million tons.
When neutron stars merge, they release a fire hose of neutrons. Heated to extreme temperatures, the neutrons bombard surrounding atoms, and form heavy elements. The baby elements go on to become part of other objects of the universe, like stars and planets, including our own.
The discovery confirms a long-standing astronomical theory. Astronomers have suspected for decades that neutron-star mergers were responsible for the production of most of the heavy elements found in the universe. The lightest of the elements, like hydrogen, helium, and lithium, came from the Big Bang. Heavier elements, like carbon and oxygen, came later, fused in the hearts of stars. Some even heavier elements erupted from supernovae. But computer simulations showed these explosions weren’t powerful enough to forge some of the elements that are heavier than iron, like the precious metals. The universe needed another kind of explosion called a kilonova, which shines 1,000 times brighter than a typical supernova.
Until this summer, astronomers only had theoretical models for such an event. Now, they say the new data suggests neutron-star mergers could account for about half of all elements heavier than iron in the universe.
“I think this can prove our idea that most of these elements are made in neutron-star mergers,” said Enrico Ramirez-Ruiz, a theoretical astrophysicist at the University of California, Santa Cruz, who worked on the discovery, in a press release Monday. “We are seeing the heavy elements like gold and platinum being made in real time.”
Scientists will spend years poring over the data from this discovery. We’re just getting to know these cosmic collisions and the science behind them. But we have been long acquainted with their effects. The aftermath of neutron-star mergers is all around us, in our mines, computers, toasters, wedding bands—the list goes on.


In September of 2015, astronomers detected, for the first time, gravitational waves, cosmic ripples that distort the very fabric of space and time. They came from a violent merger of two black holes somewhere in the universe, more than a billion light-years away from Earth. Astronomers observed the phenomenon again in December, and then again in November 2016, and then again in August of this year. The discoveries confirmed a century-old prediction by Albert Einstein, earned a Nobel prize, and ushered in a new field of astronomy.
But while astronomers could observe the effects of the waves in the sensitive instruments built to detect them, they couldn’t see the source. Black holes, as their name suggests, don’t emit any light. To directly observe the origin of gravitational waves, astronomers needed a different kind of collision to send the ripples Earth’s way. This summer, they finally got it.
Scientists announced Monday they have observed gravitational waves for the fifth time—and they’ve seen the light from the cosmic crash that produced them. The waves came from the collision of two neutron stars in a galaxy called NGC 4993, located about 130 million light-years from Earth.
Neutron stars are strange, mysterious objects, the collapsed cores of stars that exploded in spectacular fashion—supernovae—and died. These stars measure about the size of a metropolitan city, but have about the same mass as our sun. Astronomers had long predicted that when two neutron stars collide, the resulting explosion would produce electromagnetic radiation, in the form of optical light. The afterglow would shine bright enough to be seen through powerful telescopes, the first visible proof of a source of gravitational waves, provided the latter could also be detected.
Here it is, captured by the European Southern Observatory’s Very Large Telescope in Chile, in the center of the image:
Astronomers made the observation August 17. Three gravitational-wave detectors, two at the Nobel prize–winning Laser Interferometer Gravitational-Wave Observatory (LIGO) in the United States, and one at the Virgo Interferometer in Italy, detected the cosmic ripples as they washed over Earth. About two seconds later, two space telescopes—NASA’s Fermi Gamma-ray Space Telescope and ESA’s International Gamma-Ray Astrophysics Laboratory—observed a short burst of gamma rays, the most energetic wave in the electromagnetic spectrum, coming from the same part of the sky.
The almost simultaneous detections caught astronomers’ attention, and they threw everything they had at it. Dozens of ground-based telescopes around the world quickly turned their gaze toward the same slice of sky. ESO’s army of telescopes, sprinkled across the Chilean desert, scanned through the night. When the sun set in Hawaii, the Pan-STARRS and Subaru telescopes joined in. So did space observatories like Hubble. Within hours, astronomers pinpointed the location of the collision using an ESO telescope that sees in infrared wavelengths. They aimed the Swope Telescope, also in Chile, at the region and started snapping pictures. They found the afterglow in their ninth shot.
Astronomers observed the afterglow of the merger for days. They watched as the glowing orb faded and changed colors from blue to red, a tell-tale sign that the remnants of the crash were pushing radioactive material out and cooling down.
Here’s an animation from ESO that shows two neutron stars spiraling closer together until they crash:

Astronomers examined the gravitational waves to estimate the size of the colliding objects and found they had masses far smaller than black holes. “The biggest neutron star is a lot smaller than the smallest black hole,” said Richard O’Shaughnessy, a theoretical gravitational-wave astrophysicist at Rochester Institute of Technology who works in the LIGO group. The mass measurement, coupled with the near-simultaneous observations of the gravitational waves and a light source, told scientists they were dealing with neutron stars. The event was also much closer to Earth than previous mergers recorded by LIGO, which originated between 1 billion and 3 billion light-years away.
All told, about 70 observatories captured the event, named GW170817 for the day it made itself known to Earth. The collision’s aftermath was recorded at nearly every wavelength. O’Shaughnessy described the discovery as a Rosetta stone for astronomy; the observation produced reams of data with richness seemingly unprecedented for a single astronomical event. The findings, which are spread across many papers in several journals, provide evidence for several theories in astronomy.
The discovery supports the theory that neutron-star collisions produce short gamma-ray bursts, brief streams of light that shine brighter than a million trillion times the sun. Gamma-ray bursts have been detected and imaged before, but without gravitational-wave detectors like LIGO and Virgo, astronomers couldn’t know whether they came from cosmic collisions.
The presence of the short gamma ray-burst suggests the merger led to a kilonova, a powerful explosion 1,000 times brighter than a supernova. Astronomers have long suspected kilonovae follow neutron-star collisions, spewing material out into space. In the case of GW170817, scientists estimate the kilonova ejected material at one-fifth the speed of light, faster than a typical supernova.
The findings support another prediction that neutron-star collisions produce chemical elements heavier than iron, like gold and platinum. Astronomers believe neutrons released during the merger combine with surrounding atoms in a phenomenon known as r-process nucleosynthesis. Telescope observations of GW170817’s spectra—the chemical composition of the star material—revealed it contained heavy elements, including 10 times the mass of the Earth in gold, according to O’Shaughnessy. These kinds of collisions, astronomers believe, may be responsible for populating the universe with heavy elements.
The discovery gave scientists a chance to measure the expansion of the universe, too. Since astronomers knew which galaxy the latest gravitational waves came from, they could calculate the distance between that galaxy and Earth and then plug it into equations for the rate of expansion, known as the Hubble constant. Good news: The answer matched up with previous estimates from other methods.
When scientists announced their fourth detection of gravitational waves in August, they promised that these kinds of announcements would become routine. LIGO and Virgo’s instruments, they predicted, will detect the rippling of space-time once or multiple times a week. It’s a certainty that we will experience the effects of mergers between black holes and neutron stars—and maybe between one of each—again. LIGO and Virgo scientists may even have a few confirmed detections they haven’t told us about yet. And the more, the better.
“This rain of events will continue at such a high rate that we’ll have a census of comic explosions,” O’Shaughnessy said. “And by data mining the census, we can learn something about how they form, about the origins of these mysterious events.”


The Madras Observatory offers little to the visitor’s eye. Stone slabs and broken pillars lie ignored in a fenced-off section of a local weather center in the southern Indian city of Chennai. Few tourists venture out to see the ruins of the 18th-century complex. On the other side of the subcontinent, in northern Indian cities such as New Delhi, Varanasi, and Jaipur, remains of the Jantar Mantars, vast astronomical stations, are far more popular attractions. Built in the same century as the Madras Observatory, their stark geometric structures, with looming proportions and vibrant colors, make for mandatory stops on travelers’ itineraries. Yet it is the Madras Observatory, and not the spectacular Jantar Mantars, that marks the triumphal fusion of scientific knowledge and imperial power.
South Asians had been studying the heavens long before the 18th century. The subcontinent’s first texts on astronomical phenomena date back more than 3,000 years. As was common throughout the ancient world, observations about the movements of stars and planets often served the needs of astrologers and priests. Nevertheless, they formed an impressive body of scientific knowledge, one that was further enriched by contact with other cultures. The Islamic conquest of South Asia in the medieval era brought Persian and Arab discoveries along with it, and the Mughal Empire promoted a blend of South Asian and Islamic astronomical knowledge in the 16th and 17th centuries. The city of Lahore, in modern-day Pakistan, became a center in the production of sophisticated astronomical instruments such as celestial spheres. By the early 18th century, as Mughal rulers lost control of most of the subcontinent, local rulers used astronomy to promote their own authority. They built the flamboyant Jantar Mantars across northern India to show that, just like the great dynasties before them, they too were patrons of knowledge.
The era’s greatest promoter of astronomy was Jai Singh II, the 18th-century raja of Jaipur. He oversaw the construction of monumental observatories across his domains, using them not only to overawe subjects but also to gather useful knowledge about the lands he ruled. His Jantar Mantars, like others in South Asia, featured massive sundials, sextants, and other instruments of observation, but lacked telescopes, which had been invented in Europe a century before. Eager to capitalize on European knowledge, and to show the global reach of his influence, Jai Singh II came into contact with French missionary scientists.
A team of Jesuit astronomers arrived at Jaipur in 1734, and demonstrated the practical value of their scientific advances. By establishing the exact time that the sun was at its highest over a given spot, the missionaries could determine its longitude, or distance east or west of other points on the Earth’s surface. They established the longitude of several of Jai Singh II’s cities, just as other Jesuit teams were doing for the Qing emperors in China. Offering their astronomical knowledge to Asian rulers, these Catholic missionaries hoped to win approval for their Christian faith, while the rulers they served used foreign expertise to increase their own power. The Jesuits also learned from South Asian science, studying Sanskrit, the classical language of science in South Asia, in order to translate the greatest works of South Asian astronomy.
This peaceful exchange of scientific patronage, technology, and texts between Europe and Asia was short-lived. After the raja’s death in 1743, scientific activity in his network of observatories faded, and Jaipur’s collaboration with the Jesuits came to an end. New forces entered the fray, as both the subcontinent and astronomy became arenas for the rising empires of Britain and France. Throughout the second half of the 18th century, as the two rival powers fought for control of North America, they also competed against each other in South Asia, staging proxy wars through networks of local allies. They also competed to gather scientific data, sending rival astronomical expeditions across their far-flung empires and using the knowledge gained to control their colonies. While only a few generations before, it might have seemed that the global circulation of astronomical knowledge would bring a new era of understanding between Europe and Asia, this was not to be.
In 1792, the British East India Company delivered a stinging defeat to Tipu Sultan of Mysore, France’s only remaining ally in South Asia. In the same year, it completed the construction of the Madras Observatory, one of the first modern observatories in Asia. It was armed with impressive telescopes, still rare in the Indian subcontinent. The observatory was the brainchild of Michael Topping, a British surveyor tasked with mapping the shoreline of southern India. He argued that an observatory was crucial to his task, since astronomy was the “parent and nurse of navigation.” But the site was also a tool of colonial rule, a means of showing that Britain was now the dominant power in South Asia. As Topping insisted, astronomy held the key to “the sovereignty of a rich and extensive empire.”
The East India Company destroyed what remained of Tipu Sultan’s power in 1799, when Tipu himself died in a desperate last battle at his capital of Srirangapatna. Most of his sultanate was annexed by the Company, which soon began an extensive survey of his former dominion. Fanning out from the Madras Observatory, British surveyors used it as a fixed location from which they could calculate the exact location of sites in Mysore. This was a first step to assessing the value of the lands for tax purposes, and bringing the region under direct British control, where it would remain for the next century and a half. The observatories of Jai Singh II, symbols of his kingdom’s independence and cosmopolitan collaboration with European science, were a thing of the past. Alongside other massive British projects of scientific knowledge collection, such as James Cook’s expeditions to the Pacific (1768–1778), the Madras Observatory heralded the rise of a new kind of science, serving the needs of a global empire and imposing its sway on subject peoples.
This post appears courtesy of Aeon.



Frances Glessner Lee grew up in the Gilded Age as the heiress to a fortune made in industrial farm equipment. Her childhood was “sheltered and indulged.” She would marry at age 20, have three children, and then, in a turn against convention, divorce her husband.
It was in her 40s—free of a husband and then free of a brother and father who both died, leaving her a vast fortune—that Lee embarked on the project that would consume the rest of her life. She had become enthralled by the grisly crime stories of George Burgess Magrath, her brother’s friend and a medical examiner in Boston. And so Lee began pouring her family fortune into a project that combined the very unladylike world of crime with the domestic arts: the Nutshell Studies of Unexplained Death.
The nutshells are dioramas, based on actual death scenes that Lee painstakingly researched. (“Convict the guilty, clear the innocent, and find the truth in a nutshell” went a police saying at the time.) Lee finished 20 nutshells before her death in 1962. Eighteen are still used today by Maryland’s Office of the Chief Medical Examiner to train detectives to look for clues, one was destroyed in transit, and the last “lost” nutshell has been recovered from an attic and restored for an upcoming exhibit of Lee’s work at the Renwick Gallery of the Smithsonian American Art Museum in Washington, D.C.
Lee was fastidious—to the point of obsession. In one nutshell titled “Saloon and Jail,” a man lies facedown in the street. Debris is strewn on the pavement: miniature cigarettes (hand-rolled and filled with paper), a banana peel (painted leather), scraps of paper with visible faces. A storefront in the background displays newspapers and magazines with real covers from the date of the man’s death. A bucket of tiny, colorful lollipops sits under the magazines, each piece of candy individually wrapped in cellophane.
The story goes that at one point Lee requested the carpenter who built the wooden structures and furniture in the nutshells remake a certain rocking chair: She wanted it to rock the exact same number of times as the rocking chair in the real-life scene. Lee also made use of factory-made dollhouse pieces, like the boxes of Ivory soap that show up on the pantries of various nutshells. But the textiles—from clothing on dolls to upholstery on couches—she hand-sewed all by herself. After her death, people found half-finished doll clothing that Lee was knitting with pink yarn and straight pins.
For Lee, the nutshells were more than just objects for display. They were part of a campaign to bring rigor to forensic science—a campaign, ultimately, for justice. She funded the Department of Legal Medicine at Harvard Medical School, with her friend George Burgess Magrath as its first professor. And she hosted seminars using the nutshells to teach police how to examine crime scenes.
As a high-society woman, Lee knew how to throw memorable parties to woo men who might not be so inclined to listen to a middle-aged woman. At the end of her seminars, she wined and dined them at the Ritz Carlton. The parties featured china specially made for the occasion. And she left them with a parting gift—a miniature nutshell that opened to reveal a pair of miniature, working cufflinks.
“People didn’t know where to put her,” says Nora Atkinson, the Smithsonian curator who organized the Lee exhibit. Lee was a woman in a man’s world, who had succeeded by marshaling her talents in the traditionally feminine pursuits. She won respect in professional circles. At one point, New Hampshire made her an honorary police captain. But she was still seen first as a “grandmother” with murder as a “hobby.”
Lee knew that to be taken seriously, her nutshells had to be more than just meticulously crafted. They had to be scientifically accurate. She bought porcelain doll heads and other parts, but she made sure to fashion their bodies according to real biology. “You can’t buy a doll in rigor mortis,” says Ariel O’Connor, a Smithsonian conservator who has been working on the nutshells. In one nutshell, a woman has fallen into a position with an unusually stiff neck—a sign she may not have died in this position. Some dolls show lividity, which is when blood in the body sinks to whatever part of the body is facing down, turning the skin purplish red. It can also hint at whether a body has been moved. The dolls dead by hanging were filled with lead shot, to give them the heft of real human bodies.
For all the care in creating them, the 80-year-old nutshells are showing their age now. Tablecloths have faded. The blood on the bodies, which O’Connor suspects to be red nail polish, has darkened to a less realistic deep purple.
To prepare the nutshells for exhibition at the Smithsonian, O’Connor has led a months-long effort to conserve the dioramas. The work, she says, has been a bit like detective work itself. She looks for subtle hints like glue residue to match up, say, an object that has fallen off of the shelf where it originally stood. Because any object out of place can in fact be an important clue to the murder, every move requires consultation with Maryland’s medical examiners, who have the solutions to the nutshells.
“It is one of the most challenging things I have ever tried to work on,” says O’Connor. The sheer number of materials in the nutshells add to the challenge. Materials have “inherent vice,” or a natural tendency to degrade over time. The plastic panes of the nutshells’ windows, for example, are especially prone to warping. O’Connor also had to figure out how to preserve a nutshell that features a burnt house. (Lee literally had a blowtorch taken to it.) She ended up borrowing techniques from archaeology used to preserve burnt wood.
A big part of the conservation project has focused on lighting. Lee used the incandescent bulbs available in her time, which cast a beautiful warm glow but have the disadvantage of heat. Heat causes damage. The painted linoleum in one nutshell had cracked and curled, in part, O’Connor thinks, because of the heat of the lightbulb above it.
To be in the Smithsonian exhibit, the lights would have to be on hours each day, which would have caused too much damage. So the team replaced 70 incandescent bulbs with custom-made LEDs, matching the quality of light without the heat. It’s possible now because of recent advances in LED technology. “Just a few years ago, we couldn’t have achieved this quality with LEDs,” says Scott Rosenfeld, a lighting designer at the Smithsonian. The 80-year-old dioramas now have a state-of-the-art lighting system.
The trickiest nutshell to restore was the lost nutshell, which features a dead man on his couch. The solution is completely lost. To make matters worse, when it was taken out of the attic where it was found, it was tipped over, displacing many of the objects Lee had carefully placed inside as clues. There was dirt all over the living-room floor. The bannister for the stairs had fallen down, which led some to suspect it was the murder weapon. But then O’Connor discovered that the bannister is supposed to be intact. They glued it back on.
When I stopped by the unfinished exhibit, O’Connor and Atkinson were still discussing where to place an object that had come loose and they suspected was a key clue. (I’m being purposely vague to avoid spoilers. I will say, you would have to look very, very carefully to even notice.) I asked O’Connor if she had solved the death after spending so many hours with the lost nutshell. She laughed and demurred. Visitors to the Smithsonian’s Renwick Gallery will have the opportunity to test their own detective skills.
The Renwick Gallery specifically showcases craft and decorative arts. Atkinson told me she was particularly pleased to exhibit Lee’s nutshells because museums have traditionally focused on studio craft, which has been so dominated by the work of white men. Half a century after her death, Lee is still transgressing in the world of men.


WASHINGTON, D.C.—If the United States is going to have even a chance of meeting its climate-change goals under the Paris Agreement, there are few actions more pressing than taking coal plants offline. Coal-burning power plants emit more heat-trapping carbon dioxide into the atmosphere than any other part of the electricity sector.
It’s been widely reported that coal is inexorably on its way out—that cheap natural gas, made available by fracking, is eroding coal’s dominance in the U.S. grid. But the speed of that transition matters immensely to the planet, as the United States is currently not on track to slow its emissions fast enough to avert catastrophic climate change.
This week, two powerful men set up on Capitol Hill to fight over the speed of that transition. One of them vied to keep up the pressure on coal retirements in a press conference on Wednesday; the other threw his support behind still-surviving plants in a congressional hearing on Thursday. The two were inhabiting unusual roles: The environmentalist billionaire said he was fighting for the free market, while the long-time Republican and former Texas governor asked for emergency government subsidies.
The first of the men is Michael Bloomberg, the former mayor of New York and the UN special envoy on climate change and cities. In this latter role, Bloomberg has put together an “America’s Pledge” from cities, states, companies, and universities who will continue to track their greenhouse-gas emissions under the Paris Agreement, even though Donald Trump has removed the federal government from that accord.
Bloomberg, one of the world’s richest men, is also a philanthropist—and Wednesday, he gave an additional $30 million to the Sierra Club’s Beyond Coal campaign. The Beyond Coal campaign is a targeted activism organization that uses public pressure and environmental litigation to force coal plants to close across the country.
The Beyond Coal campaign is widely considered one of the most successful environmental activist pushes of the post–Cold War period. In the past decade, it has closed or secured the retirement of nearly half of the country’s 523 coal plants. Eleven of those plants have announced plans to shutter since President Donald Trump’s inauguration.
“Coal’s share of the energy mix is at a record low,” said Michael Brune, the executive director of the Sierra Club, at the press conference. “In fact, coal plants are retiring at the exact same rate under President Trump as they were under President Obama.”
Bloomberg’s funding will keep the Beyond Coal campaign running until 2020. He also announced $34 million to support the League of Conservation Voters and job-retraining and economic-development programs in Appalachia.
“The war on coal has never been led by Washington,” said Bloomberg. “It has been led by market forces that are providing cleaner and cheaper sources of energy, and by communities, cities, and states that want to protect public health.”
The second of the men is Secretary of Energy Rick Perry. In a fractious age, in a polarized country, Perry has pulled off a rare feat: He has proposed an emergency policy that just about everyone hates—except, that is, for the coal and nuclear industry.
On Thursday, Perry sat before the House Energy and Commerce Committee and defended his “grid reliability” plan, which his department is trying to hustle through the Federal Energy Regulatory Commission, or FERC, in the next 60 days. FERC creates the rules that govern some of the nation’s power grid.
Perry’s proposed rule would pay coal and nuclear plants to remain on the electrical grid in the name of resilience, financially rewarding them for keeping three-month supplies of fuel on premises.
Experts say it would virtually assure that coal and nuclear plants—nearly all of which have been squeezed by the availability of cheaper natural gas and solar and wind power—remain profitable. It could also create a brief boom in the coal market, as plants snap up three months of fuel.
To build support for the rule, Perry has cited the polar vortex of 2014, when frigid weather descended across much of the continental United States. Though the power grid stayed intact through that episode, the largest grid operators saw roughly 20 percent of their power plants shut down. Natural gas plants, in particular, suffered in the weather, but some coal stacks froze as well.
Some experts also worry that as solar and wind power play a larger role in the electricity mix, power generation will become more unreliable, as the sun doesn’t shine and the wind doesn’t blow all the time. (Many technology companies are working on large batteries that could solve this problem.) Combined with the historical anecdote that natural-gas plants sometimes shut down in the cold, Perry says the grid is suffering a “reliability crisis.”
Some economists have worried that the proposed rule could spike rates for electricity customers. The energy-consulting firm ICF has estimated it will cost power customers $800 million to $3.8 billion every year through 2030. Asked about those concerns Wednesday, Perry replied: “I think you take costs into account, but what’s the cost of freedom? What’s the cost to keep America free? I’m not sure I want to leave that up to the free market.”
Perry has ordered FERC to make a decision about his proposed rule in the next 60 days. But when pressed by lawmakers, he said he was just “starting a conversation” about grid resiliency.
His plan has found barely any support from either side of the political spectrum. Multiple Republicans declined to endorse the plan Thursday. Congressman Greg Walden, a Republican of Oregon and the chairman of the House subcommittee on energy, didn’t even mention the plan in his statement introducing Perry.
A large and bizarre alliance of environmental groups, renewable-energy advocates, and natural-gas suppliers oppose the plan and will try to block it in court, according to the energy trade publication E&E News.
Economic experts from both the right and the left have also opposed the plan. Josiah Neeley, the director of energy policy at the conservative R Street Institute, said he found Perry’s remarks at the hearing “a little strange.”
“Secretary Perry didn’t sound very much like Governor Perry that I remember back here in Texas, because Governor Perry, of course, was a big fan of free markets in electricity,” he told me.
The R Street Institute has been “very critical” of the proposal, he said, which he said was also “pretty vague.”
“There’s certainly not a reliability crisis. And even if there was, the proposed rule doesn’t address any of the issues with reliability that are out there,” he said. “To the extent that there is an issue here with reliability, the principled way to deal with it is to create some sort of market product ... and not have the Department of Energy pick winners and losers.”
Those thoughts were echoed by Michael Greenstone, a professor of economics at the University of Chicago and a former chief economist to the White House Council of Advisers under President Barack Obama.
“The first question you always want to ask yourself is, is there some imperfection in the market that needs solving?” he said.
The biggest market failure in energy markets is climate change, he told me: Power plants can emit carbon dioxide for free, even though it’s a greenhouse gas that worsens climate change and imposes high indirect costs on people today. Therefore, it should be more expensive to release greenhouse gases into the atmosphere, he said, not cheaper. “Doing nothing is already not merited by economics,” he said. “This is like doubling down.”
Greenstone agreed with Neeley that insofar as there is a potential failure to plan for grid reliability, it should be solved with market mechanisms. He didn’t think it constituted a crisis either, though.
“In many respects it reminds me of what we just saw with the Clean Power Plan,” he said, referring to the landmark Obama climate policy that the Environmental Protection Agency repealed this week. Environmental advocates and public-health groups have alleged that the math used to justify repealing the Clean Power Plan distorts the well-documented benefits of clean-air regulation.
Greenstone continued: “The high-level political goal is set—and then poor Secretary Perry and poor [EPA] Administrator Pruitt, having received high-level political direction, have to do the plumbing to make it work. And I think this is kind of ugly plumbing precisely because it does not efficiently address any of the two potential market failures floating around—and in fact makes one of them worse.”
“Coal’s real problem, nuclear’s real problem, and—by the way—renewable’s real problem is natural gas. We’re swimming in it,” he said.
As for Bloomberg’s opinion of the plan, he did not make his views a secret. “The Trump administration wants taxpayers to pay more for an earlier death,” he told journalists Wednesday. “This is one of the worst ideas Washington has ever come up with, which is saying a lot.”


As always, a horror film managed to express the idea before the scientists ever could, and in better, more visceral terms. “The television screen,” the haunting image of Brian O’Blivion tells us in David Cronenberg’s 1983 classic Videodrome, “is the retina of the mind's eye. Therefore, the television screen is part of the physical structure of the brain.” So far, so much media theory: secondhand McLuhan, thirdhand Baudrillard. It’s what happens next that’s interesting.
Our hero, wilting under the caustic nihilism of the video age, finds that strange things start happening not to his mind, but his body. A howling cavern opens up in his stomach, rimmed by grisly pulsing labial folds. It eats weapons. His hand sprouts metallic screws, driving into his wrist, locking his gun into a hand that swells into a grotesque of formless and seeping flesh. He is told to kill, and he kills. It’s not that his mind has been invaded. It just exists beyond itself; it now contains endless shelves of video tapes. This is, somehow, obscurely, us; this monstrous body is our own.
Among philosophers, biologists, and cognitive scientists, this nightmare is an exciting new field of study, known as embodied or extended cognition: broadly, the theory that what we think of as brain processes can take place outside of the brain. In some cases, this isn’t a particularly radical idea. The octopus, for instance, has a bizarre and miraculous mind, sometimes inside its brain, sometimes extending beyond it in sucker-tipped trails. Neurons are spread throughout its body; the creature has more of them in its arms than in its brain itself. It’s possible that each arm might be, to some extent, an independently thinking creature, all of which are collapsed into an octopean superconsciousness in times of danger. Embodied cognition, though, tells us that we’re all more octopus-like than we realize. Our minds are not like the floating conceptual “I” imagined by Descartes. We’re always thinking with, and inseparable from, our bodies.
The body codes how the brain works, more than the brain controls the body. When we walk—whether taking a pleasant afternoon stroll, or storming off in tears, or trying to sneak into a stranger’s house late at night, with intentions that seem to have exploded into our minds from some distant elsewhere—the brain might be choosing where each foot lands, but the way in which it does so is always constrained by the shape of our legs. We can’t ever stalk like a creature with triple-jointed legs, or sulk in the dejected crawl of a millipede, or stride with a giraffe’s airy gangly indifference. The way in which the brain approaches the task of walking is already coded by the physical layout of the body—and as such, wouldn’t it make sense to think of the body as being part of our decision-making apparatus? The mind is not simply the brain, as a generation of biological reductionists, clearing out the old wreckage of what had once been the soul, once insisted. It’s not a kind of software being run on the logical-processing unit of the brain. It’s bigger, and richer, and grosser, in every sense. It has joints and sinews. The rarefied rational mind sweats and shits; this body, this mound of eventually rotting flesh, is really you.
That’s embodied cognition. Extended cognition is stranger.
Many years ago, when I found myself standing on the roof of a tall building, or on a platform of the London Underground, or by the banks of the river, I would feel a strange urge to throw myself off. Not because I was miserable or because I particularly wanted at that moment to die; it was like an itch, an obsessive-compulsive tic, the deep gravitational hunger of the death drive. I would visualize myself falling, stupidly and fatally, for no reason other than to indulge in the most pointless destruction. I would almost savor the feeling of being hemmed in on all sides by ordinary life—commuters on the platform, tourists gawping at the Thames—and at the same time, right on the edge of the void, the domain of gods or nothingness or both. Maybe you’ve felt the same urge. But I don’t get it any more. These days, I still sometimes feel that cold vertiginous breath down my neck, but it’s not my body I want to throw down into the void. It’s my phone. Isn’t the phone, now, part of the physical structure of the brain?
In 1998, 15 years after Videodrome, the philosophers and cognitive scientists Andy Clark and David J. Chalmers finally made sense of what was happening in their landmark paper “The Extended Mind.” The mind, they argue, has no reason to stop at the edges of the body, hemmed in by skin, flapping open and closed with mouths and anuses.
Recent studies have added weight to their provocation: It’s been shown that spiders can use their webs to process and store information, essentially “outsourcing” mental processes to physical structures. Why is it, Clark and Chalmers ask, that mentally rearranging Scrabble tiles is considered a “part of action” rather than a “part of thought”?
When we jot something down—a shopping list, maybe—on a piece of paper, aren’t we in effect remembering it outside our heads? Most of all, isn’t language itself something that’s always external to the individual mind? We can’t invent our own private languages; as Wittgenstein showed in his Philosophical Investigations, we can invent our own words for things, but only as substitutes for words that already exist; it’s impossible to make the incommunicable meaningful. Language sits hazy in the world, a symbolic and intersubjective ether, but at the same time it forms the substance of our thought and the structure of our understanding. Isn’t language thinking for us?
This is not, entirely, a new idea. Plato, in his Phaedrus, is hesitant or even afraid of writing, precisely because it’s a kind of artificial memory, a hypomnesis. (Incidentally Freud inverts the metaphor 2,000 years later: The unconscious mind is like a child’s toy, the Mystic Writing Pad.) Writing, for Plato, is a pharmakon, a “remedy” for forgetfulness, but if taken in too strong a dose it becomes a poison: A person no longer remembers things for themselves; it’s the text that remembers, with an unholy autonomy. The same criticisms are now commonly made of smartphones. Not much changes.
Most of all, though, a theory similar to extended cognition is present in the work of Hegel and his descendants—and, in particular, Marx. In the dialectical tradition, the hermetic and self-contained Cartesian consciousness is impossible: We only become conscious in and through the world and its history. Marx, in the Economic and Philosophic Manuscripts of 1844, describes the process of unalienated labor in familiar terms. “The object of labor is, therefore, the objectification of man’s species life: for he ... contemplates himself in a world that he has created.” Work, without ownership or scarcity, is a kind of play: You’re always turning the exterior world into something else, something more responsive to your needs and your being. In a liberated future, the world of objects can be an externalization of our own consciousness; it can be a true home for humanity, because it is already ourselves. But not yet; first we have to overthrow capitalism. In the 20th century, Theodor Adorno picks up this theme: The “separation between subject and object” exists—I am not the world around me, in fact for the most part I’m terrified by it while it’s monstrously indifferent to me—but this is “the result of a coercive historical process.” It wasn’t always this way, it doesn’t have to be forever. The difference is that, according to theories of extended cognition, this separation is already over and always was, that subject and object are united right now.
But not entirely. Extended cognition promises to rip up the idea of a mind that lives only in the furrows of the brain, but it doesn’t always follow through. Cognition is extended, outsourced, leaking from cranial slime into the material world—but like an octopus’s tentacle, it can always dart back in. There are stranger and more dangerous possibilities. Take the grocery list. For Clark and Chalmers, it’s a brain process—information storage and retrieval—offloaded onto a piece of paper. But by whom? In Limited Inc, Jacques Derrida uses the same object to construct a very different interpretation. “At the very moment ‘I’ make a shopping list,” he writes, “I know that it will only be a list if it implies my absence, if it already detaches itself from me in order to function beyond my ‘present’ act and if it is utilizable at another time, in the absence of my being-present-now.” The list will still do its cognitive work if you are not currently reading it. The list will still do its work if you are dead. If we can accept that a grocery list is in some way thinking, is the part of the mental apparatus that remains lodged in the human brain really so central? The thought-capacity of objects is indifferent to whichever bit of brain is plugged into it. A war memorial remembers its list of the dead for us, in the same way that a scrap of paper remembers milk, and it keeps remembering, long after the weeds have grown and the rest of the world has tumbled past caring.
In Molloy, Samuel Beckett’s strange and gorgeous thicket of a novel, a long section sees the titular character sheltering by a beach, trying to work out a system for passing sucking-stones between his various pockets and his mouth, so that he never sucks on the same stone twice. Fantastic methods are devised, new ways of ordering the world: stones moving around by fours across the ordered and Ptolemaic universe of his coat; stones moving singly in postmodern disorder. Is it Molloy who is thinking here, or the system, the dynamic complex of pockets and stones? This passage has attracted a fair amount of attention from philosophers, who tend to see it as either a parody of logical systems in general or a form of thought beyond reason. Deleuze and Guattari, in Anti-Oedipus, see in it the model of a new kind of reasoning: schizoid, unbodied, and diffuse. Molloy’s circulation of pebbles is a “complete machine,” one in which “the mouth, too, plays a role as a stone-sucking machine.”
We’re used to thinking of active machines as digital machines; when we talk about the possibility that unliving things might think, we mean computers. We might be very shortsighted. All the processes we attribute to brains and computers alone might fill the world. In the same way that the legs code the program of walking, unknown information is inscribed in the patterns of grains of sand as the wind tosses them on an empty beach; the frenetic interconnections of the internet and the spoken world are thrumming in a field of grass. The thinking machine thinks; it has its processes and its functions. And the world of inert objects might think too, in slow and strange ways which we can only borrow for a moment, and which disappear again into what sounds like silence.


Imagine if a word in a book—say, bubble—had the ability to magically copy itself, and paste those copies elsewhere in the text. Eventually, you might bubble end up bubble bubble with bubble bubble bubble sentences bubble bubble bubble bubble like these.
This is exactly what happens in our genomes. There are genes known as retrotransposons that can copy themselves and paste the duplicates in other parts of our DNA, creating large tracts of repetitive gobbledygook. Around half of the human genome consists of these jumping genes. And a quarter of a cow’s DNA consists of one particular jumping gene, known as BovB. It, and its descendants, have bloated out the cow genome with thousands of copies of themselves.
This jumping gene seems to have entered the cow genome from the unlikeliest of sources: snakes and lizards.
Retrotransposons typically jump around within a single genome, but sometimes they can travel further afield. Through means that scientists still don’t fully understand, they can leave the DNA of one species and enter that of another. And so it is with BovB. No one knows the animal in which it originated. But from that mystery source, it has jumped into the DNA of snakes and cows, elephants and butterflies, ants and rhinos.
David Adelson, from the University of Adelaide, charted the gene’s travels in 2013 by comparing the subtly different versions of BovB in dozens of animals. That was when his team showed that BovB in cows and other cud-chewing mammals is most similar to the versions in pythons and vipers—and likely descended from them. Now, Adelson’s colleague Atma Ivancevic has extended the search for BovB to more than 500 animal species. And her results show that the gene’s travels are even more erratic than anyone thought.
Genes change over time, and closely related species have more similar versions than distantly related ones. So, if you compare different versions of the same gene across a range of animals, you can usually create a family tree that shows how they’re all related. But that only works for genes that are inherited in the usual vertical way, from parent to child.
When Ivancevic did the same exercise for BovB, which jumps between species horizontally, she got one of the weirdest family trees I’ve ever seen. It’s like a window into a bizarre parallel universe where sheep are more closely related to cobras than they are to elephants, where kangaroos have more in common with bedbugs than with horses, and where pythons, zebrafish, leeches, scorpions, and sea urchins all belong to the same tight-knit family. “It was just bizarre,” Ivancevic says.
She estimated that BovB has repeatedly jumped between the genomes of distantly related animals on at least 11 occasions, and likely many more. In some cases, when it arrived in a new lineage, it ran amok: Cows, sheep, and elephants all have thousands of copies in their DNA. In other groups, like bats and horses, it kept its head down, and produced just a few dozen copies.
No one knows how BovB travels between species, but Ivancevic and Adelson suspect that it might spread via blood-sucking parasites. They have found strong similarities between the BovB versions in leeches and zebrafish, bedbugs and snakes, ticks and lizards. By biting different hosts, parasites might help jumping genes to vault over the species barrier.
This idea makes sense in principle, but it’s hard to imagine in practice. What parasite, for example, could possibly have transferred BovB between, say, a sea urchin and a rattlesnake?
We should be careful before making too much of similarities between BovB in different species, argues Sarah Schaack, from Reed College. “Let’s say some worm DNA was picked up by a virus, that virus began infecting a bird, and the DNA was transferred,” she says. “Subsequently, a tick that specialized on the bird picked up some of the viral DNA. If you only sequenced the worm and the tick, you would infer the worm got the DNA from the tick or vice versa, which might be appealing because ticks are parasites, but would be strange because ticks don’t bite worms.” When genes go on the move, it can be hard to track their footprints accurately.
Ivancevic agrees, and wants to sequence BovB in many more species, parasites included. “I feel like the [patterns we have] look really sporadic because we don’t have all the intermediate species,” she says. By looking at more animals, it may become easier to trace the gene’s history.
That was certainly the case when Ivancevic turned her attention to a different retrotransposon called L1. This jumping gene is more directly relevant to humans because it fills up 17 percent of our genome. Most copies are now broken and stationary, but some have kept their ability to move around—and their presence has been linked to diseases like schizophrenia and cancer.
L1 exists in the genomes of almost every mammal, and was presumably around in the DNA of our common ancestor. And while it can jump around any given genome, scientists believed that, unlike BovB, it doesn’t move between species.
Not so, says Ivancevic. She identified at least three possible cases in which L1 seems to have jumped between major animal groups, typically those that live in water. And she showed that it’s completely absent from the DNA of platypuses and echidnas, which means that it must have jumped into mammalian genomes after these oddities had split off from the main dynasty, between 160 and 191 million years ago.
“There’s not yet a smoking gun for this transfer, like the identification of the donor species,” says Edward Chuong, soon to be at the University of Colorado Boulder. Still, “their failure to find any trace of L1 in [platypuses and echidnas] is fairly strong evidence supporting their claim.”
“These [jumping genes] really seem to be good at moving between genomes,” says Alexander Suh, from Uppsala University. But why, he asks, are some genes like BovB more likely to do so than L1, or at least more likely to establish themselves in a new setting? Possibly, Ivancevic says, it’s because BovB is about half the size of L1 and so is easier to move. Maybe it’s because BovB hitches a ride on parasites, and L1 doesn’t.
Whatever the reason, both genes—and others like them—must surely have influenced the evolution of their hosts. Jumping genes were first discovered by the pioneering scientist Barbara McClintock in the 1940s, but it took decades for people to realize how common they are—and how influential. They could potentially cause diseases by jumping into the wrong place and disrupting vital genes. But they could also provide raw material for evolution, by reshaping or rewiring existing genes. In this way, they spurred the evolution of the placenta, and supercharged our immune systems.
“It’s mind-boggling to think about how just a few [jumps] have fundamentally altered the course of our evolution,” says Chuong. Evolutionary biologists like to wonder what would happen if we replayed the tape of life—if we went back to some earlier point in history and let evolution run its course again. Would history repeat itself? Chuong thinks not. These jumps are so unpredictable, but so potentially important when they happen, that it’s hard to imagine the events unfolding twice.
And what would happen if we ran the tape forward? Where will genes like BovB end up next, and how will they shape the destinies of their hosts? “It would be interesting to see what BovB looks like in a few million years, but I’ll probably not get a chance to do that,” says Ivancevic.


It’s a story about mystery, grime, and a phoenix rising from the ashes—so of course it started in Chicago.
The Field Museum, a Greco-Roman citadel of natural history on the shores of Lake Michigan, is famous for its Egyptology exhibit and for displaying Sue, the largest T. rex ever discovered. But among scientists and biologists, it carries a different distinction. In its basement and archives, the Field Museum holds more than 3 million specimens: drawers and drawers full of the stuffed corpses of long-dead creatures.
Among these dead are hundreds of Illinoisan birds, many of them captured and killed in the decades after the city’s founding. For years, curators and collection managers had noticed that something was off about some of them. Birds from the late 19th century were noticeably darker than other specimens of their own species from other time periods or other locations. They suspected the birds had been stained by the soot that once choked Chicago’s skies—but without looking at the birds more closely, they couldn’t be sure.
Their drabber colors remained a mystery until a few years ago, when two University of Chicago graduate students took an interest in the birds. In 2014, Carl Fuldner, a historian of photography, and Shane DuBay, an evolutionary biologist who studies alpine birds, secured a grant to photograph some specimens in the Field Museum’s collection.
Using a scanning electron microscope, they confirmed that the wings were covered in black carbon. Black carbon is a tiny waste product of coal combustion. It’s of great note to scientists: Black carbon contributes to global warming, but it’s also a potent form of toxic air pollution. A particle of black carbon is so small that it can penetrate deep into the lungs’ alveoli.
Now Fuldner and DuBay have brought their work further. They have photographed more than 1,300 birds from museum collections across the Midwest. Then they used the birds’ relative drabness to reconstruct a 130-year history of black-carbon pollution in the region surrounding the Great Lakes. Their results were published this week in the Proceedings of the National Academy of Sciences.
How Atmospheric Black Carbon Has Changed Over Time
“The birds are an incidental record—we’re using the birds as the tool, the resource, that captures the direct environmental sample from the past,” said DuBay.
Getting them to give up that data required some creativity. The pair photographed the birds in the Field Museum before realizing that their setup was imperfect, and that it allowed different amounts of light to leak onto the birds with every exposure. They had to go image all the birds again using a light box designed for commercial photography.
The pair also photographed birds at the Carnegie Museum of Natural History in Pittsburgh and the University of Michigan Museum of Zoology in Ann Arbor, Michigan.
They also used a piece of software called RawDigger to access the unedited brightness data from the photographs. This let them treat the digital camera’s output as an objective data set, rather than a subjective image. “We’re using the entire photographic apparatus as a kind of reflectance sensor,” said Fuldner.
But the technique worked. It largely matches earlier attempts to estimate the amount of coal burned in the region. “Seeing how coal consumption dropped right after the Great Depression began—and seeing that signal pop up really strongly in our dataset—really validated [this] in our minds,” said DuBay.
Their graph provides evidence that emissions rose during the late 19th century, previously the least understood period for North American emissions. The research also contained some geography-specific treats: Six of the 10 sootiest birds came from Joliet, Illinois, which held the country’s second-largest steel refinery for decades. Fuldner was even able to dig up old photographs of Joliet in the 1890s, its sky nearly black with smoke from the refinery.
“Who would’ve thought that people could mine the birds we collected over the last 150 years and get a climate record from it?” said Joe McConnell, a professor at the Desert Research Institute who was not connected to the study. His 2007 study of ice cores from Greenland also estimates the amount of black carbon released from North America over the last 200 years and lines up well with this new finding.
He said that the study, especially in its spatial precision, was important confirmation of what was already known about black-carbon concentrations during the late 19th century. He lamented that DuBay and Fuldner had not extended their record before 1880. “It would have been interesting to see if bird-feather records declined to preindustrial levels if you go back another 30 years,” he told me.
Fuldner and DuBay’s study is not the first to plumb the depths of the county’s natural-history collections. As my colleague Ed Yong has written, biologists have just started to use their contents to reveal entirely new species. And recently, a team of researchers at Princeton University used similar collections of dead plants to learn how flora had moved north and uphill in response to climate change.
The pair say they hope to continue their work in other parts of the world with deep collections and a long industrial history. (The United Kingdom comes to mind.)
“One of the things that has really excited me about this project is it’s more about the prospect of these collections being used by atmospheric scientists,” said DuBay. “This is really just the tip of the iceberg for other studies like this.”


Few human traits are more variable, more obvious, and more historically divisive than the color of our skin. And yet, for all its social and scientific importance, we know very little about how our genes influence its pigment. What we do know comes almost entirely from studying people of European descent.
To Sarah Tishkoff, a geneticist at the University of Pennsylvania, that’s a ridiculous state of affairs. “It gives you a very incomplete perspective,” she says.
To redress that imbalance, Tishkoff and her team looked to Africa—the continent where humanity is at its most physically and genetically diverse. They recruited 1,570 volunteers from 10 ethnic groups in Ethiopia, Tanzania, and Botswana, and measured the amount of the dark pigment melanin in the skin of their inner arms. Then the team looked at more than 4 million spots in the volunteers’ genomes where DNA can vary by a single letter, to identify which variations are associated with their skin color.
They found several, clustered around six specific genes: SLC24A5, MFSD12, DDB1, TMEM138, OCA2 and HERC2. And they showed that these variants collectively account for 29 percent of the variation in skin color in the three countries studied. That’s a big proportion! For comparison, a similar and much bigger study identified hundreds of genes that affect one’s height, but that collectively account for just 16 percent of the variation that you see in large populations.
Tishkoff says that her results complicate the traditional evolutionary story of human skin. In this view, humanity began with dark skin in Africa to protect against the harmful effects of the sun’s ultraviolet radiation. As people migrated to other continents, some groups evolved lighter skin, to more effectively produce vitamin D in areas where sunlight is scarce.
But most of the variants that Tishkoff’s team identified, for both light and dark skin, have an ancient African origin. They likely arose in hominids like Homo erectus long before the dawn of our own species, and have coexisted in balance for hundreds and thousands of years. In many cases, the older variant is responsible for lighter skin, not darker. That’s consistent with an idea from Nina Jablonski, an anthropologist from Pennsylvania State University, who thinks that the ancient ancestors of humans—much like other primates—had pale skin. “As our ancestors moved out of the forest and into the savannah, they lost their hair and evolved darker skin,” says Nick Crawford, a researcher in Tishkoff’s lab.
But that wasn’t an all-encompassing change. Different groups of people adapted to their own particular environments, not just around the world, but within Africa, too. “Africa is not some homogenous place where everyone has dark skin,” Tishkoff says. “There’s huge variation.” For example, her team’s measurements showed that the Nilotic peoples in eastern Africa have some of the darkest skin around, while the San of southern Africa have light skin, comparable to some East Asians.
This physical diversity is mirrored in these groups’ genes. The first gene identified as affecting human skin color—MC1R—is very diverse in European populations but remarkably similar across African ones. Based on that pattern, says Tishkoff, some geneticists have concluded that the evolutionary pressure for dark skin in Africa is so strong that any genetic variants that altered skin color were ruthlessly weeded out by natural selection. “That’s not true,” says Tishkoff—but it’s what happens when you only study skin color in Western countries. “When you look at this African-centered perspective, there’s a lot of variation.”
For example, a gene called MFSD12 has variants that are linked to darker skin; these are common in dark-skinned people from East Africa, but rare among the lighter-skinned San. MFSD12 also shows how the search for pigmentation genes can reveal new insights about the basic biology of our skin. Two years ago, the gene didn’t even have a name, but it was linked to vitiligo—a condition where people develop white patches on dark skin. By deleting the gene in fish and mice, Tishkoff’s colleagues confirmed that it controls the balance between light and dark pigments.
Another gene called SLC24A5 has a variant that has traditionally been seen as “European,” because it is so starkly associated with lighter skin in Western European populations. But Tishkoff’s team showed that the variant entered the East African gene pool from the Middle East several millennia ago and well before the era of colonization. Today, it is common in Ethiopian and Tanzanian groups, but rare in other areas.
Critically, in East African groups, the variant doesn’t lighten skin color to the same degree that it does in Europeans. It’s a stark reminder that “a person can carry a gene that confers a particular trait in one population and yet not obviously show evidence of that trait themselves,” says Jablonski. “It reminds us that we can’t be cavalier about stating that a particular crime suspect has a particular skin color based on the presence of a single genetic variant in their DNA.”
Sandra Beleza, from the University of Leicester, has done one of the only other genetic studies of skin color to include people of mixed African ancestry. She says that neither her work nor Tishkoff’s have come close to identifying all the genes behind this trait. Further studies, involving other African populations that haven’t been included in genetic studies yet, may help to plug that gap.
While many have used skin color as a means of dividing people, Tishkoff sees the potential for unity and connectedness. “One of the traits that most people would associate with race—skin color—is a terrible classifier,” she says. Even without supposedly “dark” skin, there is a lot of hidden variation. “The study really discredits the idea of a biological construct of race,” she adds. “There are no discrete boundaries between groups that are consistent with biological markers.”
Jedidiah Carlson from the University of Michigan, who has been keeping tabs on how white-supremacist groups misappropriate genetic studies, agrees. “Because visually distinguishable traits common in present-day Europeans, such as light skin color, are also assumed to have arisen within European populations, white supremacists treat these traits as a proxy for superior intelligence,” he says. The history of SLC24A5 reminds us that “light skin pigmentation, and likely other ‘European’ traits, are not unique to Europeans. Human populations have been interbreeding for as long as we have existed as a species.”
White-supremacist communities “often rally around the demonstrably false claim that Africans are more genetically similar to ancestral hominids than Europeans—and these results turn the tables,” Carlson adds. At several genes that influence skin pigments, “Europeans are actually more likely to be genetically similar to great apes.”


There’s a famous viral video in which a diver slowly swims up to a clump of rock and seaweed, only for part of that clump to turn white, open its eye, and jet away, squirting ink behind it. Few videos so dramatically illustrate an octopus’s mastery of camouflage. But ignore, if you can, the creature’s color, and focus on its texture. As its skin shifts from mottled brown to spectral white, it also goes from lumpy to smooth. In literally the blink of an eye, all those little bumps, spikes, and protuberances disappear.
The project was entirely funded by the U.S. Army Research Office—and it’s not hard to imagine why. There are obvious benefits to having materials that can adaptively hide the outlines of vehicles and robots by breaking up their outlines. But there are other applications beyond military ones, Shepherd says. It might cut down on shipping costs if you could deliver materials as flat sheets, and then readily transform them into three-dimensional shapes—like flat-pack furniture, but without the frustrating assembly. Or, as the roboticist Cecilia Laschi notes in a related commentary, biologists could use camouflaged robots to better spy on animals in their natural habitats.
“I don’t see this being implemented in any real application for quite some time,” says Shepherd. Instead, he mainly wants to learn more about how octopuses themselves work, by attempting to duplicate their biological feats with synthetic materials. “I’m just a big nerd who likes biology,” he says.
Octopuses change their texture using small regions in their skin known as papillae. In these structures, muscle fibers run in a spiderweb pattern, with both radial spokes and concentric circles. When these fibers contract, they draw the soft tissue in the papillae towards the center. And since that tissue doesn’t compress very well, the only direction it can go is up. By arranging the muscle fibers in different patterns, the octopus can turn flat, two-dimensional skin into all manner of three-dimensional shapes, including round bumps, sharp spikes, and even branching structures.
Shepherd’s team—which includes the postdoc James Pikul and the octopus expert Roger Hanlon, who took the famous video at the start of this piece—designed their material to work in a similar way. In place of the octopus’s soft flesh, they used a stretchy silicone sheet. And in place of the muscles, they used a mesh of synthetic fibers that were laid down in concentric rings. Normally, the silicone membrane would balloon outward into a sphere when inflated. But the rings of fibers constrain it, limiting its ability to expand and forcing it to shoot upward instead.
By changing the layout of the fibers, the team could create structures that would inflate into various shapes, like round bumps and pointy cones. Pikul grabbed a stone from a local riverbed and programmed the material to mimic its contours. He set the material to create hierarchical shapes—lumps on lumps. He even programmed it to duplicate the more complicated contours of a field of stones, and a plant with spiraling leaves.
For the moment, the material can only be programmed to mimic one predetermined shape at a time. Still, “the results are impressive,” writes Laschi, and “represent a first step toward more general camouflage abilities.” Indeed, Shepherd is now adapting the material so it can transform more flexibly—just like a real octopus. For example, the team could replace the fixed mesh of fibers with rubber tubes, parts of which could be inflated or deflated at whim. That way, they could change which bits of the surface are flexible, to determine how it will eventually inflate.
Shepherd’s team is just one of many groups who are attempting to build soft robots, which eschew the traditional hard surfaces of most machines in favor of materials that are soft, bouncy, and floppy. Such bots would theoretically be better at navigating tough terrain, resisting shocks and injuries, and even caring for people. Often, these researchers use the octopus as an inspiration. Last year, Harvard researchers 3-D printed a soft, autonomous “octobot” that moved by burning small amounts of onboard fuel, and channeling the resulting gas into its arms. Laschi, meanwhile, has built a robot with soft floppy arms that can wiggle through the water.
The robots are certainly cool, but they’re nowhere near as versatile as the real deal. Shepherd’s material, for example, can change texture about as fast as an actual octopus, but it can only make one rough shape at a time. The animal, meanwhile, can produce far finer undulations in its skin, which are tuned to whatever it sees in its environment. For now, nothing we produce comes anywhere close.


In the wild, a predator that eats too much of its prey can drive that species toward extinction. But there are other, less understood influences that predators can have on their prey’s survival. Take, for instance, odor: New research shows that the very smell of predators may be enough to increase the chances of a whole population of animals going extinct. Fear alone, it suggests, can shape the fate of a species.
Traditional ecological theory holds that smaller populations of any creature will usually breed more productively than larger, denser populations, since individuals have less competition and more luck in mating, says Ryan Norris, a biologist at the University of Guelph. But when hungry predators hover around, small populations of prey may not be as insulated to stress as larger ones.
To test this, Norris and his colleagues recently conducted an experiment using fruit flies and one of the flies’ natural predators: praying mantises. They put a group of flies into a cage with a mantis underneath, in a separate partition. The fruit flies couldn’t see the mantis and it couldn’t attack them, but they could smell it. Days later, during the following breeding period, female fruit flies that had been in the cage laid fewer eggs than ones that had been placed in a separate cage with no mantis below. Those eggs subsequently hatched into insects that weighed less on average, too.
The researchers speculate that part of the reason females laid fewer eggs and had lighter offspring is because worrying about a lurking mantis takes away from the time fruit flies would normally spend foraging for food and having sex. Less food for fruit mothers could mean smaller eggs, though the study, which was published in Proceedings of the Royal Society B, didn’t specifically test to see whether a lack of food caused lighter offspring and less eggs. “I suspect they are eating less because they are being more vigilant of predators,” Norris says.
This only occurred with low-density populations, he notes; at high densities, other factors like competition probably overwhelms the effect of fear. But the researchers then found through modeling that when a mantis was around to strike fear into the hearts of low-density fruit-fly populations for several generations, the risk of extinction increased sevenfold over 10 generations. In the wild, the pressure predators put on a small population of prey could be even stronger, Norris says, since the models they ran didn’t include losses from direct attacks.
Justin Suraci, an ecologist at the University of California, Santa Cruz, says that Norris’s study is one of the few that approaches the effects of fear on the population level. As he sees it, it provides a break from the traditional view of predator-prey interactions, which only involves direct killing. “There’s some evidence that the fear effects outweigh the effects of actual killing and eating,” he says, adding that killing often only affects a single individual, while the fear of predators can affect entire groups.
These results aren’t limited to fruit flies. While laboratory tests using the insects may be particularly good at honing in on the specific factors that cause population declines, some researchers have identified similar effects on wild birds. A 2011 study exposed song sparrows to the sounds of predators like ravens, hawks, and raccoons, and found that just like the worried fruit flies, the birds laid fewer eggs, and fewer of the eggs produced birds that made it to adulthood.
Is It Possible to Be Scared to Death?
Suraci, too, conducted a study earlier this year to investigate how fear might affect predators themselves. He and his coauthors tracked pumas in the Santa Cruz Mountains via radio collars and determined when the big cats killed deer. Pumas often return to feed on their kills for several consecutive nights, so the scientists came in during the day and set up video-camera traps with speakers that played the sounds of humans—a large source of mortality for pumas—when triggered by motion around the deer kill. For comparison, other motion-triggered speakers were set up to play more natural sounds, like tree frogs.
The human sounds appeared to make the fierce mountain lions nervous. “They are much more likely to flee immediately and abandon their kill completely,” Suraci says.
The study only tracked the effect on individual pumas, so Suraci isn’t certain what disruptions like this would do at a population level. But the human fear factor clearly seems to drain cats’ energy levels, because they will waste more time on killing more deer and spend less time eating their fill.
Norris says that more research will have to be done to determine the extent of the effect that fear can have on small populations of prey, but it may be an important component in wildlife conservation. Government agencies and conservation groups spend an enormous amount of time and effort on wildlife-management plans, many of which are based on ecosystem models to predict the effect predators will have on a given prey population. If the mere sounds and smells of a predator are enough to offset the success of subsequent generations of animals, models that ignore the power of fear might be more likely to fail.
“[If] you make the wrong decision, you not only waste a lot of money, you risk the species going extinct,” Norris says.


Last week, a day after The New York Times reported many years’ worth of sexual-harassment allegations against film producer Harvey Weinstein, another alarming report appeared, this one in Science magazine. The setting was different—Antarctic research expeditions, not Hollywood—but the narrative was the same. A man, well aware of his position of power, had preyed on women in his field, and his behavior had gone unchecked for years.
According to Science, Boston University is investigating sexual-harassment allegations against David Marchant, an Antarctic geologist and now department chair at the school, brought by two women, his former graduate students. The women say Marchant verbally and physically harassed them during research expeditions in Antarctica two decades ago. Science writer Meredith Wadman reported that documents related to the case suggest Marchant denies the allegations.
For young scientists, research expeditions are important experiences and career builders. They are, in some ways, workplaces like any other in a male-dominated industry, which means they are not immune to sexual harassment. In a 2014 survey of field scientists, 64 percent of respondents said they had personally experienced sexual harassment during their work, and 20 percent reported they were sexually assaulted. But the nature of field work can amplify the damaging effects of sexual harassment, particularly at very remote sites, where there’s little to no communication to the outside world. The distance from reality can become both physical and emotional. The feeling of helplessness that comes with abuse is magnified. In the moment, victims may, quite literally, have nowhere to turn.
One set of allegations against Marchant came from a small expedition in Antarctica’s Beacon Valley, where people slept in unheated tents, traversed rugged terrain, and received supplies by helicopter, Science reported. For weeks, their only contact with others was a radio connection to a base station. Jane Willenbring, now an associate professor at the University of California, San Diego, alleges that during this trip Marchant, then her thesis adviser, called her a “slut” and a “whore” and urged her to have sex with his brother, who was with them. Willenbring said Marchant told her each day, “Today I’m going to make you cry.”
Another woman, whom Science does not name, alleges that during a different expedition in Antarctica, Marchant belittled her and called her a “bitch” repeatedly. “I began to believe the things he told me,” she wrote in a formal complaint.
In some cases, Marchant’s harassment was violent, Willenbring said. She alleges Marchant shoved her, threw rocks at her when she urinated in the field, and:
In another instance, Willenbring alleges in the complaint, Marchant declared it was “training time.” Excited that he might be about to teach her something, Willenbring allowed him to pour volcanic ash, which includes tiny shards of glass, into her hand. She had been troubled by ice blindness, caused by excessive ultraviolet light exposure, which sensitizes the eyes. She says she leaned in to observe, and Marchant blew the ash into her eyes. “He knew that glass shards hitting my already sensitive eyes would be really painful—and it was,” she writes.
The details of these allegations are shocking in their vulgarity. But the fact that they exist isn’t surprising at all, said Julienne Rutherford, a biological anthropologist at the University of Illinois at Chicago and one of the authors of the 2014 survey. Rutherford said the accounts of sexual harassment, in both the Weinstein and Marchant reports, read like a script, carefully constructed from similar stories over the years. She described it from the perspective of the alleged perpetrator.
“You identify the target as wanting something you control. You break them down to the point that they don’t trust themselves. You break them down to the point where their work suffers. And you isolate them to the point that they either don’t report, or when they do report, they’re told, wouldn’t it be better if you kept this to yourself?” Rutherford said. “It’s the same story over and over again, and it’s devastating every time.”
On a remote research expedition, there may be no option to report the harassment immediately. The person in charge might be the abuser. Witnesses to the abuse may feel powerless in the moment, perhaps fearful of making themselves targets.
The trauma of the harassment follows victims from the field to their homes and institutions. Avoiding their harassers may be difficult. According to a followup report from Rutherford and her 2014 coauthors on victims, published online Wednesday, “these interactions occurred on their university campuses, at conferences, or online, and a few targets of harassment received love letters even after repeatedly rebuffing the advances of their colleagues.”
Some victims fear career-ending retribution if they report the abuse. The anonymous woman in Science alleges Marchant threatened to keep her from getting research funding. Willenbring said she waited to file her complaint against Marchant until after she received tenure last year, fearing reprisal for describing the events of the field work. “It could be traumatizing to even look at the data” from a trip, said Robin Nelson, a biological anthropologist at Santa Clara University and Rutherford’s coauthor. Entire careers can be abandoned.
Then there are the fears of not being believed. “There still seems to be a fair amount of questioning of these victims, unless the cases are really egregious,” said Meredith Hastings, an associate professor at Brown University who is part of a national project to prevent sexual harassment in earth-sciences fields.
Reports of sexual harassment by male scientists, well-known and respected in their fields, have emerged in recent years in several areas of research, from astronomy to infectious disease, with many instances going unchecked for decades. The bad behavior is discussed quietly and discreetly in a whisper network, where women trade anecdotes and warnings about male advisers or professors or researchers. They exchange stories, advising their colleagues about who’s inappropriate, who’s handsy, and who should be avoided at all costs.
The recent spate of sexual-harassment stories will not be the last, in science and in Hollywood. Similar abuses have happened before, whether in a hotel room or a research camp, and they will happen again. The ones yet to come will prompt outrage, as these have, but they shouldn’t come as a surprise.
“This is common. This is not isolated behavior,” Rutherford said. “These are not a few bad apples. These are examples of systematic campaigns of abuse against junior people.”


In 2004, a New York Times reporter asked Stephen Hawking what his IQ was. “I have no idea,” the theoretical physicist replied. “People who boast about their IQ are losers.”
President Donald Trump seems to think otherwise. After recent reports that Rex Tillerson, his secretary of state, called him a moron, Trump told Forbes: “I think it’s fake news, but if he did that, I guess we’ll have to compare IQ tests. And I can tell you who is going to win.”
As Philip Bump at The Washington Post reported, Trump has a history of boasting about his IQ, and challenging others to IQ tests. His supporters have also taken up this cause for him in the past. In December 2016, a chart made the rounds saying that Trump’s IQ was 156, putting him above most past presidents. (The median score is 100.) The fact-checking website Snopes rated this claim as false: While the chart was based on a real study, the study didn’t have real IQ scores for most presidents (it estimated their IQs based on other factors), Trump wasn’t included in the study, and most importantly,“Donald Trump’s true intelligence quotient is unknown,” the article reads.
Scientists disagree on how useful IQ tests are as a measurement of intelligence. There’s research to show that IQ can change over the lifespan, for example. And some say that it doesn’t account for things like emotional intelligence or creativity. But “IQ” remains an easy shorthand for referring to intelligence, with the added bonus of sounding scientific, quantifiable, and official.
It isn’t totally surprising that Trump seems to think his IQ is exceptional. If asked, most people would say they are smarter than the average person. (They would also likely say they’re more competent, kinder, more honest, and more responsible.) This is a well-studied phenomenon in psychology known as the “better-than-average effect” or “self-enhancement.”
Even so, people might be reluctant to publicize their sense of superiority, because such boasts tend to be poorly received. One study found that while bragging about your own good qualities didn’t necessarily make people dislike you, bragging about yourself in relation to others did. For example, saying “I am a good student” probably wouldn’t bother anybody, but saying “I’m a better student than others” would. Bragging about your better-than-average intelligence, then, would likely make other people think you’re insulting them—and make them feel aggressive toward you as a result.
“We see this consistently, that the claim, at least in this culture, of being above average is just frowned upon,” says Joachim Krueger, a professor of psychology at Brown University who has studied bragging. “There are also conversational norms.” There’s an expectation that your good qualities will reveal themselves over time, without you announcing them.
So if bragging about your intelligence can alienate people, and make you look like a loser (at least to Stephen Hawking), why risk it?
It feels great, for one thing. People love to talk about themselves—to the point that in one study, they turned down money for the opportunity to answer questions about themselves. And the same study found that self-disclosure appears to activate brain regions associated with reward.
How to Boast on the Sly
They may also miscalculate how others will react to their boasts. One study found that people often self-promote because they’re trying to make a good impression on others. But they tend to overestimate how much their self-promotion will make other people feel happy for them and proud of them, and underestimate how annoying people will find it.
Krueger coauthored a study in 2016 that found bragging about your superiority comes with a tradeoff. It will make people see you as more competent, but less moral—unless the thing you are bragging about is provably false. Then it makes people see you as incompetent and immoral.
In a case like Trump’s brag about being smarter than Tillerson, where the validity of his claim is unknown, the research suggests people would err on the side of believing him.
“My personal opinion of the president’s claims about his own IQ is that he is making a gamble: brag and be thought of as competent (intelligent), and hope that nothing will ever surface that proves his claims wrong,” Patrick Heck, Krueger’s coauthor on the study, told me in an email.
“Of course there won’t be an IQ test given and then the nation will be informed what score he had and what score Tillerson had, so it’s pure rhetoric,” Krueger says. “In the context of our study if there truly were no other information about him, that would be a slight advantage.”
But nobody is evaluating Donald Trump based on this claim and this claim alone. Krueger points out that because opinions of the president are already so polarized, people will likely incorporate this latest brag into their existing views of him. If they think he’s a clever politician and a savvy businessman, they’ll likely take his brag at face value. If they already think he’s incompetent and immoral, an unverified claim about his IQ probably won’t change that.
But what if Trump and Tillerson were actually to throw down—intellectually speaking? Mensa International—a society whose members must prove their IQs are in the top 2 percent of the population—has offered to host an IQ-test showdown between Trump and Tillerson, heaven help us all. What if it really happened? And what if Trump were to lose?
“That’s a crazy hypothetical—you and I know perfectly well that’s not going to happen and Mensa never thought it would happen,” Krueger says. “That's another rhetorical play to throw into the game. But in your hypothetical universe, [if they] take the test and we find that he is 20 points below Tillerson, then according to our study, it would be a false positive and it would be particularly embarrassing.”


The morning of January 27, 1967, Gus Grissom and his Apollo 1 crew put on their flight suits. Foil-clad, with breathing masks, they looked like a mid-century vision of the future brought to you by Reynolds Wrap. The crew of three were to perform a launch test that day, to see what it would be like when they rode a metal cone to space.*
Grissom had been to space before during the Gemini program. That day’s practice wasn’t going great, not like one would hope an actual launch would go. First, the astronauts smelled something rotten in their oxygen. That delayed them by more than an hour. Then, their communications system began to fritz out. Of this, Grissom famously groused, “How are we going to get to the moon if we can't talk between two or three buildings?”
Later, though—into that same microphone and over those same lines—came a single word: “fire.”
It was true: Damaged wires had likely ignited a spark, which fed on the all-oxygen air, growing with its consumption of space-age material—nylon, foam.
The crew tried to escape the capsule. But the hatch wouldn’t open. All three astronauts suffocated inside the vessel that was supposed to carry them—and with them, us—into the future.**
The agency’s two other fatal accidents occurred during the same January week as Apollo’s: Challenger 19 years later, Columbia 17 years after that.*** And just three years ago, the private-spaceflight industry endured its first human loss, when Virgin Galactic’s SpaceShipTwo lost its copilot.****
After each fatal incident, the nation has responded with shock and grief. These explorers—our explorers, Earth’s explorers—paid for that exploration with their lives. Questions arose. Some—How did this happen?—are left to inspectors and investigators. But others—How big a cost are humans willing to bear to leave the planet?—lie in the public domain. The answers seem to have changed throughout the decades, as space travel seemed to evolve from something novel to something routine.
Today, industry and government are both upshifting gears, back into novelty, which means the public’s attitudes toward space travel and its inevitable accidents may return to what they were in NASA’s early, more adventurous days. After decades in a stable and predictable orbit, American spaceflight will return to new vehicles and, maybe, new destinations. The country is deciding which far-off world to point ships toward next, with the moon and Mars the most likely candidates. Private companies are doing the same, and preparing to take high rollers on suborbital romps. And with that leap into the unknown, Americans may become more tolerant of the loss of astronaut life. If they don’t, the government and private industry might not be able to make the leap at all.
We all know people probably will die on these new missions, especially if they become commonplace, as many hope. What no one knows is how we will all respond to those losses.
* * *
When Grissom and his compatriots signed on to the astronaut corps, times were different. They were different: cowboy test pilots—military men, mostly, with that rightest of stuff. Space, and the flight paths to and through it, was basically uncharted. Rockets blew up—a lot—listing sideways, spinning tail over teakettle, exploding heads in the ground like ostriches.
And the astronauts themselves were, for the most part, inured to their mortality. In The Right Stuff, Tom Wolfe repeatedly references the euphemism the early astronauts used to describe fatal crashes: The fliers had “screwed the pooch.” It’s gallows humor: The pilots and astronauts couldn’t completely control their survival—but they could at once face death, distance themselves from it, and use tone to strip it of power.
The public perceived these guys (and they were all guys) as all-American swaggerers, laying their lives on the line for the primacy of the country.
“It was a battle in the Cold War,” says Rand Simberg, author of Safe Is Not An Option: Overcoming the Futile Obsession With Getting Everyone Back Alive That Is Killing Our Expansion Into Space.
The nation, of course, mourned the Apollo 1 crew’s loss—especially given its gruesome nature. But the public and the government were perhaps not surprised, or philosophically disturbed, that people had to die if Americans were to get to the moon in a decade. In an article called “Space Travel: Risk, Ethics, and Governance in Commercial Human Spaceflight,” space strategist Sara Langston looks to other fields to understand attitudes and regulations about space exploration. “In the Encyclopedia of Public Health, [Daniel] Krewski defines acceptable risk as the likelihood of an event whose probability of occurrence is small, whose consequences are so slight, or whose benefits (perceived or real) are so great that individuals or groups in society are willing to take or be subjected to the risk that the event might occur,” she writes. The risk of space accidents, by inference, are subject to the same kind of cost-benefit analysis.
After Apollo, though, came the staid shuttle program. And with it, the tenor of spaceflight changed. The Cold War ended in the ’90s. The spacecraft was called a shuttle. You know, like the bus that takes you to the airport. The Americans had already conquered spaceflight—we got to the moon, which was very hard and very far away and involved orbiting other bodies and sometimes landing. Spinning ellipses around our own planet in a sturdy vehicle? Easy.
The shuttle program left Americans—and perhaps the world—with the false sense that the space-pioneer days were over.
* * *
In technical terms, as the shuttle program developed, people began to think of its flights as operational rather than experimental. In experimental mode, engineers are still figuring the details out, fingering the edges of a craft’s envelope and seeing how hard and fast they can press before they get a cut. In operational mode, though, engineers are supposed to know most everything—the ups, downs, ins, and outs of performance given sundry contingencies.
While the shuttle mostly functioned well, that performance was never actually a given. The vehicle remained, to its last days, experimental, a status reflected in its success/failure rate. “I think people that know our industry kind of understand the edge we're on here, because these systems are tremendously complex,” says David Bearden, general manager of the NASA and civil-space division at the Aerospace Corporation. “If you look back, empirically or historically, at any launch system, about the best you're ever going to get is 1 in 200. On an airline it is a one-in-a-million chance. People who know the industry and know the way those systems operate understand that, I think.”
* * *
I was only six months old when space-shuttle mission STS-51-L sat on the launchpad on January 28, 1986. Aboard were six astronauts and Christa McAuliffe, a teacher from Concord, New Hampshire. The shuttle lifted off on the cold Florida morning. But then, nine miles above Earth’s surface, that seemingly reliable spacecraft broke apart, undone by the uncharacteristic chill at Cape Canaveral that day.
As a Miami Herald Tropic investigation later detailed, the astronauts didn’t die right away: The crew vehicle stayed intact, and continued to go up, before tipping back toward Earth, traveling 12 miles of sky before crashing into the cold ocean water—hard as the cement on the launchpad. The astronauts, the article said, were very likely alive until the very end and might have even been conscious.
Coverage from the days after the tragedy expresses, of course, sadness. “The Shuttle Explosion; Suddenly, Flash of Fear Dashes Watchers’ Hopes,” read a New York Times headline.
“What Went Wrong? Shuttle Loss in Ball of Fire Stuns Nation,” went one from the local Orlando Sentinel.
Both papers, though, declared that the show must go on: “Reflecting on Loss: Welling of Tears, a Desire to Press On,” said the Times.
“Three shuttle veterans lose peers but not faith in program,” said the Sentinel.
The losses, while tragic and (as the Rogers Commission Report would later reveal) avoidable, shouldn’t squash the program. Sacrifices, after all, must be made, for a new program whose utility the nation was still proving.
* * *
I was 17 when NASA lost the space shuttle Columbia in 2003. I’d grown up in Central Florida, not far from Kennedy Space Center. I’d seen almost every shuttle launch in person—with my classmates outside on the sidewalks of my schools, with my sisters in the backyard, and very occasionally from the far side of Cape Canaveral, with my parents. The sonic booms from landings sometimes set off the burglar alarms that hung from our door handles.
But in 2003, I had things to do that didn’t include watching out for spacecraft. I was on my way to a band (marching band, not the cool kind of band) rehearsal session when I heard about Columbia. News of the accident came slow and halting over whatever alt-rock station I was blasting from my Grand Am.
Later, investigators would reveal that a piece of foam had fallen from the shuttle’s wing during launch, leaving a hole that let gas come through when the shuttle re-entered the atmosphere. The shuttle was going Mach 18, 37 miles above the ground, when it broke apart, shaking debris across thousands of square miles.
I remember sitting in my car in a church parking lot, thinking how it couldn’t be real. I remember thinking the radio host didn’t sound like he thought it was real. We’d probably both watched shuttles launch and land safely for much of our lives. To us, the whole program seemed routine—operational. It had moved into that realm of seeming safety, and risks seemed not just less likely but also less justified. And while we always knew this could happen, we never thought it would.
The Columbia disaster represented, unlike the Challenger explosion before it, the start of the finish for the shuttle program. NASA announced its end the very next year. Two strikes, shuttle’s out.
* * *
Sometimes, you hear the phrase “Failure is not an option” associated with NASA. But it was never a slogan at the agency; no one in mission control, that we know of, ever said it, and no manager passed it down. It was just a line in the movie Apollo 13. Failure is always an option: It has to be.
Of course, no one wants a rocket to blow up or a crew capsule to fall to Earth. But to undertake space travel, the undertakers have to acknowledge those possibilities and mitigate the risks. As NASA administrator William Gerstenmaier said in his paper “Staying Hungry: the Interminable Management of Risk in Human Spaceflight,” “We never simply accept it, but NASA, our stakeholders, and the public must acknowledge the risk as we move forward.”
The public, to some extent, also knows that’s the equation. But a 1/200 mission-failure rate means that one doesn’t happen very often, which means that every one comes as a shock.
Still, astronauts’ deaths don’t always cause communal moral outrage. “A particularly risky venture can become socially acceptable in correlation with the value placed on it,” Langston wrote in her risk paper. If people value a space-exploration program, in other words, they’re okay with others risking their lives to push it forward.
Simberg contends that wasn’t fully true with the shuttle, as compared to Apollo—an inspiring and aspiring mission with political importance. “The reason we were so upset about losing these seven astronauts was that what they were doing was kind of trivial,” he says of Columbia.
We don’t always demand, though, that people be doing something Valuable that Benefits Humanity to let them risk their lives (and there were lots of ways the shuttle and, in particular, its trips to the always-peaceful International Space Station did benefit humanity). About 1 percent of people who try to climb Mount Everest historically die in the attempt, for example. And this despite the fact that topping Everest is not exactly exploration, with its $40,000 price tag and Sherpa guides and commercial expeditions. And it’s been done before.
Shuttle astronauts, meanwhile, have a 1.5 percent chance of dying on a given trip to space. And trying to keep them at least as safe as that—or safer—means the agency can’t go as boldly as private industry can.
* * *
The major players in the crewed-commercial space are SpaceX, which wants to eventually build a martian colony; Blue Origin, whose Jeff Bezos envisions an outer space full of industry and long-term habitation; and Virgin Galactic, which wants to democratize access to space closer to home, with a carrier plane that rides up to 50,000 feet, then travels up on its own and glides back down at the behest and guidance of its pilots.
On October 31, 2014, Virgin Galactic paid a human price for that system. During that October test flight, copilot Michael Alsbury unlocked SpaceShip’s feathering system, which changes the shape of the plane to aid reentry, early. The wind then pushed the system open, and the vehicle destabilized. While pilot Peter Siebold parachuted to safety, Alsbury remained with the ship, and died on impact.
After the accident, Virgin allowed its already-booked customers to back out, but just 3 percent did.
SpaceX, meanwhile, has had its own explosive setbacks, and yet the company and leader Elon Musk still remain the industry’s darlings. SpaceX blew up an uncrewed Falcon 9 rocket in September 2016. In June of the year before, the company lost another Falcon that was supposed to resupply the Space Station. In test launches and landings of its reusable rockets, SpaceX has also had a vessel tip over into the ocean and explode (January 2016); crash into a ship (January 2015); and land “too hard for survival” (April 2015).
Based on this, the NewSpace industry seems to exist firmly in the experimental phase. But, more than that, the public seems to know—and accept—that status. “You understand that you're in a test-pilot phase,” says Bearden. “The public can process that and say, ‘That's not me. By the time I fly, they're going to have worked it out.’”
The public permits mistakes for the private space companies—because they produce rockets and results on non-geologic timescales, and lay out visions like “you can go to space” and “you can have a house on Mars.”
The FAA, which regulates commercial space activity via the Human Spaceflight Requirements for Crew and Spaceflight Participants, is also relatively forgiving. “Congress mandated these regulations in the Commercial Space-Launch Amendments Act of 2004,” says the FAA’s description of this law. “Recognizing that this is a fledgling industry, the law required a phased approach in regulating commercial human spaceflight, with regulatory standards evolving as the industry matures,” attempting not to crush innovation with regulation. Flight providers do, though, have to get extremely informed consent from would-be astronauts.
NASA recognizes the value in this model, and in its different posture toward risk. The agency has teamed up with such space companies—letting them, among other things, shuttle cargo and crew to low Earth orbit. NASA no longer has to be all things to all people and missions, and can let those experimental upstarts do a little legwork.
The agency may also see, though, that the public perceives New Space cadets as pioneers—a lens through which they don’t see NASA like they used to—and so forgive their mistakes, tallying them as the cost of innovation, rather than a cost not worth bearing. And perhaps the agency hopes the same thing for itself, as it turned those duties over to private companies so that it can focus on its own bold goals, its own new risky, experimental phase of operations with both the costs and the benefits that come with that.
* This article originally stated that there were four crew members aboard Apollo 1.
** This article originally misstated the cause of death for the Apollo 1 crew.
*** This article originally implied that the Columbia disaster occurred 36 years after the Challenger explosion.
**** This article originally stated that the Virgin Galactic crash resulted in the death of the craft’s pilot. We regret the errors.


Ten months in, and the presidency of Donald Trump has acquired a reputation for ineffectiveness. Trump’s attempt to repeal Obamacare has failed three times; he has taunted and alienated some congressional Republicans; he has hemorrhaged senior administration officials while struggling to contain an FBI investigation.
But Trump has found near-total success in a few areas of lawmaking—and few are as expansive, with consequences as long-lasting, as environmental policy. Since January, the Trump administration has tried to dismantle former President Barack Obama’s broad legacy of EPA rules meant to lessen the blow of climate change. It has largely succeeded. This week, Trump’s team opens a new front in that war.
Scott Pruitt, the administrator of the U.S. Environmental Protection Agency, announced Monday that he will fully repeal the Clean Power Plan, the Obama administration’s signature plan to reduce greenhouse-gas emissions from the U.S. electricity sector.
It’s the new administration’s most aggressive attempt to materially transform U.S. climate policy since its departure from the Paris Agreement on climate change in June.
The Clean Power Plan was designed to lower greenhouse-gas emissions from American power plants by 32 percent by 2030, as compared to their historical peak in 2005. It constituted the central legal mechanism to meet the American commitments made under the Paris Agreement.
Speaking in the coal-mining town of Hazard, Kentucky, Pruitt alleged that the Clean Power Plan represented illegal executive overreach. He also linked it to a “war on coal” that he says was waged by the Obama administration.
“The past administration was unapologetic. They were using every bit of power [and] authority to use the EPA to pick winners and losers on how we generate electricity in this country. That is wrong,” he said. Mitch McConnell, the Senate majority leader and a longtime Republican of Kentucky, appeared by his side.
Environmentalists condemned the repeal and promised to fight it in court. Ditching the Clean Power Plan, they said, would further degrade the planet’s climate by leading to the release of more heat-trapping gas into the atmosphere.
Public-health groups, including the American Lung Association, also condemned the planned repeal. Nearly half of Americans already live in counties with unhealthy levels of air pollution, the organization has said, adding that climate change appears to makes conventional air pollutants worse.
“Science shows that warmer temperatures can reduce air quality, due to increases in ozone and particulate matter,” said Laura Anderko, a professor at Georgetown University. Air pollution poses a particular health threat to children, she noted.
The Clean Power Plan was sold in part on its public-health benefits. A 2015 study in Nature Climate Change found that a set of rules similar to the Clean Power Plan would prevent 220 heart attacks and 3,500 premature deaths per year. Pennsylvania, Ohio, and Texas would all see the greatest gains under the simulated plan, each avoiding hundreds of early deaths annually.
But the Clean Power Plan has also long contained a tension inherent to American environmental law. Since 2007, the Supreme Court has held that the EPA has the legal authority to regulate carbon dioxide and other greenhouse gases under the Clean Air Act.
The Obama administration didn’t pursue regulation at first, attempting to pass the Waxman-Markey climate-change bill through Congress. That bill would have allowed companies to bid on the right to emit carbon dioxide into the atmosphere, mirroring previous federal laws to reduce acid rain. But after it failed in the Senate—and Democrats lost control of Congress in 2010—Obama’s EPA turned to the Clean Air Act. Over the course of several years, the agency studied, issued, and revised state-by-state rules guiding how local governments should reduce greenhouse-gas emissions from their power plants. The final version of the Clean Power Plan was released in August 2015, months before the Paris conference on climate change.
Almost immediately after the Clean Power Plan was published, dozens of state attorneys general sued the Obama administration, alleging the rules were illegal. They were led by Pruitt, then the Oklahoma attorney general and the chairman of the Republican Attorneys General Association. Despite the Supreme Court’s finding that the Clean Air Act allowed the EPA to regulate carbon dioxide, they argued that this plan, the Clean Power Plan, was illegal.
Did the Obama rules break the law? We may never know. The D.C. Circuit Court of Appeals was still considering the case on Election Day last year. Trump ordered the EPA to revisit the plan in an omnibus executive order in March.
Due to the Supreme Court’s 2007 ruling, the Trump administration will have to issue a new regulation to replace the Clean Power Plan. But it has great leeway in when it may issue that new rule, and it appears to be taking its time.
Of course, the Clean Power Plan is not the only change convulsing the U.S. power sector. While solar and wind energy are on the rise worldwide, they have mostly displaced nuclear power in the United States. Coal use has indeed been plummeting in recent years, but this has mostly been due to cheap natural gas made available by fracking.
“You want to know how you bring the coal jobs back? Ban fracking,” said Ted Thomas, the chairman of the Arkansas Public Service Commission and a former Republican legislator in the state, in March. “When you talk to utility professionals, there are more of them who think Elvis is still alive than believe that” the Obama administration’s rules are to blame for the decline in coal, he told me then.
Indeed, energy experts say that the United States may meet the ostensible emissions-reduction goals of the Clean Power Plan. The Rhodium Group, an energy consulting firm at the forefront of climate-economic forecasting, projects that the U.S. power sector will fall by between 27 and 35 percent by 2030 without the Clean Power Plan.
But its experts also caution the 32-percent figure was just an estimate, and the real emissions gains of the Clean Power Plan may have been higher than anticipated. Historically, energy forecasts have undershot the rise of solar and renewable energy.
And there are graver implications to the rollback. The United States is not on track to reduce its carbon emissions fast enough to avert catastrophic global warming—even under the most optimistic economic forecasts. A recent study on the world’s carbon budget found that the world would need to reduce its emissions in ways that parallel “the Great Depression, the years following World War II, and during the collapse of the Soviet Union” to stave off the worst of global warming.


Rumors that the Trump administration was more interested in the moon than Mars began circulating days after the inauguration. Leaked memos published in February revealed the president’s advisers wanted NASA to send astronauts there by 2020, one part in a bigger plan to focus on activities near Earth rather than missions deeper in the solar system. Vice President Mike Pence spoke vaguely of a return to the moon in a speech in July. In September, the administration nominated a NASA chief who extolled the construction of lunar outposts. All signs pointed to a significant shift in the country’s Mars-focused space agenda of the last seven years.
This week, the Trump administration made it official.
“We will return NASA astronauts to the moon—not only to leave behind footprints and flags, but to build the foundation we need to send Americans to Mars and beyond,” Pence said Thursday at the inaugural meeting of the National Space Council, an advisory body his administration recently revived.
The vice president’s comments marked a pivot from Barack Obama’s directive for a “Journey to Mars,” established in 2010, and harkens to the aspirations set forth by the George W. Bush administration. The Obama administration had maintained that some kind of human activity in cislunar space—the region between the Earth and the moon—was necessary to test technology for a mission to Mars, but the efforts would amount to a pit stop, not a destination. While Pence did not provide details on what kind of “foundation” Americans would build on the moon, the new direction was clear: Americans should be spending more time in their cosmic backyard before flying off into the solar system.
“It’s a 180-degree shift from no moon to moon first,” said John Logsdon, a space-policy expert and former director of the Space-Policy Institute at George Washington University.
The announcement is obviously good news for space-transportation companies and lunar researchers lamenting the country’s 45-year absence from the moon. For those in the Mars camp, many of whom aim for a human mission to the planet by 2033, the news puts their ambitions on shakier ground.
The administration’s push for a return to the moon may be unambiguous now, but plenty of questions remain, ranging from the basic, like when and how, to the intriguing, like the role commercial spaceflight companies might play. NASA also wouldn’t be starting from scratch. The space agency has spent the last decade building the Space-Launch System and Orion, a rocket and spacecraft intended to carry people into deep space but also to build a cislunar way station called the Deep-Space Gateway. NASA planned to use the Deep-Space Gateway as a place for astronauts to prep for deep-space journeys, but the new shift could see the station being used for lunar landings.
Both time-tested contractors and growing commercial companies will be eager to work on potential lunar activities. Boeing is currently developing a capsule that would ferry people into low-Earth orbit, and SpaceX said its proposed mega-rocket, which is mostly intended to fulfill Elon Musk’s Sim City-esque ambitions for Mars, could contribute to travel to the moon. Musk, well aware of the political benefits of it, leaned heavily into lunar ambitions. “It’s 2017. We should have a lunar base by now,” he said recently. “What the hell’s going on?”
Federal support for moon research is also good news for lunar scientists like Paul Spudis, a scientist at the Lunar and Planetary Institute in Houston who writes frequently in support of a return. He dismissed NASA’s Mars ambitions as unrealistic and a public-relations stunt. “I see the vice president’s remarks not so much as a pivot in policy as a belated recognition of simple reality,” Spudis wrote in an email. “They don't have have an architecture, they don't have the spacecraft, they don't have the technology, and most assuredly, they don't have the money to bludgeon any difficulties into submission.”
Mars proponents, naturally, disagree. “We weren’t anywhere close to being ready” to going to Mars, Logsdon said, but to dismiss the objective as simply publicity is wrong. NASA has invested over the years in some research for the requirements of a mission, including life-support systems and landing technology, he said.
“Most of the people who are Mars-centered worry that we’ll get stuck on the moon, all the resources available will be focused on lunar exploration, and the idea of getting to Mars will slip indefinitely into the future,” Logsdon said.
Chris Carberry, the CEO of the nonprofit group Explore Mars, fears the same. Carberry said he’s pleased with the administration’s focus on human spaceflight and doesn’t oppose a pit stop on the moon. But he wonders whether the construction of a full-fledged lunar base could consume enough resources that would delay a Mars mission for decades. “If we got to the surface of the moon, we need to do it in a way that really is a stepping-stone to getting to Mars, not just an excuse to build a base there,” Carberry said.
In Washington, NASA’s Mars goals have received bipartisan support, which is typical for space programs in general. In March, Trump signed a NASA funding bill that included some of the strongest language about a human mission to ever appear in U.S. legislation, listing “achieving human exploration of Mars” as a key objective. The space agency has also been buoyed by interest from a general public bombarded with Hollywood movies about Mars and deep-space travel.
A renewed focus on the moon brings the United States into some alignment with space agencies in Europe, Russia, India, and China. The European Space Agency envisions building a “moon village.” Roscosmos, the Russian agency, is recruiting cosmonauts to be the first Russians to land on the moon in the 2030s. ISRO, the Indian agency, launched its first lunar orbiter in 2008 and plans to send a second mission, this time to land on the surface, next year. China has spent the last decade experimenting in cislunar space with both robotic and crewed missions, and officials have said they would put astronauts on the moon by the mid-2030s. A mission to send a rover to the far side of the moon, a first for humankind, is planned for next year. The United States is unparalleled when it comes to exploration of the solar system beyond Earth, but some in the country, particularly in the security community, worry that the country’s cislunar capabilities are rapidly atrophying.
Pence said this week he believed other spacefaring nations have outpaced the United States. “In the absence of American leadership, other nations have seized the opportunity to stake their claim in the infinite frontier,” he said. “Rather than lead in space, too often, we have chosen to drift.”
Space-policy experts say it’s too early to say who the winners and losers might be in this new chapter in the country’s space agenda. The administration has promised a robust human spaceflight program, an effort that both moon and Mars proponents can get behind. And the administration didn’t rule out a Mars mission entirely. It just chooses to go to the moon in this decade and do the other things—later.


Squirrels are among the peculiar menagerie of creatures who’ve made a home for themselves where humans live. Like pigeons, they’ve figured out how to continue their ways in our parks, cities, and towns, tucking away nuts in the walls of houses, in basements—and in the lawns of institutions of higher learning.
At the University of California, Berkeley, that behavior is the focus of a recent paper by comparative psychologists. Having performed a number of experiments with their small furry neighbors, Mikel Delgado and her colleagues discovered that squirrels are not putting nuts away willy-nilly. When they have the chance, they'll sort their bounty by type, like nuts with like nuts. That suggests they’re remembering not only the locations of their caches but, intriguingly, their contents.
The work began some years ago, with this paper just the latest in a long string.“The question always is, given that squirrels bury so many nuts, how do they decide where to bury them and how are they able to find them again?” says Delgado, who performed the research as a graduate student and now is a postdoctoral researcher at University of California, Davis.
If you’ve ever watched a squirrel with a nut, you might have noticed that they turn them over and over in their paws, bobbing their heads. That seems to be a way for them to judge the weight and other qualities of the nut they are holding, which suggests that information is useful for them in their subsequent stashing. Perhaps, the scientists thought, the squirrels were sorting the nuts somehow.
Delgado and her colleagues designed a series of experiments in which they gave nuts to 45 squirrels and watched to see where they put them. Each squirrel was given an almond, a walnut, a pecan, or a hazelnut, then the researchers followed them from a distance to see where on the Berkeley campus they cached it. After it was hidden, the researchers took a GPS reading at the site. Half the squirrels got their next nut right then and there, while the others were lured back to the starting point. Not all squirrels are very patient with this, it turns out. “[Sometimes it] takes a while, because you have to convince them to come back,” says Delgado. The intent was to mimic a situation where a squirrel has found a tree flush with nuts that they’ll return to.
This process repeated 16 times, so each participant got 16 nuts. Some squirrels got theirs in runs of the same kind—four walnuts, then four almonds, and so on—and some got them in no particular order. This was to test whether they would cache the walnuts together, for instance, even if they got lots of other kinds of nuts in between.
When the researchers tallied up the contents of the caches, they discovered that the squirrels that got their nuts in the same place every time would group the same kinds of nuts together, a process called “chunking” in cognitive science. “That is pretty cool because it suggests they can remember” where they put certain types of foods for at least a few hours, says Delgado.
When the foraging was spread out over the campus, the squirrels wouldn’t go all the way to the last almond when they got a new one, however. Instead, they made caches of mixed nuts along their route. Chunking wasn’t a universal strategy, which suggests it takes some mental resources to keep track of where that last nut of that particular kind went. Sometimes it’s helpful; sometimes—perhaps when a squirrel is already pretty far away from its first caches—it’s not worth the strain.
It’s fascinating to think that right under our noses squirrels are engaging in some complex sorting processes. Then again, this is a species that lives for months on food hidden all across the landscape, in patterns that researchers are still working to understand. That may shame those of us who can’t find that butter we know we bought, or that stray can of beans that turns up years later. Perhaps if we paid a little more attention to creatures living around us, we could pick up a few tips.


At first, they look like stars. I see them as I gaze upward at the ceiling of a flooded, pitch-black cave—hundreds of blue pinpricks. As my eyes habituate to the darkness, more and more of them resolve, and I see that they are brighter and more densely packed than any starry field. And unlike the night sky, these lights don’t appear as a flat canvas, but as a textured one. Some are clearly closer to us than others and they move relative to each other, so the whole tableau seems to undulate gently as our boat sails beneath it. These lights are not astrological, but entomological. They are produced by insects called glowworms.
The word “glowworm” is sometimes used colloquially to describe fireflies and click beetles. The insects in the caves aren’t part of either group. They’re maggots—the larvae of small flies called fungus gnats. Hatching out of eggs that are laid on the ceilings of caves, the larvae spin hammocks of silk. From these hammocks, they lower up to 70 threads of silk—extremely strong, and coated with blobs of sticky mucus. These threads, which dangle downward like bead curtains, are traps, and the glowworms bait them by triggering a chemical reaction in their rear ends that emits blue light. The light lures in other insects that get entangled in the silk, and are eventually reeled in and devoured by the glowworms.
After 6 to 12 months of eating whatever they can ensnare, the larvae transform into adults, which lack mouths and never eat. Their only job, in the final few days of their lives, is to mate and create the next generation of glowing-bottomed, trap making juveniles.
These luminous insects are found in the dark and damp corners of New Zealand and Australia, and the Māori know them as “titiwai”—a word that refers to light reflected in water. That etymology reflects the glowworms’ habit of inhabiting damp and dark places, including riverbanks, tropical rain forests, and—most famously—caves like those at Waitomo. For thousands of tourists, these places are major attractions, where one can gaze at the ethereal beauty of a living, indoor star field. But for the thousands of moths, midges, and mayflies flitting around in the darkness, these are places of death.
The origin of the glowworms’ fatal beauty is unclear, but you can piece together a plausible story by looking at their relatives. Most fungus gnats live and feed on mushrooms, and some of them build sticky webs to trap edible spores. Such webs would inevitably and accidentally have snagged passing insects. Tempted by this source of protein-rich food, some fungus gnats went all-in on carnivory, and transformed their spore-catching webs into dangling fishing lines for capturing flying prey. Such prey are plentiful in the tropical caves of South and Central America, and Southeast Asia, where many fungus-gnat species still dangle their sticky threads. But in temperate regions like Australia and New Zealand, where prey are scarcer, fungus gnats would have needed a way of attracting their victims. That, perhaps, is why they started to glow.
They did so, bizarrely enough, by refashioning their kidneys. Insects have a series of tubes called Malpighian tubules that branch off their guts and produce urine, much like our kidneys do. In glowworms, the cells at the very end of these tubes have become swollen and transparent—they’re the ones that glow. “It’s absolutely unique,” says David Merritt from the University of Queensland. “Insects are such a broad and diverse group that you’ll always find some insect that has done something weird with a preexisting structure.”
Merritt has been studying the glowworms for years. Over that time, he has shown that they control their light with great finesse, using nerve cells that run into the light-producing kidney-esque organs. If you shine a light onto a glowworm, it will switch off its own light after a few minutes. If you anesthetize one, it will glow very brightly before dimming down again. And if you give them good vibrations, they’ll, er, get the excitations. There are some tours in New Zealand, Merritt tells me, where guides will deliberately hit the water or cave walls with an inflated inner tube; in response, the field of living stars will double in brightness. Merritt can achieve the same effect in his lab by pressing a vibrating cellphone against the aquarium where his captive glowworms live. “They really brighten up intensively if they detect vibration,” he says. “I’m not sure of the function.”
He also found that the glowworms glow on a cycle. Even in the constant darkness of a cave, cut off from the daily rhythms of the outside world, they’ll glow particularly brightly at around 6 or 7 p.m. They also synchronize with each other. “We’ve done experiments where we get them out of sync and put them together—and they sync up over five to six days,” says Merritt. “I think they’re maximizing the population’s light output for the time when there are most insects flying around inside the cave.”
All of which makes me wonder: Are there cheating glowworms that save energy by going dark, and rely on the light of their neighbors to lure victims into their hanging snares? Are there cave insects that have evolved to resist the fatal attraction of these blue lights? Do you get populations of glowworms that switch their cycles to glow at different times of the day, to attract insects during the quiet hours when the main colony has dimmed down? And how do these insects even produce their glow?
That last question, at least, has an answer. Aptly enough, glowworms light up their underworld with a pair of chemicals that have a satanic etymology—luciferins and luciferases, after the Latin for “light bringer.” When luciferins react with oxygen in the presence of luciferases, they produce light. Fireflies, jellyfish, and many other luminescent animals glow in the same way, and each group has its own distinctive luciferins and luciferases. “They’re generic terms, like ‘oven,’” says Kurt Krause from the University of Otago. “I’ve got one, you’ve got another, and they both make heat, but they’re different.” Similarly, Krause and his colleagues have found that the glowworms have their own unique light-making chemistry, which they’re starting to tease apart.
Why bother? Partly, it’s because luciferases are really useful to scientists. Researchers can use them to engineer the cells of non-glowing creatures so that they light up when certain genes are active, allowing them to track cancer cells, detect viral infections, and more. And since different kinds of luciferase glow with their own colors, the more you discover, the more things you can track at once. “We think the glowworm has an interesting biochemical story to tell, and we think that hopefully it’s something that can be commercialized,” says Krause. “But even if it’s not, it’s cool! What’s cooler than organisms that make light? When you get involved with them, you get mesmerized.”
That’s certainly the case when I see the lights for myself. In Waitomo, I sail beneath a field of living stars, produced by the repurposed kidneys of synced-up maggots that are trying to lure other insects into death traps. The true nature of these lights is weird and perhaps even grotesque. But they are strong enough that when I look to my right, I can see my partner’s face in the darkness of the cave. And she is smiling.


The Space Age began on October 4, 1957, exactly 60 years ago. On that date, the Soviet Union used a rocket originally intended for use with ballistic missiles to launch Sputnik 1, a small polished metal sphere weighing 184 pounds. Upon reaching its orbit, it became Earth’s first artificial satellite. Traveling at five miles per second, with an altitude that ranged between about 140 miles at its lowest point and 600 miles at its highest point, it orbited the earth once every 98 minutes. It was very nearly a failure. Telemetry after the fact indicated that an initial booster imbalance came within one second of causing the rocket to pitch so low that it would have veered downward, causing it to crash near the launch site.
Sputnik itself was just a small metal sphere filled with nitrogen, with a radio transmitter, power supply, fan, and antennae. All it did was beep, at two frequencies that could be detected by radio receivers, including ham-radio operators, around the world. That’s it.
On the surface, it would seem it was not much to write home about, and in fact the Soviet government initially didn’t write much about it. The official Communist newspaper Pravda printed only a few paragraphs about Sputnik 1 on launch day. But given the geopolitical situation at the time, outside the Soviet Union, in particular in the United States, the whole landscape of international power and prestige changed.
Five years before Sputik launched, the International Council of Scientific Unions had decided to establish the period July 1, 1957, to December 31, 1958, as the International Geophysical Year (IGY). (The fact that it was actually a period of 18 months somehow didn’t seem to bother them at the time.) To celebrate the IGY the White House announced plans in 1955 to launch an Earth-orbiting satellite during this period. A week later, the Soviets, who had as early as 1954 begun considering a development plan for an artificial satellite, approved plans for their own satellite program.
How Sputnik Launched an Era of Technological Fragility
The initial Soviet program was very ambitious, involving large satellites, with significant scientific payloads, the development of which would have resulted in launches sometime during the IGY. However, it soon became clear that the complexity of the proposed program could not be met in time. Based on a fear that the United States would beat the Soviets to the punch, the Soviet program changed completely. Instead of a complex satellite, a very simple and light satellite would be designed, which, while it could provide some scientific data, would essentially just get into orbit.
They had a rocket designed for ICBMs that was up to the task, and eight months after the new program was approved, and after several embarrassing failures, Sputnik 1 was successfully launched. (In November, the Soviets followed that successful launch with Sputnik 2, which housed a dog, the first living casualty of the new Space Race.)
The initial U.S.-government reaction to the launch of Sputnik was itself subdued. The Soviet program was not secret, and details of Sputnik were made publicly available before the launch, but no one took much notice. Moreover, U.S. spy planes had carefully been monitoring the Soviet rocket program so that the U.S. government was aware of the imminent launch, publicly stating that it did not come as a surprise. The Naval Research Laboratory, among other facilities, tracked Sputnik’s U.S. crossings.
But Sputnik’s launch did trigger concern among the U.S. public and their representatives in Congress. It demolished the notion, nurtured by the U.S. propaganda machine, that the Soviet Union was technically backward. The televised failure of the U.S. government’s first attempted satellite launch of the so-called Vanguard satellite only made the situation worse. Moreover, if a Russian satellite could fly over the United States, Russian missiles carrying nuclear weapons could perhaps also rain down upon the country.
Politics and science had already begun a courtship with the Manhattan Project during World War II, but the Space Race wedded them, a trend that has continued right up to the present time. By 1958 the United States had created two new agencies, NASA and the agency that would eventually become known as DARPA (the Defense Advanced Research-Projects Agency), and the U.S. government dramatically ramped up its support of scientific research and education programs.
The public became galvanized by fears of falling behind the Soviet Communist regime. Suddenly there was a new push for science and technology training in U.S. schools, and Congress enacted a law in 1958 providing low-interest loans to encourage students to study science and math at university. Ultimately the U.S. would build the space infrastructure that led to the moon landing in July of 1969.
Sputnik also had other political impacts. Democrats used the successful launch to argue that there was a dangerous “missile gap” with the Soviets, a central plank of John F. Kennedy’s successful presidential campaign.
Sputnik not only heralded a new era of intense competition between the Soviet Union and the United States, but also spread to many more countries—from Europe to China to India to North Korea—who all used space technology to build research infrastructure and to pursue international prestige.
As a child of the 1950s, I remember how the “Sputnik Moment” jump-started whole new areas of basic and applied research, not to mention the teaching of science and mathematics. And while I continue to find it unfortunate that space exploration is largely driven by national prestige, rather than the progress of science, it is nevertheless true that the side effects have been dramatic, and largely positive. Exactly three months after its launch, Sputnik reentered the atmosphere and burned up. The satellite may have disappeared, but its legacy continues to color almost every aspect of the world in which we now live.
As we face the 21st-century challenges of climate change and nuclear proliferation and much else, it’s worth asking whether we need a terrestrial Sputnik moment to catalyze public and government action to realistically face these challenges. With any luck, the galvanizing act will be as harmless as the small metal sphere the Soviets launched, and not something much more traumatic.


Deep in the Mariana Trench, at depths lower than the Rockies are high, rests a tin of reduced-sodium Spam.
NOAA scientists caught sight of it last year near the mouth of the Mariana’s Sirena Deep. It isn’t an isolated incursion, but it was nevertheless startling, the sight of those timeless golden letters bright against the deep ocean bottom.
Shortly after came news from another team of scientists who had found in the Mariana an innovation less familiar than shelf-stable meat, but far more significant. In the bodies of deep-dwelling creatures were found traces of industrial chemicals responsible for the rise of modern America—polychlorinated biphenyls.
PCBs had been detected in Hirondellea gigas, tiny shrimp-like amphipods scooped up by deepwater trawlers. Results from the expedition, led by Newcastle University’s hadal-zone expert Alan Jamieson, were preliminarily released last year and then published in February.
PCBs have been found the world over—from the bed of the Hudson River to the fat of polar bears roaming the high Arctic—but never before in the creatures of the extreme deep, a bioregion about which science knows relatively little.
How PCBs reached the Mariana is still under investigation. Jamieson and colleagues speculated on multiple, regional sources. A nearby military base. The industrial corridors along the Asian coastline. And the Great Pacific Garbage Patch, where PCBs glom onto plastic particles caught in the current. Over time, the plastic degrades and descends into the depths, ferrying PCBs with them.
But the true origin of PCBs lies in another time and place, in Depression-era Alabama, and before that, 19th-century Germany at the pinnacle of German chemistry.
* * *
PCB production began in late 1929 in a factory east of Birmingham. The same era that gave us New York’s Chrysler Building, The Little Engine That Could, and eventually Spam brought mass-made PCBs to market.
General Electric and Westinghouse were early adopters. Both firms formulated PCBs into dielectric fluids, the insulating liquids added to capacitors and other electrical components to keep them cool and to prevent fires. With PCBs’ aid, the electric grid spread from the industrialized north into the rural regions of the Deep South and the American West. By mid-century, PCBs had a bird’s-eye view of any block in America with a utility pole and PCB-bathed transformer.
Soon PCBs were added to paints, caulks, plastics, even floor finishes and dish detergents. They were branded, and assigned names like Aroclor. That commercial products contained PCBs was never advertised, explained Ellen Griffith Spears, who wrote the definitive book on PCBs’ genesis.
PCBs slipped into the world, becoming ubiquitous while remaining anonymous. Until the mid-1960s, when the Danish-born scientist Sören Jensen detected PCBs in the bodies of pike taken from the waters off Sweden.
In the wake of Rachel Carson’s Silent Spring, published in 1962, Jensen had been dispatched to look for DDT, one of the post-WWII pesticides whose increasing use Carson’s book had questioned. Jensen found DDT. But his data also signaled the presence of unexpected, yet chemically similar, contaminants. It took two years to determine the “ghosts” in his data were PCBs.
After that, everywhere Jensen thought to look, he found their fingerprints: in the feathers of archived white-tailed eagles, and in hair plucked from his own head, and others sampled from his wife and infant daughter. His conclusions, published in 1966, instigated a global investigation into the fate and toxicity of PCBs, research now carried forward (and into the deep oceans) by Jamieson and colleagues.
Today, PCBs are well-characterized pollutants—toxic, extremely persistent, and pervasive. All 209 variations of PCBs are known carcinogens. PCBs can alter liver function, and they can interfere with how humans reproduce, develop, think, and mount an immune response. Based on their cancer-causing potential alone, Congress voted to end American production in 1976 by attaching an amendment to the Toxic-Substances Control Act (TSCA).
“ToSCA,” as the law was called, gave the fledgling Environmental Protection Agency, created six years earlier, the authority to regulate industrial chemicals. PCBs were the only class of chemicals called out by TSCA; about 60,000 others were grandfathered, meaning their use was never questioned.
Another three years passed before Congress’s limits on PCB production took effect. Four decades later, though banned, PCBs live on, including in tiny amphipods swimming in some of the deepest waters of Earth’s biggest ocean.
* * *
PCBs, now endemic to environments everywhere, belong to a class of chemicals called (depending on the era) halocarbons, organohalogens, or halogenated organic compounds.
Organic, in this instance, refers not to foods raised without chemicals but to compounds made with carbon. Halo- (or halogen) signifies the presence of one or more of the halogen elements, the most familiar being chlorine, bromine, and fluorine. From these starting materials, chemists can make an array of compounds. But so can nature. To date, there are more than 5,000 so-called biogenic or naturally-occurring organohalogens.
Nature’s versions are forged in volcanoes or near deep-sea vents where temperatures run high, and chlorine and bromine are abundant. Organisms in all kingdoms can make halocarbons, though in minute quantities and typically for highly specialized purposes such as self-defense or signaling mates.
There are many pathways to making such complex molecules. The Iowa atmospheric chemist Scott Spak keeps a running tally of the “recipes” that might yield a PCB. While there are no known analogs in nature, one does get to wondering whether nature—at some point, somewhere on the planet or deep in the cosmos—could have served up the right mix of raw materials, in the right order and under the right conditions to allow for PCBs’ spontaneous formation. It is conceivable, Spak concedes, though purely hypothetical. Such a discovery, should it even occur, wouldn’t explain PCBs’ global dispersion, nor absolve what humans made with impunity. But it does hint at the complexity of Earth’s chemistry, and the humility with which we still endeavor to understand it.
Into intricate ecological and biological systems human industry introduced PCBs in extraordinary volumes, and in evolutionary terms, rapidly—over the span of three or four human generations, said Spak. But the problem isn’t so much that PCBs are “unnatural,” though one could make that argument. It is that they are molecules nature recognized, familiar enough to be folded into its systems and to confuse them.
Human biology has not adapted to their presence. Species far older than us, microbes mostly, have evolved over millennia to coexist with, and even to synthesize and break down, specific types of biogenic halocarbons. Some strains of bacteria are capable of disassembling PCBs. Other species, such as the Atlantic tomcod, bottom-feeders in the Hudson, have even developed a genetic variation that allows for survival in PCB-polluted waters, though their livers are also loaded with the chemicals.
But for humans, research tracking the health effects of industrial PCB exposures, Spak said, is tantamount to watching evolutionary consequences playing out in real time.
* * *
While sunlight (and some microorganisms) break down PCBs over time, they can be stunningly stable when stored in sediment, glaciers and other so-called “sinks” like the deep ocean. And because PCBs are lipophilic (or fat-loving), they can also accumulate in the fatty tissue of marine life, and in the bodies of mammals like us. Depending on the total load, some measure of PCBs can last a lifetime, or pass between generations through cord blood or milk.
The chemistry that enabled humankind to engineer elements into such durable molecules and enduring pollutants dates to the early 19th century, to a time when the natural world was the chemist’s muse. As chemistry industrialized, chemists were drawn by profit, and later, into geopolitics. In time, chemistry became a tool for nature’s mastery, and—both knowingly and inadvertently—an engine for its alteration.
At the center of this transformation is an elegant molecule called benzene. It is the same carbon-rich compound that lends gas stations their curiously evocative aroma, and PCBs their structural integrity.
A molecule of benzene is comprised of six carbon and six hydrogen atoms. Ask a chemist what benzene looks like, and she will draw a hexagon. It’s a schematic of how benzene’s six carbons circle themselves, as if linked hand in hand, into a ring. The benzene ring.
Benzene rings are also abundant in nature. They were present in Earth’s prebiotic soup, and they float in deep space. Many human hormones have benzene rings. And so do many human-engineered molecules.
Chemical engineers approach benzene as a building block from which to make thousands of useful products, including aspirin, the plastic lid to your takeaway coffee, and the Legos children leave strewn across floors the world over. Though, it should be noted that benzene is rarely made intentionally. Benzene is incidental to other industrial processes, such as refining oil into gasoline, processing coal into coke, or in the making of ethylene, another widely used chemical building block. Which makes benzene an industrial by-product and also a common industrial pollutant, especially following industrial accidents, as happened after Hurricane Harvey struck the plants along Texas’s Gulf coast.
All of which means the benzene ring is something of a paradox. When incorporated into certain molecules, it is essential to life. But in other configurations, slight tweaks in the composition and arrangement of atoms render benzene part of a toxic, possibly carcinogenic molecule. And when on its own, there is ample evidence that benzene causes cancer.
* * *
Until the middle of the 19th century, building on the benzene ring wasn’t possible because, though benzene had been isolated, its structure still eluded chemists. So significant was the “discovery” of benzene’s ring of carbons that in 1890, on the occasion of its 25th anniversary, the German Chemical Society threw a party—the Benzolfest! It was “a festival of magnificence perhaps unparalleled in the history of science,” wrote one commentator.
It was held at Berlin’s City Hall in Germany. The emperor was invited. Dignitaries came. All of the era’s most preeminent chemists gathered in their finest attire. Augustus Hoffman, a bearded and towering figure in the field of organic chemistry, waxed poetic about benzene and the chemist, Friedrich August Kekulé, who had “pluck[ed] the heart out of its mystery.”
The benzene tree, as he called it, was “thronged with blossoms,” its branches heavy with fruit. It was “a blaze of color,” and it gave off “an almost overwhelming fragrance,” a fitting metaphor given benzene’s signature scent.
To Hoffman, the benzene tree was a “giant.” It soared “into the clouds to where the eye cannot yet follow it.” Up the tree scaled “no dearth of industrious workers,” all “busily striving to collect the harvest,” he had said, referring to how entrepreneurial chemists were converting benzene chemistry into industrial products. “Keen climbers have already clambered up to the third or fourth branch,” Hoffman continued, some chemists “working at a dizzy[ing] height.”
With new insight into molecular configurations, chemists, starting around 1865, could better anticipate the steps required to synthesize new molecules. With the benzene ring, “the number of organic compounds all at once,” Hoffman had noted, “increased to infinity.”
Among those in the tree’s farthest reaches was Gustav Schultz, in attendance that night. In 1881, Schutlz, with the chemist Hermann Schmidt, had described the synthesis of a PCB. The pair published the new molecule in a premier German-language journal. And then, like DDT, also achieved during this era, PCBs were left to languish for decades on dust-cloaked shelves of obscure chemical libraries. Their discovery advanced chemical knowledge but was otherwise of little practical value.
* * *
Hoffman was born into the fast-changing world of 19th-century Europe. He grew up in parallel with organic chemistry, and ascended to its highest ranks. Unlike inorganic chemists’ fascination with Earth’s metals and minerals, the first organic chemists studied molecules from living organisms, which were principally comprised of carbon.
Benzene was first isolated from the by-products left after making “portable gas,” which was rendered from fish or whale blubber to fuel lamps. In time, chemists learned to derive benzene and other carbon-rich materials from coal tar instead, a project central to Hoffman’s work and that of his student, Charles Mansfield.
A lump of coal, while not alive, is evidence of life once lived. It is the long-sequestered remnants of ancient flora, from an epoch when trees could grow as tall, or taller, than the one Hoffman had conjured. Coal tar, though, is what remains after human extraction and use. Wherever cities gasified coal to light 19th-century streetlamps, or converted coal into hotter-burning coke to smelt metals, coal tar piled up as waste. Hoffman, curious of its composition, set out to characterize it.
But when organic chemistry adopted coal tar as its primary feedstock, it wedded itself to the residues of industrialization. And so the field became one step removed from the thrum of life that had first inspired it.
By the Benzolfest, organic chemistry was high technology, and the German Empire its Silicon Valley. The field was “the earliest pure science to have a massive impact on technology and on a national economy,” noted the historian Alan Rocke. Germany’s prowess in chemical manufacturing would embolden its nationalistic ambitions. But the war such nationalism inspired would seed the downfall of the German chemical empire, and the rise of a new kind of chemical industry on American soil.
* * *
The original chemical products plucked from the benzene tree had a very specific application: They were textile dyes, what turns humdrum fabrics into every hue imaginable. The first colorfast dye synthesized from coal tar had been an accident. A failed attempt by one of Hoffman’s students to make quinine—a hard-to-source malarial drug derived from the South American fever-bark tree—led to the discovery in 1856 of a dye that could permanently stain silk a rich, red-purple mauve.
Hoffman was based in England, and it was his English student William Perkin who happened into the colorant. Perkin left the lab against Hoffman’s counsel to build a dye-works instead. His factory marked the first attempt to do organic chemistry at scale, wrote Simon Garfield in his history of this world-changing invention. Eventually, after multiple failed attempts, Perkin developed the multistepped process, and proceeded to the equally difficult task of convincing an established, but reluctant, textile industry to adopt his industrial dye.
In time, Garfield noted, a new class of chemists engineered the radiant color palette of Victorian fashion from the dregs of Europe’s Industrial Revolution. Dye-works sprouted across Europe, clustering along Germany and Switzerland’s swift rivers. In time, these ran foul—discolored and odorous. Neighbors complained. Researchers took note. Governments acted, but the coal-tar industry bloomed all the same.
By the close of the 19th century, German dyestuffs dominated the world market, though the first effects of acute exposures were already evident among the earliest generations of dye workers. By 1897, the term chloracne appeared in the German literature to describe a condition unique to chemical workers, “an industrial leprosy,” where the skin is pocked with painful, disfiguring lesions. Not long after, medicine documented new “aniline (dye) tumors” and “dye-workers cancer.” In time, chemists realized benzene, too, was more hazardous in factory quantities than in those used in a laboratory.
And yet, coal-tar chemistry flourished, spreading in multiple directions at once. Chemical innovation led to new dyes, and also to new drugs like aspirin, a synthetic version of a molecule once sourced from willow bark. New classes of pharmaceuticals followed. In time, coal-tar drugs transformed health care.
But coal-tar drugs and coal-tar chemicals thereafter diverged, at least in the popular imagination and in the eyes of states trying to manage these burgeoning technologies. To this day, and despite recent reforms to U.S. policy, drugs and industrial chemicals fall under the purview of separate laws administered by separate agencies. And they are held to separate standards of safety. Forgotten to history are their common origins and chemical ancestry.
Less than a century after the Benzolfest, scientists, led by Theo Colborn, synthesized the growing body of ecological research into a disquieting discovery. At trace levels of exposure, levels lower than workers’ acute exposures and equivalent to dosed drugs, many of these new classes of organic chemicals (including PCBs) could mimic, block, or disrupt the work of hormones, the biochemical signals that coordinate multicellular life.
* * *
Though Perkin missed the mark, making mauve instead, it was no accident that an English chemist sought a synthetic route to quinine. Malarial drugs were essential to colonial expansion into the tropics, one example of how organic chemicals are influenced by prevailing geopolitics and pressed to do the work of empire.
By the second decade of the 20th century, organic chemicals were moved to the front lines of global conflict. Mustard gas, picric acid, and TNT—all organic molecules—made for unimaginably destructive weapons. After Versailles, even peacetime users of organic chemicals were hailed as patriots and power brokers. Germany paid reparations with dyes, while the United States seized German chemical patents and industrial-plant designs as the spoils of war.
Before the Great War, few U.S. firms had ventured into organic chemicals. Only after fighting began, and the British embargo blocked import of German chemicals, did American companies leap into the unknown of organic-chemicals production.
“Americans keenly felt their dependence on German chemicals,” wrote the industrial historian Kathryn Steen. Mastering how to make them domestically was motivated “partly because of the[se] shortages,” she added, but “primarily because of what [the chemicals] represented to Americans—the seemingly inferior industrial and scientific abilities relative to the enemy and rival.”
After the war, growing the nation’s capacity to manufacture chemicals became a national project. The founder of Hooker Chemical (later responsible for Love Canal, the nation’s first Superfund site) argued this point with regard to chlorine (a gruesome war gas). The same case was made for coal-tar chemicals in 1917 by John F. Queeny, the founder of Monsanto Chemical—the company that two decades later would take PCBs global. Factories that made drugs and dyes from coal, Queeny argued, could easily make the materials required by modern warfare. Ramp up peacetime production of coal-tar chemicals and a fleet of war-ready factories would be lying in wait. It would mark “the declaration of American chemical independence,” Queeny said.
For the general public, though, the new coal-tar chemicals were a harder sell.
Coal-tar chemicals smacked of artifice and, in their use as chemical weapons, seemed abhorrent, not the patriots industrialists had painted them to be. Chemistry had a postwar public-relations problem.
In the same editorial where Queeny equated national security with a robust chemical industry, he also pictured coal-tar chemistry as a thickly crowned tree. Like those that chart ancestry, Queeny’s tree arranged chemical products by family and parentage.
Semet-Solvay widely distributed a similar image. Their “coal products tree” rooted the company’s offerings in the nation’s bituminous coal beds. Along boughs of ammonia, tar, and benzene bloomed hundreds of products—Saccharin! Explosives! Mothballs! Perfume!—all promising mastery over nature’s unyielding cycles, its wild swarms and infestations, its off-putting odors and inconvenient secretions.
The collective conscience still carried residues of the real and symbolic horrors of chemical warfare, even at the height of the roaring ’20s. Chemistry had a spectral presence, and it was evident in popular sentiment and the era’s literature. All Quiet on the Western Front, A Farewell to Arms, and Goodbye to All That were postmortems on chemical warfare. All three were published (or translated) in 1929, the same year PCB production began far from the battlefields of Western Europe in Anniston, Alabama, on the lush, southern limits of Appalachia’s rolling hills.
Anniston was a planned, industrial utopia. But in reality, it was a segregated southern city populated by pig-iron and pipe foundries. PCBs were first made by Anniston’s Federal Phosphorus Company, in a plant that belonged to the son of a Confederate solider, a charismatic entrepreneur and a champion of the South’s revival.
Theodore Swann had gotten his start selling locomotives and later electricity. During World War I, he managed a munitions plant in Anniston, but then founded a factory to refine manganese for the steel industry, and when that failed, phosphorous, which is how Swann finally came into his fortune. His electric furnaces captured a pure form of phosphoric acid, which was sold to make fertilizer, detergent, baking soda and even soda pop.
Swann wasn’t a chemist, but a “boom man in a boomtown,” as one biographer put it, always looking for the next thing. And in the mid-1920s, what was on the rise were cars, American crude, and by extension organic chemicals. The success of these ascendant industries depended on better chemical technologies to make better fuels and better car parts. And Swann had seen it coming.
In the mid-1920s, at the invitation of an oil company, Indian Refining (later known as Texaco), he agreed to manufacture a benzene-based chemical called diphenyl (or biphenyl, in today’s parlance). Indian Refining needed a regular supply of diphenyl, a heat transfer fluid, to improve its oil-refining process. Except only one company made diphenyl, Eastman Kodak, and it was only available in small batches at the prohibitive price of $40 per pound.
The structure of diphenyl was simple: just two fused rings of benzene. But it required an entirely different chemistry, and experience and expertise that were in short supply at the time.
As Spears described in Baptized in PCBs, Swann assigned a team to the difficult (and hazardous) task, and issued them a deadline. Development proceeded by trial and error, with error sometimes resulting in explosions, including one that took off a wall and showered Swann’s men with fire, glass, benzene, and other shrapnel.
They eventually mastered diphenyl, a “magic fluid” that brought the nation to the brink of a new chemical age. Not long after, Swann’s chemists scaled another process to add chlorine onto the diphenyl backbone, creating a molecule so stable, it would travel the globe and accumulate in places surely unfathomable to Swann. Swann had financed the conversion of benzene into biphenyl, and now biphenyl into PCBs. The Anniston Works would soon produce 3,000 pounds of PCBs per day.
Production levels climbed higher by the year, as PCBs were put to the project of nation-building. They erected the bases and surveillance equipment that protected the new world order, while at home, PCBs were built into the schools, offices, and factories constructed to accommodate the postwar boom. PCBs would transform U.S. industry in a matter of decades and global ecology before the century was out.
* * *
In the United States, the first batches of PCBs, made at the close of the 1920s, likely left Anniston for Pittsburgh, home of H.H. Robertson Company. Robertson made prefabricated metal siding and roofing from which to erect factories, smelters, refineries and chemical works. The company, like Indian Refining before them, had asked for technical assistance. They needed a new protective coating for their metal sheeting, one that could prevent erosion and the spread of fire better than Halowaxes, or chlorinated naphthalene, one of the earliest classes of industrial halocarbons produced in the United States.
PCBs were unusually durable and durably useful. They were heat-resistant, non-conductive, and excellent as a weatherproof and fireproof coating. And because they were a value-added waste product, PCBs were economically viable.
Flame-resistant factory parts may seem like an insignificant side note, but Robertson’s Protected Metals, later called Galbestos, were instrumental in growing the nation’s manufacturing sector in size and scale. Factories were changing. Small, clustered, brick structures gave way to multistory, steel Goliaths. And these needed to withstand the elements, and to contain the fire and explosion risks that went along with the 20th century’s new methods for transforming nature’s resources into the mass-made materials of modern societies.
Four days after the stock-market crash of 1929, Swann’s firm filed a patent for PCB-laced transformer oils. Westinghouse and GE, like Robertson, would soon find PCBs indispensable, wrote Spears.
But despite these early successes, problems were mounting in Anniston. Within the first years of PCB production, those handling PCBs developed the same chloracne and other debilitating symptoms as dye workers a generation earlier. Three workers at Halowax who handled PCBs died from acute yellow atrophy of the liver (extreme jaundice), fatalities that were studied, but ultimately cast aside by company officials.
And though Swann’s company had weathered the worst years of the Depression, it wouldn’t survive the decade. Roosevelt’s National Recovery Administration had issued economic directives that, as Swann put it, leveled southern industry like Sherman’s March to the Sea. And he had been a profligate spender—living high in the Birmingham hills. But with shifting costs, and loans coming due, Swann went under. His rise to fortune and influence had been fast and somewhat famed. (He had even been profiled in Forbes.) His decline was equally precipitous. Swann sunk into debt, losing his mansion, and all claims to his many factories and technologies.
Queeny’s coal-tar drug company, Monsanto, was also flush from lucrative contracts with Coca-Cola, who bought their caffeine, vanillin, and saccharin. Monsanto took over PCB production in 1935, securing government contracts, growing sales through World War II, and shielding all along the suite of chemicals from regulation until the mid-1970s.
Chemical production in these intervening years soared, owning to the fact that U.S. firms switched their substrate from coal tar to the by-products of the new, advanced crude refineries. In the 1950s, Socony-Vacuum (later ExxonMobil) published the petroleum tree, emblematic of the flowering of American petrochemistry, at the time a “uniquely American phenomenon,” as industrialist Peter Spitz put it.
Two years later, in 1959, Goodrich-Gulf published the rubber tree, as symbolic to this era as Hoffman’s tree had been in his time. Advances in chemical engineering had obviated the need for natural latex, making rubber from oil rather than rubber trees. Drugs, sweeteners, flavorings, fertilizers, fabrics, and furnishings now all had synthetic equivalents. Chemistry, it seemed, had freed humanity from nature altogether.
When chemical trees finally disappeared from popular culture, what was lost was any connection of organic chemicals to their fossil-fuel roots, and of greater significance, to their molecular basis in carbon and the chemistry of life.
* * *
If you bore into the fat-rich bark of a thick-trunked tree, you’ll likely find PCBs, same as you’d find in deep-ocean amphipods, which makes trees like shrimp, and shrimp like us. PCBs are thought to be present in detectable levels in every person on the planet. Though everywhere, the implications are distributed unevenly. PCBs can concentrate, creating hot spots, including in Anniston and other factory towns and regions of the Arctic and subarctic, with significant implications for the indigenous communities living there.
Despite national and international curbs on their production, PCBs now congregate in the deep ocean, raising new concerns. In some areas of the Mariana, PCB levels registered 50 times higher than those found in crabs living in surface waters near heavy industry in China.
By the time the international community stepped in to end global PCB production, well over 1 million metric tons (about 3 billion pounds) had been manufactured worldwide. The 2001 UN Stockholm Convention on Persistent Organic Pollutants that resulted from these negotiations, but which the United States has yet to ratify, initially named DDT, PCBs, and 10 other chemicals (or classes of chemicals), all based on benzene.
“The challenge moving forward is to determine the physiological consequences of such contamination and understand knock-on effects on ecosystem function,” Jamieson and his colleagues concluded in the pages of Nature Ecology and Evolution. Except human activity may be altering the chemistry of the deep before we have had the chance to document it.
The problem is not limited to PCBs, but extends to the larger family of organohalogens to which they belong. In the early 1970s, PCBs were replaced by other organohalogens, one being polybrominated biphenyls, or PBBs, molecules similar to PCBs, but made with bromine instead of chlorine. Soon after their introduction, PBBs got into cattle feed, poisoning the food supply and the people of Michigan, and saddling the state with a long-lived legacy. PBDEs, polybrominated diphenyl ethers, followed as yet another alternative. These were used as flame retardants for two decades before a subsequent generation of Swedish scientists charted rising levels in breast milk.
Besides PCBs, amphipods living in the Mariana also harbor PBDEs, though the most prevalent commercial mixtures have been phased out of U.S. production and named to the UN Stockholm Convention as well.
The deep now archives nearly a century of chemical innovation, and documents the rise and fall of chemical classes, which industry develops and retracts in waves without seeming to absorb the larger lesson.
* * *
There are those who want a bright line to divide what’s natural from what isn’t as a means to make clear what’s safe. But with their origins in Earth’s deep carbon, and their enduring presence in life forms everywhere, such distinctions are murky at best. And yet, PCBs are part of a post-natural state in which industrial chemistry and ecology have become one and the same.
“Nature,” the organic chemist Pat Costner reminded me, “is a chemist, too” and the world its roiling, bubbling, reactive laboratory. Science has only begun to grapple with the complexity of our overlapping chemistries. Costner trained in organic chemistry at the peak of the chemical age. She took her first job as a bench chemist with Shell Oil in the 1960s, though she didn’t stay long. She turned her attention to organic pollution, particularly the chemical by-products industry never intended to make but released into nature anyway.
Her specialty became the family of benzene-based compounds called dioxins, perhaps the most poisonous products of the benzene tree. Some PCBs, she reminded me, are dioxin-like in their toxicity profile. And like dioxins, PCBs can also be inadvertent by-products, made incidentally during the manufacture of titanium dioxide, for example, or a shade of yellow organic inks and dyes reminiscent of the letters that spell out Spam. Try though we may, control over chemistries this complex is something of a chimera.
“We have been so clever at learning to play with atoms and molecules without ever thinking about what they do once they are out,” she told me. “Put a complex molecule into the environment,” said Costner, “and it is going to undergo any number of transitions in hard-to-predict ways.”
The same is true when human-made molecules interact with the exquisite biochemistry of our bodies.
As the biologist Sandra Steingraber explains, the organs of the human system are designed to “shuttle around, break apart, recycle, and reconstruct carbon-containing molecules,” work orchestrated by enzymes and hormones. If carbon molecules come with add-ons like chlorine and bromine, the chemical makeup can influence whether the body stores the molecules, metabolizes them, renders them benign, or makes them inherently more dangerous.
But regulating organic chemicals for their biological activity has been political and controversial because carbon is not just the basis of our biology, but also deeply embedded in our economy.
One hundred and fifty years ago, when Hoffman and his contemporaries gathered to celebrate benzene, humanity’s collective relationship to carbon and nature had already begun to shift. Chemistry, which once mimicked nature’s molecules, had begun to manipulate them. All the more striking, then, is Hoffman’s fantastical tree. Just two years before his death, he rooted chemistry to the Earth. However high chemists might climb, however much industry might harvest, chemistry was grounded in the laws of nature. What goes around comes around. Nature travels in cycles.
But those in attendance that cold March eve, a decade before the new century dawned, could not have known that the branch of chemistry they honored—founded to study living matter!—would spawn an industry so prolific as to irreparably alter the chemistry of life itself.
The chemical innovations of Hoffman’s era would get swept into global conflict—and the American impulse toward maximum and unfettered production—casting PCBs and other fruits of the benzene tree far and wide, into the ocean deep and possibly the depths of time.



